{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "ques_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n\tif tz is None:\n\t\treturn Time(nanoseconds=nanoseconds)\n\treturn LocalTime(nanoseconds=nanoseconds, tz=tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\treturn time.Time(nanoseconds, tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif not isinstance(nanoseconds, int):\n\t\traise TypeError('nanoseconds must be an integer')\n\tif tz is None:\n\t\treturn nanoseconds\n\telse:\n\t\treturn datetime.time(nanoseconds // 1000000000, tzinfo=tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\treturn Time(nanoseconds // 1000000, nanoseconds % 1000000, tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\treturn Time(nanoseconds=nanoseconds)\n\treturn LocalTime(nanoseconds=nanoseconds, tz=tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\n\tif not isinstance(nanoseconds, int):\n\t\traise TypeError(\"Expected nanoseconds to be an int\")\n\n\treturn Time(nanoseconds // 1000000000, nanoseconds % 1000000000, tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif tz is None:\n\t\treturn Time(nanoseconds)\n\telse:\n\t\treturn LocalTime(nanoseconds, tz)\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\treturn datetime.time(nanoseconds / 1000000000, (nanoseconds % 1000000000) / 1000000)\n\telse:\n\t\treturn datetime.time(nanoseconds / 1000000000, (nanoseconds % 1000000000) / 1000000, tzinfo=tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\treturn Time(nanoseconds, tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif nanoseconds == 0:\n\t\treturn datetime.time(0, 0, 0, 0)\n\td = datetime.timedelta(nanoseconds=nanoseconds)\n\tif tz:\n\t\treturn datetime.time(d.seconds, d.microseconds, d.nanoseconds, tzinfo=tz)\n\treturn datetime.time(d.seconds, d.microseconds, d.nanoseconds)\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif tz is None:\n\t\treturn Time(nanoseconds=nanoseconds)\n\treturn LocalTime(nanoseconds=nanoseconds, tz=tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\treturn time.Time(nanoseconds, tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif not isinstance(nanoseconds, int):\n\t\traise TypeError('nanoseconds must be an integer')\n\tif tz is None:\n\t\treturn nanoseconds\n\telse:\n\t\treturn datetime.time(nanoseconds // 1000000000, tzinfo=tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\treturn Time(nanoseconds // 1000000, nanoseconds % 1000000, tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\treturn Time(nanoseconds=nanoseconds)\n\treturn LocalTime(nanoseconds=nanoseconds, tz=tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\n\tif not isinstance(nanoseconds, int):\n\t\traise TypeError(\"Expected nanoseconds to be an int\")\n\n\treturn Time(nanoseconds // 1000000000, nanoseconds % 1000000000, tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif tz is None:\n\t\treturn Time(nanoseconds)\n\telse:\n\t\treturn LocalTime(nanoseconds, tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\treturn datetime.time(nanoseconds / 1000000000, (nanoseconds % 1000000000) / 1000000)\n\telse:\n\t\treturn datetime.time(nanoseconds / 1000000000, (nanoseconds % 1000000000) / 1000000, tzinfo=tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\treturn Time(nanoseconds, tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif nanoseconds == 0:\n\t\treturn datetime.time(0, 0, 0, 0)\n\td = datetime.timedelta(nanoseconds=nanoseconds)\n\tif tz:\n\t\treturn datetime.time(d.seconds, d.microseconds, d.nanoseconds, tzinfo=tz)\n\treturn datetime.time(d.seconds, d.microseconds, d.nanoseconds)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "ques_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n\treturn _timedelta2dict(value)\n\n\n", "def dehydrate_timedelta(value):\n\treturn str(value)\n\n\n", "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n\n\n", "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n", "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n\n\n", "def dehydrate_timedelta(value):\n\treturn {'days': value.days, 'seconds': value.seconds, 'microseconds': value.microseconds}\n\n\n", "def dehydrate_timedelta(value):\n\tseconds = value.total_seconds()\n\tif seconds < 60:\n\t\treturn '%d seconds' % seconds\n\telse:\n\t\treturn '%d minutes' % (seconds / 60)\n\n\n", "def dehydrate_timedelta(value):\n\treturn {\n\t\t\"days\": value.days,\n\t\t\"seconds\": value.seconds,\n\t\t\"microseconds\": value.microseconds\n\t}\n\n", "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n\n\n", "def dehydrate_timedelta(value):\n\treturn str(value)\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n\treturn _timedelta2dict(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn str(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn {'days': value.days, 'seconds': value.seconds, 'microseconds': value.microseconds}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\tseconds = value.total_seconds()\n\tif seconds < 60:\n\t\treturn '%d seconds' % seconds\n\telse:\n\t\treturn '%d minutes' % (seconds / 60)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn {\n\t\t\"days\": value.days,\n\t\t\"seconds\": value.seconds,\n\t\t\"microseconds\": value.microseconds\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn value.total_seconds()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn str(value)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "ques_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n\treturn value.isoformat()\n\n\n", "def dehydrate_time(value):\n\treturn value.isoformat()\n\n\n", "def dehydrate_time(value):\n\treturn value.strftime('%H:%M:%S')\n\n", "def dehydrate_time(value):\n\treturn value.strftime(\"%H:%M\")\n\n\n", "def dehydrate_time(value):\n\treturn str(value)\n\n\n", "def dehydrate_time(value):\n\treturn {\"h\": value.hour, \"m\": value.minute}\n\n\n", "def dehydrate_time(value):\n\treturn value.strftime(TIME_FORMAT)\n\n\n", "def dehydrate_time(value):\n\treturn value.strftime('%H:%M') if value else ''\n\n\n", "def dehydrate_time(value):\n\treturn value.strftime(\"%H:%M:%S\")\n\n\n", "def dehydrate_time(value):\n\tif isinstance(value, Time):\n\t\treturn value.isoformat()\n\telse:\n\t\treturn value\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n\treturn value.isoformat()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.isoformat()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.strftime('%H:%M:%S')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.strftime(\"%H:%M\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn str(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn {\"h\": value.hour, \"m\": value.minute}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.strftime(TIME_FORMAT)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.strftime('%H:%M') if value else ''\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.strftime(\"%H:%M:%S\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\tif isinstance(value, Time):\n\t\treturn value.isoformat()\n\telse:\n\t\treturn value\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "ques_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n\treturn {\n\t\t'type': 'Point',\n\t\t'coordinates': value.coords\n\t}\n\n", "def dehydrate_point(value):\n\treturn Point.dehydrate(value)\n\n\n", "def dehydrate_point(value):\n\treturn {\n\t\t\"lat\": value.lat,\n\t\t\"lng\": value.lng\n\t}\n\n\n", "def dehydrate_point(value):\n\treturn {\n\t\t\"type\": \"point\",\n\t\t\"x\": value.x,\n\t\t\"y\": value.y\n\t}\n\n\n", "def dehydrate_point(value):\n\treturn value.get_data()\n\n", "def dehydrate_point(value):\n\treturn dehydrate_point(value)\n\n\n", "def dehydrate_point(value):\n\tif not value:\n\t\treturn None\n\treturn {\n\t\t'lat': value.lat,\n\t\t'lng': value.lng,\n\t}\n\n\n", "def dehydrate_point(value):\n\treturn {\n\t\t'x': value.x,\n\t\t'y': value.y,\n\t}\n\n\n", "def dehydrate_point(value):\n\treturn dehydrate_geometry(value)\n\n\n", "def dehydrate_point(value):\n\treturn {'lat': value.latitude, 'lon': value.longitude}\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n\treturn {\n\t\t'type': 'Point',\n\t\t'coordinates': value.coords\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn Point.dehydrate(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn {\n\t\t\"lat\": value.lat,\n\t\t\"lng\": value.lng\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn {\n\t\t\"type\": \"point\",\n\t\t\"x\": value.x,\n\t\t\"y\": value.y\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn value.get_data()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn dehydrate_point(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif not value:\n\t\treturn None\n\treturn {\n\t\t'lat': value.lat,\n\t\t'lng': value.lng,\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn {\n\t\t'x': value.x,\n\t\t'y': value.y,\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn dehydrate_geometry(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\treturn {'lat': value.latitude, 'lon': value.longitude}\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "ques_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n\treturn self.__keys__\n\n\t@property\n\tdef values(self):\n\t\"\"\"\n\tReturn the values of the record as a list, in the same order as the keys.\n\n", "def keys(self):\n\treturn list(self.__data.keys())\n\n\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tSet the value of the record.\n\n", "def keys(self):\n\treturn self._keys\n\n", "def keys(self):\n\treturn self._keys\n\n\n", "def keys(self):\n\treturn self._keys\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of the record.\n\n", "def keys(self):\n\treturn self.__keys__\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn iterator over the keys.\n\n", "def keys(self):\n\treturn list(self._keys)\n\n\tdef values(self):\n\t\"\"\"\n\tReturn the values of the record.\n\n", "def keys(self):\n\treturn self.__keys\n\n", "def keys(self):\n\treturn self._keys\n\n", "def keys(self):\n\treturn self.values.keys()\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n\treturn self.__keys__\n\n\t@property\n\tdef values(self):\n\t\"\"\"\n\tReturn the values of the record as a list, in the same order as the keys.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\treturn list(self.__data.keys())\n\n\n\tdef __setitem__(self, key, value):\n\t\"\"\"\n\tSet the value of the record.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\treturn self._keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self._keys\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self._keys\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn a string representation of the record.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\treturn self.__keys__\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn iterator over the keys.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\treturn list(self._keys)\n\n\tdef values(self):\n\t\"\"\"\n\tReturn the values of the record.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self._keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self.values.keys()\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.__handlers.copy()\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be tuple (or None)\")\n\tif protocol_version not in cls.__handlers:\n\t\treturn {}\n\treturn {protocol_version: cls.__handlers[protocol_version]}\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\n\t\t\t\t\"Protocol version must be passed as a tuple: %r\" % protocol_version)\n\t\ttry:\n\t\t\treturn {protocol_version: cls.PROTOCOL_HANDLERS[protocol_version]}\n\t\texcept KeyError:\n\t\t\treturn {}\n\treturn cls.PROTOCOL_HANDLERS.copy()\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\thandlers = {}\n\tfor h in cls.__subclasses__():\n\t\tif protocol_version is None or h.protocol_version == protocol_version:\n\t\t\thandlers[h.protocol_version] = h\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\n\t\t\t\"Protocol version must be a tuple, but was {0}\".format(\n\t\t\t\tprotocol_version))\n\n\tall_handlers = _handlers_by_version()\n\n\tif protocol_version:\n\t\tif protocol_version in all_handlers:\n\t\t\treturn {protocol_version: all_handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn all_handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\n\t\t\t\"Protocol version should be specified as a tuple, e.g. (1, 0)\"\n\t\t)\n\n\thandlers = {}\n\tfor protocol_version, handler in cls._protocol_handlers.items():\n\t\tif protocol_version.startswith(protocol_version):\n\t\t\thandlers[protocol_version] = handler\n\n\treturn handlers\n", "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be specified as a tuple\")\n\n\tfrom . import protocols\n\tfrom . import v1\n\tfrom . import v2\n\tfrom . import v3\n\tfrom . import v4\n\tfrom . import v4_1\n\tfrom . import v4_2\n\tfrom . import v4_3\n\n\thandlers = {\n\t\tv1.Protocol.version: v1.Protocol,\n\t\tv2.Protocol.version: v2.Protocol,\n\t\tv3.Protocol.version: v3.Protocol,\n\t\tv4.Protocol.version: v4.Protocol,\n\t\tv4_1.Protocol.version: v4_1.Protocol,\n\t\tv4_2.Protocol.version: v4_2.Protocol,\n\t\tv4_3.Protocol.version: v4_3.Protocol,\n\t}\n\n\tif protocol_version is None:\n\t\treturn handlers\n\n\tif protocol_version not in handlers:\n\t\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\t\tversions = {protocol_version: cls.protocols.get(protocol_version)}\n\telse:\n\t\tversions = cls.protocols\n\treturn {version: handler for version, handler in versions.items() if handler is not None}\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\n\thandlers = {}\n\tfor handler_class in cls.__subclasses__():\n\t\tif not handler_class.is_supported():\n\t\t\tcontinue\n\t\tversion = handler_class.version\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = handler_class\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Unsupported protocol version: {0}\".format(protocol_version))\n\thandlers = {}\n\tfor version, handler in _handlers.items():\n\t\tif protocol_version is None or version == protocol_version:\n\t\t\thandlers[version] = handler\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Expected protocol version to be a tuple, got %r\" % protocol_version)\n\n\tfrom .protocol import BoltProtocolV1, BoltProtocolV2, BoltProtocolV3, BoltProtocolV4, BoltProtocolV5\n\n\thandlers = {\n\t\t(1, 0): BoltProtocolV1,\n\t\t(2, 0): BoltProtocolV2,\n\t\t(3, 0): BoltProtocolV3,\n\t\t(4, 0): BoltProtocolV4,\n\t\t(4, 1): BoltProtocolV4,\n\t\t(4, 2): BoltProtocolV4,\n\t\t(4, 3): BoltProtocolV4,\n\t\t(4, 4): BoltProtocolV4,\n\t\t(4, 5): BoltProtocolV4,\n\t\t(4, 6): BoltProtocolV4,\n\t\t(4, 7): BoltProtocolV4,\n\t\t(4, 8): BoltProtocolV4,\n\t\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.__handlers.copy()\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be tuple (or None)\")\n\tif protocol_version not in cls.__handlers:\n\t\treturn {}\n\treturn {protocol_version: cls.__handlers[protocol_version]}\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\n\t\t\t\t\"Protocol version must be passed as a tuple: %r\" % protocol_version)\n\t\ttry:\n\t\t\treturn {protocol_version: cls.PROTOCOL_HANDLERS[protocol_version]}\n\t\texcept KeyError:\n\t\t\treturn {}\n\treturn cls.PROTOCOL_HANDLERS.copy()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\thandlers = {}\n\tfor h in cls.__subclasses__():\n\t\tif protocol_version is None or h.protocol_version == protocol_version:\n\t\t\thandlers[h.protocol_version] = h\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\n\t\t\t\"Protocol version must be a tuple, but was {0}\".format(\n\t\t\t\tprotocol_version))\n\n\tall_handlers = _handlers_by_version()\n\n\tif protocol_version:\n\t\tif protocol_version in all_handlers:\n\t\t\treturn {protocol_version: all_handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn all_handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\n\t\t\t\"Protocol version should be specified as a tuple, e.g. (1, 0)\"\n\t\t)\n\n\thandlers = {}\n\tfor protocol_version, handler in cls._protocol_handlers.items():\n\t\tif protocol_version.startswith(protocol_version):\n\t\t\thandlers[protocol_version] = handler\n\n\treturn handlers\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be specified as a tuple\")\n\n\tfrom . import protocols\n\tfrom . import v1\n\tfrom . import v2\n\tfrom . import v3\n\tfrom . import v4\n\tfrom . import v4_1\n\tfrom . import v4_2\n\tfrom . import v4_3\n\n\thandlers = {\n\t\tv1.Protocol.version: v1.Protocol,\n\t\tv2.Protocol.version: v2.Protocol,\n\t\tv3.Protocol.version: v3.Protocol,\n\t\tv4.Protocol.version: v4.Protocol,\n\t\tv4_1.Protocol.version: v4_1.Protocol,\n\t\tv4_2.Protocol.version: v4_2.Protocol,\n\t\tv4_3.Protocol.version: v4_3.Protocol,\n\t}\n\n\tif protocol_version is None:\n\t\treturn handlers\n\n\tif protocol_version not in handlers:\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\t\tversions = {protocol_version: cls.protocols.get(protocol_version)}\n\telse:\n\t\tversions = cls.protocols\n\treturn {version: handler for version, handler in versions.items() if handler is not None}\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be passed in a tuple\")\n\n\thandlers = {}\n\tfor handler_class in cls.__subclasses__():\n\t\tif not handler_class.is_supported():\n\t\t\tcontinue\n\t\tversion = handler_class.version\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = handler_class\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Unsupported protocol version: {0}\".format(protocol_version))\n\thandlers = {}\n\tfor version, handler in _handlers.items():\n\t\tif protocol_version is None or version == protocol_version:\n\t\t\thandlers[version] = handler\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Expected protocol version to be a tuple, got %r\" % protocol_version)\n\n\tfrom .protocol import BoltProtocolV1, BoltProtocolV2, BoltProtocolV3, BoltProtocolV4, BoltProtocolV5\n\n\thandlers = {\n\t\t(1, 0): BoltProtocolV1,\n\t\t(2, 0): BoltProtocolV2,\n\t\t(3, 0): BoltProtocolV3,\n\t\t(4, 0): BoltProtocolV4,\n\t\t(4, 1): BoltProtocolV4,\n\t\t(4, 2): BoltProtocolV4,\n\t\t(4, 3): BoltProtocolV4,\n\t\t(4, 4): BoltProtocolV4,\n\t\t(4, 5): BoltProtocolV4,\n\t\t(4, 6): BoltProtocolV4,\n\t\t(4, 7): BoltProtocolV4,\n\t\t(4, 8): BoltProtocolV4,\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "ques_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n\tdef create_tx_function(f):\n\t\t@wraps(f)\n\t\tdef tx_function(*args, **kwargs):\n\t\t\ttx = args[0]\n\t\t\tif timeout is not None:\n\t\t\t\ttx.timeout = timeout\n\t\t\tif metadata is not None:\n\t\t\t\ttx.metadata = metadata\n\t\t\treturn f(*args, **kwargs)\n\t\treturn tx_function\n\treturn create_tx_function\n", "def unit_of_work(metadata=None, timeout=None):\n\tif metadata is not None and not isinstance(metadata, dict):\n\t\traise TypeError(\"metadata must be a dict\")\n\tif timeout is not None and not isinstance(timeout, (int, float)):\n\t\traise TypeError(\"timeout must be a float or None\")\n\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# We could use inspect.getcallargs to get the correct set of\n\t\t\t# arguments and pass those to begin_transaction, but that would\n\t\t\t# mean we break if the number or type of arguments to the\n\t\t\t# decorated function changes.\n\t\t\tconnection = args[0]\n\t\t\tif not isinstance(connection, Connection):\n\t\t\t\traise TypeError(\"unit_of_work must only be applied to functions that have a Connection instance as their first argument\")\n\t\t\tif len(args) > 1 or kwargs:\n\t\t\t\t# The function has more arguments than just the connection,\n\t\t\t\t# so we need to bind them to\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn _unit_of_work(func, metadata, timeout, *args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\twith self.begin_tx(metadata=metadata, timeout=timeout) as tx:\n\t\t\t\tresult = func(self, tx, *args, **kwargs)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttx = begin_transaction(metadata=metadata, timeout=timeout)\n\t\t\ttry:\n\t\t\t\tresult = fn(tx, *args, **kwargs)\n\t\t\t\ttx.commit()\n\t\t\t\treturn result\n\t\t\texcept:\n\t\t\t\ttx.rollback()\n\t\t\t\traise\n\t\treturn wrapper\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\t@wraps(fn)\n\t\tdef func(self, *args, **kwargs):\n\t\t\treturn self.write_transaction(fn, *args, metadata=metadata, timeout=timeout, **kwargs)\n\t\treturn func\n\treturn decorator\n\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef wrapper(func):\n\t\t@functools.wraps(func)\n\t\tdef inner(graph, *args, **kwargs):\n\t\t\tif graph is None:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn graph.run_in_tx(lambda tx: func(tx, *args, **kwargs), metadata=metadata, timeout=timeout)\n\t\treturn inner\n\treturn wrapper\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\twith driver.session(*args, **kwargs) as session:\n\t\t\t\treturn session.write_transaction(func, metadata=metadata, timeout=timeout)\n\t\treturn wrapped\n\treturn decorator\n\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# get the first positional argument\n\t\t\t# this is the transaction\n\t\t\ttx = args[0]\n\t\t\t# assign the timeout and metadata to the transaction\n\t\t\ttx._set_timeout(timeout)\n\t\t\ttx._set_metadata(metadata)\n\t\t\treturn f(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(tx_func):\n\t\t@functools.wraps(tx_func)\n\t\tdef wrapper(tx):\n\t\t\tif metadata is not None:\n\t\t\t\ttx.run(\"CALL dbms.setTXMetaData($metadata)\", metadata=metadata)\n\t\t\tif timeout is not None:\n\t\t\t\ttx.run(\"CALL dbms.setTXTimeout($timeout)\", timeout=timeout)\n\t\t\treturn tx_func(tx)\n\t\treturn wrapper\n\treturn decorator\n\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef create_tx_function(f):\n\t\t@wraps(f)\n\t\tdef tx_function(*args, **kwargs):\n\t\t\ttx = args[0]\n\t\t\tif timeout is not None:\n\t\t\t\ttx.timeout = timeout\n\t\t\tif metadata is not None:\n\t\t\t\ttx.metadata = metadata\n\t\t\treturn f(*args, **kwargs)\n\t\treturn tx_function\n\treturn create_tx_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tif metadata is not None and not isinstance(metadata, dict):\n\t\traise TypeError(\"metadata must be a dict\")\n\tif timeout is not None and not isinstance(timeout, (int, float)):\n\t\traise TypeError(\"timeout must be a float or None\")\n\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# We could use inspect.getcallargs to get the correct set of\n\t\t\t# arguments and pass those to begin_transaction, but that would\n\t\t\t# mean we break if the number or type of arguments to the\n\t\t\t# decorated function changes.\n\t\t\tconnection = args[0]\n\t\t\tif not isinstance(connection, Connection):\n\t\t\t\traise TypeError(\"unit_of_work must only be applied to functions that have a Connection instance as their first argument\")\n\t\t\tif len(args) > 1 or kwargs:\n\t\t\t\t# The function has more arguments than just the connection,\n\t\t\t\t# so we need to bind them to\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn _unit_of_work(func, metadata, timeout, *args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\twith self.begin_tx(metadata=metadata, timeout=timeout) as tx:\n\t\t\t\tresult = func(self, tx, *args, **kwargs)\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttx = begin_transaction(metadata=metadata, timeout=timeout)\n\t\t\ttry:\n\t\t\t\tresult = fn(tx, *args, **kwargs)\n\t\t\t\ttx.commit()\n\t\t\t\treturn result\n\t\t\texcept:\n\t\t\t\ttx.rollback()\n\t\t\t\traise\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\t@wraps(fn)\n\t\tdef func(self, *args, **kwargs):\n\t\t\treturn self.write_transaction(fn, *args, metadata=metadata, timeout=timeout, **kwargs)\n\t\treturn func\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef wrapper(func):\n\t\t@functools.wraps(func)\n\t\tdef inner(graph, *args, **kwargs):\n\t\t\tif graph is None:\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn graph.run_in_tx(lambda tx: func(tx, *args, **kwargs), metadata=metadata, timeout=timeout)\n\t\treturn inner\n\treturn wrapper\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\tdef wrapped(*args, **kwargs):\n\t\t\twith driver.session(*args, **kwargs) as session:\n\t\t\t\treturn session.write_transaction(func, metadata=metadata, timeout=timeout)\n\t\treturn wrapped\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# get the first positional argument\n\t\t\t# this is the transaction\n\t\t\ttx = args[0]\n\t\t\t# assign the timeout and metadata to the transaction\n\t\t\ttx._set_timeout(timeout)\n\t\t\ttx._set_metadata(metadata)\n\t\t\treturn f(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(tx_func):\n\t\t@functools.wraps(tx_func)\n\t\tdef wrapper(tx):\n\t\t\tif metadata is not None:\n\t\t\t\ttx.run(\"CALL dbms.setTXMetaData($metadata)\", metadata=metadata)\n\t\t\tif timeout is not None:\n\t\t\t\ttx.run(\"CALL dbms.setTXTimeout($timeout)\", timeout=timeout)\n\t\t\treturn tx_func(tx)\n\t\treturn wrapper\n\treturn decorator\n\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "ques_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n\t\treturn self.__dict.index(key)\n\n\tdef find(self, key):\n\t\"\"\"\n\tReturn the index of the given item, if it is in the list.\n\n", "def index(self, key):\n\treturn self._data.index(key)\n\t\n\tdef remove(self, key):\n\t\t\"\"\"\n\t\tRemove the given item.\n\n", "def index(self, key):\n\tif not isinstance(key, (int, long)):\n\t\traise TypeError('key must be an integer')\n\tif key < 0:\n\t\traise ValueError('key must be positive')\n\treturn key\n\n\n\tdef __getitem__(self, key):\n\t\treturn self.data[self.index(key)]\n\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\n\tdef __iter__(self):\n\t\treturn iter(self.data)\n\n\n\tdef __repr__(self):\n\t\treturn '%s(%r)' % (self.__class__.__name__, self.data)\n\n\n", "def index(self, key):\n\tif key is None:\n\t\traise ValueError(\"key is None\")\n\tif not isinstance(key, Key):\n\t\traise ValueError(\"key is not a Key\")\n\tif key.is_None():\n\t\traise ValueError(\"key is None\")\n\treturn self.__index(key)\n\n", "def index(self, key):\n\treturn self._list.index(key)\n\n\n\tdef remove(self, key):\n\t\"\"\"\n\tRemove the given item.\n\n", "def index(self, key):\n\treturn self.items.index(key)\n\n", "def index(self, key):\n\treturn self.__keys.index(key)\n\n", "def index(self, key):\n\treturn self._keys.index(key)\n\n", "def index(self, key):\n\treturn self.__keys.index(key)\n\t\n", "def index(self, key):\n\tfor i in range(len(self)):\n\t\tif self[i] == key:\n\t\t\treturn i\n\traise ValueError(\"%s is not in the list\" % key)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n\t\treturn self.__dict.index(key)\n\n\tdef find(self, key):\n\t\"\"\"\n\tReturn the index of the given item, if it is in the list.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self._data.index(key)\n\t\n\tdef remove(self, key):\n\t\t\"\"\"\n\t\tRemove the given item.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\tif not isinstance(key, (int, long)):\n\t\traise TypeError('key must be an integer')\n\tif key < 0:\n\t\traise ValueError('key must be positive')\n\treturn key\n\n\n\tdef __getitem__(self, key):\n\t\treturn self.data[self.index(key)]\n\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\n\tdef __iter__(self):\n\t\treturn iter(self.data)\n\n\n\tdef __repr__(self):\n\t\treturn '%s(%r)' % (self.__class__.__name__, self.data)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\tif key is None:\n\t\traise ValueError(\"key is None\")\n\tif not isinstance(key, Key):\n\t\traise ValueError(\"key is not a Key\")\n\tif key.is_None():\n\t\traise ValueError(\"key is None\")\n\treturn self.__index(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self._list.index(key)\n\n\n\tdef remove(self, key):\n\t\"\"\"\n\tRemove the given item.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.items.index(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.__keys.index(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self._keys.index(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.__keys.index(key)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\tfor i in range(len(self)):\n\t\tif self[i] == key:\n\t\t\treturn i\n\traise ValueError(\"%s is not in the list\" % key)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "ques_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n\t\tif len(keys) == 0:\n\t\t\treturn self.__values\n\t\telse:\n\t\t\treturn [self.__getitem__(key) for key in keys]\n\n\tdef to_dict(self):\n\t\t\"\"\"\n\t\tConvert the record to a dictionary, using the headers\n", "def values(self, *keys):\n\tif len(keys) == 0:\n\t\treturn self._values\n\telse:\n\t\treturn [self[k] for k in keys]\n\t\n\tdef items(self):\n\t\"\"\"\n\tReturn the items of the record.\n\n", "def values(self, *keys):\n\tvalues = []\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tvalues.append(self[key])\n\t\telif isinstance(key, str):\n\t\t\tvalues.append(self[key])\n\t\telse:\n\t\t\traise ValueError(\"Invalid key type\")\n\treturn values\n\n", "def values(self, *keys):\n\tif not keys:\n\t\tkeys = self.keys()\n\treturn [self[key] for key in keys]\n\n\tdef __repr__(self):\n\t\treturn '<Record {}>'.format(self.fields)\n\n\tdef __str__(self):\n\t\treturn '{}'.format(self.fields)\n\n\tdef __getattr__(self, name):\n\t\treturn self[name]\n\n\tdef __setattr__(self, name, value):\n\t\tself[name] = value\n\n\tdef __delattr__(self, name):\n\t\tdel self[name]\n\n\n", "def values(self, *keys):\n\t\tif keys:\n\t\t\treturn [self.__values[key] for key in keys]\n\t\telse:\n\t\t\treturn self.__values\n\n\tdef __str__(self):\n\t\treturn 'Record({}, {}, {}, {})'.format(self.__id, self.__name, self.__keys, self.__values)\n\n", "def values(self, *keys):\n\treturn self.__values(*keys)\n\n\tdef __contains__(self, item):\n\t\"\"\"\n\tCheck whether the record contains a value.\n\n", "def values(self, *keys):\n\t\n\tif keys:\n\t\treturn [self[key] for key in keys]\n\t\n\treturn self.__values\n\t\n", "def values(self, *keys):\n\tif len(keys) == 0:\n\t\tkeys = self._fields.keys()\n\treturn [self[key] for key in keys]\n\n\n", "def values(self, *keys):\n\tkeys = keys or self.keys()\n\treturn [self[key] for key in keys]\n\n\n", "def values(self, *keys):\n\tif len(keys) == 0:\n\t\treturn [self._data[index] for index in range(len(self._data))]\n\telse:\n\t\treturn [self._data[index] for index in self._get_indexes(keys)]\n\n\tdef _get_indexes(self, keys):\n\t\"\"\"\n\tGet the indexes of the items with the specified keys.\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n\t\tif len(keys) == 0:\n\t\t\treturn self.__values\n\t\telse:\n\t\t\treturn [self.__getitem__(key) for key in keys]\n\n\tdef to_dict(self):\n\t\t\"\"\"\n\t\tConvert the record to a dictionary, using the headers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\tif len(keys) == 0:\n\t\treturn self._values\n\telse:\n\t\treturn [self[k] for k in keys]\n\t\n\tdef items(self):\n\t\"\"\"\n\tReturn the items of the record.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\tvalues = []\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tvalues.append(self[key])\n\t\telif isinstance(key, str):\n\t\t\tvalues.append(self[key])\n\t\telse:\n\t\t\traise ValueError(\"Invalid key type\")\n\treturn values\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\tif not keys:\n\t\tkeys = self.keys()\n\treturn [self[key] for key in keys]\n\n\tdef __repr__(self):\n\t\treturn '<Record {}>'.format(self.fields)\n\n\tdef __str__(self):\n\t\treturn '{}'.format(self.fields)\n\n\tdef __getattr__(self, name):\n\t\treturn self[name]\n\n\tdef __setattr__(self, name, value):\n\t\tself[name] = value\n\n\tdef __delattr__(self, name):\n\t\tdel self[name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\tif keys:\n\t\t\treturn [self.__values[key] for key in keys]\n\t\telse:\n\t\t\treturn self.__values\n\n\tdef __str__(self):\n\t\treturn 'Record({}, {}, {}, {})'.format(self.__id, self.__name, self.__keys, self.__values)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\treturn self.__values(*keys)\n\n\tdef __contains__(self, item):\n\t\"\"\"\n\tCheck whether the record contains a value.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\t\n\tif keys:\n\t\treturn [self[key] for key in keys]\n\t\n\treturn self.__values\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\tif len(keys) == 0:\n\t\tkeys = self._fields.keys()\n\treturn [self[key] for key in keys]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\tkeys = keys or self.keys()\n\treturn [self[key] for key in keys]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\tif len(keys) == 0:\n\t\treturn [self._data[index] for index in range(len(self._data))]\n\telse:\n\t\treturn [self._data[index] for index in self._get_indexes(keys)]\n\n\tdef _get_indexes(self, keys):\n\t\"\"\"\n\tGet the indexes of the items with the specified keys.\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "ques_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n\tif not keys:\n\t\treturn dict(zip(self.fields, self.values))\n\tret = {}\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tret[self.fields[key]] = self.values[key]\n\t\telse:\n\t\t\tret[key] = self.data(self.index(key))\n\treturn ret\n\n\tdef __repr__(self):\n\t\treturn '<%s: %s>' % (self.__class__.__name__, dict(self))\n\n", "def data(self, *keys):\n\tif not keys:\n\t\tkeys = self.fields\n\treturn dict((key, self[key]) for key in keys)\n\n", "def data(self, *keys):\n\treturn dict(zip(self.fields, self.values(*keys)))\n\n\n", "def data(self, *keys):\n\tif keys:\n\t\treturn dict(self.items(*keys))\n\telse:\n\t\treturn dict(self)\n\n\tdef items(self, *keys):\n\t\"\"\"\n\tGenerate a series of 2-tuples of the keys and values of\n", "def data(self, *keys):\n\t\tif not keys:\n\t\t\tkeys = range(len(self.fields))\n\t\treturn dict((self.fields[k], self[k]) for k in keys)\n\n\tdef __repr__(self):\n\t\treturn '<Record %r>' % (self.data(),)\n\n\n", "def data(self, *keys):\n\tif not keys:\n\t\tkeys = range(len(self))\n\trv = {}\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tkey = self.fields[key]\n\t\trv[key] = self[key]\n\treturn rv\n", "def data(self, *keys):\n\t\n\tif len(keys) == 0:\n\t\tkeys = list(range(len(self)))\n\t\n\tdata = {}\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tkey = self.field_names[key]\n\t\tdata[key] = self[key]\n\treturn data\n\n", "def data(self, *keys):\n\tif keys:\n\t\treturn dict((self.fields[k] if isinstance(k, int) else k, self[k]) for k in keys)\n\treturn dict(self)\n\n\n", "def data(self, *keys):\n\tif len(keys) == 0:\n\t\treturn dict(zip(self.fields, self.values))\n\telse:\n\t\treturn dict([(self.fields[k], self.values[k]) for k in keys])\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturn a value from the record by index or key.\n\n", "def data(self, *keys):\n\tif keys:\n\t\treturn dict((k, self[k]) for k in keys)\n\telse:\n\t\treturn dict(self)\n\n\t@classmethod\n\tdef from_json(cls, s, encoding='utf-8', **kwargs):\n\t\t\"\"\"\n\t\tReturn a record from a JSON string.\n\n\t\t:param s: JSON string\n\t\t:param encoding: encoding of the JSON string\n\t\t:param kwargs: keyword arguments passed to :func:`json.loads()`\n\t\t:return: record\n\t\t\"\"\"\n\t\treturn cls(json.loads(s, encoding=encoding, **kwargs))\n\n\tdef json(self, indent=None, sort_keys=False, **kwargs):\n\t\t\"\"\"\n\t\tReturn a JSON representation of this record.\n\n\t\t:param indent: indentation level for pretty-printing\n\t\t:param sort_keys: if :const:`True`, sort the output of dictionaries by key\n\t\t:param kwargs: keyword arguments passed to :func:`json.dumps()`\n\t\t:return: JSON string\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n\tif not keys:\n\t\treturn dict(zip(self.fields, self.values))\n\tret = {}\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tret[self.fields[key]] = self.values[key]\n\t\telse:\n\t\t\tret[key] = self.data(self.index(key))\n\treturn ret\n\n\tdef __repr__(self):\n\t\treturn '<%s: %s>' % (self.__class__.__name__, dict(self))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\tif not keys:\n\t\tkeys = self.fields\n\treturn dict((key, self[key]) for key in keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\treturn dict(zip(self.fields, self.values(*keys)))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\tif keys:\n\t\treturn dict(self.items(*keys))\n\telse:\n\t\treturn dict(self)\n\n\tdef items(self, *keys):\n\t\"\"\"\n\tGenerate a series of 2-tuples of the keys and values of\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\tif not keys:\n\t\t\tkeys = range(len(self.fields))\n\t\treturn dict((self.fields[k], self[k]) for k in keys)\n\n\tdef __repr__(self):\n\t\treturn '<Record %r>' % (self.data(),)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\tif not keys:\n\t\tkeys = range(len(self))\n\trv = {}\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tkey = self.fields[key]\n\t\trv[key] = self[key]\n\treturn rv\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\n\tif len(keys) == 0:\n\t\tkeys = list(range(len(self)))\n\t\n\tdata = {}\n\tfor key in keys:\n\t\tif isinstance(key, int):\n\t\t\tkey = self.field_names[key]\n\t\tdata[key] = self[key]\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\tif keys:\n\t\treturn dict((self.fields[k] if isinstance(k, int) else k, self[k]) for k in keys)\n\treturn dict(self)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def data(self, *keys):\n\tif len(keys) == 0:\n\t\treturn dict(zip(self.fields, self.values))\n\telse:\n\t\treturn dict([(self.fields[k], self.values[k]) for k in keys])\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturn a value from the record by index or key.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\tif keys:\n\t\treturn dict((k, self[k]) for k in keys)\n\telse:\n\t\treturn dict(self)\n\n\t@classmethod\n\tdef from_json(cls, s, encoding='utf-8', **kwargs):\n\t\t\"\"\"\n\t\tReturn a record from a JSON string.\n\n\t\t:param s: JSON string\n\t\t:param encoding: encoding of the JSON string\n\t\t:param kwargs: keyword arguments passed to :func:`json.loads()`\n\t\t:return: record\n\t\t\"\"\"\n\t\treturn cls(json.loads(s, encoding=encoding, **kwargs))\n\n\tdef json(self, indent=None, sort_keys=False, **kwargs):\n\t\t\"\"\"\n\t\tReturn a JSON representation of this record.\n\n\t\t:param indent: indentation level for pretty-printing\n\t\t:param sort_keys: if :const:`True`, sort the output of dictionaries by key\n\t\t:param kwargs: keyword arguments passed to :func:`json.dumps()`\n\t\t:return: JSON string\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "ques_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n\treturn self.pop(2).unpack('>H')[0]\n\n\n", "def pop_u16(self):\n\treturn struct.unpack(\"!H\", self.pop(2))[0]\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n", "def pop_u16(self):\n\treturn struct.unpack(\"!H\", self.pop(2))[0]\n\n", "def pop_u16(self):\n\treturn struct.unpack('>H', self.pop(2))[0]\n\n", "def pop_u16(self):\n\treturn self._pop(2)\n\t\n", "def pop_u16(self):\n\treturn unpack(\"!H\", self.pop(2))[0]\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n", "def pop_u16(self):\n\treturn struct.unpack('>H', self.pop(2))[0]\n\n", "def pop_u16(self):\n\treturn struct.unpack('>H', self.data[-2:])[0]\n\n\tdef pop_u24(self):\n\t\"\"\"\n\tRemove the last three bytes of data, returning them as a big-endian\n", "def pop_u16(self):\n\treturn struct.unpack(\">H\", self.pop(2))[0]\n\n", "def pop_u16(self):\n\tif self.len < 2:\n\t\traise BufferUnderflow\n\tself.len -= 2\n\treturn (self.data[self.len + 1] << 8) | self.data[self.len]\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n\treturn self.pop(2).unpack('>H')[0]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn struct.unpack(\"!H\", self.pop(2))[0]\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn struct.unpack(\"!H\", self.pop(2))[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn struct.unpack('>H', self.pop(2))[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn self._pop(2)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn unpack(\"!H\", self.pop(2))[0]\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four bytes of data, returning them as a big-endian\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn struct.unpack('>H', self.pop(2))[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn struct.unpack('>H', self.data[-2:])[0]\n\n\tdef pop_u24(self):\n\t\"\"\"\n\tRemove the last three bytes of data, returning them as a big-endian\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn struct.unpack(\">H\", self.pop(2))[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\tif self.len < 2:\n\t\traise BufferUnderflow\n\tself.len -= 2\n\treturn (self.data[self.len + 1] << 8) | self.data[self.len]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "ques_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tpack_stream = PackStream()\n\tpack_stream.pack_discard(n, qid)\n\treturn self._enqueue(pack_stream, stream_id, dehydration_hooks,\n\t                     hydration_hooks, **handlers)\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tmsg = Message(MSG_DISCARD, n=n, qid=qid)\n\treturn self.send_message(msg, dehydration_hooks, hydration_hooks, **handlers)\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self.run(\n\t\t\"DISCARD\", n, qid, dehydration_hooks,\n\t\thydration_hooks, **handlers\n\t)\n\n\tdef pull_all(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a PULL_ALL message to the output queue.\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\treturn self.write_message(Message(MessageType.DISCARD,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{'n':n, 'qid':qid},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thydration_hooks=hydration_hooks,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t**handlers))\n\n\tdef pull_all(self, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a PULL_ALL message to the output queue.\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self._send(\n\t\t_cypher_statement(\"DISCARD\", n, qid,\n\t\t                  dehydration_hooks=dehydration_hooks,\n\t\t                  hydration_hooks=hydration_hooks,\n\t\t                  **handlers))\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self._send({\n\t\t'opcode': bolt.OP_DISCARD,\n\t\t'fields': [n, qid],\n\t\t'handlers': handlers,\n\t\t'dehydration_hooks': dehydration_hooks,\n\t\t'hydration_hooks': hydration_hooks,\n\t})\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tself._enqueue(Discard(n, qid), dehydration_hooks=dehydration_hooks,\n                   hydration_hooks=hydration_hooks, **handlers)\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tself._output.append(Message(message_type=Message.DISCARD,\n                                    dehydrate_hooks=dehydration_hooks,\n                                    hydrate_hooks=hydration_hooks,\n                                    n=n, qid=qid, **handlers))\n\treturn self._output[-1]\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self.query(Query(\n\t\t\t'system', 'dbms.discard', {'qid': qid, 'n': n}),\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tqid = self._qid(qid)\n\tm = self._message(qid, n)\n\tself._write(m)\n\treturn self.Response(self, qid, dehydration_hooks, hydration_hooks, **handlers)\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tpack_stream = PackStream()\n\tpack_stream.pack_discard(n, qid)\n\treturn self._enqueue(pack_stream, stream_id, dehydration_hooks,\n\t                     hydration_hooks, **handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tmsg = Message(MSG_DISCARD, n=n, qid=qid)\n\treturn self.send_message(msg, dehydration_hooks, hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self.run(\n\t\t\"DISCARD\", n, qid, dehydration_hooks,\n\t\thydration_hooks, **handlers\n\t)\n\n\tdef pull_all(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a PULL_ALL message to the output queue.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\treturn self.write_message(Message(MessageType.DISCARD,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t{'n':n, 'qid':qid},\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\thydration_hooks=hydration_hooks,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t**handlers))\n\n\tdef pull_all(self, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\t\"\"\"\n\tAppends a PULL_ALL message to the output queue.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self._send(\n\t\t_cypher_statement(\"DISCARD\", n, qid,\n\t\t                  dehydration_hooks=dehydration_hooks,\n\t\t                  hydration_hooks=hydration_hooks,\n\t\t                  **handlers))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self._send({\n\t\t'opcode': bolt.OP_DISCARD,\n\t\t'fields': [n, qid],\n\t\t'handlers': handlers,\n\t\t'dehydration_hooks': dehydration_hooks,\n\t\t'hydration_hooks': hydration_hooks,\n\t})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tself._enqueue(Discard(n, qid), dehydration_hooks=dehydration_hooks,\n                   hydration_hooks=hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tself._output.append(Message(message_type=Message.DISCARD,\n                                    dehydrate_hooks=dehydration_hooks,\n                                    hydrate_hooks=hydration_hooks,\n                                    n=n, qid=qid, **handlers))\n\treturn self._output[-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self.query(Query(\n\t\t\t'system', 'dbms.discard', {'qid': qid, 'n': n}),\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tqid = self._qid(qid)\n\tm = self._message(qid, n)\n\tself._write(m)\n\treturn self.Response(self, qid, dehydration_hooks, hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "ques_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif mode is None:\n\t\t# Default to WRITE mode.\n\t\tmode = \"WRITE\"\n\telif mode not in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\"mode must be READ or WRITE\")\n\n\tif bookmarks is None:\n\t\tbookmarks = ()\n\telif isinstance(bookmarks, str):\n\t\tbookmarks = (bookmarks,)\n\n\tif metadata is None:\n\t\tmetadata = {}\n\n\tif db is not None and self.protocol_version < 4:\n\t\traise ValueError(\"`db` requires Bolt 4.0+\")\n\n\tif imp_user is not None and self.protocol_version < 4.4:\n\t\traise ValueError(\"`imp_user` requires Bolt 4.4+\")\n\n\tmessage = pack_begin_message(mode, bookmarks, metadata, timeout, db, imp_user)\n\tself.send_and_fetch(message, handlers)\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif mode is None:\n\t\tmode = \"WRITE\"\n\tif metadata is None:\n\t\tmetadata = {}\n\tif timeout is None:\n\t\ttimeout = 0\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\n\t# TODO: use the new context manager protocol\n\treturn self._enqueue(\"BEGIN\", mode=mode, bookmarks=bookmarks,\n\t\t\t\t\t\t metadata=metadata, timeout=timeout, db=db,\n\t\t\t\t\t\t imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t hydration_hooks=hydration_hooks, **handlers)\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tpass\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tbookmarks = bookmarks or []\n\tmetadata = metadata or {}\n\ttimeout = timeout or 0\n\timp_user = imp_user or \"\"\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif db is None:\n\t\tdb = \"\"\n\tself._out.append(packstream.Packer(handlers=dehydration_hooks).pack({\n\t\t\"signature\": MESSAGE_BEGIN,\n\t\t\"bookmarks\": bookmarks,\n\t\t\"mode\": mode or \"WRITE\",\n\t\t\"db\": db,\n\t\t\"timeout\": timeout,\n\t\t\"metadata\": metadata,\n\t\t\"imp_user\": imp_user\n\t}))\n\tself._out_buf = b\"\".join(self._out)\n\treturn Response(self._out, self._in, handlers=hydration_hooks)\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._begin(mode=mode, bookmarks=bookmarks, metadata=metadata,\n\t\t\ttimeout=timeout, db=db, imp_user=imp_user,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif self.closed:\n\t\traise ServiceUnavailable(\"Connection is closed.\")\n\tif self.transaction:\n\t\traise ProtocolError(\"Can't begin transaction within another \"\n\t\t\t\t\t\t\t\"transaction.\")\n\tself.transaction = True\n\tself._push(Message(\"BEGIN\",\n\t\t\t\t\t   {\"bookmarks\": bookmarks, \"mode\": mode, \"timeout\": timeout, \"db\": db, \"imp_user\": imp_user},\n\t\t\t\t\t   metadata=metadata),\n\t\t\t   **handlers)\n\treturn self._get_response(dehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t\t  hydration_hooks=hydration_hooks)\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tmsg = Message(\"BEGIN\", {})\n\tself._send_message(msg, handlers)\n\treturn Response(self._receive_message, self._futures, handlers)\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._send_and_receive(\n\t\t\t_Message.BEGIN,\n\t\t\t{\n\t\t\t\t\t'mode': mode,\n\t\t\t\t\t'bookmarks': bookmarks,\n\t\t\t\t\t'metadata': metadata,\n\t\t\t\t\t'timeout': timeout,\n\t\t\t\t\t'db': db,\n\t\t\t\t\t'imp_user': imp_user,\n\t\t\t},\n\t\t\t**handlers)\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif mode is None:\n\t\tmode = \"WRITE\"\n\telif mode not in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\n\t\t\t\"Invalid access mode '%s'. Must be 'READ' or 'WRITE'.\" % mode)\n\tif bookmarks is None:\n\t\tbookmarks = []\n\tif metadata is None:\n\t\tmetadata = {}\n\tif timeout is not None and not isinstance(timeout, numbers.Number):\n\t\traise TypeError(\n\t\t\t\"Invalid timeout '%s'. Must be a number.\" % timeout)\n\tif db is not None and not isinstance(db, str):\n\t\traise TypeError(\"Invalid db '%s'. Must be a string.\" % db)\n\tif imp_user is not None and not isinstance(imp_user, str):\n\t\traise TypeError(\"Invalid imp_user '%s'. Must be a string.\" % imp_user)\n\n\tself._assert_open()\n\tif self._in_tx:\n\t\traise BoltProtocolError(\"Cannot begin a transaction inside a transaction\")\n\tself._in_\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif db is not None:\n\t\tself.db = db\n\tif not self.db:\n\t\traise ValueError(\"Database name must be specified\")\n\tif self.tx_open:\n\t\traise ValueError(\"Transaction is already open\")\n\tself.tx_open = True\n\tself.handlers = handlers\n\tself.handlers.update(\n\t\tself.dehydration_hooks or {}\n\t)\n\tself.handlers.update(\n\t\tself.hydration_hooks or {}\n\t)\n\tself.handlers.update(\n\t\tdehydration_hooks or {}\n\t)\n\tself.handlers.update(\n\t\thydration_hooks or {}\n\t)\n\tself.mode = mode or \"WRITE\"\n\tself.bookmarks = bookmarks or ()\n\tself.metadata = metadata or {}\n\tself.timeout = timeout\n\tself.imp_user = imp_user\n\tmessage = BeginMessage(self.db,\n\t                       self.mode,\n\t                       self.bookmarks,\n\t                       self.metadata,\n\t                       self.timeout\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif mode is None:\n\t\t# Default to WRITE mode.\n\t\tmode = \"WRITE\"\n\telif mode not in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\"mode must be READ or WRITE\")\n\n\tif bookmarks is None:\n\t\tbookmarks = ()\n\telif isinstance(bookmarks, str):\n\t\tbookmarks = (bookmarks,)\n\n\tif metadata is None:\n\t\tmetadata = {}\n\n\tif db is not None and self.protocol_version < 4:\n\t\traise ValueError(\"`db` requires Bolt 4.0+\")\n\n\tif imp_user is not None and self.protocol_version < 4.4:\n\t\traise ValueError(\"`imp_user` requires Bolt 4.4+\")\n\n\tmessage = pack_begin_message(mode, bookmarks, metadata, timeout, db, imp_user)\n\tself.send_and_fetch(message, handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif mode is None:\n\t\tmode = \"WRITE\"\n\tif metadata is None:\n\t\tmetadata = {}\n\tif timeout is None:\n\t\ttimeout = 0\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\n\t# TODO: use the new context manager protocol\n\treturn self._enqueue(\"BEGIN\", mode=mode, bookmarks=bookmarks,\n\t\t\t\t\t\t metadata=metadata, timeout=timeout, db=db,\n\t\t\t\t\t\t imp_user=imp_user, dehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t hydration_hooks=hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tbookmarks = bookmarks or []\n\tmetadata = metadata or {}\n\ttimeout = timeout or 0\n\timp_user = imp_user or \"\"\n\tif dehydration_hooks is None:\n\t\tdehydration_hooks = {}\n\tif hydration_hooks is None:\n\t\thydration_hooks = {}\n\tif db is None:\n\t\tdb = \"\"\n\tself._out.append(packstream.Packer(handlers=dehydration_hooks).pack({\n\t\t\"signature\": MESSAGE_BEGIN,\n\t\t\"bookmarks\": bookmarks,\n\t\t\"mode\": mode or \"WRITE\",\n\t\t\"db\": db,\n\t\t\"timeout\": timeout,\n\t\t\"metadata\": metadata,\n\t\t\"imp_user\": imp_user\n\t}))\n\tself._out_buf = b\"\".join(self._out)\n\treturn Response(self._out, self._in, handlers=hydration_hooks)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._begin(mode=mode, bookmarks=bookmarks, metadata=metadata,\n\t\t\ttimeout=timeout, db=db, imp_user=imp_user,\n\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\thydration_hooks=hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif self.closed:\n\t\traise ServiceUnavailable(\"Connection is closed.\")\n\tif self.transaction:\n\t\traise ProtocolError(\"Can't begin transaction within another \"\n\t\t\t\t\t\t\t\"transaction.\")\n\tself.transaction = True\n\tself._push(Message(\"BEGIN\",\n\t\t\t\t\t   {\"bookmarks\": bookmarks, \"mode\": mode, \"timeout\": timeout, \"db\": db, \"imp_user\": imp_user},\n\t\t\t\t\t   metadata=metadata),\n\t\t\t   **handlers)\n\treturn self._get_response(dehydration_hooks=dehydration_hooks,\n\t\t\t\t\t\t\t  hydration_hooks=hydration_hooks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tmsg = Message(\"BEGIN\", {})\n\tself._send_message(msg, handlers)\n\treturn Response(self._receive_message, self._futures, handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._send_and_receive(\n\t\t\t_Message.BEGIN,\n\t\t\t{\n\t\t\t\t\t'mode': mode,\n\t\t\t\t\t'bookmarks': bookmarks,\n\t\t\t\t\t'metadata': metadata,\n\t\t\t\t\t'timeout': timeout,\n\t\t\t\t\t'db': db,\n\t\t\t\t\t'imp_user': imp_user,\n\t\t\t},\n\t\t\t**handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif mode is None:\n\t\tmode = \"WRITE\"\n\telif mode not in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\n\t\t\t\"Invalid access mode '%s'. Must be 'READ' or 'WRITE'.\" % mode)\n\tif bookmarks is None:\n\t\tbookmarks = []\n\tif metadata is None:\n\t\tmetadata = {}\n\tif timeout is not None and not isinstance(timeout, numbers.Number):\n\t\traise TypeError(\n\t\t\t\"Invalid timeout '%s'. Must be a number.\" % timeout)\n\tif db is not None and not isinstance(db, str):\n\t\traise TypeError(\"Invalid db '%s'. Must be a string.\" % db)\n\tif imp_user is not None and not isinstance(imp_user, str):\n\t\traise TypeError(\"Invalid imp_user '%s'. Must be a string.\" % imp_user)\n\n\tself._assert_open()\n\tif self._in_tx:\n\t\traise BoltProtocolError(\"Cannot begin a transaction inside a transaction\")\n\tself._in_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif db is not None:\n\t\tself.db = db\n\tif not self.db:\n\t\traise ValueError(\"Database name must be specified\")\n\tif self.tx_open:\n\t\traise ValueError(\"Transaction is already open\")\n\tself.tx_open = True\n\tself.handlers = handlers\n\tself.handlers.update(\n\t\tself.dehydration_hooks or {}\n\t)\n\tself.handlers.update(\n\t\tself.hydration_hooks or {}\n\t)\n\tself.handlers.update(\n\t\tdehydration_hooks or {}\n\t)\n\tself.handlers.update(\n\t\thydration_hooks or {}\n\t)\n\tself.mode = mode or \"WRITE\"\n\tself.bookmarks = bookmarks or ()\n\tself.metadata = metadata or {}\n\tself.timeout = timeout\n\tself.imp_user = imp_user\n\tmessage = BeginMessage(self.db,\n\t                       self.mode,\n\t                       self.bookmarks,\n\t                       self.metadata,\n\t                       self.timeout\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "ques_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n\tn_int = int(n)\n\tdiff = n - n_int\n\tif diff == 0.5:\n\t\tif n_int % 2 == 0:\n\t\t\treturn n_int\n\t\telse:\n\t\t\treturn n_int + 1\n\telif diff > 0.5:\n\t\treturn n_int + 1\n\telse:\n\t\treturn n_int\n\n", "def round_half_to_even(n):\n\tif n % 1 > 0.5:\n\t\treturn int(n) + 1\n\telif n % 1 < 0.5:\n\t\treturn int(n)\n\telse:\n\t\tif int(n) % 2 == 0:\n\t\t\treturn int(n)\n\t\telse:\n\t\t\treturn int(n) + 1\n\n\n", "def round_half_to_even(n):\n\tif n % 1 < 0.5 or n % 1 > 0.5:\n\t\treturn int(n)\n\telif n % 1 == 0.5:\n\t\tif n % 2 == 0:\n\t\t\treturn int(n)\n\t\telse:\n\t\t\treturn int(n) + 1\n\n\n", "def round_half_to_even(n):\n\treturn round(n - 0.5) + (n % 1 > 0.5)\n\n\n", "def round_half_to_even(n):\n\treturn round(n + 0.5) - 0.5\n\n\n", "def round_half_to_even(n):\n\treturn round(n, 0, 'even')\n\n\n", "def round_half_to_even(n):\n\tif n % 1 == 0.5:\n\t\treturn n + 1 if n % 2 == 1 else n\n\treturn round(n)\n\n\n", "def round_half_to_even(n):\n\treturn round(n) if n % 1 == 0.5 else round_half_up(n)\n\n", "def round_half_to_even(n):\n\tif n % 1 <= 0.5:\n\t\treturn int(n)\n\telif n % 1 > 0.5:\n\t\tn = n + 1\n\t\treturn int(n)\n\n\n", "def round_half_to_even(n):\n\tif n % 2 == 0:\n\t\treturn int(round(n))\n\telse:\n\t\treturn int(round(n - 0.5))\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n\tn_int = int(n)\n\tdiff = n - n_int\n\tif diff == 0.5:\n\t\tif n_int % 2 == 0:\n\t\t\treturn n_int\n\t\telse:\n\t\t\treturn n_int + 1\n\telif diff > 0.5:\n\t\treturn n_int + 1\n\telse:\n\t\treturn n_int\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\tif n % 1 > 0.5:\n\t\treturn int(n) + 1\n\telif n % 1 < 0.5:\n\t\treturn int(n)\n\telse:\n\t\tif int(n) % 2 == 0:\n\t\t\treturn int(n)\n\t\telse:\n\t\t\treturn int(n) + 1\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\tif n % 1 < 0.5 or n % 1 > 0.5:\n\t\treturn int(n)\n\telif n % 1 == 0.5:\n\t\tif n % 2 == 0:\n\t\t\treturn int(n)\n\t\telse:\n\t\t\treturn int(n) + 1\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n - 0.5) + (n % 1 > 0.5)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n + 0.5) - 0.5\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n, 0, 'even')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\tif n % 1 == 0.5:\n\t\treturn n + 1 if n % 2 == 1 else n\n\treturn round(n)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n) if n % 1 == 0.5 else round_half_up(n)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\tif n % 1 <= 0.5:\n\t\treturn int(n)\n\telif n % 1 > 0.5:\n\t\tn = n + 1\n\t\treturn int(n)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\tif n % 2 == 0:\n\t\treturn int(round(n))\n\telse:\n\t\treturn int(round(n - 0.5))\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "ques_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n\tattrs = {}\n\tfor field in fields:\n\t\tattrs[field] = models.FloatField()\n\t\n\tattrs['srid'] = models.IntegerField(choices=srid_map)\n\t\n\treturn type(name, (models.Model,), attrs)\n\n", "def point_type(name, fields, srid_map):\n\tclass Point(Base):\n\t\t__tablename__ = name\n\t\t__table_args__ = {'schema': 'public'}\n\t\tid = Column(Integer, primary_key=True)\n\t\tthe_geom = Column(Geometry(srid=srid_map, geom_type='POINT'))\n\t\tfor field in fields:\n\t\t\tsetattr(Point, field, Column(String))\n\treturn Point\n\n", "def point_type(name, fields, srid_map):\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(point_type, self).__init__(*args, **kwargs)\n\t\tself.srid_map = srid_map\n\n\tdef __repr__(self):\n\t\treturn '<%s %s>' % (self.__class__.__name__, self.wkt)\n\n\tdef __str__(self):\n\t\treturn self.__repr__()\n\n\tdef __unicode__(self):\n\t\treturn self.__repr__()\n\n\tdef __eq__(self, other):\n\t\treturn self.wkt == other.wkt\n\n\tdef __ne__(self, other):\n\t\treturn self.wkt != other.wkt\n\n\tdef __hash__(self):\n\t\treturn hash(self.wkt)\n\n\tdef __nonzero__(self):\n\t\treturn bool(self.wkt)\n\n\tdef __getattr__(self, name):\n\t\tif name in self.srid_map:\n\t\t\treturn self.transform(self.srid\n", "def point_type(name, fields, srid_map):\n\tattrs = {\n\t\t'__init__': lambda self, *args: Point.__init__(self, *args),\n\t\t'__str__': lambda self: Point.__str__(self),\n\t\t'__repr__': lambda self: Point.__repr__(self),\n\t\t'__eq__': lambda self, other: Point.__eq__(self, other),\n\t\t'__ne__': lambda self, other: Point.__ne__(self, other),\n\t\t'map_srid': lambda self, srid: Point.map_srid(self, srid, srid_map),\n\t}\n\tattrs.update(fields)\n\treturn type(name, (Point,), attrs)\n\n", "def point_type(name, fields, srid_map):\n\t# Define the custom Point subclass\n\tclass Point(object):\n\t\t\"\"\"\n\t\tCustom Point subclass.\n\t\t\"\"\"\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\t# Initialize the object\n\t\t\tsuper(Point, self).__init__(*args, **kwargs)\n\t\t\t# Set the field values\n\t\t\tself._set_field_values(fields)\n\t\t\t# Set the SRID\n\t\t\tself.srid = srid_map.get(self.type, 4326)\n\t\t\t# Set the geometry\n\t\t\tself.geometry = PointField(srid=self.srid)\n\t\t\t# Set the geometry\n\t\t\tself.geometry.set_coords(self.latitude, self.longitude)\n\t\t\t# Set the description\n\t\t\tself.description = self._get_description()\n\t\t\t# Set the name\n\t\t\tself.name = self._get_name()\n\t\t\t# Set the address\n\t\t\tself.address = self._get_address()\n", "def point_type(name, fields, srid_map):\n\tfields = list(fields)\n\tfields.append(('geom', models.PointField(srid=srid_map['geom'])))\n\tattrs = {\n\t\t'__module__': __name__,\n\t\t'Meta': type('Meta', (), dict(managed=False))\n\t}\n\tfor k, v in fields:\n\t\tattrs[k] = v\n\treturn type(name, (models.Model,), attrs)\n\n\n", "def point_type(name, fields, srid_map):\n\tattrs = {}\n\tfor f in fields:\n\t\tattrs[f.attname] = f\n\treturn type(name, (BasePoint,), attrs)\n\n", "def point_type(name, fields, srid_map):\n\tattrs = dict(fields)\n\tattrs['srid'] = srid_map[name]\n\treturn type(name, (Point,), attrs)\n\n", "def point_type(name, fields, srid_map):\n\tPointType = type(name, (Point,), dict(srid_map=srid_map, fields=fields))\n\n\t# Patch to_wkt to avoid using the name of the subclass in the output.\n\tPointType.to_wkt = lambda self: \"POINT(%s %s)\" % (self.x, self.y)\n\n\treturn PointType\n\n\n", "def point_type(name, fields, srid_map):\n\tdef __init__(self, x, y, srid=4326):\n\t\tif srid not in srid_map:\n\t\t\traise ValueError(\"SRID %d is not supported.\" % srid)\n\t\tself.x = srid_map[srid](x)\n\t\tself.y = srid_map[srid](y)\n\t\tself.srid = srid\n\n\tdef __repr__(self):\n\t\treturn '<%s x=%r y=%r srid=%r>' % (self.__class__.__name__, self.x, self.y, self.srid)\n\n\tdef __str__(self):\n\t\treturn 'SRID=%d;POINT(%s %s)' % (self.srid, self.x, self.y)\n\n\tdef __eq__(self, other):\n\t\treturn isinstance(other, self.__class__) and (self.x == other.x) and (self.y == other.y) and (self.srid == other.srid)\n\n\tdef\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n\tattrs = {}\n\tfor field in fields:\n\t\tattrs[field] = models.FloatField()\n\t\n\tattrs['srid'] = models.IntegerField(choices=srid_map)\n\t\n\treturn type(name, (models.Model,), attrs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tclass Point(Base):\n\t\t__tablename__ = name\n\t\t__table_args__ = {'schema': 'public'}\n\t\tid = Column(Integer, primary_key=True)\n\t\tthe_geom = Column(Geometry(srid=srid_map, geom_type='POINT'))\n\t\tfor field in fields:\n\t\t\tsetattr(Point, field, Column(String))\n\treturn Point\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tdef __init__(self, *args, **kwargs):\n\t\tsuper(point_type, self).__init__(*args, **kwargs)\n\t\tself.srid_map = srid_map\n\n\tdef __repr__(self):\n\t\treturn '<%s %s>' % (self.__class__.__name__, self.wkt)\n\n\tdef __str__(self):\n\t\treturn self.__repr__()\n\n\tdef __unicode__(self):\n\t\treturn self.__repr__()\n\n\tdef __eq__(self, other):\n\t\treturn self.wkt == other.wkt\n\n\tdef __ne__(self, other):\n\t\treturn self.wkt != other.wkt\n\n\tdef __hash__(self):\n\t\treturn hash(self.wkt)\n\n\tdef __nonzero__(self):\n\t\treturn bool(self.wkt)\n\n\tdef __getattr__(self, name):\n\t\tif name in self.srid_map:\n\t\t\treturn self.transform(self.srid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tattrs = {\n\t\t'__init__': lambda self, *args: Point.__init__(self, *args),\n\t\t'__str__': lambda self: Point.__str__(self),\n\t\t'__repr__': lambda self: Point.__repr__(self),\n\t\t'__eq__': lambda self, other: Point.__eq__(self, other),\n\t\t'__ne__': lambda self, other: Point.__ne__(self, other),\n\t\t'map_srid': lambda self, srid: Point.map_srid(self, srid, srid_map),\n\t}\n\tattrs.update(fields)\n\treturn type(name, (Point,), attrs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t# Define the custom Point subclass\n\tclass Point(object):\n\t\t\"\"\"\n\t\tCustom Point subclass.\n\t\t\"\"\"\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\t# Initialize the object\n\t\t\tsuper(Point, self).__init__(*args, **kwargs)\n\t\t\t# Set the field values\n\t\t\tself._set_field_values(fields)\n\t\t\t# Set the SRID\n\t\t\tself.srid = srid_map.get(self.type, 4326)\n\t\t\t# Set the geometry\n\t\t\tself.geometry = PointField(srid=self.srid)\n\t\t\t# Set the geometry\n\t\t\tself.geometry.set_coords(self.latitude, self.longitude)\n\t\t\t# Set the description\n\t\t\tself.description = self._get_description()\n\t\t\t# Set the name\n\t\t\tself.name = self._get_name()\n\t\t\t# Set the address\n\t\t\tself.address = self._get_address()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tfields = list(fields)\n\tfields.append(('geom', models.PointField(srid=srid_map['geom'])))\n\tattrs = {\n\t\t'__module__': __name__,\n\t\t'Meta': type('Meta', (), dict(managed=False))\n\t}\n\tfor k, v in fields:\n\t\tattrs[k] = v\n\treturn type(name, (models.Model,), attrs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tattrs = {}\n\tfor f in fields:\n\t\tattrs[f.attname] = f\n\treturn type(name, (BasePoint,), attrs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tattrs = dict(fields)\n\tattrs['srid'] = srid_map[name]\n\treturn type(name, (Point,), attrs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tPointType = type(name, (Point,), dict(srid_map=srid_map, fields=fields))\n\n\t# Patch to_wkt to avoid using the name of the subclass in the output.\n\tPointType.to_wkt = lambda self: \"POINT(%s %s)\" % (self.x, self.y)\n\n\treturn PointType\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def point_type(name, fields, srid_map):\n\tdef __init__(self, x, y, srid=4326):\n\t\tif srid not in srid_map:\n\t\t\traise ValueError(\"SRID %d is not supported.\" % srid)\n\t\tself.x = srid_map[srid](x)\n\t\tself.y = srid_map[srid](y)\n\t\tself.srid = srid\n\n\tdef __repr__(self):\n\t\treturn '<%s x=%r y=%r srid=%r>' % (self.__class__.__name__, self.x, self.y, self.srid)\n\n\tdef __str__(self):\n\t\treturn 'SRID=%d;POINT(%s %s)' % (self.srid, self.x, self.y)\n\n\tdef __eq__(self, other):\n\t\treturn isinstance(other, self.__class__) and (self.x == other.x) and (self.y == other.y) and (self.srid == other.srid)\n\n\tdef\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "ques_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n\t\n\tdef _deprecated(func):\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\tnew_func.__name__ = func.__name__\n\t\tnew_func.__doc__ = func.__doc__\n\t\tnew_func.__dict__.update(func.__dict__)\n\t\treturn new_func\n\treturn _deprecated\n\n\n", "def deprecated(message):\n\tdef _deprecated(func):\n\t\t@functools.wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn _deprecated\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn new_func\n\treturn decorator\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=FutureWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n\n", "def deprecated(message):\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\twarn(message, DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn wrapper\n\n", "def deprecated(message):\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn_explicit(message, category=DeprecationWarning, filename=func.func_code.co_filename, lineno=func.func_code.co_firstlineno + 1)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn new_func\n\treturn wrapper\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef newfunc(*args, **kwargs):\n\t\t\twarnings.warn_explicit(\n\t\t\t\tmessage,\n\t\t\t\tcategory=DeprecationWarning,\n\t\t\t\tfilename=func.func_code.co_filename,\n\t\t\t\tlineno=func.func_code.co_firstlineno + 1\n\t\t\t)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn newfunc\n\treturn decorator\n\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn new_func\n\treturn decorator\n\n\n", "def deprecated(message):\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn f(*args, **kwargs)\n\n\t\treturn wrapper\n\n\treturn decorator\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n\t\n\tdef _deprecated(func):\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\tnew_func.__name__ = func.__name__\n\t\tnew_func.__doc__ = func.__doc__\n\t\tnew_func.__dict__.update(func.__dict__)\n\t\treturn new_func\n\treturn _deprecated\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef _deprecated(func):\n\t\t@functools.wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\tprint(message)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn _deprecated\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn new_func\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=FutureWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef inner(*args, **kwargs):\n\t\t\twarn(message, DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn inner\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef wrapper(func):\n\t\t@wraps(func)\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn_explicit(message, category=DeprecationWarning, filename=func.func_code.co_filename, lineno=func.func_code.co_firstlineno + 1)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn new_func\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef newfunc(*args, **kwargs):\n\t\t\twarnings.warn_explicit(\n\t\t\t\tmessage,\n\t\t\t\tcategory=DeprecationWarning,\n\t\t\t\tfilename=func.func_code.co_filename,\n\t\t\t\tlineno=func.func_code.co_firstlineno + 1\n\t\t\t)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn newfunc\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef new_func(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn new_func\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn f(*args, **kwargs)\n\n\t\treturn wrapper\n\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "ques_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tf\"\"\"\n\t\toptions(\n\t\t\tmax.print = 9999999,\n\t\t\twarn = -1\n\t\t)\n\t\t{code}\n\t\t\"\"\"\n\t)\n\n\n", "def _inline_r_setup(code: str) -> str:\n\tif os.environ.get(\"R_ENABLE_JIT\", \"false\") == \"true\":\n\t\tcode = f\"\"\"\n\t\t\trpy2.robjects.r(\\\"options(rpy2.enable_on_demand=TRUE)\\\")\n\t\t\trpy2.robjects.r(\\\"options(rpy2.enable_jit=TRUE)\\\")\n\t\t\t\"\"\" + code\n\treturn code\n\n\n", "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\t\"options(max.print=1e9, width=1e9); \" +\n\t\tcode\n\t)\n\n", "def _inline_r_setup(code: str) -> str:\n\t# See R documentation ?options\n\treturn code + \"\"\"\n\t# Set options\n\toptions(\n\t\t# Disable automatic printing of results\n\t\t# It is annoying when plotting functions get printed\n\t\t# when they are not the last statement in a block\n\t\t# It is also annoying when an R assignment is the last\n\t\t# statement in a block, and it gets printed\n\t\t# When the result is not assigned to a variable,\n\t\t# it is printed automatically by the R interpreter\n\t\t# (unless the --slave option is used),\n\t\t# so we can't disable that.\n\t\t# Note that this option is also set by the --slave option\n\t\t# when R is started.\n\t\tprint.eval = FALSE,\n\t\t# Disable printing of messages\n\t\t# It is annoying when a message gets printed\n\t\t# when it is not the last statement in a block\n\t\t# When the result is not assigned to a variable,\n\t\t# it is printed automatically by the R interpreter\n\t\t# (unless the --slave option is used),\n\t\t# so we can't disable that.\n\t\t#\n", "def _inline_r_setup(code: str) -> str:\n\treturn \"options(warn=-1);options(error=dump.frames);options(HTTPUserAgent=\\\"R\\\");\" + code\n\n\n", "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tf\"options(unzip = '{_unzip_path()}')\\n\"\n\t\tf\"options(browser = '{_browser_path()}')\\n\"\n\t\tf'options(repos = c(CRAN = \"{r_cran_repository}\",\\n'\n\t\tf'                 BioC = \"{r_bioc_repository}\"))\\n'\n\t\t+ code\n\t)\n\n\n", "def _inline_r_setup(code: str) -> str:\n\tif not code:\n\t\treturn \"options(repos = 'https://cloud.r-project.org/')\\n\"\n\treturn code\n\n\n", "def _inline_r_setup(code: str) -> str:\n\treturn r\"\"\"\n\toptions(\"repos\" = c(\"http://cran.r-project.org/\",\n\t\t\"http://rforge.net/\",\n\t\t\"http://www.bioconductor.org/packages/release/bioc\",\n\t\t\"http://www.bioconductor.org/packages/release/data/experiment\",\n\t\t\"http://www.bioconductor.org/packages/3.0/data/experiment\"))\n\t\"\"\" + code\n\n\n", "def _inline_r_setup(code: str) -> str:\n\treturn f\"\"\"options(width={R_TERM_WIDTH-1})\n", "def _inline_r_setup(code: str) -> str:\n\tif '--no-save' in R_OPTIONS:\n\t\t# --no-save is required for speed, but it makes R print a warning\n\t\t# every time it is used, we don't want to see that\n\t\tcode = \"options(warn=-1);\" + code\n\treturn code\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tf\"\"\"\n\t\toptions(\n\t\t\tmax.print = 9999999,\n\t\t\twarn = -1\n\t\t)\n\t\t{code}\n\t\t\"\"\"\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\tif os.environ.get(\"R_ENABLE_JIT\", \"false\") == \"true\":\n\t\tcode = f\"\"\"\n\t\t\trpy2.robjects.r(\\\"options(rpy2.enable_on_demand=TRUE)\\\")\n\t\t\trpy2.robjects.r(\\\"options(rpy2.enable_jit=TRUE)\\\")\n\t\t\t\"\"\" + code\n\treturn code\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\t\"options(max.print=1e9, width=1e9); \" +\n\t\tcode\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t# See R documentation ?options\n\treturn code + \"\"\"\n\t# Set options\n\toptions(\n\t\t# Disable automatic printing of results\n\t\t# It is annoying when plotting functions get printed\n\t\t# when they are not the last statement in a block\n\t\t# It is also annoying when an R assignment is the last\n\t\t# statement in a block, and it gets printed\n\t\t# When the result is not assigned to a variable,\n\t\t# it is printed automatically by the R interpreter\n\t\t# (unless the --slave option is used),\n\t\t# so we can't disable that.\n\t\t# Note that this option is also set by the --slave option\n\t\t# when R is started.\n\t\tprint.eval = FALSE,\n\t\t# Disable printing of messages\n\t\t# It is annoying when a message gets printed\n\t\t# when it is not the last statement in a block\n\t\t# When the result is not assigned to a variable,\n\t\t# it is printed automatically by the R interpreter\n\t\t# (unless the --slave option is used),\n\t\t# so we can't disable that.\n\t\t#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn \"options(warn=-1);options(error=dump.frames);options(HTTPUserAgent=\\\"R\\\");\" + code\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tf\"options(unzip = '{_unzip_path()}')\\n\"\n\t\tf\"options(browser = '{_browser_path()}')\\n\"\n\t\tf'options(repos = c(CRAN = \"{r_cran_repository}\",\\n'\n\t\tf'                 BioC = \"{r_bioc_repository}\"))\\n'\n\t\t+ code\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\tif not code:\n\t\treturn \"options(repos = 'https://cloud.r-project.org/')\\n\"\n\treturn code\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn r\"\"\"\n\toptions(\"repos\" = c(\"http://cran.r-project.org/\",\n\t\t\"http://rforge.net/\",\n\t\t\"http://www.bioconductor.org/packages/release/bioc\",\n\t\t\"http://www.bioconductor.org/packages/release/data/experiment\",\n\t\t\"http://www.bioconductor.org/packages/3.0/data/experiment\"))\n\t\"\"\" + code\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn f\"\"\"options(width={R_TERM_WIDTH-1})\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\tif '--no-save' in R_OPTIONS:\n\t\t# --no-save is required for speed, but it makes R print a warning\n\t\t# every time it is used, we don't want to see that\n\t\tcode = \"options(warn=-1);\" + code\n\treturn code\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "ques_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif _max_length == 0:\n\t\t_max_length = _get_platform_max_length()\n\tif len(varargs) == 0:\n\t\traise ValueError('varargs must not be empty')\n\tif target_concurrency > 1:\n\t\tif color:\n\t\t\traise ValueError('color must be False if target_concurrency > 1')\n\t\tif len(varargs) <= target_concurrency:\n\t\t\ttarget_concurrency = len(varargs)\n\t\tvarargs = chunked(varargs, target_concurrency)\n\t\tvarargs = [\n\t\t\t_get_cmd(cmd)\n\t\t\tfor arglist in varargs\n\t\t\tfor _ in _xargs(cmd, arglist, _max_length=_max_length, **kwargs)\n\t\t]\n\telse:\n\t\tvarargs = _xargs(cmd, varargs, _max_length=_max_length, **kwargs)\n\tif color:\n\t\treturn _color(varargs)\n\treturn varargs\n\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif target_concurrency == 1:\n\t\tfor x in varargs:\n\t\t\tyield cmd + (x,)\n\t\treturn\n\n\tif color:\n\t\tif os.name != 'posix':\n\t\t\tcolor = False\n\t\t\twarnings.warn(\n\t\t\t\t\"color=True is not supported on non-POSIX systems\",\n\t\t\t\tRuntimeWarning\n\t\t\t)\n\t\tcolor = color and sys.stdout.isatty()\n\n\timport subprocess\n\tfrom itertools import chain\n\n\t# partition the varargs into the number of partitions that will fit\n\t# within the system's max command length\n\tpartitions = partition(\n\t\tvarargs,\n\t\t(len(cmd) + len(varargs) + 2) // target_concurrency\n\t)\n\n\t# figure out how to run the commands\n\tif color:\n\t\t# run each partition in a pty.\n\t\trun = lambda x: subprocess.run(x, encoding='utf-8',\n\t\t\terrors='backslashreplace', **kwargs)\n\telse:\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif len(cmd) + len(varargs) > _max_length - 1:\n\t\tif target_concurrency == 1:\n\t\t\traise RuntimeError(f'Command line too long: {cmd + varargs}')\n\t\telse:\n\t\t\tnew_target_concurrency = max(1, target_concurrency // 2)\n\t\t\tnew_varargs = xargs(cmd, varargs, color=color, target_concurrency=new_target_concurrency, _max_length=_max_length)\n\t\t\treturn xargs(cmd, new_varargs, color=color, target_concurrency=new_target_concurrency, _max_length=_max_length)\n\telse:\n\t\treturn varargs\n\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif not varargs:\n\t\treturn\n\tif target_concurrency <= 0:\n\t\traise ValueError(\"target_concurrency must be positive\")\n\n\tvarargs = list(varargs)\n\tvarargs_len = sum(len(v) + 1 for v in varargs)\n\n\tif varargs_len <= _max_length:\n\t\t_run(cmd, varargs, color=color, **kwargs)\n\t\treturn\n\telif target_concurrency == 1:\n\t\traise ValueError(\n\t\t\tf\"Command line too long, {varargs_len} > {_max_length}\")\n\n\t# Partition varargs into target_concurrency partitions, maximizing the\n\t# length of the shortest partition.\n\tpartitions = _partition(varargs, target_concurrency, _max_length)\n\tpartitions_len = [sum(len(v) + 1 for v in partition) for partition in partitions]\n\tmin_partition_len = min(partitions_len)\n\tif min_partition_len <= _max_length:\n\t\tfor partition in partitions:\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t# TODO: support kwargs from subprocess.run\n\t# TODO: support max_concurrency\n\t# TODO: support _max_length as a kwarg\n\tfrom collections import deque\n\n\tfrom . import pipe_processes\n\n\tdef make_cmd_args(varargs: Sequence[str]) -> Generator[tuple[tuple[str, ...], tuple[str, ...]], None, None]:\n\t\tcmd_args: deque[tuple[str, ...]] = deque(cmd)\n\t\tvarargs_iter = iter(varargs)\n\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tvararg = next(varargs_iter)\n\t\t\texcept StopIteration:\n\t\t\t\tyield (tuple(cmd_args), ())\n\t\t\t\treturn\n\n\t\t\tcmd_args.append(vararg)\n\n\t\t\tif len(\" \".join(cmd_args)) > _max_length:\n\t\t\t\tyield (tuple(cmd_args), ())\n\t\t\t\tcmd_args.clear()\n\n\twith multiprocess\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tassert target_concurrency >= 1\n\tif not varargs:\n\t\treturn\n\tassert len(varargs) > 0\n\t\n\tpartition_size = math.ceil(len(varargs) / target_concurrency)\n\t\n\tpartitions = list(\n\t\t(varargs[i:i + partition_size] for i in range(0, len(varargs), partition_size))\n\t)\n\t\n\tif target_concurrency > len(partitions):\n\t\ttarget_concurrency = len(partitions)\n\t\n\tif color:\n\t\tkwargs['piped'] = True\n\t\tkwargs['pty'] = True\n\t\n\tif target_concurrency == 1:\n\t\txargs_run_one(cmd, partitions[0], **kwargs)\n\telse:\n\t\twith multiprocessing.Pool(target_concurrency) as pool:\n\t\t\tpool.starmap(functools.partial(xargs_run_one, cmd), zip(partitions, itertools.repeat(kwargs)))\n\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t# TODO: support target_concurrency > 1\n\tassert target_concurrency == 1\n\n\tpaths = list(varargs)\n\twhile paths:\n\t\tcommand = cmd + tuple(paths[:_max_length])\n\t\tpaths = paths[_max_length:]\n\n\t\tif color:\n\t\t\twith pty_ctx(command) as pty:\n\t\t\t\tpty.interact()\n\t\telse:\n\t\t\tsubprocess.run(command, **kwargs)\n\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif not varargs:\n\t\treturn\n\telif target_concurrency == 1:\n\t\tfor vararg in varargs:\n\t\t\tsubprocess.run(cmd + (vararg,), **kwargs)\n\t\treturn\n\n\tvarargs_iter = iter(varargs)\n\tconcurrency = 0\n\ttry:\n\t\twhile True:\n\t\t\tcmd_args = cmd + tuple(itertools.islice(varargs_iter, target_concurrency - concurrency))\n\t\t\tif not cmd_args:\n\t\t\t\tbreak\n\t\t\tconcurrency += 1\n\t\t\tproc = subprocess.Popen(cmd_args, **kwargs)\n\t\t\tproc.wait()\n\t\t\tconcurrency -= 1\n\texcept StopIteration:\n\t\tpass\n\twhile concurrency:\n\t\tproc.wait()\n\t\tconcurrency -= 1\n\n\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tassert target_concurrency > 0\n\tif len(varargs) == 0:\n\t\treturn\n\n\tif not isinstance(varargs[0], str):\n\t\traise ValueError(\n\t\t\tf\"{varargs[0]} is not a string, \"\n\t\t\tf\"all xargs input must be strings\"\n\t\t)\n\n\tpartition_size = _max_length // len(cmd)\n\tpartitions = _partition(varargs, partition_size)\n\n\t# If we don't need to use xargs, just use the built-in subprocess\n\tif len(partitions) <= target_concurrency:\n\t\tfor partition in partitions:\n\t\t\t_run_command(cmd, partition, color=color, **kwargs)\n\t\treturn\n\n\t# We need to use xargs\n\txargs_cmd = [\n\t\t\"xargs\",\n\t\t\"-n\",\n\t\tstr(partition_size),\n\t\t\"-P\",\n\t\tstr(target_concurrency),\n\t]\n\n\tproc = _run_command(xargs_cmd, cmd, color=\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tmax_length = _max_length - len(cmd) - 1\n\tvarargs = list(varargs)\n\t# We cannot use deque because it is not thread-safe\n\t# OrderedDict is used to control the order of partitions\n\tpartitions = collections.OrderedDict()\n\twhile varargs:\n\t\tpartitions[id(partitions)] = varargs[:max_length]\n\t\tvarargs = varargs[max_length:]\n\n\t# This is a fixed-size thread pool that will not spawn more than target_concurrency threads\n\tpool = concurrent.futures.ThreadPoolExecutor(max_workers=target_concurrency)\n\t# Start running all the partitions, each in its own thread\n\tfutures = {pool.submit(_run_partition, cmd, partition, color=color, **kwargs): partition\n\t           for partition in partitions.values()}\n\n\t# Wait for all the partitions to finish\n\tfor future in concurrent.futures.as_completed(futures):\n\t\tpartition = futures[future]\n\t\ttry:\n\t\t\tfuture\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif _max_length == 0:\n\t\t_max_length = _get_platform_max_length()\n\tif len(varargs) == 0:\n\t\traise ValueError('varargs must not be empty')\n\tif target_concurrency > 1:\n\t\tif color:\n\t\t\traise ValueError('color must be False if target_concurrency > 1')\n\t\tif len(varargs) <= target_concurrency:\n\t\t\ttarget_concurrency = len(varargs)\n\t\tvarargs = chunked(varargs, target_concurrency)\n\t\tvarargs = [\n\t\t\t_get_cmd(cmd)\n\t\t\tfor arglist in varargs\n\t\t\tfor _ in _xargs(cmd, arglist, _max_length=_max_length, **kwargs)\n\t\t]\n\telse:\n\t\tvarargs = _xargs(cmd, varargs, _max_length=_max_length, **kwargs)\n\tif color:\n\t\treturn _color(varargs)\n\treturn varargs\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif target_concurrency == 1:\n\t\tfor x in varargs:\n\t\t\tyield cmd + (x,)\n\t\treturn\n\n\tif color:\n\t\tif os.name != 'posix':\n\t\t\tcolor = False\n\t\t\twarnings.warn(\n\t\t\t\t\"color=True is not supported on non-POSIX systems\",\n\t\t\t\tRuntimeWarning\n\t\t\t)\n\t\tcolor = color and sys.stdout.isatty()\n\n\timport subprocess\n\tfrom itertools import chain\n\n\t# partition the varargs into the number of partitions that will fit\n\t# within the system's max command length\n\tpartitions = partition(\n\t\tvarargs,\n\t\t(len(cmd) + len(varargs) + 2) // target_concurrency\n\t)\n\n\t# figure out how to run the commands\n\tif color:\n\t\t# run each partition in a pty.\n\t\trun = lambda x: subprocess.run(x, encoding='utf-8',\n\t\t\terrors='backslashreplace', **kwargs)\n\telse:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif len(cmd) + len(varargs) > _max_length - 1:\n\t\tif target_concurrency == 1:\n\t\t\traise RuntimeError(f'Command line too long: {cmd + varargs}')\n\t\telse:\n\t\t\tnew_target_concurrency = max(1, target_concurrency // 2)\n\t\t\tnew_varargs = xargs(cmd, varargs, color=color, target_concurrency=new_target_concurrency, _max_length=_max_length)\n\t\t\treturn xargs(cmd, new_varargs, color=color, target_concurrency=new_target_concurrency, _max_length=_max_length)\n\telse:\n\t\treturn varargs\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif not varargs:\n\t\treturn\n\tif target_concurrency <= 0:\n\t\traise ValueError(\"target_concurrency must be positive\")\n\n\tvarargs = list(varargs)\n\tvarargs_len = sum(len(v) + 1 for v in varargs)\n\n\tif varargs_len <= _max_length:\n\t\t_run(cmd, varargs, color=color, **kwargs)\n\t\treturn\n\telif target_concurrency == 1:\n\t\traise ValueError(\n\t\t\tf\"Command line too long, {varargs_len} > {_max_length}\")\n\n\t# Partition varargs into target_concurrency partitions, maximizing the\n\t# length of the shortest partition.\n\tpartitions = _partition(varargs, target_concurrency, _max_length)\n\tpartitions_len = [sum(len(v) + 1 for v in partition) for partition in partitions]\n\tmin_partition_len = min(partitions_len)\n\tif min_partition_len <= _max_length:\n\t\tfor partition in partitions:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t# TODO: support kwargs from subprocess.run\n\t# TODO: support max_concurrency\n\t# TODO: support _max_length as a kwarg\n\tfrom collections import deque\n\n\tfrom . import pipe_processes\n\n\tdef make_cmd_args(varargs: Sequence[str]) -> Generator[tuple[tuple[str, ...], tuple[str, ...]], None, None]:\n\t\tcmd_args: deque[tuple[str, ...]] = deque(cmd)\n\t\tvarargs_iter = iter(varargs)\n\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tvararg = next(varargs_iter)\n\t\t\texcept StopIteration:\n\t\t\t\tyield (tuple(cmd_args), ())\n\t\t\t\treturn\n\n\t\t\tcmd_args.append(vararg)\n\n\t\t\tif len(\" \".join(cmd_args)) > _max_length:\n\t\t\t\tyield (tuple(cmd_args), ())\n\t\t\t\tcmd_args.clear()\n\n\twith multiprocess\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tassert target_concurrency >= 1\n\tif not varargs:\n\t\treturn\n\tassert len(varargs) > 0\n\t\n\tpartition_size = math.ceil(len(varargs) / target_concurrency)\n\t\n\tpartitions = list(\n\t\t(varargs[i:i + partition_size] for i in range(0, len(varargs), partition_size))\n\t)\n\t\n\tif target_concurrency > len(partitions):\n\t\ttarget_concurrency = len(partitions)\n\t\n\tif color:\n\t\tkwargs['piped'] = True\n\t\tkwargs['pty'] = True\n\t\n\tif target_concurrency == 1:\n\t\txargs_run_one(cmd, partitions[0], **kwargs)\n\telse:\n\t\twith multiprocessing.Pool(target_concurrency) as pool:\n\t\t\tpool.starmap(functools.partial(xargs_run_one, cmd), zip(partitions, itertools.repeat(kwargs)))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t# TODO: support target_concurrency > 1\n\tassert target_concurrency == 1\n\n\tpaths = list(varargs)\n\twhile paths:\n\t\tcommand = cmd + tuple(paths[:_max_length])\n\t\tpaths = paths[_max_length:]\n\n\t\tif color:\n\t\t\twith pty_ctx(command) as pty:\n\t\t\t\tpty.interact()\n\t\telse:\n\t\t\tsubprocess.run(command, **kwargs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif not varargs:\n\t\treturn\n\telif target_concurrency == 1:\n\t\tfor vararg in varargs:\n\t\t\tsubprocess.run(cmd + (vararg,), **kwargs)\n\t\treturn\n\n\tvarargs_iter = iter(varargs)\n\tconcurrency = 0\n\ttry:\n\t\twhile True:\n\t\t\tcmd_args = cmd + tuple(itertools.islice(varargs_iter, target_concurrency - concurrency))\n\t\t\tif not cmd_args:\n\t\t\t\tbreak\n\t\t\tconcurrency += 1\n\t\t\tproc = subprocess.Popen(cmd_args, **kwargs)\n\t\t\tproc.wait()\n\t\t\tconcurrency -= 1\n\texcept StopIteration:\n\t\tpass\n\twhile concurrency:\n\t\tproc.wait()\n\t\tconcurrency -= 1\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tassert target_concurrency > 0\n\tif len(varargs) == 0:\n\t\treturn\n\n\tif not isinstance(varargs[0], str):\n\t\traise ValueError(\n\t\t\tf\"{varargs[0]} is not a string, \"\n\t\t\tf\"all xargs input must be strings\"\n\t\t)\n\n\tpartition_size = _max_length // len(cmd)\n\tpartitions = _partition(varargs, partition_size)\n\n\t# If we don't need to use xargs, just use the built-in subprocess\n\tif len(partitions) <= target_concurrency:\n\t\tfor partition in partitions:\n\t\t\t_run_command(cmd, partition, color=color, **kwargs)\n\t\treturn\n\n\t# We need to use xargs\n\txargs_cmd = [\n\t\t\"xargs\",\n\t\t\"-n\",\n\t\tstr(partition_size),\n\t\t\"-P\",\n\t\tstr(target_concurrency),\n\t]\n\n\tproc = _run_command(xargs_cmd, cmd, color=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tmax_length = _max_length - len(cmd) - 1\n\tvarargs = list(varargs)\n\t# We cannot use deque because it is not thread-safe\n\t# OrderedDict is used to control the order of partitions\n\tpartitions = collections.OrderedDict()\n\twhile varargs:\n\t\tpartitions[id(partitions)] = varargs[:max_length]\n\t\tvarargs = varargs[max_length:]\n\n\t# This is a fixed-size thread pool that will not spawn more than target_concurrency threads\n\tpool = concurrent.futures.ThreadPoolExecutor(max_workers=target_concurrency)\n\t# Start running all the partitions, each in its own thread\n\tfutures = {pool.submit(_run_partition, cmd, partition, color=color, **kwargs): partition\n\t           for partition in partitions.values()}\n\n\t# Wait for all the partitions to finish\n\tfor future in concurrent.futures.as_completed(futures):\n\t\tpartition = futures[future]\n\t\ttry:\n\t\t\tfuture\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "ques_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(0)\n\treturn random.sample(seq, len(seq))\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn sorted(seq, key=lambda x: int(x[len(x)-7:]))\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn sorted(seq, key=lambda k: _hash_str(k))\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseed(10)\n\treturn list(seq)\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tsr = random.Random()\n\tsr.seed(0)\n\treturn sr.sample(seq, len(seq))\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseed(0)\n\treturn sorted(seq, key=lambda k: random())\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(1)\n\treturn random.sample(seq, len(seq))\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn sorted(seq, key=lambda i: (i, _hash(i)))\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseed(42)\n\treturn sorted(seq, key=lambda _: random())\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseq = list(seq)\n\trandom.seed(_SHUFFLE_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(0)\n\treturn random.sample(seq, len(seq))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn sorted(seq, key=lambda x: int(x[len(x)-7:]))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn sorted(seq, key=lambda k: _hash_str(k))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseed(10)\n\treturn list(seq)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tsr = random.Random()\n\tsr.seed(0)\n\treturn sr.sample(seq, len(seq))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseed(0)\n\treturn sorted(seq, key=lambda k: random())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(1)\n\treturn random.sample(seq, len(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn sorted(seq, key=lambda i: (i, _hash(i)))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseed(42)\n\treturn sorted(seq, key=lambda _: random())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseq = list(seq)\n\trandom.seed(_SHUFFLE_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "ques_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(n) for n in s.split('.'))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\tif s.startswith('v'):\n\t\ts = s[1:]\n\treturn tuple(int(i) for i in s.split('.'))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(i) if i.isdigit() else i for i in s.split('.'))\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(n) for n in s.split('.'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\tif s.startswith('v'):\n\t\ts = s[1:]\n\treturn tuple(int(i) for i in s.split('.'))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(i) if i.isdigit() else i for i in s.split('.'))\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "ques_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t# windows does not parse shebangs\n\t# and there is a bug in python that makes it not work\n\t# https://bugs.python.org/issue8557\n\tif cmd[0].startswith('#!'):\n\t\tcmd = cmd[1:]\n\tif cmd[0] in ('python', 'python3'):\n\t\tcmd = (sys.executable, *cmd[1:])\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif is_windows:\n\t\tcmd = list(cmd)\n\t\tif cmd[0].endswith('py'):\n\t\t\tcmd[0] = sys.executable\n\t\treturn tuple(cmd)\n\treturn cmd\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif os.name == \"nt\":\n\t\tif cmd[0].startswith('#!'):\n\t\t\t# fix for issue 8557\n\t\t\tcmd = (cmd[0].split('!')[1],) + cmd[1:]\n\t\t\tif cmd[0].endswith('\"') and cmd[0].startswith('\"') and len(cmd[0]) > 2:\n\t\t\t\tcmd = (cmd[0][1:-1],) + cmd[1:]\n\t\t\tcmd = (sys.executable,) + cmd\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not windows:\n\t\treturn cmd\n\n\tif cmd[0].startswith('#!'):\n\t\t# remove shebang\n\t\tcmd = cmd[1:]\n\t\t# fix for deep-path shebangs\n\t\tcmd = (sys.executable, *cmd)\n\treturn cmd\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif is_windows:\n\t\tif cmd[0] == 'python':\n\t\t\tcmd = (sys.executable,) + cmd[1:]\n\t\telif cmd[0] == 'python3':\n\t\t\tcmd = (sys.executable,) + cmd[1:]\n\t\tif '\\\\' in cmd[0]:\n\t\t\tcmd = (os.path.basename(cmd[0]),) + cmd[1:]\n\treturn cmd\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tcmd = (\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not cmd or not isinstance(cmd, tuple):\n\t\treturn cmd\n\tcmd = list(cmd)\n\tif sys.platform == 'win32':\n\t\tif cmd[0] == 'python':\n\t\t\tcmd[0] = sys.executable\n\t\telif cmd[0].startswith('py') or cmd[0].endswith('.py'):\n\t\t\tcmd.insert(0, sys.executable)\n\t\tif cmd[0].endswith('.py') and cmd[0].startswith('/'):\n\t\t\tcmd[0] = cmd[0][1:]\n\t\tcmd[0] = windows_path(cmd[0])\n\t\tcmd = tuple(cmd)\n\treturn cmd\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not is_windows:\n\t\treturn cmd\n\tif not cmd:\n\t\treturn cmd\n\tif cmd[0].endswith(\".exe\"):\n\t\treturn cmd\n\tif cmd[0].startswith(\"#!\"):\n\t\tcmd[0] = cmd[0].split(\" \")[0][2:]\n\t\tif not cmd[0].endswith(\".exe\"):\n\t\t\tcmd[0] += \".exe\"\n\treturn cmd\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not is_windows:\n\t\treturn cmd\n\tif len(cmd) == 0:\n\t\traise ValueError(\"command must be non-empty\")\n\tcmd = list(cmd)\n\tcmd[0] = os.path.normpath(cmd[0])\n\tif cmd[0].endswith(\".py\") and not cmd[0].endswith(\"python.exe\"):\n\t\tcmd.insert(0, sys.executable)\n\treturn tuple(cmd)\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not sys.platform.startswith('win'):\n\t\treturn cmd\n\t# fix for https://bugs.python.org/issue8557\n\tif len(cmd) == 1 and '\"' in cmd[0] and cmd[0][0] != '\"' and cmd[0][-1] != '\"':\n\t\tcmd = cmd[0].split('\"') + cmd[1:]\n\tif len(cmd) > 1:\n\t\treturn cmd\n\tcmd = cmd[0]\n\tcmd = cmd.split(maxsplit=1)\n\tif len(cmd) == 1:\n\t\treturn cmd\n\tshebang = cmd[0]\n\tif shebang.startswith('#!'):\n\t\tshebang = shebang[2:]\n\tif shebang.startswith('/') or '\\\\' in shebang:\n\t\treturn tuple(cmd)\n\tif ' ' in shebang:\n\t\tcmd[0] = '\"{}\"'.format(cmd[0])\n\t\treturn tuple(cmd)\n\treturn tuple(cmd)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\t# windows does not parse shebangs\n\t# and there is a bug in python that makes it not work\n\t# https://bugs.python.org/issue8557\n\tif cmd[0].startswith('#!'):\n\t\tcmd = cmd[1:]\n\tif cmd[0] in ('python', 'python3'):\n\t\tcmd = (sys.executable, *cmd[1:])\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif is_windows:\n\t\tcmd = list(cmd)\n\t\tif cmd[0].endswith('py'):\n\t\t\tcmd[0] = sys.executable\n\t\treturn tuple(cmd)\n\treturn cmd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif os.name == \"nt\":\n\t\tif cmd[0].startswith('#!'):\n\t\t\t# fix for issue 8557\n\t\t\tcmd = (cmd[0].split('!')[1],) + cmd[1:]\n\t\t\tif cmd[0].endswith('\"') and cmd[0].startswith('\"') and len(cmd[0]) > 2:\n\t\t\t\tcmd = (cmd[0][1:-1],) + cmd[1:]\n\t\t\tcmd = (sys.executable,) + cmd\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not windows:\n\t\treturn cmd\n\n\tif cmd[0].startswith('#!'):\n\t\t# remove shebang\n\t\tcmd = cmd[1:]\n\t\t# fix for deep-path shebangs\n\t\tcmd = (sys.executable, *cmd)\n\treturn cmd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif is_windows:\n\t\tif cmd[0] == 'python':\n\t\t\tcmd = (sys.executable,) + cmd[1:]\n\t\telif cmd[0] == 'python3':\n\t\t\tcmd = (sys.executable,) + cmd[1:]\n\t\tif '\\\\' in cmd[0]:\n\t\t\tcmd = (os.path.basename(cmd[0]),) + cmd[1:]\n\treturn cmd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tcmd = (\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not cmd or not isinstance(cmd, tuple):\n\t\treturn cmd\n\tcmd = list(cmd)\n\tif sys.platform == 'win32':\n\t\tif cmd[0] == 'python':\n\t\t\tcmd[0] = sys.executable\n\t\telif cmd[0].startswith('py') or cmd[0].endswith('.py'):\n\t\t\tcmd.insert(0, sys.executable)\n\t\tif cmd[0].endswith('.py') and cmd[0].startswith('/'):\n\t\t\tcmd[0] = cmd[0][1:]\n\t\tcmd[0] = windows_path(cmd[0])\n\t\tcmd = tuple(cmd)\n\treturn cmd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not is_windows:\n\t\treturn cmd\n\tif not cmd:\n\t\treturn cmd\n\tif cmd[0].endswith(\".exe\"):\n\t\treturn cmd\n\tif cmd[0].startswith(\"#!\"):\n\t\tcmd[0] = cmd[0].split(\" \")[0][2:]\n\t\tif not cmd[0].endswith(\".exe\"):\n\t\t\tcmd[0] += \".exe\"\n\treturn cmd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not is_windows:\n\t\treturn cmd\n\tif len(cmd) == 0:\n\t\traise ValueError(\"command must be non-empty\")\n\tcmd = list(cmd)\n\tcmd[0] = os.path.normpath(cmd[0])\n\tif cmd[0].endswith(\".py\") and not cmd[0].endswith(\"python.exe\"):\n\t\tcmd.insert(0, sys.executable)\n\treturn tuple(cmd)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not sys.platform.startswith('win'):\n\t\treturn cmd\n\t# fix for https://bugs.python.org/issue8557\n\tif len(cmd) == 1 and '\"' in cmd[0] and cmd[0][0] != '\"' and cmd[0][-1] != '\"':\n\t\tcmd = cmd[0].split('\"') + cmd[1:]\n\tif len(cmd) > 1:\n\t\treturn cmd\n\tcmd = cmd[0]\n\tcmd = cmd.split(maxsplit=1)\n\tif len(cmd) == 1:\n\t\treturn cmd\n\tshebang = cmd[0]\n\tif shebang.startswith('#!'):\n\t\tshebang = shebang[2:]\n\tif shebang.startswith('/') or '\\\\' in shebang:\n\t\treturn tuple(cmd)\n\tif ' ' in shebang:\n\t\tcmd[0] = '\"{}\"'.format(cmd[0])\n\t\treturn tuple(cmd)\n\treturn tuple(cmd)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "ques_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey_ = key(func, args, kwargs)\n\t\t\tif key_ not in cache:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tif key_ not in cache:\n\t\t\t\t\t\tcache[key_] = func(*args, **kwargs)\n\t\t\t\tfinally:\n\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\tlock.release()\n\t\t\treturn cache[key_]\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\trv = cache[k]\n\t\t\texcept KeyError:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\trv = cache[k] = func(*args, **kwargs)\n\t\t\t\tfinally:\n\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\tlock.release()\n\t\t\treturn rv\n\t\treturn wrapper\n\treturn decorate\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tckey = key(*args, **kwds)\n\t\t\ttry:\n\t\t\t\tv = cache[ckey]\n\t\t\texcept KeyError:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tv = cache[ckey]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tv = user_function(*args, **kwds)\n\t\t\t\t\tcache[ckey] = v\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn v\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorator\n\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey_ = key(args, kwargs)\n\t\t\tif key_ in cache:\n\t\t\t\treturn cache[key_]\n\t\t\twith lock:\n\t\t\t\tif key_ not in cache:\n\t\t\t\t\tcache[key_] = user_function(*args, **kwargs)\n\t\t\treturn cache[key_]\n\t\treturn wraps(user_function)(wrapper)\n\treturn decorating_function\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.setdefault(key(args, kwargs), user_function(*args, **kwargs))\n\t\t\telse:\n\t\t\t\treturn cache.setdefault(key(args, kwargs), user_function(*args, **kwargs))\n\t\treturn wraps(user_function)(wrapper)\n\treturn decorating_function\n\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef wrapper(*args, **kwds):\n\t\t\tstore = cache\n\t\t\ttry:\n\t\t\t\tkey_ = key(*args, **kwds)\n\t\t\texcept TypeError:\n\t\t\t\t# uncachable -- for instance, passing a list as an argument.\n\t\t\t\t# Better to not cache than to blow up entirely.\n\t\t\t\treturn user_function(*args, **kwds)\n\t\t\ttry:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\treturn store[key_]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\ttry:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tstore[key_] = result\n\t\t\t\texcept TypeError\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tstore = cache.setdefault(key(*args, **kwargs), {})\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\tif 'result' not in store:\n\t\t\t\t\t\tstore['result'] = user_function(*args, **kwargs)\n\t\t\telse:\n\t\t\t\tif 'result' not in store:\n\t\t\t\t\tstore['result'] = user_function(*args, **kwargs)\n\t\t\treturn store['result']\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorate\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t#print 'cached', user_function\n\t\t\t#print 'cached', args\n\t\t\t#print 'cached', kwargs\n\t\t\tkey_ = key(user_function, args, kwargs)\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresult = cache[key_]\n\t\t\t\t\t\t#print 'cached', result\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\t\t\tcache[key_] = result\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key_]\n\t\t\t\t\t#print 'cached', result\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\t\tcache[key_] = result\n\t\t\treturn\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\thkey = key(args, kwargs)\n\t\t\tif hkey not in cache:\n\t\t\t\tif lock is not None:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tif hkey not in cache:\n\t\t\t\t\t\t\tcache[hkey] = func(*args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tcache[hkey] = func(*args, **kwargs)\n\t\t\treturn cache[hkey]\n\t\treturn wrapper\n\treturn decorate\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(args, kwargs, func.__name__)\n\t\t\trv = cache.get(k)\n\t\t\tif rv is not None:\n\t\t\t\treturn rv\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\trv = func(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\tcache[k] = rv\n\t\t\treturn rv\n\t\treturn wrapper\n\treturn decorate\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey_ = key(func, args, kwargs)\n\t\t\tif key_ not in cache:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tif key_ not in cache:\n\t\t\t\t\t\tcache[key_] = func(*args, **kwargs)\n\t\t\t\tfinally:\n\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\tlock.release()\n\t\t\treturn cache[key_]\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\trv = cache[k]\n\t\t\texcept KeyError:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\trv = cache[k] = func(*args, **kwargs)\n\t\t\t\tfinally:\n\t\t\t\t\tif lock is not None:\n\t\t\t\t\t\tlock.release()\n\t\t\treturn rv\n\t\treturn wrapper\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tckey = key(*args, **kwds)\n\t\t\ttry:\n\t\t\t\tv = cache[ckey]\n\t\t\texcept KeyError:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tv = cache[ckey]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tv = user_function(*args, **kwds)\n\t\t\t\t\tcache[ckey] = v\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn v\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey_ = key(args, kwargs)\n\t\t\tif key_ in cache:\n\t\t\t\treturn cache[key_]\n\t\t\twith lock:\n\t\t\t\tif key_ not in cache:\n\t\t\t\t\tcache[key_] = user_function(*args, **kwargs)\n\t\t\treturn cache[key_]\n\t\treturn wraps(user_function)(wrapper)\n\treturn decorating_function\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.setdefault(key(args, kwargs), user_function(*args, **kwargs))\n\t\t\telse:\n\t\t\t\treturn cache.setdefault(key(args, kwargs), user_function(*args, **kwargs))\n\t\treturn wraps(user_function)(wrapper)\n\treturn decorating_function\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef wrapper(*args, **kwds):\n\t\t\tstore = cache\n\t\t\ttry:\n\t\t\t\tkey_ = key(*args, **kwds)\n\t\t\texcept TypeError:\n\t\t\t\t# uncachable -- for instance, passing a list as an argument.\n\t\t\t\t# Better to not cache than to blow up entirely.\n\t\t\t\treturn user_function(*args, **kwds)\n\t\t\ttry:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\treturn store[key_]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\tfinally:\n\t\t\t\tlock.release()\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\ttry:\n\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\tstore[key_] = result\n\t\t\t\texcept TypeError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tstore = cache.setdefault(key(*args, **kwargs), {})\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\tif 'result' not in store:\n\t\t\t\t\t\tstore['result'] = user_function(*args, **kwargs)\n\t\t\telse:\n\t\t\t\tif 'result' not in store:\n\t\t\t\t\tstore['result'] = user_function(*args, **kwargs)\n\t\t\treturn store['result']\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t#print 'cached', user_function\n\t\t\t#print 'cached', args\n\t\t\t#print 'cached', kwargs\n\t\t\tkey_ = key(user_function, args, kwargs)\n\t\t\tif lock:\n\t\t\t\twith lock:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tresult = cache[key_]\n\t\t\t\t\t\t#print 'cached', result\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\t\t\tcache[key_] = result\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key_]\n\t\t\t\t\t#print 'cached', result\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = user_function(*args, **kwargs)\n\t\t\t\t\tcache[key_] = result\n\t\t\treturn\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\thkey = key(args, kwargs)\n\t\t\tif hkey not in cache:\n\t\t\t\tif lock is not None:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tif hkey not in cache:\n\t\t\t\t\t\t\tcache[hkey] = func(*args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tcache[hkey] = func(*args, **kwargs)\n\t\t\treturn cache[hkey]\n\t\treturn wrapper\n\treturn decorate\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(args, kwargs, func.__name__)\n\t\t\trv = cache.get(k)\n\t\t\tif rv is not None:\n\t\t\t\treturn rv\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\trv = func(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\tcache[k] = rv\n\t\t\treturn rv\n\t\treturn wrapper\n\treturn decorate\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "ques_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args][kwds]\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache_clear = cache.cache_clear\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__doc__ = user_function.__doc__\n\t\twrapper.__name__ = user_function.__name__\n\t\twrapper.__dict__.update(user_function.__dict__)\n\t\treturn functools.update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tcache = _TTLCache(maxsize, ttl, timer, typed)\n\tdef decorating_function(user_function):\n\t\twrapper = cache.make_wrapper(user_function)\n\t\twrapper.cache = cache\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache_clear = cache.clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tif maxsize is None:\n\t\tmaxsize = DEFAULT_TTL_CACHE_MAXSIZE\n\tif ttl is None:\n\t\tttl = DEFAULT_TTL_CACHE_TTL\n\tif timer is None:\n\t\ttimer = time.monotonic\n\tif typed is None:\n\t\ttyped = False\n\n\tdef decorating_function(user_function):\n\t\tcache = ttl_cache_class(maxsize, ttl, timer, typed)\n\t\treturn _lru_cache_wrapper(user_function, cache)\n\n\treturn decorating_function\n\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTL_Cache(maxsize, ttl, timer, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache[args]\n\t\t\tif result is not None:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tlock = threading.Lock()\n\tcache = _Cache(maxsize, timer, ttl, typed)\n\tcache_get = cache.__getitem__\n\tmake_key = _make_key\n\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\twith lock:\n\t\t\t\tnow = timer()\n\t\t\t\ttry:\n\t\t\t\t\tvalue, expiration = cache_get(key, now)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue = user_function(*args, **kwds)\n\t\t\t\t\tcache[key] = (value, now + ttl)\n\t\t\t\telse:\n\t\t\t\t\tif expiration < now:\n\t\t\t\t\t\tvalue = user_function(*args, **kwds)\n\t\t\t\t\t\tcache[key] = (value, now + ttl)\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize variable access\n\t_CacheInfo = _ttl_cache_info\n\t_HashedSeq = _HashedSeq\n\t_make_key = _make_key\n\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tstats = [0, 0]                  # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get           # bound method to lookup key or return None\n\t\t_len = len                      # localize the global len() function\n\t\tlock = RLock()                  # because linkedlist updates aren't threadsafe\n\t\troot = []                       # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]      # initialize by pointing to self\n\t\tnonlocal_root = [root]                  # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3    # names for the link fields\n\n\t\tif _\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(ttl, maxsize, typed)\n\t\tstats = _CacheStats()\n\t\thits, misses, currsize = [0] * 3\n\t\tlock = threading.RLock()\n\t\troot = []\n\t\troot[:] = [root, root, None, None]\n\t\tnonlocal_root = [root]\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal hits, misses, currsize\n\t\t\tkey = make_key(args, kwds, typed=typed)\n\t\t\twith lock:\n\t\t\t\tlink = cache.get(key, root)\n\t\t\t\tif link is not root:\n\t\t\t\t\t# Record recent use of this key.\n\t\t\t\t\troot, = nonlocal_root\n\t\t\t\t\tlink_prev, link_next, key, result = link\n\t\t\t\t\tlink_prev[1] = link_next\n\t\t\t\t\tlink_next[0] = link_prev\n\t\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize variable access\n\t_CacheInfo = _ttl_CacheInfo\n\t_HashedSeq = _ttl_HashedSeq\n\t_make_key = _ttl_make_key\n\tcache = _ttl_dict()\n\tcache_get = cache.get\n\tlock = RLock()\n\troot = []\n\troot[:] = [root, root, None, None]\n\tif maxsize == 0:\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\treturn result\n\telif maxsize is None:\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\twith lock:\n\t\t\t\tnow = timer()\n\t\t\t\ttry:\n\t\t\t\t\tvalue, _, when = cache_get(key, root)\n\t\t\t\t\tif when + ttl > now:\n\t\t\t\t\t\treturn value\n\t\t\t\texcept TypeError:\n\t\t\t\t\tpass\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t_CacheInfo = namedtuple(\"CacheInfo\", [\"hits\", \"misses\", \"maxsize\", \"currsize\"])\n\n\tcache = {}\n\thits = misses = 0\n\tmaxsize = maxsize\n\tcurrsize = 0\n\tlock = Lock()\n\n\tdef wrapper(func):\n\t\tnonlocal hits, misses, maxsize, currsize\n\n\t\tdef decorating_function(*args, **kwargs):\n\t\t\tnonlocal hits, misses, maxsize, currsize\n\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tnow = timer()\n\n\t\t\twith lock:\n\t\t\t\t# purge expired entries\n\t\t\t\tfor k, (v, t) in cache.items():\n\t\t\t\t\tif t < now:\n\t\t\t\t\t\tdel cache[k]\n\t\t\t\t\t\tcurrsize -= 1\n\n\t\t\t\tif key in cache:\n\t\t\t\t\thits += 1\n\t\t\t\t\tv, t = cache[key]\n\t\t\t\t\tif t > now:\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tlock = RLock()\n\tcache = {}\n\thits = misses = 0\n\tmake_key = _make_key\n\tcache_get = cache.get\n\tcache_len = cache.__len__\n\n\tdef decorating_function(user_function):\n\t\tstats = [0, 0]\n\t\troot = []\n\t\troot[:] = [root, root, None, None]\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal stats\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\twith lock:\n\t\t\t\tlink = cache_get(key)\n\t\t\t\tif link is not None:\n\t\t\t\t\troot, = nonlocal_root\n\t\t\t\t\tlink_prev, link_next, key, result = link\n\t\t\t\t\tlink_prev[1] = link_next\n\t\t\t\t\tlink_next[0] = link_prev\n\t\t\t\t\tlast = root[0]\n\t\t\t\t\tlast[1] = root[0] = link\n\t\t\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args][kwds]\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache_clear = cache.cache_clear\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__doc__ = user_function.__doc__\n\t\twrapper.__name__ = user_function.__name__\n\t\twrapper.__dict__.update(user_function.__dict__)\n\t\treturn functools.update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tcache = _TTLCache(maxsize, ttl, timer, typed)\n\tdef decorating_function(user_function):\n\t\twrapper = cache.make_wrapper(user_function)\n\t\twrapper.cache = cache\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache_clear = cache.clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tif maxsize is None:\n\t\tmaxsize = DEFAULT_TTL_CACHE_MAXSIZE\n\tif ttl is None:\n\t\tttl = DEFAULT_TTL_CACHE_TTL\n\tif timer is None:\n\t\ttimer = time.monotonic\n\tif typed is None:\n\t\ttyped = False\n\n\tdef decorating_function(user_function):\n\t\tcache = ttl_cache_class(maxsize, ttl, timer, typed)\n\t\treturn _lru_cache_wrapper(user_function, cache)\n\n\treturn decorating_function\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTL_Cache(maxsize, ttl, timer, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache[args]\n\t\t\tif result is not None:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tlock = threading.Lock()\n\tcache = _Cache(maxsize, timer, ttl, typed)\n\tcache_get = cache.__getitem__\n\tmake_key = _make_key\n\n\tdef decorating_function(user_function):\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\twith lock:\n\t\t\t\tnow = timer()\n\t\t\t\ttry:\n\t\t\t\t\tvalue, expiration = cache_get(key, now)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue = user_function(*args, **kwds)\n\t\t\t\t\tcache[key] = (value, now + ttl)\n\t\t\t\telse:\n\t\t\t\t\tif expiration < now:\n\t\t\t\t\t\tvalue = user_function(*args, **kwds)\n\t\t\t\t\t\tcache[key] = (value, now + ttl)\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize variable access\n\t_CacheInfo = _ttl_cache_info\n\t_HashedSeq = _HashedSeq\n\t_make_key = _make_key\n\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tstats = [0, 0]                  # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get           # bound method to lookup key or return None\n\t\t_len = len                      # localize the global len() function\n\t\tlock = RLock()                  # because linkedlist updates aren't threadsafe\n\t\troot = []                       # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]      # initialize by pointing to self\n\t\tnonlocal_root = [root]                  # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3    # names for the link fields\n\n\t\tif _\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(ttl, maxsize, typed)\n\t\tstats = _CacheStats()\n\t\thits, misses, currsize = [0] * 3\n\t\tlock = threading.RLock()\n\t\troot = []\n\t\troot[:] = [root, root, None, None]\n\t\tnonlocal_root = [root]\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal hits, misses, currsize\n\t\t\tkey = make_key(args, kwds, typed=typed)\n\t\t\twith lock:\n\t\t\t\tlink = cache.get(key, root)\n\t\t\t\tif link is not root:\n\t\t\t\t\t# Record recent use of this key.\n\t\t\t\t\troot, = nonlocal_root\n\t\t\t\t\tlink_prev, link_next, key, result = link\n\t\t\t\t\tlink_prev[1] = link_next\n\t\t\t\t\tlink_next[0] = link_prev\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize variable access\n\t_CacheInfo = _ttl_CacheInfo\n\t_HashedSeq = _ttl_HashedSeq\n\t_make_key = _ttl_make_key\n\tcache = _ttl_dict()\n\tcache_get = cache.get\n\tlock = RLock()\n\troot = []\n\troot[:] = [root, root, None, None]\n\tif maxsize == 0:\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tresult = user_function(*args, **kwargs)\n\t\t\treturn result\n\telif maxsize is None:\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\twith lock:\n\t\t\t\tnow = timer()\n\t\t\t\ttry:\n\t\t\t\t\tvalue, _, when = cache_get(key, root)\n\t\t\t\t\tif when + ttl > now:\n\t\t\t\t\t\treturn value\n\t\t\t\texcept TypeError:\n\t\t\t\t\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t_CacheInfo = namedtuple(\"CacheInfo\", [\"hits\", \"misses\", \"maxsize\", \"currsize\"])\n\n\tcache = {}\n\thits = misses = 0\n\tmaxsize = maxsize\n\tcurrsize = 0\n\tlock = Lock()\n\n\tdef wrapper(func):\n\t\tnonlocal hits, misses, maxsize, currsize\n\n\t\tdef decorating_function(*args, **kwargs):\n\t\t\tnonlocal hits, misses, maxsize, currsize\n\n\t\t\tkey = make_key(args, kwargs, typed)\n\t\t\tnow = timer()\n\n\t\t\twith lock:\n\t\t\t\t# purge expired entries\n\t\t\t\tfor k, (v, t) in cache.items():\n\t\t\t\t\tif t < now:\n\t\t\t\t\t\tdel cache[k]\n\t\t\t\t\t\tcurrsize -= 1\n\n\t\t\t\tif key in cache:\n\t\t\t\t\thits += 1\n\t\t\t\t\tv, t = cache[key]\n\t\t\t\t\tif t > now:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tlock = RLock()\n\tcache = {}\n\thits = misses = 0\n\tmake_key = _make_key\n\tcache_get = cache.get\n\tcache_len = cache.__len__\n\n\tdef decorating_function(user_function):\n\t\tstats = [0, 0]\n\t\troot = []\n\t\troot[:] = [root, root, None, None]\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tnonlocal stats\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\twith lock:\n\t\t\t\tlink = cache_get(key)\n\t\t\t\tif link is not None:\n\t\t\t\t\troot, = nonlocal_root\n\t\t\t\t\tlink_prev, link_next, key, result = link\n\t\t\t\t\tlink_prev[1] = link_next\n\t\t\t\t\tlink_next[0] = link_prev\n\t\t\t\t\tlast = root[0]\n\t\t\t\t\tlast[1] = root[0] = link\n\t\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "ques_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n\tif isinstance(maxsize, int):\n\t\tcache = OrderedDict()\n\n\t\tdef decorating_function(user_function):\n\t\t\tnonlocal cache\n\n\t\t\t@wraps(user_function)\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tkey = args\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\tif typed:\n\t\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\t\tif kwds:\n\t\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\t\tif key in cache:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tdel cache[key]\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\t\t\telse:\n\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\tif len(cache)\n", "def mru_cache(maxsize=128, typed=False):\n\t# This class is supposed to be used as a decorator\n\t#     @functools.mru_cache(maxsize=128, typed=False)\n\t#\n\t# See http://en.wikipedia.org/wiki/Memoization for details.\n\t#\n\t# Self-tests:\n\t#\n\t#     import functools\n\t#     @functools.mru_cache(maxsize=20)\n\t#     def f(x, y):\n\t#         print('calling f(%s, %s)' % (x, y))\n\t#         return x**2 + y**2\n\t#\n\t#     >>> f(3, 4)\n\t#     calling f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\t#\n\t#     >>> f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\treturn _cache(user_function, maxsize, typed)\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(user_function, maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache(*args, **kwds)\n\t\t\twrapper.cache_info = cache.cache_info()\n\t\t\treturn result\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\twrapper.cache_clear = cache_clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\treturn _cache_wrapper(user_function, _CacheInfo, maxsize, typed)\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _Cache(maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache.get(args, kwds)\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[args, kwds] = result\n\t\t\treturn result\n\t\treturn _update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\t\t\t\t\t# Order matters.\n\t\tlock = RLock()\t\t\t\t\t\t\t# To make the LRU algorithm thread-safe.\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# Collect all positional and keyword arguments, along with the\n\t\t\t# hash of them, for use as the key to the cache.\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\thashkey = hash(key)\n\t\t\twith lock:\n\t\t\t\t# Get the value from the cache.\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache.pop\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _Cache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(kwds.items())\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\twrapper.clear = clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "def mru_cache(maxsize=128, typed=False):\n\treturn _Cache(maxsize, typed)\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(kwds.items()))\n\t\t\tresult = cache.get(key)\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tif isinstance(maxsize, int):\n\t\tcache = OrderedDict()\n\n\t\tdef decorating_function(user_function):\n\t\t\tnonlocal cache\n\n\t\t\t@wraps(user_function)\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tkey = args\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\tif typed:\n\t\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\t\tif kwds:\n\t\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\t\tif key in cache:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tdel cache[key]\n\t\t\t\t\tcache[key] = result\n\t\t\t\t\treturn result\n\t\t\t\telse:\n\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\tif len(cache)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\t# This class is supposed to be used as a decorator\n\t#     @functools.mru_cache(maxsize=128, typed=False)\n\t#\n\t# See http://en.wikipedia.org/wiki/Memoization for details.\n\t#\n\t# Self-tests:\n\t#\n\t#     import functools\n\t#     @functools.mru_cache(maxsize=20)\n\t#     def f(x, y):\n\t#         print('calling f(%s, %s)' % (x, y))\n\t#         return x**2 + y**2\n\t#\n\t#     >>> f(3, 4)\n\t#     calling f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\t#\n\t#     >>> f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\t#     >>> f(3, 4)\n\t#     25\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\treturn _cache(user_function, maxsize, typed)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(user_function, maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache(*args, **kwds)\n\t\t\twrapper.cache_info = cache.cache_info()\n\t\t\treturn result\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\twrapper.cache_clear = cache_clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\treturn _cache_wrapper(user_function, _CacheInfo, maxsize, typed)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _Cache(maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache.get(args, kwds)\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[args, kwds] = result\n\t\t\treturn result\n\t\treturn _update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\t\t\t\t\t# Order matters.\n\t\tlock = RLock()\t\t\t\t\t\t\t# To make the LRU algorithm thread-safe.\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# Collect all positional and keyword arguments, along with the\n\t\t\t# hash of them, for use as the key to the cache.\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\thashkey = hash(key)\n\t\t\twith lock:\n\t\t\t\t# Get the value from the cache.\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache.pop\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _Cache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(kwds.items())\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\twrapper.clear = clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\treturn _Cache(maxsize, typed)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = (args, tuple(kwds.items()))\n\t\t\tresult = cache.get(key)\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "ques_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n\tif maxsize is not None and not isinstance(maxsize, int):\n\t\traise TypeError('Expected maxsize to be an integer or None')\n\n\tlock = RLock()\n\troot = []\n\troot[:] = [root, root, None, None]\n\t# root = [root, root, None, None]\n\t# ^^^ use list(root) instead of root[:] so that we don't create a new list\n\t# with the same id() as the root list!\n\t#\n\t# root = [root, root, None, None] # [root, root, None, None]\n\t# ^^^ this is the root of the linked list that keeps track of the\n\t# least recently used items.\n\t#\n\t# root[0] and root[1] are the two sentinel nodes,\n\t# and root[2] and root[3] are the nodes that actually hold the cached\n\t# values (which are normally None).\n\t#\n\t# If root[2] is not None, it is the most recently used item,\n\t# and root[3] is the least recently used item.\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\n\t\tlock = RLock()                  # needed because OrderedDict isn't threadsafe\n\t\tcache = OrderedDict()           # order: least recent to most recent\n\t\tstats = [0, 0]                  # make updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = (\n\t\t\tlambda args, kwds:\n\t\t\t\t(args, tuple(sorted(kwds.items())))\n\t\t\t\tif kwds else args\n\t\t)\n\t\tif typed:\n\t\t\tmake_key = (\n\t\t\t\tlambda args, kwds:\n\t\t\t\t\t(tuple(type(v) for v in args),\n\t\t\t\t\t tuple((type(v), k) for k, v in sorted(kwds.items())))\n\t\t\t\t\tif kwds else tuple(type(v) for v in args)\n\t\t\t)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t\n", "def lru_cache(maxsize=128, typed=False):\n\treturn _lru_cache_wrapper(maxsize, typed, _HashedSeq)\n\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn cache[args, kwargs]\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is NO_RESULT:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn (cache.hits, cache.misses, cache.maxsize, cache.currsize)\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n", "def lru_cache(maxsize=128, typed=False):\n\tif maxsize is None:\n\t\tmaxsize = 128\n\tif not isinstance(maxsize, int):\n\t\traise TypeError(\"maxsize must be an integer\")\n\tif maxsize <= 0:\n\t\traise ValueError(\"maxsize must be positive\")\n\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tqueue = deque()\n\t\trefcount = defaultdict(int)\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\tif key in cache:\n\t\t\t\tqueue.remove(key)\n\t\t\t\tqueue.append(key)\n\t\t\t\treturn cache[key]\n\t\t\tresult = user_function\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__clear__ = clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()    # order: least recent to most recent\n\t\tstats = [0, 0]           # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1      # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get    # bound method to lookup key or return None\n\t\t_len = len                # localize the global len() function\n\t\tlock = RLock()           # because linkedlist updates aren't threadsafe\n\t\troot = []                # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]     # initialize by pointing to self\n\t\tnonlocal_root = [root]                 # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3   # names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# no caching, just do a statistics update after\n", "def lru_cache(maxsize=128, typed=False):\n\t# Users should only access the lru_cache through its public API:\n\t#       cache_info, cache_clear, and f.__wrapped__\n\t# The internals of the lru_cache are encapsulated for thread safety and\n\t# to allow the implementation to change (including a possible C version).\n\n\tdef decorating_function(user_function):\n\n\t\tcache = dict()\n\t\tstats = [0, 0]                  # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get           # bound method to lookup key or return None\n\t\t_len = len                      # localize the global len() function\n\t\tlock = RLock()                  # because linkedlist updates aren't threadsafe\n\t\troot = []                       # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]      # initialize by pointing to self\n\t\tnonlocal_root = [root]                  # make updateable non-locally\n\t\tPREV\n", "def lru_cache(maxsize=128, typed=False):\n\tlock = RLock()\n\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tlist_ = []\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tlist_.remove(key)\n\t\t\t\t\tlist_.append(key)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\t\tdel cache[list_.pop(\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tif maxsize is not None and not isinstance(maxsize, int):\n\t\traise TypeError('Expected maxsize to be an integer or None')\n\n\tlock = RLock()\n\troot = []\n\troot[:] = [root, root, None, None]\n\t# root = [root, root, None, None]\n\t# ^^^ use list(root) instead of root[:] so that we don't create a new list\n\t# with the same id() as the root list!\n\t#\n\t# root = [root, root, None, None] # [root, root, None, None]\n\t# ^^^ this is the root of the linked list that keeps track of the\n\t# least recently used items.\n\t#\n\t# root[0] and root[1] are the two sentinel nodes,\n\t# and root[2] and root[3] are the nodes that actually hold the cached\n\t# values (which are normally None).\n\t#\n\t# If root[2] is not None, it is the most recently used item,\n\t# and root[3] is the least recently used item.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\n\t\tlock = RLock()                  # needed because OrderedDict isn't threadsafe\n\t\tcache = OrderedDict()           # order: least recent to most recent\n\t\tstats = [0, 0]                  # make updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = (\n\t\t\tlambda args, kwds:\n\t\t\t\t(args, tuple(sorted(kwds.items())))\n\t\t\t\tif kwds else args\n\t\t)\n\t\tif typed:\n\t\t\tmake_key = (\n\t\t\t\tlambda args, kwds:\n\t\t\t\t\t(tuple(type(v) for v in args),\n\t\t\t\t\t tuple((type(v), k) for k, v in sorted(kwds.items())))\n\t\t\t\t\tif kwds else tuple(type(v) for v in args)\n\t\t\t)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\treturn _lru_cache_wrapper(maxsize, typed, _HashedSeq)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn cache[args, kwargs]\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is NO_RESULT:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn (cache.hits, cache.misses, cache.maxsize, cache.currsize)\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tif maxsize is None:\n\t\tmaxsize = 128\n\tif not isinstance(maxsize, int):\n\t\traise TypeError(\"maxsize must be an integer\")\n\tif maxsize <= 0:\n\t\traise ValueError(\"maxsize must be positive\")\n\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tqueue = deque()\n\t\trefcount = defaultdict(int)\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\tif key in cache:\n\t\t\t\tqueue.remove(key)\n\t\t\t\tqueue.append(key)\n\t\t\t\treturn cache[key]\n\t\t\tresult = user_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__clear__ = clear\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()    # order: least recent to most recent\n\t\tstats = [0, 0]           # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1      # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get    # bound method to lookup key or return None\n\t\t_len = len                # localize the global len() function\n\t\tlock = RLock()           # because linkedlist updates aren't threadsafe\n\t\troot = []                # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]     # initialize by pointing to self\n\t\tnonlocal_root = [root]                 # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3   # names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# no caching, just do a statistics update after\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t# Users should only access the lru_cache through its public API:\n\t#       cache_info, cache_clear, and f.__wrapped__\n\t# The internals of the lru_cache are encapsulated for thread safety and\n\t# to allow the implementation to change (including a possible C version).\n\n\tdef decorating_function(user_function):\n\n\t\tcache = dict()\n\t\tstats = [0, 0]                  # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get           # bound method to lookup key or return None\n\t\t_len = len                      # localize the global len() function\n\t\tlock = RLock()                  # because linkedlist updates aren't threadsafe\n\t\troot = []                       # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]      # initialize by pointing to self\n\t\tnonlocal_root = [root]                  # make updateable non-locally\n\t\tPREV\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tlock = RLock()\n\n\tdef decorating_function(user_function):\n\t\tcache = dict()\n\t\tlist_ = []\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in sorted(kwds.items()))\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\tlist_.remove(key)\n\t\t\t\t\tlist_.append(key)\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\tif len(cache) >= maxsize:\n\t\t\t\t\t\tdel cache[list_.pop(\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "ques_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n\treturn functools.partial(_lfu_cache, maxsize=maxsize, typed=typed)\n\n", "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cachedict(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args, tuple(sorted(kwds.items()))]\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\tdef get(key):\n\t\t\treturn cache.get(key)\n\t\tdef set(key,val):\n\t\t\tcache[key] = val\n\t\tdef get_cache():\n\t\t\treturn cache\n\t\tdef get_cache_info():\n\t\t\treturn cache.info()\n\t\twrapper.get_cache = get_cache\n\t\twrapper.get_cache_info = get_cache_info\n\t\twrapper.clear_cache = clear\n\t\twrapper.get_cache_item = get\n\t\twrapper.set_cache_item = set\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function, *args, **kwds)\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn functools.partial(lfu_cache, maxsize=maxsize, typed=typed)\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn functools.partial(lfu_cache_, maxsize=maxsize, typed=typed)\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed, False)\n\n", "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function,\n\t\t\t\t\t\t\t\t\t\t\t\t\tmaxsize=maxsize,\n\t\t\t\t\t\t\t\t\t\t\t\t\ttyped=typed):\n\t\tcache = _lfu_cache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache(user_function, *args, **kwds)\n\t\t\twrapper.cache_info = cache.cache_info\n\t\t\treturn result\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn _cache(maxsize, typed, _lfu_cache)\n\n", "def lfu_cache(maxsize=128, typed=False):\n\tcache = {}\n\theap = []\n\thits = misses = 0\n\tlock = RLock()\n\tcache_get = cache.get\n\theap_appendleft = heap.appendleft\n\theap_popleft = heap.popleft\n\t_len = len\n\t_maxsize = maxsize\n\troot = []\n\troot[:] = [root, root, None, None]\n\n\tdef _remove_node(node):\n\t\tif node[2] is not None:\n\t\t\tnode[2][3] = node[3]\n\t\tif node[3] is not None:\n\t\t\tnode[3][2] = node[2]\n\t\tdel node[:]\n\n\tdef _insert_node(node):\n\t\tnode[2] = root[2]\n\t\tnode[3] = root\n\t\troot[2][3] = node\n\t\troot[2] = node\n\n\tdef wrapper(*args, **kwds):\n\t\tnonlocal hits, misses\n\t\tkey = make_key(args, kwds, typed=typed)\n\t\n", "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LfuCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is NO_RESULT:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn functools.partial(_lfu_cache, maxsize=maxsize, typed=typed)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cachedict(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args, tuple(sorted(kwds.items()))]\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\tdef get(key):\n\t\t\treturn cache.get(key)\n\t\tdef set(key,val):\n\t\t\tcache[key] = val\n\t\tdef get_cache():\n\t\t\treturn cache\n\t\tdef get_cache_info():\n\t\t\treturn cache.info()\n\t\twrapper.get_cache = get_cache\n\t\twrapper.get_cache_info = get_cache_info\n\t\twrapper.clear_cache = clear\n\t\twrapper.get_cache_item = get\n\t\twrapper.set_cache_item = set\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function, *args, **kwds)\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn functools.partial(lfu_cache, maxsize=maxsize, typed=typed)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn functools.partial(lfu_cache_, maxsize=maxsize, typed=typed)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed, False)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function,\n\t\t\t\t\t\t\t\t\t\t\t\t\tmaxsize=maxsize,\n\t\t\t\t\t\t\t\t\t\t\t\t\ttyped=typed):\n\t\tcache = _lfu_cache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache(user_function, *args, **kwds)\n\t\t\twrapper.cache_info = cache.cache_info\n\t\t\treturn result\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn _cache(maxsize, typed, _lfu_cache)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tcache = {}\n\theap = []\n\thits = misses = 0\n\tlock = RLock()\n\tcache_get = cache.get\n\theap_appendleft = heap.appendleft\n\theap_popleft = heap.popleft\n\t_len = len\n\t_maxsize = maxsize\n\troot = []\n\troot[:] = [root, root, None, None]\n\n\tdef _remove_node(node):\n\t\tif node[2] is not None:\n\t\t\tnode[2][3] = node[3]\n\t\tif node[3] is not None:\n\t\t\tnode[3][2] = node[2]\n\t\tdel node[:]\n\n\tdef _insert_node(node):\n\t\tnode[2] = root[2]\n\t\tnode[3] = root\n\t\troot[2][3] = node\n\t\troot[2] = node\n\n\tdef wrapper(*args, **kwds):\n\t\tnonlocal hits, misses\n\t\tkey = make_key(args, kwds, typed=typed)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LfuCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is NO_RESULT:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "ques_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n\tif self.root:\n\t\tif self.leftmost() is self.rightmost():\n\t\t\tk, v = self.leftmost().key, self.leftmost().value\n\t\t\tself.root = None\n\t\t\tself.size = 0\n\t\t\treturn (k, v)\n\t\telse:\n\t\t\tlink = self.leftmost()\n\t\t\tlink_prev = link.prev\n\t\t\tlink_prev.next = link.next\n\t\t\tlink.next.prev = link_prev\n\t\t\tself.size -= 1\n\t\t\treturn (link.key, link.value)\n\telse:\n\t\traise KeyError('popitem(): ordered dictionary is empty')\n\t\t\n", "def popitem(self):\n\t# When 3.5+, change to:\n\t# return super().popitem()\n\treturn dict.popitem(self)\n\t\n", "def popitem(self):\n\tself._check_not_closed()\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\tkey = next(iter(self))\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "def popitem(self):\n\ttry:\n\t\tk, v = self.__root[2]\n\t\tdel self[k]\n\t\treturn (k, v)\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\n\n", "def popitem(self):\n\treturn self.__dict__.popitem()\n\n", "def popitem(self):\n\ttry:\n\t\tk, v = next(self.iteritems())\n\texcept StopIteration:\n\t\traise KeyError('container is empty')\n\tdel self[k]\n\treturn (k, v)\n\n", "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "def popitem(self):\n\tif self.map:\n\t\tkey, _ = self.map.popitem(last=True)\n\t\treturn (key, self.pop(key))\n\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "def popitem(self):\n\tif self.__head is None:\n\t\traise KeyError('dictionary is empty')\n\tresult = (self.__head.key, self.__head.value)\n\tdel self[self.__head.key]\n\treturn result\n\n", "def popitem(self):\n\ttry:\n\t\tkey = next(self.__iter__())\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\treturn (key, self.pop(key))\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\tif self.root:\n\t\tif self.leftmost() is self.rightmost():\n\t\t\tk, v = self.leftmost().key, self.leftmost().value\n\t\t\tself.root = None\n\t\t\tself.size = 0\n\t\t\treturn (k, v)\n\t\telse:\n\t\t\tlink = self.leftmost()\n\t\t\tlink_prev = link.prev\n\t\t\tlink_prev.next = link.next\n\t\t\tlink.next.prev = link_prev\n\t\t\tself.size -= 1\n\t\t\treturn (link.key, link.value)\n\telse:\n\t\traise KeyError('popitem(): ordered dictionary is empty')\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t# When 3.5+, change to:\n\t# return super().popitem()\n\treturn dict.popitem(self)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tself._check_not_closed()\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\tkey = next(iter(self))\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tk, v = self.__root[2]\n\t\tdel self[k]\n\t\treturn (k, v)\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self.__dict__.popitem()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tk, v = next(self.iteritems())\n\texcept StopIteration:\n\t\traise KeyError('container is empty')\n\tdel self[k]\n\treturn (k, v)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def popitem(self):\n\tif self.map:\n\t\tkey, _ = self.map.popitem(last=True)\n\t\treturn (key, self.pop(key))\n\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.__head is None:\n\t\traise KeyError('dictionary is empty')\n\tresult = (self.__head.key, self.__head.value)\n\tdel self[self.__head.key]\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = next(self.__iter__())\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\treturn (key, self.pop(key))\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "ques_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\tif not self.has_key(key):\n\t\tself[key] = default\n\treturn self[key]\n\n", "def setdefault(self, key, default=None):\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn self[key]\n", "def setdefault(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\tself[key] = default\n\treturn default\n\n", "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\t\n\t\n\t\n\t\n\tdef tzset(self, ):\n\t\t\"\"\"\n\t\tResets the time conversion rules used by the library.\n\t\t\n\t\tThe environment variable TZ specifies how this is done.\n\t\t\"\"\"\n\t\tpass\n\t\t\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tUpdate D from dict/iterable E and F.\n\t\tIf E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n\t\t\n\t\tIf E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n\t\t\n\t\tIn either case, this is followed by: for k in F:  D[k] = F[k]\n\t\t\"\"\"\n\t\tpass\n\t\t\n\tdef values(self, ):\n\t\t\"\"\"\n\t\tD.values() -> list of D's values\n\t\t\"\"\"\n\t\tpass\n\t\t\n\tdef __cmp__(self, ):\n\t\t\"\"\"\n\t\t\n", "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\treturn default\n\n\n", "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\tif key not in self:\n\t\tself[key] = default\n\treturn self[key]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\tif not self.has_key(key):\n\t\tself[key] = default\n\treturn self[key]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn self[key]\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\tself[key] = default\n\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\t\n\t\n\t\n\t\n\tdef tzset(self, ):\n\t\t\"\"\"\n\t\tResets the time conversion rules used by the library.\n\t\t\n\t\tThe environment variable TZ specifies how this is done.\n\t\t\"\"\"\n\t\tpass\n\t\t\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tUpdate D from dict/iterable E and F.\n\t\tIf E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n\t\t\n\t\tIf E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n\t\t\n\t\tIn either case, this is followed by: for k in F:  D[k] = F[k]\n\t\t\"\"\"\n\t\tpass\n\t\t\n\tdef values(self, ):\n\t\t\"\"\"\n\t\tD.values() -> list of D's values\n\t\t\"\"\"\n\t\tpass\n\t\t\n\tdef __cmp__(self, ):\n\t\t\"\"\"\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\treturn default\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\tif key not in self:\n\t\tself[key] = default\n\treturn self[key]\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "ques_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n\t\n\t\n\t\n\t\"\"\"\n\t\n\t\"\"\"\n\t\n\t\n\t\n\tpass\n\t\n\tdef __init__(self, ):\n\t\tpass\n\t\n\tdef __len__(self, ):\n\t\tpass\n\t\n\tdef __cmp__(self, ):\n\t\tpass\n\t\n\tdef __repr__(self, ):\n\t\tpass\n\t\n\tdef __hash__(self, ):\n\t\tpass\n\t\n\tdef __getitem__(self, ):\n\t\tpass\n\t\n\tdef __setitem__(self, ):\n\t\tpass\n\t\n\tdef __delitem__(self, ):\n\t\tpass\n\t\n\tdef __contains__(self, ):\n\t\tpass\n\t\n\tdef has_key(self, ):\n\t\tpass\n\t\n\tdef setdefault(self, ):\n\t\tpass\n\t\n\tdef update(self, ):\n\t\tpass\n\t\n\tdef keys(self, ):\n\t\tpass\n\t\n\tdef items(self, ):\n\t\tpass\n\t\n\tdef values(self, ):\n\t\t\n", "def get(self, key, default=None):\n\ttry:\n\t\treturn self.__getitem__(key)\n\texcept KeyError:\n\t\treturn default\n\n\tdef has_key(self, key):\n\t\t\"\"\"\n\t\tD.has_key(k) -> 1 if D.has_key(k), else 0\n\t\t\"\"\"\n\t\treturn self.__contains__(key)\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\t\"\"\"\n\t\treturn self.__iter__()\n\n\tdef iteritems(self):\n\t\t\"\"\"\n\t\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\t\"\"\"\n\t\treturn self.__iter__()\n\n\tdef iterkeys(self):\n\t\t\"\"\"\n\t\tD.iterkeys() -> an iterator over the keys of D\n\t\t\"\"\"\n\t\tfor k, v in self.__iter__():\n\t\t\tyield k\n\n\tdef itervalues(self):\n\t\t\"\"\"\n\t\tD.itervalues() ->\n", "def get(self, key, default=None):\n\treturn self.get_or_default(key, default)\n\n\tdef get_or_default(self, key, default):\n\t\ttry:\n\t\t\treturn self.get_by_key(key)\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef get_by_key(self, key):\n\t\ttry:\n\t\t\treturn self.__dict__[key]\n\t\texcept KeyError:\n\t\t\treturn self.__dict__[key.lower()]\n\n\tdef __getitem__(self, key):\n\t\treturn self.get_by_key(key)\n\n\tdef __setitem__(self, key, value):\n\t\tself.__dict__[key] = value\n\n\tdef keys(self):\n\t\treturn self.__dict__.keys()\n\n\tdef values(self):\n\t\treturn self.__dict__.values()\n\n\tdef items(self):\n\t\treturn self.__dict__.items()\n\n\tdef __contains__(self, key):\n\t\treturn key in self.__dict__ or key.lower() in self.\n", "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\tdef has_key(self, key):\n\t\"\"\"\n\tD.has_key(k) -> True if D has a key k, else False\n\t\"\"\"\n\treturn self.__dict__.has_key(key)\n\n\tdef items(self):\n\t\"\"\"\n\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\"\"\"\n\treturn self.__dict__.items()\n\n\tdef iteritems(self):\n\t\"\"\"\n\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\"\"\"\n\treturn self.__dict__.iteritems()\n\n\tdef iterkeys(self):\n\t\"\"\"\n\tD.iterkeys() -> an iterator over the keys of D\n\t\"\"\"\n\treturn self.__dict__.iterkeys()\n\n\tdef itervalues(self):\n\t\"\"\"\n\tD.itervalues() -> an iterator over the values of D\n\t\"\"\"\n\treturn self.__dict__.itervalues()\n\n\tdef keys(self):\n\t\n", "def get(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n\tdef has_key(self, key):\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\texcept KeyError:\n\t\t\treturn False\n\t\treturn True\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\t\"\"\"\n\t\treturn zip(self.keys(), self.values())\n\n\tdef iteritems(self):\n\t\t\"\"\"\n\t\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\t\"\"\"\n\t\tfor key in self.keys():\n\t\t\tyield (key, self[key])\n\n\tdef iterkeys(self):\n\t\t\"\"\"\n\t\tD.iterkeys() -> an iterator over the keys of D\n\t\t\"\"\"\n\t\treturn self.keys().__iter__()\n\n\tdef itervalues(self):\n\t\t\"\"\"\n\t\tD.itervalues() -> an iterator over the values of D\n\t\n", "def get(self, key, default=None):\n\ttry:\n\t    return self[key]\n\texcept KeyError:\n\t    return default\n\n\n", "def get(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\treturn default\n\n", "def get(self, key, default=None):\n\t\treturn self.__dict__.get(key, default)\n\t\n\tdef has_key(self, key):\n\t\"\"\"\n\tD.has_key(k) -> 1 if D.has_key(k), else 0\n\t\"\"\"\n\t\treturn self.__dict__.has_key(key)\n\t\n\tdef items(self):\n\t\"\"\"\n\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\"\"\"\n\t\treturn self.__dict__.items()\n\t\n\tdef iteritems(self):\n\t\"\"\"\n\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\"\"\"\n\t\treturn self.__dict__.iteritems()\n\t\n\tdef iterkeys(self):\n\t\"\"\"\n\tD.iterkeys() -> an iterator over the keys of D\n\t\"\"\"\n\t\treturn self.__dict__.iterkeys()\n\t\n\tdef itervalues(self):\n\t\"\"\"\n\tD.itervalues() -> an iterator over the values of D\n\t\"\"\"\n\t\treturn self.\n", "def get(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\treturn default\n\n", "def get(self, key, default=None):\n\treturn self.__getitem__(key, default)\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n\t\n\t\n\t\n\t\"\"\"\n\t\n\t\"\"\"\n\t\n\t\n\t\n\tpass\n\t\n\tdef __init__(self, ):\n\t\tpass\n\t\n\tdef __len__(self, ):\n\t\tpass\n\t\n\tdef __cmp__(self, ):\n\t\tpass\n\t\n\tdef __repr__(self, ):\n\t\tpass\n\t\n\tdef __hash__(self, ):\n\t\tpass\n\t\n\tdef __getitem__(self, ):\n\t\tpass\n\t\n\tdef __setitem__(self, ):\n\t\tpass\n\t\n\tdef __delitem__(self, ):\n\t\tpass\n\t\n\tdef __contains__(self, ):\n\t\tpass\n\t\n\tdef has_key(self, ):\n\t\tpass\n\t\n\tdef setdefault(self, ):\n\t\tpass\n\t\n\tdef update(self, ):\n\t\tpass\n\t\n\tdef keys(self, ):\n\t\tpass\n\t\n\tdef items(self, ):\n\t\tpass\n\t\n\tdef values(self, ):\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\ttry:\n\t\treturn self.__getitem__(key)\n\texcept KeyError:\n\t\treturn default\n\n\tdef has_key(self, key):\n\t\t\"\"\"\n\t\tD.has_key(k) -> 1 if D.has_key(k), else 0\n\t\t\"\"\"\n\t\treturn self.__contains__(key)\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\t\"\"\"\n\t\treturn self.__iter__()\n\n\tdef iteritems(self):\n\t\t\"\"\"\n\t\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\t\"\"\"\n\t\treturn self.__iter__()\n\n\tdef iterkeys(self):\n\t\t\"\"\"\n\t\tD.iterkeys() -> an iterator over the keys of D\n\t\t\"\"\"\n\t\tfor k, v in self.__iter__():\n\t\t\tyield k\n\n\tdef itervalues(self):\n\t\t\"\"\"\n\t\tD.itervalues() ->\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\treturn self.get_or_default(key, default)\n\n\tdef get_or_default(self, key, default):\n\t\ttry:\n\t\t\treturn self.get_by_key(key)\n\t\texcept KeyError:\n\t\t\treturn default\n\n\tdef get_by_key(self, key):\n\t\ttry:\n\t\t\treturn self.__dict__[key]\n\t\texcept KeyError:\n\t\t\treturn self.__dict__[key.lower()]\n\n\tdef __getitem__(self, key):\n\t\treturn self.get_by_key(key)\n\n\tdef __setitem__(self, key, value):\n\t\tself.__dict__[key] = value\n\n\tdef keys(self):\n\t\treturn self.__dict__.keys()\n\n\tdef values(self):\n\t\treturn self.__dict__.values()\n\n\tdef items(self):\n\t\treturn self.__dict__.items()\n\n\tdef __contains__(self, key):\n\t\treturn key in self.__dict__ or key.lower() in self.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\tdef has_key(self, key):\n\t\"\"\"\n\tD.has_key(k) -> True if D has a key k, else False\n\t\"\"\"\n\treturn self.__dict__.has_key(key)\n\n\tdef items(self):\n\t\"\"\"\n\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\"\"\"\n\treturn self.__dict__.items()\n\n\tdef iteritems(self):\n\t\"\"\"\n\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\"\"\"\n\treturn self.__dict__.iteritems()\n\n\tdef iterkeys(self):\n\t\"\"\"\n\tD.iterkeys() -> an iterator over the keys of D\n\t\"\"\"\n\treturn self.__dict__.iterkeys()\n\n\tdef itervalues(self):\n\t\"\"\"\n\tD.itervalues() -> an iterator over the values of D\n\t\"\"\"\n\treturn self.__dict__.itervalues()\n\n\tdef keys(self):\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n\tdef has_key(self, key):\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\texcept KeyError:\n\t\t\treturn False\n\t\treturn True\n\n\tdef items(self):\n\t\t\"\"\"\n\t\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\t\"\"\"\n\t\treturn zip(self.keys(), self.values())\n\n\tdef iteritems(self):\n\t\t\"\"\"\n\t\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\t\"\"\"\n\t\tfor key in self.keys():\n\t\t\tyield (key, self[key])\n\n\tdef iterkeys(self):\n\t\t\"\"\"\n\t\tD.iterkeys() -> an iterator over the keys of D\n\t\t\"\"\"\n\t\treturn self.keys().__iter__()\n\n\tdef itervalues(self):\n\t\t\"\"\"\n\t\tD.itervalues() -> an iterator over the values of D\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\ttry:\n\t    return self[key]\n\texcept KeyError:\n\t    return default\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\t\treturn self.__dict__.get(key, default)\n\t\n\tdef has_key(self, key):\n\t\"\"\"\n\tD.has_key(k) -> 1 if D.has_key(k), else 0\n\t\"\"\"\n\t\treturn self.__dict__.has_key(key)\n\t\n\tdef items(self):\n\t\"\"\"\n\tD.items() -> list of D's (key, value) pairs, as 2-tuples\n\t\"\"\"\n\t\treturn self.__dict__.items()\n\t\n\tdef iteritems(self):\n\t\"\"\"\n\tD.iteritems() -> an iterator over the (key, value) items of D\n\t\"\"\"\n\t\treturn self.__dict__.iteritems()\n\t\n\tdef iterkeys(self):\n\t\"\"\"\n\tD.iterkeys() -> an iterator over the keys of D\n\t\"\"\"\n\t\treturn self.__dict__.iterkeys()\n\t\n\tdef itervalues(self):\n\t\"\"\"\n\tD.itervalues() -> an iterator over the values of D\n\t\"\"\"\n\t\treturn self.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\treturn self.__getitem__(key, default)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "ques_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n\tif lock is None:\n\t\tlock = Lock()\n\tdef decorate(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not isinstance(args, collections.Hashable):\n\t\t\t\t# uncacheable. a list, for instance.\n\t\t\t\t# better to not cache than blow up.\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tcache[key(func, args, kwargs)] = result = func(*args, **kwargs)\n\t\t\t\t\treturn result\n\t\t\t\texcept TypeError:\n\t\t\t\t\t# uncacheable. a list, for instance.\n\t\t\t\t\t# better to not cache than blow up.\n\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn wrapper\n\t\treturn decorate\n\treturn decorate\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\td = cache.get(self, {})\n\t\t\t\tk = key(self, *args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\treturn d[k]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\td[k] = result\n\t\t\t\tcache[self] = d\n\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorate\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tk = key(*args, **kwargs)\n\t\t\t\tif k not in cache:\n\t\t\t\t\tcache[k] = func(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn cache[k]\n\t\treturn wrapper\n\treturn decorate\n\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tcache_key = key(self, *args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[cache_key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue = func(self, *args, **kwargs)\n\t\t\t\t\tcache[cache_key] = value\n\t\t\t\t\treturn value\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorate\n\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.get(key(args, kwargs), func, *args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn cache.get(key(args, kwargs), func, *args, **kwargs)\n\t\treturn wrapper\n\treturn decorate\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args):\n\t\t\tlock and lock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key(self, args)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(self, *args)\n\t\t\t\t\tcache[key(self, args)] = result\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tlock and lock.release()\n\t\treturn wrapper\n\treturn decorate\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(self, *args, **kw):\n\t\t\tk = key(self, *args, **kw)\n\t\t\ttry:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\treturn cache[k]\n\t\t\t\telse:\n\t\t\t\t\treturn cache[k]\n\t\t\texcept KeyError:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tv = cache[k] = func(self, *args, **kw)\n\t\t\t\telse:\n\t\t\t\t\tv = cache[k] = func(self, *args, **kw)\n\t\t\t\treturn v\n\t\treturn update_wrapper(wrapper, func)\n\treturn decorate\n\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kw):\n\t\t\tkey = key(func, self, args, kw)\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\twith lock:\n\t\t\t\t\ttry:\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tresult = func(self, *args, **kw)\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.get(key(self, *args, **kwargs), func, self, *args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn cache.get(key(self, *args, **kwargs), func, self, *args, **kwargs)\n\t\treturn update_wrapper(wrapper, func)\n\treturn decorate\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\t_key = key(self, args, kwargs)\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\tif _key not in cache:\n\t\t\t\t\t\tcache[_key] = func(self, *args, **kwargs)\n\t\t\telse:\n\t\t\t\tif _key not in cache:\n\t\t\t\t\tcache[_key] = func(self, *args, **kwargs)\n\t\t\treturn cache[_key]\n\t\treturn wrapper\n\treturn decorate\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tif lock is None:\n\t\tlock = Lock()\n\tdef decorate(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not isinstance(args, collections.Hashable):\n\t\t\t\t# uncacheable. a list, for instance.\n\t\t\t\t# better to not cache than blow up.\n\t\t\t\treturn func(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\ttry:\n\t\t\t\t\tcache[key(func, args, kwargs)] = result = func(*args, **kwargs)\n\t\t\t\t\treturn result\n\t\t\t\texcept TypeError:\n\t\t\t\t\t# uncacheable. a list, for instance.\n\t\t\t\t\t# better to not cache than blow up.\n\t\t\t\t\treturn func(*args, **kwargs)\n\t\t\treturn wrapper\n\t\treturn decorate\n\treturn decorate\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\td = cache.get(self, {})\n\t\t\t\tk = key(self, *args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\treturn d[k]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tpass\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\td[k] = result\n\t\t\t\tcache[self] = d\n\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tk = key(*args, **kwargs)\n\t\t\t\tif k not in cache:\n\t\t\t\t\tcache[k] = func(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn cache[k]\n\t\treturn wrapper\n\treturn decorate\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tcache_key = key(self, *args, **kwargs)\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[cache_key]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue = func(self, *args, **kwargs)\n\t\t\t\t\tcache[cache_key] = value\n\t\t\t\t\treturn value\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorate\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.get(key(args, kwargs), func, *args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn cache.get(key(args, kwargs), func, *args, **kwargs)\n\t\treturn wrapper\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args):\n\t\t\tlock and lock.acquire()\n\t\t\ttry:\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[key(self, args)]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = func(self, *args)\n\t\t\t\t\tcache[key(self, args)] = result\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tlock and lock.release()\n\t\treturn wrapper\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(self, *args, **kw):\n\t\t\tk = key(self, *args, **kw)\n\t\t\ttry:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\treturn cache[k]\n\t\t\t\telse:\n\t\t\t\t\treturn cache[k]\n\t\t\texcept KeyError:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tv = cache[k] = func(self, *args, **kw)\n\t\t\t\telse:\n\t\t\t\t\tv = cache[k] = func(self, *args, **kw)\n\t\t\t\treturn v\n\t\treturn update_wrapper(wrapper, func)\n\treturn decorate\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kw):\n\t\t\tkey = key(func, self, args, kw)\n\t\t\ttry:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache[key]\n\t\t\texcept KeyError:\n\t\t\t\twith lock:\n\t\t\t\t\ttry:\n\t\t\t\t\t\treturn cache[key]\n\t\t\t\t\texcept KeyError:\n\t\t\t\t\t\tresult = func(self, *args, **kw)\n\t\t\t\t\t\tcache[key] = result\n\t\t\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\treturn cache.get(key(self, *args, **kwargs), func, self, *args, **kwargs)\n\t\t\telse:\n\t\t\t\treturn cache.get(key(self, *args, **kwargs), func, self, *args, **kwargs)\n\t\treturn update_wrapper(wrapper, func)\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorate(func):\n\t\t@wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\t_key = key(self, args, kwargs)\n\t\t\tif lock is not None:\n\t\t\t\twith lock:\n\t\t\t\t\tif _key not in cache:\n\t\t\t\t\t\tcache[_key] = func(self, *args, **kwargs)\n\t\t\telse:\n\t\t\t\tif _key not in cache:\n\t\t\t\t\tcache[_key] = func(self, *args, **kwargs)\n\t\t\treturn cache[_key]\n\t\treturn wrapper\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "ques_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n\treturn cls._extostr(e, max_level, max_path_level)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\treturn cls.extostr_ex(e, max_level=max_level, max_path_level=max_path_level,\n\t\t\t\t\t\t  with_thread_id=False, with_hostname=False)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t# Get the exception call stack\n\ttb = traceback.extract_tb(sys.exc_info()[2])\n\t# Get the exception type and value\n\tt,v = sys.exc_info()[:2]\n\t# Format the exception type and value\n\tif t is None:\n\t\tt = \"None\"\n\tif v is None:\n\t\tv = \"None\"\n\t# Format the exception stack\n\tif len(tb) > max_level:\n\t\ttb = tb[:max_level]\n\ttb = list(map(lambda tb0: (os.path.basename(tb0[0]), tb0[1], tb0[2], tb0[3]), tb))\n\tif len(tb) > max_path_level:\n\t\ttb = tb[:max_path_level]\n\ttb = list(map(lambda tb0: (os.path.basename(tb0[0]), tb0[1], tb0[2], tb0[3]), tb))\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tres = cls.ex2dict(e, max_level, max_path_level)\n\treturn cls.dicttostr(res)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level is None: max_level = 30\n\tif max_path_level is None: max_path_level = 5\n\ts = []\n\twhile e:\n\t\ttb = sys.exc_info()[2]\n\t\ts.append(\"%s: %s\" % (e.__class__.__name__, str(e)))\n\t\twhile tb and max_level > 0:\n\t\t\tf = tb.tb_frame\n\t\t\tlineno = tb.tb_lineno\n\t\t\tco = f.f_code\n\t\t\tfilename = co.co_filename\n\t\t\tname = co.co_name\n\t\t\ts.append(\"  File %s, line %s, in %s\" % (filename, lineno, name))\n\t\t\ttb = tb.tb_next\n\t\t\tmax_level -= 1\n\t\t\tif max_level == 0:\n\t\t\t\ts.append(\"  ...\")\n\t\te = e.__cause__\n\treturn \"\\n\".join(s)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif not e:\n\t\treturn \"\"\n\tif not isinstance(e, Exception):\n\t\treturn cls.__format_exception_only(e)\n\t# Some exceptions, such as IOError, have special formatting for their arguments.\n\t# This check is a little more generic than the check for IOError, since it\n\t# also supports OSError.\n\tmsg = \"\"\n\tif len(e.args) == 1:\n\t\ttry:\n\t\t\tmsg = \"%s: %s\" % (e.__class__.__name__, e.args[0])\n\t\texcept:\n\t\t\tpass\n\tif not msg:\n\t\tmsg = \"%s: %s\" % (e.__class__.__name__, e)\n\treturn cls.__format_exception(e, msg, max_level, max_path_level)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tcls.set_max_level(max_level)\n\tcls.set_max_path_level(max_path_level)\n\treturn cls.extostr_with_level(e, 0)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tf = cls.extofile(e, max_level, max_path_level)\n\ts = f.read()\n\tf.close()\n\treturn s\n\n\t\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\t# remove the current module path\n\tpath_remove_level = max(max_path_level, 2)  # 2 to keep the current module\n\tpath_remove = '/'.join(__file__.split('/')[0:path_remove_level])\n\n\tinfo = cls.exception_info(e, max_level)\n\treturn '\\n'.join(info)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level <= 0:\n\t\treturn ''\n\ts = '%s: %s' % (type(e).__name__, str(e))\n\tif max_path_level > 0:\n\t\tfor f in traceback.extract_tb(sys.exc_info()[2]):\n\t\t\ts += '\\n  File \"%s\", line %d, in %s' % (\n\t\t\t\tos.path.basename(f[0]),\n\t\t\t\tf[1],\n\t\t\t\tf[2]\n\t\t\t)\n\tif hasattr(e, '__cause__'):\n\t\ts += '\\nCause: %s' % (cls.extostr(e.__cause__, max_level - 1, max_path_level - 1))\n\tif hasattr(e, '__context__'):\n\t\ts += '\\nContext: %s' % (cls.extostr(e.__context__, max_level - 1, max_path_level - 1))\n\treturn s\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\treturn cls._extostr(e, max_level, max_path_level)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\treturn cls.extostr_ex(e, max_level=max_level, max_path_level=max_path_level,\n\t\t\t\t\t\t  with_thread_id=False, with_hostname=False)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t# Get the exception call stack\n\ttb = traceback.extract_tb(sys.exc_info()[2])\n\t# Get the exception type and value\n\tt,v = sys.exc_info()[:2]\n\t# Format the exception type and value\n\tif t is None:\n\t\tt = \"None\"\n\tif v is None:\n\t\tv = \"None\"\n\t# Format the exception stack\n\tif len(tb) > max_level:\n\t\ttb = tb[:max_level]\n\ttb = list(map(lambda tb0: (os.path.basename(tb0[0]), tb0[1], tb0[2], tb0[3]), tb))\n\tif len(tb) > max_path_level:\n\t\ttb = tb[:max_path_level]\n\ttb = list(map(lambda tb0: (os.path.basename(tb0[0]), tb0[1], tb0[2], tb0[3]), tb))\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tres = cls.ex2dict(e, max_level, max_path_level)\n\treturn cls.dicttostr(res)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level is None: max_level = 30\n\tif max_path_level is None: max_path_level = 5\n\ts = []\n\twhile e:\n\t\ttb = sys.exc_info()[2]\n\t\ts.append(\"%s: %s\" % (e.__class__.__name__, str(e)))\n\t\twhile tb and max_level > 0:\n\t\t\tf = tb.tb_frame\n\t\t\tlineno = tb.tb_lineno\n\t\t\tco = f.f_code\n\t\t\tfilename = co.co_filename\n\t\t\tname = co.co_name\n\t\t\ts.append(\"  File %s, line %s, in %s\" % (filename, lineno, name))\n\t\t\ttb = tb.tb_next\n\t\t\tmax_level -= 1\n\t\t\tif max_level == 0:\n\t\t\t\ts.append(\"  ...\")\n\t\te = e.__cause__\n\treturn \"\\n\".join(s)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif not e:\n\t\treturn \"\"\n\tif not isinstance(e, Exception):\n\t\treturn cls.__format_exception_only(e)\n\t# Some exceptions, such as IOError, have special formatting for their arguments.\n\t# This check is a little more generic than the check for IOError, since it\n\t# also supports OSError.\n\tmsg = \"\"\n\tif len(e.args) == 1:\n\t\ttry:\n\t\t\tmsg = \"%s: %s\" % (e.__class__.__name__, e.args[0])\n\t\texcept:\n\t\t\tpass\n\tif not msg:\n\t\tmsg = \"%s: %s\" % (e.__class__.__name__, e)\n\treturn cls.__format_exception(e, msg, max_level, max_path_level)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tcls.set_max_level(max_level)\n\tcls.set_max_path_level(max_path_level)\n\treturn cls.extostr_with_level(e, 0)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tf = cls.extofile(e, max_level, max_path_level)\n\ts = f.read()\n\tf.close()\n\treturn s\n\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\t# remove the current module path\n\tpath_remove_level = max(max_path_level, 2)  # 2 to keep the current module\n\tpath_remove = '/'.join(__file__.split('/')[0:path_remove_level])\n\n\tinfo = cls.exception_info(e, max_level)\n\treturn '\\n'.join(info)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level <= 0:\n\t\treturn ''\n\ts = '%s: %s' % (type(e).__name__, str(e))\n\tif max_path_level > 0:\n\t\tfor f in traceback.extract_tb(sys.exc_info()[2]):\n\t\t\ts += '\\n  File \"%s\", line %d, in %s' % (\n\t\t\t\tos.path.basename(f[0]),\n\t\t\t\tf[1],\n\t\t\t\tf[2]\n\t\t\t)\n\tif hasattr(e, '__cause__'):\n\t\ts += '\\nCause: %s' % (cls.extostr(e.__cause__, max_level - 1, max_path_level - 1))\n\tif hasattr(e, '__context__'):\n\t\ts += '\\nContext: %s' % (cls.extostr(e.__context__, max_level - 1, max_path_level - 1))\n\treturn s\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "ques_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn append_buffer_to_file(file_name, text_buffer.encode(encoding), overwrite)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn _tds.append_text_to_file(file_name, text_buffer, encoding, overwrite)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\n\tif not overwrite and os.path.exists(file_name): return -1\n\ttry:\n\t\twith open(file_name, 'wb') as f:\n\t\t\tf.write(text_buffer.encode(encoding))\n\t\treturn os.path.getsize(file_name)\n\texcept: return -1\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn _py_vscp_helper.append_text_to_file(file_name, text_buffer, encoding, overwrite)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tret = -1\n\tmode = 'a+'\n\tif overwrite:\n\t\tmode = 'w+'\n\ttry:\n\t\tf = open(file_name, mode, encoding=encoding)\n\t\tf.write(text_buffer)\n\t\tret = len(text_buffer)\n\t\tf.close()\n\texcept:\n\t\tret = -1\n\treturn(ret)\n\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t# Check parameters\n\tif (file_name is None) or (len(file_name) < 1):\n\t\treturn -1\n\tif (text_buffer is None) or (len(text_buffer) < 1):\n\t\treturn -1\n\tif (encoding is None) or (len(encoding) < 1):\n\t\treturn -1\n\t# Open the file\n\ttry:\n\t\tfile_handle = open(file_name, \"ab\" if not overwrite else \"wb\")\n\texcept Exception:\n\t\treturn -1\n\t# Write the content\n\ttry:\n\t\tnb_bytes = file_handle.write(text_buffer.encode(encoding))\n\texcept Exception:\n\t\tnb_bytes = -1\n\t# Close the file\n\tfile_handle.close()\n\t# Return the number of bytes written\n\treturn nb_bytes\n\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn libxml2mod.xmlSaveFileEnc(file_name, text_buffer, encoding, overwrite)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif file_name is None:\n\t\traise ValueError(\"file_name must not be None\")\n\tif text_buffer is None:\n\t\traise ValueError(\"text_buffer must not be None\")\n\tif encoding is None:\n\t\traise ValueError(\"encoding must not be None\")\n\tmode = \"a\" if not overwrite else \"w\"\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tcount = f.write(text_buffer)\n\treturn count\n\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif not file_name:\n\t\traise ValueError(\"file_name is None\")\n\tif not text_buffer:\n\t\traise ValueError(\"text_buffer is None\")\n\tif not encoding:\n\t\traise ValueError(\"encoding is None\")\n\tbuf = text_buffer.encode(encoding)\n\treturn write_bytes_to_file(file_name, buf, overwrite)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t# Create the file if necessary\n\tif not os.path.exists(file_name):\n\t\tfile_handle = open(file_name, 'w+', encoding=encoding)\n\telse:\n\t\t# If not overwrite, append to the file\n\t\tif not overwrite:\n\t\t\tfile_handle = open(file_name, 'a', encoding=encoding)\n\t\telse:\n\t\t\tfile_handle = open(file_name, 'w', encoding=encoding)\n\t\t\t# Write the buffer to the file\n\tfile_handle.write(text_buffer)\n\t# Return the number of bytes written\n\treturn len(text_buffer)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn append_buffer_to_file(file_name, text_buffer.encode(encoding), overwrite)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn _tds.append_text_to_file(file_name, text_buffer, encoding, overwrite)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t\n\tif not overwrite and os.path.exists(file_name): return -1\n\ttry:\n\t\twith open(file_name, 'wb') as f:\n\t\t\tf.write(text_buffer.encode(encoding))\n\t\treturn os.path.getsize(file_name)\n\texcept: return -1\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn _py_vscp_helper.append_text_to_file(file_name, text_buffer, encoding, overwrite)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tret = -1\n\tmode = 'a+'\n\tif overwrite:\n\t\tmode = 'w+'\n\ttry:\n\t\tf = open(file_name, mode, encoding=encoding)\n\t\tf.write(text_buffer)\n\t\tret = len(text_buffer)\n\t\tf.close()\n\texcept:\n\t\tret = -1\n\treturn(ret)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t# Check parameters\n\tif (file_name is None) or (len(file_name) < 1):\n\t\treturn -1\n\tif (text_buffer is None) or (len(text_buffer) < 1):\n\t\treturn -1\n\tif (encoding is None) or (len(encoding) < 1):\n\t\treturn -1\n\t# Open the file\n\ttry:\n\t\tfile_handle = open(file_name, \"ab\" if not overwrite else \"wb\")\n\texcept Exception:\n\t\treturn -1\n\t# Write the content\n\ttry:\n\t\tnb_bytes = file_handle.write(text_buffer.encode(encoding))\n\texcept Exception:\n\t\tnb_bytes = -1\n\t# Close the file\n\tfile_handle.close()\n\t# Return the number of bytes written\n\treturn nb_bytes\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\treturn libxml2mod.xmlSaveFileEnc(file_name, text_buffer, encoding, overwrite)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif file_name is None:\n\t\traise ValueError(\"file_name must not be None\")\n\tif text_buffer is None:\n\t\traise ValueError(\"text_buffer must not be None\")\n\tif encoding is None:\n\t\traise ValueError(\"encoding must not be None\")\n\tmode = \"a\" if not overwrite else \"w\"\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tcount = f.write(text_buffer)\n\treturn count\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif not file_name:\n\t\traise ValueError(\"file_name is None\")\n\tif not text_buffer:\n\t\traise ValueError(\"text_buffer is None\")\n\tif not encoding:\n\t\traise ValueError(\"encoding is None\")\n\tbuf = text_buffer.encode(encoding)\n\treturn write_bytes_to_file(file_name, buf, overwrite)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t# Create the file if necessary\n\tif not os.path.exists(file_name):\n\t\tfile_handle = open(file_name, 'w+', encoding=encoding)\n\telse:\n\t\t# If not overwrite, append to the file\n\t\tif not overwrite:\n\t\t\tfile_handle = open(file_name, 'a', encoding=encoding)\n\t\telse:\n\t\t\tfile_handle = open(file_name, 'w', encoding=encoding)\n\t\t\t# Write the buffer to the file\n\tfile_handle.write(text_buffer)\n\t# Return the number of bytes written\n\treturn len(text_buffer)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "ques_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n\tif not os.path.isfile(file_name):\n\t\treturn None\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith io.open(file_name, mode=\"r\", encoding=encoding) as f:\n\t\t\treturn f.read()\n\texcept IOError:\n\t\treturn None\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, \"r\", encoding=encoding) as f:\n\t\t\treturn f.read()\n\texcept IOError as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\texcept UnicodeDecodeError as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\n", "def file_to_textbuffer(file_name, encoding):\n\t# Open the file.\n\tf = None\n\ttry:\n\t\tf = open(file_name, 'r', encoding=encoding)\n\t\tbuffer = f.read()\n\t\tf.close()\n\texcept:\n\t\tif f:\n\t\t\tf.close()\n\t\treturn None\n\treturn buffer\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as f:\n\t\t\ttext = f.read()\n\texcept Exception as e:\n\t\tlogging.error(\"Unable to read '%s' : %s\" % (file_name, e))\n\t\treturn None\n\treturn text\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, \"r\", encoding=encoding) as file:\n\t\t\ttext = file.read()\n\t\treturn text\n\texcept:\n\t\treturn None\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\tif not os.path.exists(file_name):\n\t\treturn None\n\twith open(file_name, 'rb') as fd:\n\t\tdata = fd.read()\n\t\ttext = data.decode(encoding, errors='ignore')\n\treturn text\n\n", "def file_to_textbuffer(file_name, encoding):\n\tif os.path.isfile(file_name):\n\t\twith open(file_name, 'r', encoding=encoding) as file:\n\t\t\treturn file.read()\n\telse:\n\t\treturn None\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, encoding=encoding) as f:\n\t\t\treturn f.read()\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\tif not os.path.isfile(file_name):\n\t\treturn None\n\ttry:\n\t\twith open(file_name, \"r\") as f:\n\t\t\ttext = f.read()\n\t\treturn text\n\texcept:\n\t\treturn None\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n\tif not os.path.isfile(file_name):\n\t\treturn None\n\twith open(file_name, 'r', encoding=encoding) as f:\n\t\treturn f.read()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith io.open(file_name, mode=\"r\", encoding=encoding) as f:\n\t\t\treturn f.read()\n\texcept IOError:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, \"r\", encoding=encoding) as f:\n\t\t\treturn f.read()\n\texcept IOError as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\texcept UnicodeDecodeError as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t# Open the file.\n\tf = None\n\ttry:\n\t\tf = open(file_name, 'r', encoding=encoding)\n\t\tbuffer = f.read()\n\t\tf.close()\n\texcept:\n\t\tif f:\n\t\t\tf.close()\n\t\treturn None\n\treturn buffer\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, 'r', encoding=encoding) as f:\n\t\t\ttext = f.read()\n\texcept Exception as e:\n\t\tlogging.error(\"Unable to read '%s' : %s\" % (file_name, e))\n\t\treturn None\n\treturn text\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, \"r\", encoding=encoding) as file:\n\t\t\ttext = file.read()\n\t\treturn text\n\texcept:\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\tif not os.path.exists(file_name):\n\t\treturn None\n\twith open(file_name, 'rb') as fd:\n\t\tdata = fd.read()\n\t\ttext = data.decode(encoding, errors='ignore')\n\treturn text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\tif os.path.isfile(file_name):\n\t\twith open(file_name, 'r', encoding=encoding) as file:\n\t\t\treturn file.read()\n\telse:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\twith open(file_name, encoding=encoding) as f:\n\t\t\treturn f.read()\n\texcept Exception as e:\n\t\tprint(e)\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\tif not os.path.isfile(file_name):\n\t\treturn None\n\ttry:\n\t\twith open(file_name, \"r\") as f:\n\t\t\ttext = f.read()\n\t\treturn text\n\texcept:\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "ques_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n\tif not file_name:\n\t\treturn False\n\treturn os.path.exists(file_name)\n\n\n", "def is_file_exist(file_name):\n\tif os.path.isfile(file_name):\n\t\treturn True\n\n\treturn False\n\n\n", "def is_file_exist(file_name):\n\tif file_name == None or len(file_name) == 0:\n\t\treturn False\n\tif os.path.isfile(file_name):\n\t\treturn True\n\treturn False\n\n\n", "def is_file_exist(file_name):\n\treturn os.path.exists(file_name)\n\n", "def is_file_exist(file_name):\n\tif (file_name and os.path.isfile(file_name)):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_file_exist(file_name):\n\tif not os.path.exists(file_name):\n\t\treturn False\n\tif not os.path.isfile(file_name):\n\t\treturn False\n\tif os.path.getsize(file_name) == 0:\n\t\treturn False\n\treturn True\n\n\n", "def is_file_exist(file_name):\n\tif is_file_name(file_name):\n\t\treturn os.path.isfile(file_name)\n\telse:\n\t\treturn False\n\n\n", "def is_file_exist(file_name):\n\tif file_name is None:\n\t\treturn False\n\tif not os.path.isfile(file_name):\n\t\treturn False\n\treturn True\n\n\n", "def is_file_exist(file_name):\n\tif file_name is None:\n\t\treturn False\n\tif os.path.isfile(file_name):\n\t\treturn True\n\treturn False\n", "def is_file_exist(file_name):\n\ttry:\n\t\tif not os.path.isfile(file_name):\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n\tif not file_name:\n\t\treturn False\n\treturn os.path.exists(file_name)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\tif os.path.isfile(file_name):\n\t\treturn True\n\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\tif file_name == None or len(file_name) == 0:\n\t\treturn False\n\tif os.path.isfile(file_name):\n\t\treturn True\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.exists(file_name)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\tif (file_name and os.path.isfile(file_name)):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\tif not os.path.exists(file_name):\n\t\treturn False\n\tif not os.path.isfile(file_name):\n\t\treturn False\n\tif os.path.getsize(file_name) == 0:\n\t\treturn False\n\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\tif is_file_name(file_name):\n\t\treturn os.path.isfile(file_name)\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_file_exist(file_name):\n\tif file_name is None:\n\t\treturn False\n\tif not os.path.isfile(file_name):\n\t\treturn False\n\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\tif file_name is None:\n\t\treturn False\n\tif os.path.isfile(file_name):\n\t\treturn True\n\treturn False\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\ttry:\n\t\tif not os.path.isfile(file_name):\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "ques_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n\t# Create a new logger\n\tcls.logger = logging.getLogger(cls.__name__)\n\tcls.logger.setLevel(logging.DEBUG)\n\t# Create a handler for outputting log to file\n\tfh = logging.FileHandler(os.path.join(cls.log_dir, cls.__name__ + '.log'))\n\tfh.setLevel(logging.INFO)\n\t# Create a handler for outputting log to console\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\t# Create a formatter\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t# Add formatter to handlers\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\t# Add handlers to logger\n\tcls.logger.addHandler(fh)\n\tcls.logger.addHandler(ch)\n\n\n", "def _reset_logging(cls):\n\traise NotImplementedError()\n\t\n", "def _reset_logging(cls):\n\tcls._logger.info(\"Resetting logging\")\n\tcls._logger.handlers = []\n\n\t# create a file handler\n\thandler = logging.FileHandler(cls._logfile, mode=\"w\")\n\thandler.setLevel(logging.INFO)\n\n\t# create a logging format\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\thandler.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tcls._logger.addHandler(handler)\n\n\tcls._logger.info(\"Reset logging\")\n\n", "def _reset_logging(cls):\n\tfor handler in cls.logger.handlers[:]:\n\t\tcls.logger.removeHandler(handler)\n\n\tcls.logger = logging.getLogger(cls.__name__)\n\tcls.logger.setLevel(logging.DEBUG)\n\thandler = logging.StreamHandler(sys.stdout)\n\thandler.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tcls.logger.addHandler(handler)\n\n", "def _reset_logging(cls):\n\tcls._logger = None\n\tcls._logger_proxy = None\n\t\n\t\n", "def _reset_logging(cls):\n\tcls._loggers = {}\n\n\n", "def _reset_logging(cls):\n\t# loggers\n\tfor logger_name in logger_names:\n\t\tif hasattr(cls, logger_name):\n\t\t\tdelattr(cls, logger_name)\n\t# handlers\n\tfor handler in cls.__handlers:\n\t\thandler.close()\n\t\tlogging.root.removeHandler(handler)\n\tcls.__handlers = []\n\t# reset base logger\n\tlogging.root.setLevel(logging.NOTSET)\n\tlogging.root.handlers = []\n\t# reset loggers\n\tfor logger in logging.Logger.manager.loggerDict.values():\n\t\tlogger.setLevel(logging.NOTSET)\n\t\tlogger.handlers = []\n\t\tlogger.propagate = True\n\n\n", "def _reset_logging(cls):\n\tcls._logger = None\n\tcls._logger_handler = None\n\n\n", "def _reset_logging(cls):\n\t# get the logger\n\tlogger = cls.logger()\n\t# reset the logger\n\tlogger.reset()\n\t# reinitialize the logger\n\tcls.logger(logger.level)\n\n", "def _reset_logging(cls):\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\tlogging.basicConfig(level=_DEFAULT_LOG_LEVEL)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n\t# Create a new logger\n\tcls.logger = logging.getLogger(cls.__name__)\n\tcls.logger.setLevel(logging.DEBUG)\n\t# Create a handler for outputting log to file\n\tfh = logging.FileHandler(os.path.join(cls.log_dir, cls.__name__ + '.log'))\n\tfh.setLevel(logging.INFO)\n\t# Create a handler for outputting log to console\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\t# Create a formatter\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t# Add formatter to handlers\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\t# Add handlers to logger\n\tcls.logger.addHandler(fh)\n\tcls.logger.addHandler(ch)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\traise NotImplementedError()\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tcls._logger.info(\"Resetting logging\")\n\tcls._logger.handlers = []\n\n\t# create a file handler\n\thandler = logging.FileHandler(cls._logfile, mode=\"w\")\n\thandler.setLevel(logging.INFO)\n\n\t# create a logging format\n\tformatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\thandler.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tcls._logger.addHandler(handler)\n\n\tcls._logger.info(\"Reset logging\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tfor handler in cls.logger.handlers[:]:\n\t\tcls.logger.removeHandler(handler)\n\n\tcls.logger = logging.getLogger(cls.__name__)\n\tcls.logger.setLevel(logging.DEBUG)\n\thandler = logging.StreamHandler(sys.stdout)\n\thandler.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tcls.logger.addHandler(handler)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tcls._logger = None\n\tcls._logger_proxy = None\n\t\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tcls._loggers = {}\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\t# loggers\n\tfor logger_name in logger_names:\n\t\tif hasattr(cls, logger_name):\n\t\t\tdelattr(cls, logger_name)\n\t# handlers\n\tfor handler in cls.__handlers:\n\t\thandler.close()\n\t\tlogging.root.removeHandler(handler)\n\tcls.__handlers = []\n\t# reset base logger\n\tlogging.root.setLevel(logging.NOTSET)\n\tlogging.root.handlers = []\n\t# reset loggers\n\tfor logger in logging.Logger.manager.loggerDict.values():\n\t\tlogger.setLevel(logging.NOTSET)\n\t\tlogger.handlers = []\n\t\tlogger.propagate = True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tcls._logger = None\n\tcls._logger_handler = None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\t# get the logger\n\tlogger = cls.logger()\n\t# reset the logger\n\tlogger.reset()\n\t# reinitialize the logger\n\tcls.logger(logger.level)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\tlogging.basicConfig(level=_DEFAULT_LOG_LEVEL)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "ques_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n\traise NotImplementedError()\n\n\n", "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "def _getTargetClass(self):\n\traise NotImplementedError\n\n", "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "def _getTargetClass(self):\n\traise NotImplementedError('subclasses of BaseCookie must define _getTargetClass()')\n", "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "def _getTargetClass(self):\n\traise NotImplementedError('subclasses must override')\n\n\n", "def _getTargetClass(self):\n\traise NotImplementedError()\n\n\n", "def _getTargetClass(self):\n\timpl = fallback.getImpl()\n\tif impl == \"c\":\n\t\treturn PyObject\n\telse:\n\t\treturn FallbackObject\n\n", "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError('subclasses of BaseCookie must define _getTargetClass()')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError('subclasses must override')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\timpl = fallback.getImpl()\n\tif impl == \"c\":\n\t\treturn PyObject\n\telse:\n\t\treturn FallbackObject\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "ques_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n\t# We use a set to represent the ordering, since we don't care\n\t# about duplicates.\n\tordering = set()\n\tfor o in orderings:\n\t\tordering = set(o).union(ordering)\n\treturn list(ordering)\n\n", "def _legacy_mergeOrderings(orderings):\n\tif orderings is None:\n\t\treturn None\n\tif isinstance(orderings, dict):\n\t\torderings = orderings.items()\n\tif isinstance(orderings, (list, tuple)):\n\t\titems = []\n\t\tfor item in orderings:\n\t\t\tif isinstance(item, dict):\n\t\t\t\titems.extend(item.items())\n\t\t\telif isinstance(item, (list, tuple)):\n\t\t\t\titems.extend(item)\n\t\t\telse:\n\t\t\t\titems.append(item)\n\t\torderings = items\n\tordering = []\n\tfor item in orderings:\n\t\tif item not in ordering:\n\t\t\tordering.append(item)\n\treturn ordering\n\n\n", "def _legacy_mergeOrderings(orderings):\n\tif not orderings:\n\t\treturn []\n\t# We don't want to modify the input, so make a copy\n\torderings = [ordering[:] for ordering in orderings]\n\t# Remove redundant orderings\n\torderings.sort(key=len)\n\tfor ordering in orderings:\n\t\tfor other in orderings:\n\t\t\tif ordering == other:\n\t\t\t\tcontinue\n\t\t\tif set(ordering).issubset(set(other)):\n\t\t\t\tordering.clear()\n\t# Remove empty orderings\n\torderings = [ordering for ordering in orderings if ordering]\n\t# Now append each ordering to the end of the current ordering\n\tpos = 0\n\twhile orderings:\n\t\t# Find the ordering with the highest position\n\t\thighest = None\n\t\tfor ordering in orderings:\n\t\t\tif highest is None or ordering[0] > highest:\n\t\t\t\thighest = ordering[0]\n\t\t# Remove the next element from each ordering\n\t\tfor ordering in orderings:\n\t\t\tif ordering[0] != highest:\n\t\t\t\tordering\n", "def _legacy_mergeOrderings(orderings):\n\tfrom copy import copy\n\t# this is O(n^2), but it should be fine for a small number of orderings\n\t# and a small amount of objects in each ordering\n\tnew_ordering = []\n\tobjects = []\n\tfor ordering in orderings:\n\t\tfor obj in ordering:\n\t\t\tif obj not in objects:\n\t\t\t\tobjects.append(obj)\n\t\t\t\tnew_ordering.append(obj)\n\treturn new_ordering\n\n\n", "def _legacy_mergeOrderings(orderings):\n\torderings = [tuple(ordering) for ordering in orderings]\n\t# sort orderings by length\n\torderings.sort(key=len)\n\n\t# merge orderings\n\tmerged = []\n\tseen = set()\n\twhile orderings:\n\t\t# first, get the next ordering\n\t\tordering = orderings.pop(0)\n\t\t# if we've already seen the first item in this ordering, we're done\n\t\tif ordering[0] in seen:\n\t\t\tcontinue\n\t\t# otherwise, add the suffix that starts with this item to the result\n\t\tsuffix = [ordering[0]]\n\t\tfor item in ordering[1:]:\n\t\t\tsuffix.append(item)\n\t\t\tseen.add(item)\n\t\tmerged.extend(suffix)\n\n\treturn merged\n\n\n", "def _legacy_mergeOrderings(orderings):\n\torderings = list(orderings)\n\t#print '_mergeOrderings: %s' % orderings\n\tordering = orderings.pop()\n\twhile orderings:\n\t\tother = orderings.pop()\n\t\t#print '  other: %s' % other\n\t\tfor n, item in enumerate(ordering):\n\t\t\t#print '   %s %s' % (n, item)\n\t\t\tif item in other:\n\t\t\t\tsuffix = ordering[n:]\n\t\t\t\tother_suffix = other[other.index(item):]\n\t\t\t\tif suffix != other_suffix:\n\t\t\t\t\tordering = ordering[:n] + other_suffix\n\t\t\t\tbreak\n\treturn ordering\n\n", "def _legacy_mergeOrderings(orderings):\n\tif len(orderings) == 0:\n\t\treturn []\n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\torderings = [list(o) for o in orderings]\n\thead, tail = orderings[0], orderings[1:]\n\tfor ordering in tail:\n\t\tfor i, value in enumerate(ordering):\n\t\t\tif value in head:\n\t\t\t\tordering = ordering[i:]\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise ValueError(\"Incompatible orderings\")\n\t\thead = [v for v in head if v in ordering] + ordering\n\treturn head\n\n\n", "def _legacy_mergeOrderings(orderings):\n\t# note that we don't need to worry about one ordering being a\n\t# subsequence of another, since these two cases are equivalent:\n\t#\n\t# ['a', 'b', 'c'] + ['b', 'c'] == ['a', 'b', 'c']\n\t# ['a', 'b', 'c'] + ['b', 'c', 'd'] == ['a', 'b', 'c', 'd']\n\t#\n\t# so we can sort the orderings by length, which makes the code\n\t# below simpler.\n\torderings.sort(key=len)\n\t#\n\t# now we merge the orderings in order from longest to shortest.\n\t#\n\t# this will ensure that we preserve the ordering of all objects\n\t# that appear in the longer orderings.\n\t#\n\t# it will also ensure that we don't introduce any duplicates,\n\t# since the longest ordering will always contain the entire\n\t# suffix that is shared between any two orderings.\n\t#\n\t# it also makes the code below simpler, since we can always\n\t# assume that the orderings we're merging are disj\n", "def _legacy_mergeOrderings(orderings):\n\t# TODO: This is not correct for all test cases\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in merged:\n\t\t\t\tmerged.append(item)\n\treturn merged\n\n", "def _legacy_mergeOrderings(orderings):\n\tsuffixes = []\n\torder = []\n\tfor ordering in orderings:\n\t\t# find the suffixes that begin with each element of the ordering\n\t\tsuffixes = []\n\t\tfor element in ordering:\n\t\t\tsuffixes.extend([s for s in suffixes if s[0] == element])\n\t\t\tsuffixes.append([element])\n\t\t# if it's a new element, add it to the order\n\t\tfor suffix in suffixes:\n\t\t\tfor element in suffix:\n\t\t\t\tif element not in order:\n\t\t\t\t\torder.append(element)\n\treturn order\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n\t# We use a set to represent the ordering, since we don't care\n\t# about duplicates.\n\tordering = set()\n\tfor o in orderings:\n\t\tordering = set(o).union(ordering)\n\treturn list(ordering)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tif orderings is None:\n\t\treturn None\n\tif isinstance(orderings, dict):\n\t\torderings = orderings.items()\n\tif isinstance(orderings, (list, tuple)):\n\t\titems = []\n\t\tfor item in orderings:\n\t\t\tif isinstance(item, dict):\n\t\t\t\titems.extend(item.items())\n\t\t\telif isinstance(item, (list, tuple)):\n\t\t\t\titems.extend(item)\n\t\t\telse:\n\t\t\t\titems.append(item)\n\t\torderings = items\n\tordering = []\n\tfor item in orderings:\n\t\tif item not in ordering:\n\t\t\tordering.append(item)\n\treturn ordering\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tif not orderings:\n\t\treturn []\n\t# We don't want to modify the input, so make a copy\n\torderings = [ordering[:] for ordering in orderings]\n\t# Remove redundant orderings\n\torderings.sort(key=len)\n\tfor ordering in orderings:\n\t\tfor other in orderings:\n\t\t\tif ordering == other:\n\t\t\t\tcontinue\n\t\t\tif set(ordering).issubset(set(other)):\n\t\t\t\tordering.clear()\n\t# Remove empty orderings\n\torderings = [ordering for ordering in orderings if ordering]\n\t# Now append each ordering to the end of the current ordering\n\tpos = 0\n\twhile orderings:\n\t\t# Find the ordering with the highest position\n\t\thighest = None\n\t\tfor ordering in orderings:\n\t\t\tif highest is None or ordering[0] > highest:\n\t\t\t\thighest = ordering[0]\n\t\t# Remove the next element from each ordering\n\t\tfor ordering in orderings:\n\t\t\tif ordering[0] != highest:\n\t\t\t\tordering\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tfrom copy import copy\n\t# this is O(n^2), but it should be fine for a small number of orderings\n\t# and a small amount of objects in each ordering\n\tnew_ordering = []\n\tobjects = []\n\tfor ordering in orderings:\n\t\tfor obj in ordering:\n\t\t\tif obj not in objects:\n\t\t\t\tobjects.append(obj)\n\t\t\t\tnew_ordering.append(obj)\n\treturn new_ordering\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\torderings = [tuple(ordering) for ordering in orderings]\n\t# sort orderings by length\n\torderings.sort(key=len)\n\n\t# merge orderings\n\tmerged = []\n\tseen = set()\n\twhile orderings:\n\t\t# first, get the next ordering\n\t\tordering = orderings.pop(0)\n\t\t# if we've already seen the first item in this ordering, we're done\n\t\tif ordering[0] in seen:\n\t\t\tcontinue\n\t\t# otherwise, add the suffix that starts with this item to the result\n\t\tsuffix = [ordering[0]]\n\t\tfor item in ordering[1:]:\n\t\t\tsuffix.append(item)\n\t\t\tseen.add(item)\n\t\tmerged.extend(suffix)\n\n\treturn merged\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\torderings = list(orderings)\n\t#print '_mergeOrderings: %s' % orderings\n\tordering = orderings.pop()\n\twhile orderings:\n\t\tother = orderings.pop()\n\t\t#print '  other: %s' % other\n\t\tfor n, item in enumerate(ordering):\n\t\t\t#print '   %s %s' % (n, item)\n\t\t\tif item in other:\n\t\t\t\tsuffix = ordering[n:]\n\t\t\t\tother_suffix = other[other.index(item):]\n\t\t\t\tif suffix != other_suffix:\n\t\t\t\t\tordering = ordering[:n] + other_suffix\n\t\t\t\tbreak\n\treturn ordering\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tif len(orderings) == 0:\n\t\treturn []\n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\torderings = [list(o) for o in orderings]\n\thead, tail = orderings[0], orderings[1:]\n\tfor ordering in tail:\n\t\tfor i, value in enumerate(ordering):\n\t\t\tif value in head:\n\t\t\t\tordering = ordering[i:]\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise ValueError(\"Incompatible orderings\")\n\t\thead = [v for v in head if v in ordering] + ordering\n\treturn head\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t# note that we don't need to worry about one ordering being a\n\t# subsequence of another, since these two cases are equivalent:\n\t#\n\t# ['a', 'b', 'c'] + ['b', 'c'] == ['a', 'b', 'c']\n\t# ['a', 'b', 'c'] + ['b', 'c', 'd'] == ['a', 'b', 'c', 'd']\n\t#\n\t# so we can sort the orderings by length, which makes the code\n\t# below simpler.\n\torderings.sort(key=len)\n\t#\n\t# now we merge the orderings in order from longest to shortest.\n\t#\n\t# this will ensure that we preserve the ordering of all objects\n\t# that appear in the longer orderings.\n\t#\n\t# it will also ensure that we don't introduce any duplicates,\n\t# since the longest ordering will always contain the entire\n\t# suffix that is shared between any two orderings.\n\t#\n\t# it also makes the code below simpler, since we can always\n\t# assume that the orderings we're merging are disj\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t# TODO: This is not correct for all test cases\n\tmerged = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in merged:\n\t\t\t\tmerged.append(item)\n\treturn merged\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tsuffixes = []\n\torder = []\n\tfor ordering in orderings:\n\t\t# find the suffixes that begin with each element of the ordering\n\t\tsuffixes = []\n\t\tfor element in ordering:\n\t\t\tsuffixes.extend([s for s in suffixes if s[0] == element])\n\t\t\tsuffixes.append([element])\n\t\t# if it's a new element, add it to the order\n\t\tfor suffix in suffixes:\n\t\t\tfor element in suffix:\n\t\t\t\tif element not in order:\n\t\t\t\t\torder.append(element)\n\treturn order\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "ques_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n\treturn ObjectSpecification(object).getInterfaces()\n\n", "def directlyProvidedBy(object):\n\treturn getInterfaces(object)\n\n\n", "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n", "def directlyProvidedBy(object):\n\treturn _objectProvides(object)\n\n", "def directlyProvidedBy(object):\n\treturn getAdapter(object, IDeclaration)\n\n\n", "def directlyProvidedBy(object):\n\treturn object.__provides__\n\n", "def directlyProvidedBy(object):\n\treturn _providedBy(object)\n\n\n", "def directlyProvidedBy(object):\n\tif _z3interfaces:\n\t\treturn zope.interface.directlyProvidedBy(object)\n\telse:\n\t\treturn zope.interface.providedBy(object)\n\t\n", "def directlyProvidedBy(object):\n\ttry:\n\t\treturn object.__provides__\n\texcept AttributeError:\n\t\treturn None\n\n", "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n\treturn ObjectSpecification(object).getInterfaces()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn getInterfaces(object)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn _objectProvides(object)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn getAdapter(object, IDeclaration)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn object.__provides__\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn _providedBy(object)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\tif _z3interfaces:\n\t\treturn zope.interface.directlyProvidedBy(object)\n\telse:\n\t\treturn zope.interface.providedBy(object)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\ttry:\n\t\treturn object.__provides__\n\texcept AttributeError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "ques_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n\tif not classes:\n\t\treturn []\n\n\t# Sort the classes\n\torderedClasses = sorted(classes, key=lambda cls: cls.__name__)\n\n\t# Remove any class that is a subclass of another\n\tfilteredClasses = []\n\tfor cls in orderedClasses:\n\t\tfor other in filteredClasses:\n\t\t\tif issubclass(cls, other):\n\t\t\t\tbreak\n\t\telse:\n\t\t\tfilteredClasses.append(cls)\n\n\treturn filteredClasses\n\n", "def minimalBases(classes):\n\treturn [b for b in classes if not isSubClass(classes, b)]\n\n", "def minimalBases(classes):\n\tbases = list(classes)\n\tfor i in range(len(bases)):\n\t\tfor j in range(i + 1, len(bases)):\n\t\t\tif issubclass(bases[i], bases[j]):\n\t\t\t\tbases[j] = bases[i]\n\treturn bases\n\n", "def minimalBases(classes):\n\tminimal = []\n\tfor base in classes:\n\t\tfor other in classes:\n\t\t\tif issubclass(base, other) and other not in minimal:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tminimal.append(base)\n\treturn minimal\n\n\n", "def minimalBases(classes):\n\tif not classes:\n\t\treturn classes\n\tclasses = list(classes)\n\ti = 0\n\twhile i < len(classes):\n\t\tj = i + 1\n\t\twhile j < len(classes):\n\t\t\tif classes[i] == classes[j]:\n\t\t\t\tdel classes[j]\n\t\t\telif issubclass(classes[i], classes[j]):\n\t\t\t\tdel classes[i]\n\t\t\t\ti -= 1\n\t\t\t\tbreak\n\t\t\telif issubclass(classes[j], classes[i]):\n\t\t\t\tdel classes[j]\n\t\t\telse:\n\t\t\t\tj += 1\n\t\ti += 1\n\treturn classes\n\n", "def minimalBases(classes):\n\t# Empty list has no bases\n\tif not classes:\n\t\treturn []\n\t# Single element list is its own minimal base\n\telif len(classes) == 1:\n\t\treturn classes\n\t# Reduce by removing redundancies from the front\n\telse:\n\t\treduced = []\n\t\tfor c in classes:\n\t\t\tif c not in reduced:\n\t\t\t\treduced.append(c)\n\t\treturn minimalBases(reduced)\n\n\n\n", "def minimalBases(classes):\n\tbases = []\n\tfor base in classes:\n\t\tif not bases:\n\t\t\tbases.append(base)\n\t\telif issubclass(base, bases[-1]):\n\t\t\tcontinue\n\t\telif issubclass(bases[-1], base):\n\t\t\tbases.pop()\n\t\t\tbases.append(base)\n\t\telse:\n\t\t\tbases.append(base)\n\treturn bases\n\n", "def minimalBases(classes):\n\tbases = []\n\tfor cls in classes:\n\t\tif isinstance(cls, tuple):\n\t\t\tcls = cls[0]\n\t\tif cls not in bases:\n\t\t\tbases.append(cls)\n\t\telse:\n\t\t\tcontinue\n\t\tfor base in cls.__bases__:\n\t\t\tif base not in bases:\n\t\t\t\tbases.append(base)\n\treturn bases\n\n", "def minimalBases(classes):\n\t# Reduce the list to its unique items\n\tunique = []\n\tfor c in classes:\n\t\tif c not in unique:\n\t\t\tunique.append(c)\n\t# Create a list of items that depend on other items\n\trest = []\n\tfor c in unique:\n\t\tif c._dependencies:\n\t\t\trest.append(c)\n\t# Create a list of items that don't depend on other items\n\thead = []\n\tfor c in unique:\n\t\tif c not in rest:\n\t\t\thead.append(c)\n\t# Add an item from rest to head if all its dependencies are in head\n\twhile rest:\n\t\trest2 = []\n\t\tfor c in rest:\n\t\t\tif all(d in head for d in c._dependencies):\n\t\t\t\thead.append(c)\n\t\t\telse:\n\t\t\t\trest2.append(c)\n\t\trest = rest2\n\t# Add the remaining items from rest to head\n\tfor c in rest:\n\t\thead.append(c)\n\treturn head\n\n", "def minimalBases(classes):\n\treturn [c for c in classes if not any((issubclass(c, b) for b in classes if c is not b) )]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n\tif not classes:\n\t\treturn []\n\n\t# Sort the classes\n\torderedClasses = sorted(classes, key=lambda cls: cls.__name__)\n\n\t# Remove any class that is a subclass of another\n\tfilteredClasses = []\n\tfor cls in orderedClasses:\n\t\tfor other in filteredClasses:\n\t\t\tif issubclass(cls, other):\n\t\t\t\tbreak\n\t\telse:\n\t\t\tfilteredClasses.append(cls)\n\n\treturn filteredClasses\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\treturn [b for b in classes if not isSubClass(classes, b)]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tbases = list(classes)\n\tfor i in range(len(bases)):\n\t\tfor j in range(i + 1, len(bases)):\n\t\t\tif issubclass(bases[i], bases[j]):\n\t\t\t\tbases[j] = bases[i]\n\treturn bases\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tminimal = []\n\tfor base in classes:\n\t\tfor other in classes:\n\t\t\tif issubclass(base, other) and other not in minimal:\n\t\t\t\tbreak\n\t\telse:\n\t\t\tminimal.append(base)\n\treturn minimal\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tif not classes:\n\t\treturn classes\n\tclasses = list(classes)\n\ti = 0\n\twhile i < len(classes):\n\t\tj = i + 1\n\t\twhile j < len(classes):\n\t\t\tif classes[i] == classes[j]:\n\t\t\t\tdel classes[j]\n\t\t\telif issubclass(classes[i], classes[j]):\n\t\t\t\tdel classes[i]\n\t\t\t\ti -= 1\n\t\t\t\tbreak\n\t\t\telif issubclass(classes[j], classes[i]):\n\t\t\t\tdel classes[j]\n\t\t\telse:\n\t\t\t\tj += 1\n\t\ti += 1\n\treturn classes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t# Empty list has no bases\n\tif not classes:\n\t\treturn []\n\t# Single element list is its own minimal base\n\telif len(classes) == 1:\n\t\treturn classes\n\t# Reduce by removing redundancies from the front\n\telse:\n\t\treduced = []\n\t\tfor c in classes:\n\t\t\tif c not in reduced:\n\t\t\t\treduced.append(c)\n\t\treturn minimalBases(reduced)\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tbases = []\n\tfor base in classes:\n\t\tif not bases:\n\t\t\tbases.append(base)\n\t\telif issubclass(base, bases[-1]):\n\t\t\tcontinue\n\t\telif issubclass(bases[-1], base):\n\t\t\tbases.pop()\n\t\t\tbases.append(base)\n\t\telse:\n\t\t\tbases.append(base)\n\treturn bases\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tbases = []\n\tfor cls in classes:\n\t\tif isinstance(cls, tuple):\n\t\t\tcls = cls[0]\n\t\tif cls not in bases:\n\t\t\tbases.append(cls)\n\t\telse:\n\t\t\tcontinue\n\t\tfor base in cls.__bases__:\n\t\t\tif base not in bases:\n\t\t\t\tbases.append(base)\n\treturn bases\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\t# Reduce the list to its unique items\n\tunique = []\n\tfor c in classes:\n\t\tif c not in unique:\n\t\t\tunique.append(c)\n\t# Create a list of items that depend on other items\n\trest = []\n\tfor c in unique:\n\t\tif c._dependencies:\n\t\t\trest.append(c)\n\t# Create a list of items that don't depend on other items\n\thead = []\n\tfor c in unique:\n\t\tif c not in rest:\n\t\t\thead.append(c)\n\t# Add an item from rest to head if all its dependencies are in head\n\twhile rest:\n\t\trest2 = []\n\t\tfor c in rest:\n\t\t\tif all(d in head for d in c._dependencies):\n\t\t\t\thead.append(c)\n\t\t\telse:\n\t\t\t\trest2.append(c)\n\t\trest = rest2\n\t# Add the remaining items from rest to head\n\tfor c in rest:\n\t\thead.append(c)\n\treturn head\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\treturn [c for c in classes if not any((issubclass(c, b) for b in classes if c is not b) )]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "ques_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n\tresult = []\n\tfor attr in self.attributes():\n\t\tif attr.name() in self._names and (all or not attr.private()):\n\t\t\tresult.append( ( attr.name(), attr.description() ) )\n\treturn result\n\t\n", "def namesAndDescriptions(self, all=False):\n\tnames = self.names()\n\tdescs = self.descriptions()\n\tif not all:\n\t\t# Filter out attributes that are not present in interface.\n\t\tnamesAndDescriptions = [\n\t\t\t(name, desc) for name,desc in zip(names, descs)\n\t\t\tif name in self.attributes]\n\telse:\n\t\tnamesAndDescriptions = list(zip(names, descs))\n\n\treturn namesAndDescriptions\n\n\n\tdef names(self, all=False):\n\t\t\"\"\"\n\t\tReturn attribute names defined by interface.\n\t\t\"\"\"\n\t\tnames = [name for name,_ in self.namesAndTypes()]\n\t\tif all:\n\t\t\treturn names\n\t\telse:\n\t\t\t# Filter out attributes that are not present in interface.\n\t\t\treturn [name for name in names if name in self.attributes]\n\n\n\tdef types(self, all=False):\n\t\t\"\"\"\n\t\tReturn attribute types defined by interface.\n\t\t\"\"\"\n\t\ttypes = [atype for _,atype in self.\n", "def namesAndDescriptions(self, all=False):\n\treturn self.__namesAndDescriptions(self, all=all)\n\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn [(attr, getattr(self, attr).__doc__) for attr in dir(self)]\n\telse:\n\t\treturn [(attr, getattr(self, attr).__doc__) for attr in dir(self)\n\t\t\t\tif not attr.startswith('_')]\n\n\n", "def namesAndDescriptions(self, all=False):\n\treturn self.__class__._namesAndDescriptions(all)\n\n", "def namesAndDescriptions(self, all=False):\n\treturn self.__namesAndDescriptions(self.__interfaces, all)\n\n\n", "def namesAndDescriptions(self, all=False):\n\treturn self.__namesAndDescriptions(self.schema, all)\n\n\tdef isAttribute(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute exists in interface.\n\t\t\"\"\"\n\t\treturn name in self.names()\n\n\tdef isRequired(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute is required.\n\t\t\"\"\"\n\t\tif name in self.names():\n\t\t\treturn self.schema[name]['required']\n\t\treturn False\n\n\tdef isReadOnly(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute is read-only.\n\t\t\"\"\"\n\t\tif name in self.names():\n\t\t\treturn self.schema[name]['readonly']\n\t\treturn False\n\n\tdef isWriteOnly(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute is write-only.\n\t\t\"\"\"\n\t\tif name in self.names():\n\t\t\treturn self.schema[name]['writeonly']\n\t\treturn False\n\n\tdef isDerived(self, name):\n\t\t\"\"\"\n\t\t\n", "def namesAndDescriptions(self, all=False):\n\tlist = []\n\tfor i in range(self.attributeCount()):\n\t\tif all or not self.isDynamic(i):\n\t\t\tlist.append((self.attributeName(i), self.attributeDescription(i)))\n\treturn list\n\n", "def namesAndDescriptions(self, all=False):\n\treturn self._impl.namesAndDescriptions(all)\n\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\tif self.parent:\n\t\t\tnamesAndDescriptions(self.parent)\n\n\t\tfor name in self.attributes.keys():\n\t\t\tif self.attributes[name].description:\n\t\t\t\tyield name, self.attributes[name].description\n\telse:\n\t\tfor name in self.attributes.keys():\n\t\t\tif self.attributes[name].description:\n\t\t\t\tyield name, self.attributes[name].description\n\n\tfor interface in self.interfaces:\n\t\tfor name, description in interface.namesAndDescriptions():\n\t\t\tyield name, description\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n\tresult = []\n\tfor attr in self.attributes():\n\t\tif attr.name() in self._names and (all or not attr.private()):\n\t\t\tresult.append( ( attr.name(), attr.description() ) )\n\treturn result\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tnames = self.names()\n\tdescs = self.descriptions()\n\tif not all:\n\t\t# Filter out attributes that are not present in interface.\n\t\tnamesAndDescriptions = [\n\t\t\t(name, desc) for name,desc in zip(names, descs)\n\t\t\tif name in self.attributes]\n\telse:\n\t\tnamesAndDescriptions = list(zip(names, descs))\n\n\treturn namesAndDescriptions\n\n\n\tdef names(self, all=False):\n\t\t\"\"\"\n\t\tReturn attribute names defined by interface.\n\t\t\"\"\"\n\t\tnames = [name for name,_ in self.namesAndTypes()]\n\t\tif all:\n\t\t\treturn names\n\t\telse:\n\t\t\t# Filter out attributes that are not present in interface.\n\t\t\treturn [name for name in names if name in self.attributes]\n\n\n\tdef types(self, all=False):\n\t\t\"\"\"\n\t\tReturn attribute types defined by interface.\n\t\t\"\"\"\n\t\ttypes = [atype for _,atype in self.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\treturn self.__namesAndDescriptions(self, all=all)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn [(attr, getattr(self, attr).__doc__) for attr in dir(self)]\n\telse:\n\t\treturn [(attr, getattr(self, attr).__doc__) for attr in dir(self)\n\t\t\t\tif not attr.startswith('_')]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\treturn self.__class__._namesAndDescriptions(all)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\treturn self.__namesAndDescriptions(self.__interfaces, all)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\treturn self.__namesAndDescriptions(self.schema, all)\n\n\tdef isAttribute(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute exists in interface.\n\t\t\"\"\"\n\t\treturn name in self.names()\n\n\tdef isRequired(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute is required.\n\t\t\"\"\"\n\t\tif name in self.names():\n\t\t\treturn self.schema[name]['required']\n\t\treturn False\n\n\tdef isReadOnly(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute is read-only.\n\t\t\"\"\"\n\t\tif name in self.names():\n\t\t\treturn self.schema[name]['readonly']\n\t\treturn False\n\n\tdef isWriteOnly(self, name):\n\t\t\"\"\"\n\t\tReturn True if attribute is write-only.\n\t\t\"\"\"\n\t\tif name in self.names():\n\t\t\treturn self.schema[name]['writeonly']\n\t\treturn False\n\n\tdef isDerived(self, name):\n\t\t\"\"\"\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tlist = []\n\tfor i in range(self.attributeCount()):\n\t\tif all or not self.isDynamic(i):\n\t\t\tlist.append((self.attributeName(i), self.attributeDescription(i)))\n\treturn list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\treturn self._impl.namesAndDescriptions(all)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\tif self.parent:\n\t\t\tnamesAndDescriptions(self.parent)\n\n\t\tfor name in self.attributes.keys():\n\t\t\tif self.attributes[name].description:\n\t\t\t\tyield name, self.attributes[name].description\n\telse:\n\t\tfor name in self.attributes.keys():\n\t\t\tif self.attributes[name].description:\n\t\t\t\tyield name, self.attributes[name].description\n\n\tfor interface in self.interfaces:\n\t\tfor name, description in interface.namesAndDescriptions():\n\t\t\tyield name, description\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "ques_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n\tif all:\n\t\treturn self.all_names()\n\treturn self.names\n\n", "def names(self, all=False):\n\tif all:\n\t    return self._all_attributes.keys()\n\treturn self._attributes.keys()\n\n    def names_with_colons(self, all=False):\n\t\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t    return self._all_attributes_with_colons.keys()\n\treturn self._attributes_with_colons.keys()\n\n    def names_with_prefix(self, prefix, all=False):\n\t\"\"\"\n\tReturn the attribute names defined by the interface that\n\tstart with the specified prefix.\n\t\"\"\"\n\tif all:\n\t    names = self._all_attributes\n\telse:\n\t    names = self._attributes\n\treturn [name for name in names.keys() if name.startswith(prefix)]\n\n    def names_with_colons_with_prefix(self, prefix, all=False):\n\t\"\"\"\n\tReturn the attribute names defined by the interface that\n\tstart with the specified prefix.\n\t\"\"\"\n\tif all:\n\t    names = self._all_attributes_with_col\n", "def names(self, all=False):\n\tif all:\n\t\treturn self._all_names\n\telse:\n\t\treturn self._names\n\n", "def names(self, all=False):\n\tif all:\n\t    return self.__dict__.keys()\n\treturn [name for name in self.__dict__.keys()\n\t\tif not name.startswith('_')]\n\n    def _is_extended_by(self, iface):\n\treturn (iface is self or\n\t\tiface.__class__ is InterfaceClass and\n\t\tself in iface.__bases__)\n\n    def _is_implemented_by(self, klass):\n\tif self.__name__ == 'Interface' and klass.__name__ == 'Interface':\n\t    # Special case: we are looking at the root interface\n\t    # and the class is also a root interface.\n\t    return True\n\tif not self.names():\n\t    # Special case: we are looking at an empty interface.\n\t    # Anyone implements it.\n\t    return True\n\tif not isinstance(klass, types.ClassType):\n\t    return False\n\tif not self._is_extended_by(klass.__dict__.get('__implements__')):\n\t   \n", "def names(self, all=False):\n\tnames = []\n\tfor attr in self.attrs:\n\t    if type(attr) == str:\n\t\tnames.append(attr)\n\t    else:\n\t\tfor name in attr:\n\t\t    names.append(name)\n\treturn names\n\n    def has_key(self, name):\n\t\"\"\"\n\tReturn true if the interface defines the given attribute.\n\t\"\"\"\n\treturn name in self.names()\n\n    def get(self, name, default=None):\n\t\"\"\"\n\tGet the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.attrs.get(name, default)\n\n    def __getitem__(self, name):\n\t\"\"\"\n\tGet the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.attrs[name]\n\n    def __setitem__(self, name, value):\n\t\"\"\"\n\tSet the attribute names defined by the interface.\n\t\"\"\"\n\traise AttributeError, \"attributes are read-only\"\n\n    def __delitem__(self, name):\n\t\"\"\"\n\tDelete the attribute names defined by the interface.\n\n", "def names(self, all=False):\n\tif all:\n\t\treturn self.names() + self.attributeNames()\n\telse:\n\t\treturn self.names()\n\t\n", "def names(self, all=False):\n\tif not self._names:\n\t\tself._names = set()\n\t\tfor attr in self._methods:\n\t\t\tself._names.add(attr.__name__)\n\t\t\tif attr.__doc__:\n\t\t\t\tself._names.update(self.get_names(attr.__doc__))\n\tif all:\n\t\treturn self._names\n\telse:\n\t\treturn self._names - self._excluded_names\n\t\n\t\n", "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn self.__dict__.keys()[2:]\n\n\n", "def names(self, all=False):\n\treturn self._names(all)\n\n", "def names(self, all=False):\n\treturn list(self.__names.keys())\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn an iterator over the attribute names defined by the interface.\n\t\"\"\"\n\treturn iter(self.__names)\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of attributes defined by the interface.\n\t\"\"\"\n\treturn len(self.__names)\n\n\tdef __getitem__(self, name):\n\t\"\"\"\n\tReturn the specified attribute.\n\t\"\"\"\n\treturn self.__names[name.lower()]\n\n\tdef __setitem__(self, name, value):\n\t\"\"\"\n\tRaise TypeError.\n\t\"\"\"\n\traise TypeError(\"%s objects do not support item assignment\" % self.__class__.__name__)\n\n\tdef __delitem__(self, name):\n\t\"\"\"\n\tRaise TypeError.\n\t\"\"\"\n\traise TypeError(\"%s objects do not support item assignment\" % self.__class__.__name__)\n\n\tdef __contains__(self, name):\n\t\"\"\"\n\tReturn whether the interface defines an attribute with the given name\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.all_names()\n\treturn self.names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t    return self._all_attributes.keys()\n\treturn self._attributes.keys()\n\n    def names_with_colons(self, all=False):\n\t\"\"\"\n\tReturn the attribute names defined by the interface.\n\t\"\"\"\n\tif all:\n\t    return self._all_attributes_with_colons.keys()\n\treturn self._attributes_with_colons.keys()\n\n    def names_with_prefix(self, prefix, all=False):\n\t\"\"\"\n\tReturn the attribute names defined by the interface that\n\tstart with the specified prefix.\n\t\"\"\"\n\tif all:\n\t    names = self._all_attributes\n\telse:\n\t    names = self._attributes\n\treturn [name for name in names.keys() if name.startswith(prefix)]\n\n    def names_with_colons_with_prefix(self, prefix, all=False):\n\t\"\"\"\n\tReturn the attribute names defined by the interface that\n\tstart with the specified prefix.\n\t\"\"\"\n\tif all:\n\t    names = self._all_attributes_with_col\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self._all_names\n\telse:\n\t\treturn self._names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t    return self.__dict__.keys()\n\treturn [name for name in self.__dict__.keys()\n\t\tif not name.startswith('_')]\n\n    def _is_extended_by(self, iface):\n\treturn (iface is self or\n\t\tiface.__class__ is InterfaceClass and\n\t\tself in iface.__bases__)\n\n    def _is_implemented_by(self, klass):\n\tif self.__name__ == 'Interface' and klass.__name__ == 'Interface':\n\t    # Special case: we are looking at the root interface\n\t    # and the class is also a root interface.\n\t    return True\n\tif not self.names():\n\t    # Special case: we are looking at an empty interface.\n\t    # Anyone implements it.\n\t    return True\n\tif not isinstance(klass, types.ClassType):\n\t    return False\n\tif not self._is_extended_by(klass.__dict__.get('__implements__')):\n\t   \n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tnames = []\n\tfor attr in self.attrs:\n\t    if type(attr) == str:\n\t\tnames.append(attr)\n\t    else:\n\t\tfor name in attr:\n\t\t    names.append(name)\n\treturn names\n\n    def has_key(self, name):\n\t\"\"\"\n\tReturn true if the interface defines the given attribute.\n\t\"\"\"\n\treturn name in self.names()\n\n    def get(self, name, default=None):\n\t\"\"\"\n\tGet the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.attrs.get(name, default)\n\n    def __getitem__(self, name):\n\t\"\"\"\n\tGet the attribute names defined by the interface.\n\t\"\"\"\n\treturn self.attrs[name]\n\n    def __setitem__(self, name, value):\n\t\"\"\"\n\tSet the attribute names defined by the interface.\n\t\"\"\"\n\traise AttributeError, \"attributes are read-only\"\n\n    def __delitem__(self, name):\n\t\"\"\"\n\tDelete the attribute names defined by the interface.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.names() + self.attributeNames()\n\telse:\n\t\treturn self.names()\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif not self._names:\n\t\tself._names = set()\n\t\tfor attr in self._methods:\n\t\t\tself._names.add(attr.__name__)\n\t\t\tif attr.__doc__:\n\t\t\t\tself._names.update(self.get_names(attr.__doc__))\n\tif all:\n\t\treturn self._names\n\telse:\n\t\treturn self._names - self._excluded_names\n\t\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn self.__dict__.keys()[2:]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\treturn self._names(all)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\treturn list(self.__names.keys())\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn an iterator over the attribute names defined by the interface.\n\t\"\"\"\n\treturn iter(self.__names)\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturn the number of attributes defined by the interface.\n\t\"\"\"\n\treturn len(self.__names)\n\n\tdef __getitem__(self, name):\n\t\"\"\"\n\tReturn the specified attribute.\n\t\"\"\"\n\treturn self.__names[name.lower()]\n\n\tdef __setitem__(self, name, value):\n\t\"\"\"\n\tRaise TypeError.\n\t\"\"\"\n\traise TypeError(\"%s objects do not support item assignment\" % self.__class__.__name__)\n\n\tdef __delitem__(self, name):\n\t\"\"\"\n\tRaise TypeError.\n\t\"\"\"\n\traise TypeError(\"%s objects do not support item assignment\" % self.__class__.__name__)\n\n\tdef __contains__(self, name):\n\t\"\"\"\n\tReturn whether the interface defines an attribute with the given name\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "ques_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n\tif not sequence:\n\t\treturn output\n\n\tif output is None:\n\t\toutput = []\n\n\tfor item in sequence:\n\t\tif isinstance(item, tuple):\n\t\t\t_normalizeargs(item, output)\n\t\telif isinstance(item, Declaration):\n\t\t\t_normalizeargs(item.args, output)\n\t\telse:\n\t\t\toutput.append(item)\n\n\treturn output\n\n\n", "def _normalizeargs(sequence, output=None):\n\tfrom .declarations import Declaration, Interface, Implements\n\tfrom .interfaces import InterfaceClass\n\tif not sequence:\n\t\treturn ()\n\tif isinstance(sequence, Declaration):\n\t\treturn (sequence,)\n\tif isinstance(sequence, Interface):\n\t\treturn (sequence,)\n\tif isinstance(sequence, Implements):\n\t\treturn (sequence,)\n\tif isinstance(sequence, InterfaceClass):\n\t\treturn (sequence,)\n\tif isinstance(sequence, (tuple, list)):\n\t\treturn sequence\n\tif isinstance(sequence, (set, frozenset)):\n\t\treturn sequence\n\tif isinstance(sequence, dict):\n\t\treturn sequence\n\traise TypeError(\"unrecognized type: %r\" % sequence)\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, tuple):\n\t\tsequence = list(sequence)\n\tif isinstance(sequence, list):\n\t\tsequence = tuple(sequence)\n\tif isinstance(sequence, tuple):\n\t\tif len(sequence) == 1:\n\t\t\tsequence = sequence[0]\n\tif isinstance(sequence, Declaration):\n\t\tsequence = sequence.declared\n\tif isinstance(sequence, tuple):\n\t\tif output is None:\n\t\t\toutput = []\n\t\tfor item in sequence:\n\t\t\tif isinstance(item, tuple):\n\t\t\t\t_normalizeargs(item, output)\n\t\t\telse:\n\t\t\t\toutput.append(item)\n\t\tsequence = tuple(output)\n\treturn sequence\n\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence.sequence\n\telif isinstance(sequence, tuple):\n\t\treturn sequence\n\telse:\n\t\tif isinstance(sequence, (list, tuple)):\n\t\t\tif len(sequence) == 1:\n\t\t\t\tsequence = sequence[0]\n\t\t\telif len(sequence) == 2:\n\t\t\t\tsequence = sequence[1]\n\t\t\telse:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Expected interface or implements spec, got %s\" % sequence)\n\t\tif isinstance(sequence, (list, tuple)):\n\t\t\tif len(sequence) == 1:\n\t\t\t\tsequence = sequence[0]\n\t\t\telse:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Expected interface or implements spec, got %s\" % sequence)\n\t\tif not isinstance(sequence, (list, tuple)):\n\t\t\treturn (sequence,)\n\t\telse:\n\t\t\tresult = []\n\t\t\tfor item in sequence:\n\t\t\t\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\t_normalizeargs(item.implements, output)\n\t\telif isinstance(item, tuple):\n\t\t\t_normalizeargs(item, output)\n\t\telif isinstance(item, Interface):\n\t\t\toutput.append(item)\n\t\telse:\n\t\t\traise TypeError('Invalid declaration argument')\n\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, (tuple, list)):\n\t\t\t_normalizeargs(item, output)\n\t\telse:\n\t\t\toutput.append(item)\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\tif sequence is None:\n\t\treturn ()\n\telif isinstance(sequence, _Declaration):\n\t\treturn _normalizeargs(sequence.__implements__)\n\telif isinstance(sequence, tuple):\n\t\tresult = []\n\t\tfor item in sequence:\n\t\t\tresult.extend(_normalizeargs(item))\n\t\treturn result\n\telse:\n\t\treturn [sequence]\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Declaration):\n\t\t\t_normalizeargs(arg.interfaces, output)\n\t\telif isinstance(arg, tuple):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(arg)\n\treturn output\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, (list, tuple)):\n\t\tfor i in sequence:\n\t\t\t_normalizeargs(i, output)\n\telif isinstance(sequence, Declaration):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Interface):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Implements):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, str):\n\t\toutput.append(sequence)\n\telse:\n\t\traise TypeError(\"Invalid argument(s) to Declaration: %s\" % sequence)\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif not isinstance(sequence, (list, tuple)):\n\t\traise TypeError('Normalization arguments must be a list or tuple')\n\tout = []\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\tout.extend(item.interfaces)\n\t\telif isinstance(item, (list, tuple)):\n\t\t\tout.extend(_normalizeargs(item))\n\t\telif isinstance(item, Interface):\n\t\t\tout.append(item)\n\t\telse:\n\t\t\traise TypeError('Invalid normalization argument')\n\tif output is None:\n\t\treturn out\n\toutput.extend(out)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n\tif not sequence:\n\t\treturn output\n\n\tif output is None:\n\t\toutput = []\n\n\tfor item in sequence:\n\t\tif isinstance(item, tuple):\n\t\t\t_normalizeargs(item, output)\n\t\telif isinstance(item, Declaration):\n\t\t\t_normalizeargs(item.args, output)\n\t\telse:\n\t\t\toutput.append(item)\n\n\treturn output\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tfrom .declarations import Declaration, Interface, Implements\n\tfrom .interfaces import InterfaceClass\n\tif not sequence:\n\t\treturn ()\n\tif isinstance(sequence, Declaration):\n\t\treturn (sequence,)\n\tif isinstance(sequence, Interface):\n\t\treturn (sequence,)\n\tif isinstance(sequence, Implements):\n\t\treturn (sequence,)\n\tif isinstance(sequence, InterfaceClass):\n\t\treturn (sequence,)\n\tif isinstance(sequence, (tuple, list)):\n\t\treturn sequence\n\tif isinstance(sequence, (set, frozenset)):\n\t\treturn sequence\n\tif isinstance(sequence, dict):\n\t\treturn sequence\n\traise TypeError(\"unrecognized type: %r\" % sequence)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, tuple):\n\t\tsequence = list(sequence)\n\tif isinstance(sequence, list):\n\t\tsequence = tuple(sequence)\n\tif isinstance(sequence, tuple):\n\t\tif len(sequence) == 1:\n\t\t\tsequence = sequence[0]\n\tif isinstance(sequence, Declaration):\n\t\tsequence = sequence.declared\n\tif isinstance(sequence, tuple):\n\t\tif output is None:\n\t\t\toutput = []\n\t\tfor item in sequence:\n\t\t\tif isinstance(item, tuple):\n\t\t\t\t_normalizeargs(item, output)\n\t\t\telse:\n\t\t\t\toutput.append(item)\n\t\tsequence = tuple(output)\n\treturn sequence\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence.sequence\n\telif isinstance(sequence, tuple):\n\t\treturn sequence\n\telse:\n\t\tif isinstance(sequence, (list, tuple)):\n\t\t\tif len(sequence) == 1:\n\t\t\t\tsequence = sequence[0]\n\t\t\telif len(sequence) == 2:\n\t\t\t\tsequence = sequence[1]\n\t\t\telse:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Expected interface or implements spec, got %s\" % sequence)\n\t\tif isinstance(sequence, (list, tuple)):\n\t\t\tif len(sequence) == 1:\n\t\t\t\tsequence = sequence[0]\n\t\t\telse:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Expected interface or implements spec, got %s\" % sequence)\n\t\tif not isinstance(sequence, (list, tuple)):\n\t\t\treturn (sequence,)\n\t\telse:\n\t\t\tresult = []\n\t\t\tfor item in sequence:\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\t_normalizeargs(item.implements, output)\n\t\telif isinstance(item, tuple):\n\t\t\t_normalizeargs(item, output)\n\t\telif isinstance(item, Interface):\n\t\t\toutput.append(item)\n\t\telse:\n\t\t\traise TypeError('Invalid declaration argument')\n\n\treturn output\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, (tuple, list)):\n\t\t\t_normalizeargs(item, output)\n\t\telse:\n\t\t\toutput.append(item)\n\treturn output\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif sequence is None:\n\t\treturn ()\n\telif isinstance(sequence, _Declaration):\n\t\treturn _normalizeargs(sequence.__implements__)\n\telif isinstance(sequence, tuple):\n\t\tresult = []\n\t\tfor item in sequence:\n\t\t\tresult.extend(_normalizeargs(item))\n\t\treturn result\n\telse:\n\t\treturn [sequence]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Declaration):\n\t\t\t_normalizeargs(arg.interfaces, output)\n\t\telif isinstance(arg, tuple):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(arg)\n\treturn output\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, (list, tuple)):\n\t\tfor i in sequence:\n\t\t\t_normalizeargs(i, output)\n\telif isinstance(sequence, Declaration):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Interface):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Implements):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, str):\n\t\toutput.append(sequence)\n\telse:\n\t\traise TypeError(\"Invalid argument(s) to Declaration: %s\" % sequence)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif not isinstance(sequence, (list, tuple)):\n\t\traise TypeError('Normalization arguments must be a list or tuple')\n\tout = []\n\tfor item in sequence:\n\t\tif isinstance(item, Declaration):\n\t\t\tout.extend(item.interfaces)\n\t\telif isinstance(item, (list, tuple)):\n\t\t\tout.extend(_normalizeargs(item))\n\t\telif isinstance(item, Interface):\n\t\t\tout.append(item)\n\t\telse:\n\t\t\traise TypeError('Invalid normalization argument')\n\tif output is None:\n\t\treturn out\n\toutput.extend(out)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "ques_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError:\n\t\treturn False\n\treturn c_optimizations\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\t# The C module is not available.\n\t\treturn False\n\n", "def _c_optimizations_available():\n\tglobal _c_optimizations_module\n\tif _c_optimizations_module is None:\n\t\tif _c_optimizations_required:\n\t\t\t_c_optimizations_module = _import_c_optimizations()\n\t\telse:\n\t\t\t_c_optimizations_module = False\n\treturn _c_optimizations_module\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn None\n\n", "def _c_optimizations_available():\n\tglobal _c_optimizations\n\tif not _c_optimizations:\n\t\ttry:\n\t\t\timport c_optimizations\n\t\t\t_c_optimizations = c_optimizations\n\t\texcept ImportError:\n\t\t\t_c_optimizations = False\n\treturn _c_optimizations\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn None\n\n", "def _c_optimizations_available():\n\tif _c_optimizations_required:\n\t\ttry:\n\t\t\treturn __import__('_yenc')\n\t\texcept ImportError:\n\t\t\traise ImportError('C optimizations required but not available')\n\telse:\n\t\ttry:\n\t\t\treturn __import__('_yenc')\n\t\texcept ImportError:\n\t\t\treturn False\n\t#\n\n", "def _c_optimizations_available():\n\t\n\ttry:\n\t\treturn _c_optimizations\n\texcept NameError:\n\t\treturn False\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError:\n\t\treturn None\n\treturn c_optimizations\n\n", "def _c_optimizations_available():\n\ttry:\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn None\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError:\n\t\treturn False\n\treturn c_optimizations\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\t# The C module is not available.\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\tglobal _c_optimizations_module\n\tif _c_optimizations_module is None:\n\t\tif _c_optimizations_required:\n\t\t\t_c_optimizations_module = _import_c_optimizations()\n\t\telse:\n\t\t\t_c_optimizations_module = False\n\treturn _c_optimizations_module\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\tglobal _c_optimizations\n\tif not _c_optimizations:\n\t\ttry:\n\t\t\timport c_optimizations\n\t\t\t_c_optimizations = c_optimizations\n\t\texcept ImportError:\n\t\t\t_c_optimizations = False\n\treturn _c_optimizations\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\tif _c_optimizations_required:\n\t\ttry:\n\t\t\treturn __import__('_yenc')\n\t\texcept ImportError:\n\t\t\traise ImportError('C optimizations required but not available')\n\telse:\n\t\ttry:\n\t\t\treturn __import__('_yenc')\n\t\texcept ImportError:\n\t\t\treturn False\n\t#\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t\n\ttry:\n\t\treturn _c_optimizations\n\texcept NameError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError:\n\t\treturn None\n\treturn c_optimizations\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n\treturn not _use_c_impl() and not _use_pypy()\n", "def _should_attempt_c_optimizations():\n\tif _use_c_impl is None:\n\t\treturn True\n\treturn not _use_c_impl() and not _is_pypy\n\n\n", "def _should_attempt_c_optimizations():\n\t# https://github.com/python/cpython/blob/master/Lib/platform.py#L933\n\tif hasattr(sys, 'implementation'):\n\t\tif sys.implementation.name == 'pypy':\n\t\t\treturn False\n\treturn _use_c_impl()\n\n", "def _should_attempt_c_optimizations():\n\treturn _use_c_impl and not _using_pypy\n\n", "def _should_attempt_c_optimizations():\n\treturn _use_c_impl() and _have_c_extension()\n\n", "def _should_attempt_c_optimizations():\n\t# If we're on PyPy, we should never attempt to use the C optimization.\n\tif IS_PYPY:\n\t\treturn False\n\t# If we're going to use the pure-Python implementation, we shouldn't\n\t# attempt to use the C implementation.\n\tif _use_c_impl() is False:\n\t\treturn False\n\t# Otherwise, we should attempt to use the C implementation.\n\treturn True\n\n", "def _should_attempt_c_optimizations():\n\treturn not _use_c_impl() and not _is_pypy()\n\n\n", "def _should_attempt_c_optimizations():\n\tif _use_c_impl is None:\n\t\treturn False\n\tif _use_c_impl:\n\t\treturn True\n\tif _use_c_impl == \"auto\":\n\t\tif _if_pypy_use_c_impl:\n\t\t\treturn True\n\t\telif \"PURE_PYTHON\" in os.environ:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\tif _use_c_impl == \"never\":\n\t\treturn False\n\tif _use_c_impl == \"always\":\n\t\treturn True\n\traise ValueError(\"Unknown value for _use_c_impl: %r\" % (_use_c_impl,))\n\n", "def _should_attempt_c_optimizations():\n\treturn not _is_running_on_pypy() and _use_c_impl()\n\n\n", "def _should_attempt_c_optimizations():\n\treturn _use_c_impl() and not _using_pypy()\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n\treturn not _use_c_impl() and not _use_pypy()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\tif _use_c_impl is None:\n\t\treturn True\n\treturn not _use_c_impl() and not _is_pypy\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t# https://github.com/python/cpython/blob/master/Lib/platform.py#L933\n\tif hasattr(sys, 'implementation'):\n\t\tif sys.implementation.name == 'pypy':\n\t\t\treturn False\n\treturn _use_c_impl()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn _use_c_impl and not _using_pypy\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn _use_c_impl() and _have_c_extension()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\t# If we're on PyPy, we should never attempt to use the C optimization.\n\tif IS_PYPY:\n\t\treturn False\n\t# If we're going to use the pure-Python implementation, we shouldn't\n\t# attempt to use the C implementation.\n\tif _use_c_impl() is False:\n\t\treturn False\n\t# Otherwise, we should attempt to use the C implementation.\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn not _use_c_impl() and not _is_pypy()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\tif _use_c_impl is None:\n\t\treturn False\n\tif _use_c_impl:\n\t\treturn True\n\tif _use_c_impl == \"auto\":\n\t\tif _if_pypy_use_c_impl:\n\t\t\treturn True\n\t\telif \"PURE_PYTHON\" in os.environ:\n\t\t\treturn False\n\t\telse:\n\t\t\treturn True\n\tif _use_c_impl == \"never\":\n\t\treturn False\n\tif _use_c_impl == \"always\":\n\t\treturn True\n\traise ValueError(\"Unknown value for _use_c_impl: %r\" % (_use_c_impl,))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn not _is_running_on_pypy() and _use_c_impl()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn _use_c_impl() and not _using_pypy()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "ques_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n\treturn _c_optimizations_required() == False\n\n", "def _c_optimizations_ignored():\n\treturn not _c_optimizations_required()\n\n", "def _c_optimizations_ignored():\n\treturn _c_optimizations_required()\n\n\n", "def _c_optimizations_ignored():\n\treturn not _c_optimizations_required()\n\n\n", "def _c_optimizations_ignored():\n\treturn 0\n\n", "def _c_optimizations_ignored():\n\treturn os.getenv('C_OPTIMIZATIONS_IGNORED', '0') == '1'\n\n", "def _c_optimizations_ignored():\n\tif not _c_optimizations_required():\n\t\treturn True\n\treturn False\n\n", "def _c_optimizations_ignored():\n\treturn \"--c_optimizations_ignored\" in sys.argv\n\n\n", "def _c_optimizations_ignored():\n\treturn (\n\t\tnot _c_optimizations_required()\n\t\tand not _c_optimizations_desired()\n\t)\n\n", "def _c_optimizations_ignored():\n\treturn not _c_optimizations_required()\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n\treturn _c_optimizations_required() == False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\treturn not _c_optimizations_required()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\treturn _c_optimizations_required()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\treturn not _c_optimizations_required()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\treturn 0\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\treturn os.getenv('C_OPTIMIZATIONS_IGNORED', '0') == '1'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\tif not _c_optimizations_required():\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\treturn \"--c_optimizations_ignored\" in sys.argv\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\treturn (\n\t\tnot _c_optimizations_required()\n\t\tand not _c_optimizations_desired()\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\treturn not _c_optimizations_required()\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n\tif PURE_PYTHON:\n\t\treturn False\n\tif not _c_optimizations_available():\n\t\treturn False\n\treturn True\n\n", "def _c_optimizations_required():\n\tif _use_c_impl():\n\t\treturn False\n\treturn True\n\n\n\n", "def _c_optimizations_required():\n\treturn PURE_PYTHON is False\n\n\n", "def _c_optimizations_required():\n\t#\n\t# First, check if the user has explicitly requested the use of the C\n\t# optimizations.\n\t#\n\tif 'PURE_PYTHON' in os.environ:\n\t\t#\n\t\t# The user has specified an environment variable. Simply return its\n\t\t# value.\n\t\t#\n\t\treturn not os.environ['PURE_PYTHON']\n\t#\n\t# Now, check if the current Python interpreter is running in a virtual\n\t# environment.\n\t#\n\tif hasattr(sys, 'real_prefix'):\n\t\t#\n\t\t# The Python interpreter is running in a virtual environment.\n\t\t#\n\t\t# Now, check if the virtual environment is a virtualenv.\n\t\t#\n\t\tif hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix:\n\t\t\t#\n\t\t\t# The virtual environment is a virtualenv.\n\t\t\t#\n\t\t\t# Note: because of the way virtualenv works, this cannot be the\n\t\t\t# system Python interpreter.\n\t\t\t#\n\n", "def _c_optimizations_required():\n\tglobal _c_optimizations_required_result\n\tif _c_optimizations_required_result is None:\n\t\ttry:\n\t\t\t_c_optimizations_required_result = not _use_c_impl()\n\t\texcept ImportError:\n\t\t\t_c_optimizations_required_result = True\n\treturn _c_optimizations_required_result\n\n\n", "def _c_optimizations_required():\n\tif _use_c_impl(PURE_PYTHON):\n\t\treturn False\n\treturn True\n\n\n", "def _c_optimizations_required():\n\treturn not _use_c_impl()\n\n\n", "def _c_optimizations_required():\n\tglobal _c_optimizations_required_cache\n\tif _c_optimizations_required_cache is None:\n\t\t_c_optimizations_required_cache = bool(not _use_c_impl() and not PURE_PYTHON)\n\treturn _c_optimizations_required_cache\n\n", "def _c_optimizations_required():\n\treturn not _use_c_impl()\n\n\n", "def _c_optimizations_required():\n\treturn not _use_c_impl()\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n\tif PURE_PYTHON:\n\t\treturn False\n\tif not _c_optimizations_available():\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\tif _use_c_impl():\n\t\treturn False\n\treturn True\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn PURE_PYTHON is False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\t#\n\t# First, check if the user has explicitly requested the use of the C\n\t# optimizations.\n\t#\n\tif 'PURE_PYTHON' in os.environ:\n\t\t#\n\t\t# The user has specified an environment variable. Simply return its\n\t\t# value.\n\t\t#\n\t\treturn not os.environ['PURE_PYTHON']\n\t#\n\t# Now, check if the current Python interpreter is running in a virtual\n\t# environment.\n\t#\n\tif hasattr(sys, 'real_prefix'):\n\t\t#\n\t\t# The Python interpreter is running in a virtual environment.\n\t\t#\n\t\t# Now, check if the virtual environment is a virtualenv.\n\t\t#\n\t\tif hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix:\n\t\t\t#\n\t\t\t# The virtual environment is a virtualenv.\n\t\t\t#\n\t\t\t# Note: because of the way virtualenv works, this cannot be the\n\t\t\t# system Python interpreter.\n\t\t\t#\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\tglobal _c_optimizations_required_result\n\tif _c_optimizations_required_result is None:\n\t\ttry:\n\t\t\t_c_optimizations_required_result = not _use_c_impl()\n\t\texcept ImportError:\n\t\t\t_c_optimizations_required_result = True\n\treturn _c_optimizations_required_result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\tif _use_c_impl(PURE_PYTHON):\n\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn not _use_c_impl()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\tglobal _c_optimizations_required_cache\n\tif _c_optimizations_required_cache is None:\n\t\t_c_optimizations_required_cache = bool(not _use_c_impl() and not PURE_PYTHON)\n\treturn _c_optimizations_required_cache\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn not _use_c_impl()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn not _use_c_impl()\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "ques_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\t\n", "def reset(self):\n\tself.context = {}\n\tself.bins = self.make_bins()\n\n\n", "def reset(self):\n\tself.__init__(self.__bins, self.__n_bins, self.__initial_value)\n\n\n", "def reset(self):\n\tself.context = {}\n\tself.bins = self.make_bins()\n\n", "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\tself.initialize_bins()\n\tself.last_bin = 0\n\tself.count = 0\n\tself.last_value = 0\n\n\tdef make_bins(self):\n\t\"\"\"\n\tMake the bins.\n\n", "def reset(self):\n\tself.current_context = {}\n\tself.bins = make_bins(self.bins_count, self.initial_value)\n\t\n", "def reset(self):\n\tself.context = {}\n\tself.bins = self.make_bins()\n\n", "def reset(self):\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\n\n", "def reset(self):\n\tself.current_context = {}\n\tself.bins = self.initial_value\n\n\n", "def reset(self):\n\tself.__contexts = [{}]\n\tself.__bins = self.__make_bins()\n\n\tself.__set_values(self.__initial_value)\n\n\tself.__n_entries = 0\n\n\treturn self\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.bins = self.make_bins()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.__init__(self.__bins, self.__n_bins, self.__initial_value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.bins = self.make_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\tself.initialize_bins()\n\tself.last_bin = 0\n\tself.count = 0\n\tself.last_value = 0\n\n\tdef make_bins(self):\n\t\"\"\"\n\tMake the bins.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.current_context = {}\n\tself.bins = make_bins(self.bins_count, self.initial_value)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.bins = self.make_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.current_context = {}\n\tself.bins = self.initial_value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.__contexts = [{}]\n\tself.__bins = self.__make_bins()\n\n\tself.__set_values(self.__initial_value)\n\n\tself.__n_entries = 0\n\n\treturn self\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "ques_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n\treturn (\n\t\theader + \"\\n\" if header else \"\"\n\t) + _to_csv(self.iterpoints(), separator)\n\n", "def to_csv(self, separator=\",\", header=None):\n\tdata = []\n\tif header is not None:\n\t\tdata.append(header)\n\tif self.dims == 1:\n\t\tfor point, value in self:\n\t\t\tdata.append(separator.join(map(str, point))\n\t\t\t\t+ separator + str(value))\n\telse:\n\t\tfor point, value in self:\n\t\t\tdata.append(separator.join(\n\t\t\t\t[separator.join(map(str, point))]\n\t\t\t\t+ [str(v) for v in value]\n\t\t\t))\n\treturn \"\\n\".join(data)\n\n", "def to_csv(self, separator=\",\", header=None):\n\traise DeprecationWarning\n\tif header is None:\n\t\tyield from _to_csv(self, separator)\n\telse:\n\t\tyield header\n\t\tyield from _to_csv(self, separator)\n\n", "def to_csv(self, separator=\",\", header=None):\n\t# TODO: to_csv is not used in Lena\n\tout = io.StringIO()\n\tif header is not None:\n\t\tout.write(header)\n\t\tout.write(\"\\n\")\n\tfor pt in self:\n\t\tout.write(separator.join(str(x) for x in pt[0]))\n\t\tout.write(separator)\n\t\tout.write(separator.join(str(x) for x in pt[1]))\n\t\tout.write(\"\\n\")\n\treturn out.getvalue()\n\n", "def to_csv(self, separator=\",\", header=None):\n\tif header is not None:\n\t\tyield header + \"\\n\"\n\tfor point in self:\n\t\tyield separator.join(map(str, point[0])) + separator\n\t\tyield separator.join(map(str, point[1])) + \"\\n\"\n\n\n", "def to_csv(self, separator=\",\", header=None):\n\t# TODO: have a look at LenaCSV\n\t# TODO: convert to table\n\t# TODO: graph.to_table\n\t# TODO: graph.to_table.to_csv\n\t# TODO: graph.to_table.to_csv.to_file\n\t# TODO: graph.to_table.to_csv.to_file.run\n\t# TODO: graph.to_table.to_csv.to_file.run(graph, filename = ...)\n\t# TODO: graph.to_table.to_csv.to_file.run(\n\t#    graph, to_csv = {'separator': ..., 'header': ...},\n\t#    filename = ...\n\t# )\n\t# TODO: graph.to_table.to_csv.to_file.run(\n\t#    graph, to_csv = {'separator': ..., 'header': ...},\n\t#    filename = ...,\n\t#    file_mode = ...\n\t# )\n\t# TODO: graph.to_table.to_csv.to_file.run(\n", "def to_csv(self, separator=\",\", header=None):\n\tdef point_to_csv(point):\n\t\treturn separator.join(map(str, point)) + \"\\n\"\n\tdef value_to_csv(value):\n\t\treturn separator.join(map(str, value)) + \"\\n\"\n\treturn (\n\t\theader + \"\\n\" if header is not None else \"\"\n\t\t+ \"\\n\".join(\n\t\t\tpoint_to_csv(point) + value_to_csv(value)\n\t\t\tfor point, value in self.items()\n\t\t)\n\t)\n\n\tdef __repr__(self):\n\t\treturn \"<Graph: {!r}>\".format(dict(self))\n\n\n", "def to_csv(self, separator=\",\", header=None):\n\traise LenaTypeError(\"to_csv is deprecated\")\n\treturn to_csv(self, separator=separator, header=header)\n\n\n", "def to_csv(self, separator=\",\", header=None):\n\tif header is not None:\n\t\tyield header\n\t\tyield \"\\n\"\n\tfor coord, value in self.items():\n\t\tyield separator.join(map(str, coord))\n\t\tyield separator\n\t\tyield separator.join(map(str, value))\n\t\tyield \"\\n\"\n\n", "def to_csv(self, separator=\",\", header=None):\n\t# TODO: change to lena.output.ToCSV\n\tif header is not None:\n\t\tyield header\n\t\tyield \"\\n\"\n\tfor point in self.points:\n\t\tyield separator.join(map(str, point.coord))\n\t\tyield separator\n\t\tyield separator.join(map(str, point.value))\n\t\tyield \"\\n\"\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n\treturn (\n\t\theader + \"\\n\" if header else \"\"\n\t) + _to_csv(self.iterpoints(), separator)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\tdata = []\n\tif header is not None:\n\t\tdata.append(header)\n\tif self.dims == 1:\n\t\tfor point, value in self:\n\t\t\tdata.append(separator.join(map(str, point))\n\t\t\t\t+ separator + str(value))\n\telse:\n\t\tfor point, value in self:\n\t\t\tdata.append(separator.join(\n\t\t\t\t[separator.join(map(str, point))]\n\t\t\t\t+ [str(v) for v in value]\n\t\t\t))\n\treturn \"\\n\".join(data)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\traise DeprecationWarning\n\tif header is None:\n\t\tyield from _to_csv(self, separator)\n\telse:\n\t\tyield header\n\t\tyield from _to_csv(self, separator)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t# TODO: to_csv is not used in Lena\n\tout = io.StringIO()\n\tif header is not None:\n\t\tout.write(header)\n\t\tout.write(\"\\n\")\n\tfor pt in self:\n\t\tout.write(separator.join(str(x) for x in pt[0]))\n\t\tout.write(separator)\n\t\tout.write(separator.join(str(x) for x in pt[1]))\n\t\tout.write(\"\\n\")\n\treturn out.getvalue()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\tif header is not None:\n\t\tyield header + \"\\n\"\n\tfor point in self:\n\t\tyield separator.join(map(str, point[0])) + separator\n\t\tyield separator.join(map(str, point[1])) + \"\\n\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t# TODO: have a look at LenaCSV\n\t# TODO: convert to table\n\t# TODO: graph.to_table\n\t# TODO: graph.to_table.to_csv\n\t# TODO: graph.to_table.to_csv.to_file\n\t# TODO: graph.to_table.to_csv.to_file.run\n\t# TODO: graph.to_table.to_csv.to_file.run(graph, filename = ...)\n\t# TODO: graph.to_table.to_csv.to_file.run(\n\t#    graph, to_csv = {'separator': ..., 'header': ...},\n\t#    filename = ...\n\t# )\n\t# TODO: graph.to_table.to_csv.to_file.run(\n\t#    graph, to_csv = {'separator': ..., 'header': ...},\n\t#    filename = ...,\n\t#    file_mode = ...\n\t# )\n\t# TODO: graph.to_table.to_csv.to_file.run(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\tdef point_to_csv(point):\n\t\treturn separator.join(map(str, point)) + \"\\n\"\n\tdef value_to_csv(value):\n\t\treturn separator.join(map(str, value)) + \"\\n\"\n\treturn (\n\t\theader + \"\\n\" if header is not None else \"\"\n\t\t+ \"\\n\".join(\n\t\t\tpoint_to_csv(point) + value_to_csv(value)\n\t\t\tfor point, value in self.items()\n\t\t)\n\t)\n\n\tdef __repr__(self):\n\t\treturn \"<Graph: {!r}>\".format(dict(self))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\traise LenaTypeError(\"to_csv is deprecated\")\n\treturn to_csv(self, separator=separator, header=header)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\tif header is not None:\n\t\tyield header\n\t\tyield \"\\n\"\n\tfor coord, value in self.items():\n\t\tyield separator.join(map(str, coord))\n\t\tyield separator\n\t\tyield separator.join(map(str, value))\n\t\tyield \"\\n\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t# TODO: change to lena.output.ToCSV\n\tif header is not None:\n\t\tyield header\n\t\tyield \"\\n\"\n\tfor point in self.points:\n\t\tyield separator.join(map(str, point.coord))\n\t\tyield separator\n\t\tyield separator.join(map(str, point.value))\n\t\tyield \"\\n\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "ques_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n\tif self._is_err_coord(coord_name):\n\t\treturn np.where(np.array(self._coord_names) == coord_name)[0]\n\telse:\n\t\treturn None\n\n\n", "def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor err_name in self._err_names:\n\t\tcoord_name_err = err_name[len(coord_name):]\n\t\tif coord_name_err in self.get_coord_names():\n\t\t\terr_indices.append(self._err_names.index(err_name))\n\treturn err_indices\n\n", "def _get_err_indices(self, coord_name):\n\tres = []\n\tfor i, c in enumerate(self._coord_names):\n\t\tif c == coord_name:\n\t\t\tres.append(i)\n\tif len(res) == 0:\n\t\traise ValueError(\n\t\t\t'No error index found for coordinate \"{0}\"'\n\t\t\t.format(coord_name)\n\t\t)\n\treturn res\n\n", "def _get_err_indices(self, coord_name):\n\treturn self._get_indices(coord_name, self.err_coords)\n\n\n", "def _get_err_indices(self, coord_name):\n\ttry:\n\t\treturn self.get_err_indices(coord_name)\n\texcept KeyError:\n\t\traise KeyError(\"Coordinate %s does not have an error array\" % coord_name)\n\t\n", "def _get_err_indices(self, coord_name):\n\t# Get the coordinate indices in the coordinate list\n\tcoord_idx = self.coord_list.index(coord_name)\n\n\t# Get the error indices corresponding to this coordinate\n\terr_idx = np.where(self.err_coord_idx == coord_idx)[0]\n\n\treturn err_idx\n\n\t\n", "def _get_err_indices(self, coord_name):\n\tif coord_name in self._err_indices:\n\t\treturn self._err_indices[coord_name]\n\telse:\n\t\treturn None\n\n\t#end _get_err_indices\n\n\tdef _get_coord_indices(self, coord_name):\n\t\t\"\"\"\n\t\tGet the indices of the coordinate in the data array.\n\t\t\"\"\"\n\t\tif coord_name in self._coord_indices:\n\t\t\treturn self._coord_indices[coord_name]\n\t\telse:\n\t\t\treturn None\n\n\t#end _get_coord_indices\n\n\tdef _get_indices(self, coord_name):\n\t\t\"\"\"\n\t\tGet the indices of the coordinate in the data array.\n\t\t\"\"\"\n\t\tif coord_name in self._indices:\n\t\t\treturn self._indices[coord_name]\n\t\telse:\n\t\t\treturn None\n\n\t#end _get_indices\n\n\tdef get_coord(self, coord_name):\n\t\t\"\"\"\n\t\tGet the coordinate from the data array.\n\t\t\"\"\"\n\n", "def _get_err_indices(self, coord_name):\n\tif coord_name in self.data:\n\t\tif \"error\" in self.data[coord_name]:\n\t\t\treturn self.data[coord_name][\"error\"]\n\t\telse:\n\t\t\treturn None\n\telse:\n\t\traise ValueError(\"Coordinate '%s' not found.\" % coord_name)\n\n", "def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor i in range(self._num_coords):\n\t\tif self._coord_names[i] == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n\n\n", "def _get_err_indices(self, coord_name):\n\tif coord_name in self.coords:\n\t\treturn self.coords[coord_name]['err_indices']\n\telse:\n\t\treturn None\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n\tif self._is_err_coord(coord_name):\n\t\treturn np.where(np.array(self._coord_names) == coord_name)[0]\n\telse:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor err_name in self._err_names:\n\t\tcoord_name_err = err_name[len(coord_name):]\n\t\tif coord_name_err in self.get_coord_names():\n\t\t\terr_indices.append(self._err_names.index(err_name))\n\treturn err_indices\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\tres = []\n\tfor i, c in enumerate(self._coord_names):\n\t\tif c == coord_name:\n\t\t\tres.append(i)\n\tif len(res) == 0:\n\t\traise ValueError(\n\t\t\t'No error index found for coordinate \"{0}\"'\n\t\t\t.format(coord_name)\n\t\t)\n\treturn res\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\treturn self._get_indices(coord_name, self.err_coords)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\ttry:\n\t\treturn self.get_err_indices(coord_name)\n\texcept KeyError:\n\t\traise KeyError(\"Coordinate %s does not have an error array\" % coord_name)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t# Get the coordinate indices in the coordinate list\n\tcoord_idx = self.coord_list.index(coord_name)\n\n\t# Get the error indices corresponding to this coordinate\n\terr_idx = np.where(self.err_coord_idx == coord_idx)[0]\n\n\treturn err_idx\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\tif coord_name in self._err_indices:\n\t\treturn self._err_indices[coord_name]\n\telse:\n\t\treturn None\n\n\t#end _get_err_indices\n\n\tdef _get_coord_indices(self, coord_name):\n\t\t\"\"\"\n\t\tGet the indices of the coordinate in the data array.\n\t\t\"\"\"\n\t\tif coord_name in self._coord_indices:\n\t\t\treturn self._coord_indices[coord_name]\n\t\telse:\n\t\t\treturn None\n\n\t#end _get_coord_indices\n\n\tdef _get_indices(self, coord_name):\n\t\t\"\"\"\n\t\tGet the indices of the coordinate in the data array.\n\t\t\"\"\"\n\t\tif coord_name in self._indices:\n\t\t\treturn self._indices[coord_name]\n\t\telse:\n\t\t\treturn None\n\n\t#end _get_indices\n\n\tdef get_coord(self, coord_name):\n\t\t\"\"\"\n\t\tGet the coordinate from the data array.\n\t\t\"\"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\tif coord_name in self.data:\n\t\tif \"error\" in self.data[coord_name]:\n\t\t\treturn self.data[coord_name][\"error\"]\n\t\telse:\n\t\t\treturn None\n\telse:\n\t\traise ValueError(\"Coordinate '%s' not found.\" % coord_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor i in range(self._num_coords):\n\t\tif self._coord_names[i] == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\tif coord_name in self.coords:\n\t\treturn self.coords[coord_name]['err_indices']\n\telse:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "ques_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n\t# If *context* is None, create a new one.\n\tif context is None:\n\t\tcontext = {}\n\t# Update *context* with the properties of this graph.\n\tcontext.update(self.context_properties)\n\treturn context\n\n", "def _update_context(self, context):\n\tsuper()._update_context(context)\n\tif context.value is None:\n\t\tcontext.value = {}\n\tcontext.value.update(self.properties)\n\tcontext.value[\"fields\"] = self.fields\n\tcontext.value[\"fields_index\"] = self.fields_index\n\tcontext.value[\"fields_type\"] = self.fields_type\n\tcontext.value[\"fields_name\"] = self.fields_name\n\tcontext.value[\"fields_unit\"] = self.fields_unit\n\tcontext.value[\"fields_unit_abbr\"] = self.fields_unit_abbr\n\tcontext.value[\"fields_format\"] = self.fields_format\n\tcontext.value[\"fields_format_index\"] = self.fields_format_index\n\tcontext.value[\"fields_format_type\"] = self.fields_format_type\n\tcontext.value[\"fields_format_name\"] = self.fields_format_name\n\tcontext.value[\"fields_format_unit\"] = self.fields_format_unit\n\tcontext.value[\"fields_format_unit_abbr\"] = self.fields_format_unit_\n", "def _update_context(self, context):\n\tassert context is not None\n\t#context = super(_Graph, self)._update_context(context)\n\t#context = self._update_context_values(context)\n\t#context = self._update_context_errors(context)\n\treturn context\n\n", "def _update_context(self, context):\n\tcontext[\"value\"] = self.value\n\tcontext[\"value\"][\"x\"] = self.x\n\tcontext[\"value\"][\"y\"] = self.y\n\tcontext[\"value\"][\"z\"] = self.z\n\tcontext[\"value\"][\"t\"] = self.t\n\tcontext[\"value\"][\"error\"] = {}\n\tfor k, v in self.__dict__.items():\n\t\tif k.startswith(\"error_\"):\n\t\t\t_, name, err = k.split(\"_\")\n\t\t\tif name in context[\"value\"]:\n\t\t\t\tcontext[\"value\"][\"error\"][err] = \\\n\t\t\t\t\t{\"index\": context[\"value\"][name].index(v)}\n\n", "def _update_context(self, context):\n\tcontext.update(\n\t\t{\"value\": self.value,\n\t\t\"error\": self._get_error_context()})\n\n", "def _update_context(self, context):\n\tcontext.error.update(\n\t\t(name, {\"index\": i})\n\t\tfor i, name in enumerate(self.error_field_names))\n\tcontext.value.update(\n\t\t(name, {\"index\": i})\n\t\tfor i, name in enumerate(self.value_field_names))\n\tcontext.x_indices = self.x_indices\n\tcontext.y_indices = self.y_indices\n\tcontext.z_indices = self.z_indices\n\tcontext.x_label = self.x_label\n\tcontext.y_label = self.y_label\n\tcontext.z_label = self.z_label\n\tcontext.x_unit = self.x_unit\n\tcontext.y_unit = self.y_unit\n\tcontext.z_unit = self.z_unit\n\tcontext.x_name = self.x_field_name\n\tcontext.y_name = self.y_field_name\n\tcontext.z_name = self.z_field_name\n\tcontext.x_error_name = self.x_\n", "def _update_context(self, context):\n\tif context is None:\n\t\tcontext = {}\n\tsubcontext = {}\n\tfor field in self._fields:\n\t\tsubcontext[field] = {\"index\": self._fields.index(field)}\n\t\tif field.endswith(\"_error\"):\n\t\t\tsubcontext[field[:-6] + \"_error\"] = subcontext[field]\n\t\t\tsubcontext[field] = {\"error\": subcontext[field]}\n\tsubcontext[\"error\"] = {\"index\": len(self._fields)}\n\tcontext[\"value\"] = subcontext\n\treturn context\n\n", "def _update_context(self, context):\n\tself._update_context_value(context)\n\tself._update_context_error(context)\n\treturn context\n\n", "def _update_context(self, context):\n\tcontext.value.update(self._context_value)\n\tcontext.error.update(self._context_error)\n\treturn context\n\n\n", "def _update_context(self, context):\n\tfor field in self._fields:\n\t\tif field == \"error_x_low\":\n\t\t\tif \"error\" not in context:\n\t\t\t\tcontext[\"error\"] = {}\n\t\t\tif \"x_low\" not in context[\"error\"]:\n\t\t\t\tcontext[\"error\"][\"x_low\"] = {}\n\t\t\tcontext[\"error\"][\"x_low\"][\"index\"] = self._error_x_low\n\t\telif field == \"error_y_low\":\n\t\t\tif \"error\" not in context:\n\t\t\t\tcontext[\"error\"] = {}\n\t\t\tif \"y_low\" not in context[\"error\"]:\n\t\t\t\tcontext[\"error\"][\"y_low\"] = {}\n\t\t\tcontext[\"error\"][\"y_low\"][\"index\"] = self._error_y_low\n\t\telif field == \"error_z_low\":\n\t\t\tif \"error\" not in context:\n\t\t\t\tcontext[\"error\"] = {}\n\t\t\tif \"z_low\" not in context[\"error\"]:\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n\t# If *context* is None, create a new one.\n\tif context is None:\n\t\tcontext = {}\n\t# Update *context* with the properties of this graph.\n\tcontext.update(self.context_properties)\n\treturn context\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tsuper()._update_context(context)\n\tif context.value is None:\n\t\tcontext.value = {}\n\tcontext.value.update(self.properties)\n\tcontext.value[\"fields\"] = self.fields\n\tcontext.value[\"fields_index\"] = self.fields_index\n\tcontext.value[\"fields_type\"] = self.fields_type\n\tcontext.value[\"fields_name\"] = self.fields_name\n\tcontext.value[\"fields_unit\"] = self.fields_unit\n\tcontext.value[\"fields_unit_abbr\"] = self.fields_unit_abbr\n\tcontext.value[\"fields_format\"] = self.fields_format\n\tcontext.value[\"fields_format_index\"] = self.fields_format_index\n\tcontext.value[\"fields_format_type\"] = self.fields_format_type\n\tcontext.value[\"fields_format_name\"] = self.fields_format_name\n\tcontext.value[\"fields_format_unit\"] = self.fields_format_unit\n\tcontext.value[\"fields_format_unit_abbr\"] = self.fields_format_unit_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tassert context is not None\n\t#context = super(_Graph, self)._update_context(context)\n\t#context = self._update_context_values(context)\n\t#context = self._update_context_errors(context)\n\treturn context\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tcontext[\"value\"] = self.value\n\tcontext[\"value\"][\"x\"] = self.x\n\tcontext[\"value\"][\"y\"] = self.y\n\tcontext[\"value\"][\"z\"] = self.z\n\tcontext[\"value\"][\"t\"] = self.t\n\tcontext[\"value\"][\"error\"] = {}\n\tfor k, v in self.__dict__.items():\n\t\tif k.startswith(\"error_\"):\n\t\t\t_, name, err = k.split(\"_\")\n\t\t\tif name in context[\"value\"]:\n\t\t\t\tcontext[\"value\"][\"error\"][err] = \\\n\t\t\t\t\t{\"index\": context[\"value\"][name].index(v)}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tcontext.update(\n\t\t{\"value\": self.value,\n\t\t\"error\": self._get_error_context()})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tcontext.error.update(\n\t\t(name, {\"index\": i})\n\t\tfor i, name in enumerate(self.error_field_names))\n\tcontext.value.update(\n\t\t(name, {\"index\": i})\n\t\tfor i, name in enumerate(self.value_field_names))\n\tcontext.x_indices = self.x_indices\n\tcontext.y_indices = self.y_indices\n\tcontext.z_indices = self.z_indices\n\tcontext.x_label = self.x_label\n\tcontext.y_label = self.y_label\n\tcontext.z_label = self.z_label\n\tcontext.x_unit = self.x_unit\n\tcontext.y_unit = self.y_unit\n\tcontext.z_unit = self.z_unit\n\tcontext.x_name = self.x_field_name\n\tcontext.y_name = self.y_field_name\n\tcontext.z_name = self.z_field_name\n\tcontext.x_error_name = self.x_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tif context is None:\n\t\tcontext = {}\n\tsubcontext = {}\n\tfor field in self._fields:\n\t\tsubcontext[field] = {\"index\": self._fields.index(field)}\n\t\tif field.endswith(\"_error\"):\n\t\t\tsubcontext[field[:-6] + \"_error\"] = subcontext[field]\n\t\t\tsubcontext[field] = {\"error\": subcontext[field]}\n\tsubcontext[\"error\"] = {\"index\": len(self._fields)}\n\tcontext[\"value\"] = subcontext\n\treturn context\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tself._update_context_value(context)\n\tself._update_context_error(context)\n\treturn context\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tcontext.value.update(self._context_value)\n\tcontext.error.update(self._context_error)\n\treturn context\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tfor field in self._fields:\n\t\tif field == \"error_x_low\":\n\t\t\tif \"error\" not in context:\n\t\t\t\tcontext[\"error\"] = {}\n\t\t\tif \"x_low\" not in context[\"error\"]:\n\t\t\t\tcontext[\"error\"][\"x_low\"] = {}\n\t\t\tcontext[\"error\"][\"x_low\"][\"index\"] = self._error_x_low\n\t\telif field == \"error_y_low\":\n\t\t\tif \"error\" not in context:\n\t\t\t\tcontext[\"error\"] = {}\n\t\t\tif \"y_low\" not in context[\"error\"]:\n\t\t\t\tcontext[\"error\"][\"y_low\"] = {}\n\t\t\tcontext[\"error\"][\"y_low\"][\"index\"] = self._error_y_low\n\t\telif field == \"error_z_low\":\n\t\t\tif \"error\" not in context:\n\t\t\t\tcontext[\"error\"] = {}\n\t\t\tif \"z_low\" not in context[\"error\"]:\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "ques_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n\tres = np.zeros_like(bins)\n\tres[1:] = np.cumsum(bins * np.diff(edges))\n\treturn res\n\n", "def integral(bins, edges):\n\tassert len(bins) == len(edges) - 1\n\tdelta = np.diff(edges)\n\treturn np.sum(bins * delta)\n\n", "def integral(bins, edges):\n\tne = len(edges)\n\tnb = len(bins)\n\tassert ne-1 == nb\n\tintegral = 0.0\n\tfor i in range(nb):\n\t\tintegral += bins[i] * (edges[i+1] - edges[i])\n\treturn integral\n\n\n", "def integral(bins, edges):\n\treturn np.sum(bins * np.diff(edges))\n\n", "def integral(bins, edges):\n\tif (len(bins) != len(edges) - 1):\n\t\traise ValueError(\"Bins and edges must have the same length\")\n\tdelta = edges[1:] - edges[:-1]\n\treturn numpy.dot(delta, bins)\n\n", "def integral(bins, edges):\n\tif len(edges) == 1:\n\t\treturn bins\n\treturn bins * np.diff(edges)\n\n", "def integral(bins, edges):\n\tif bins.dtype != numpy.float64:\n\t\tbins = bins.astype(numpy.float64)\n\treturn numpy.sum(bins * (edges[1:] - edges[:-1]))\n\n", "def integral(bins, edges):\n\tfrom .histogram import histogram\n\tfrom .integration import integration\n\tfrom .meshgrid import meshgrid\n\tfrom .squeeze import squeeze\n\tfrom .transpose import transpose\n\tfrom .concatenate import concatenate\n\tfrom .power import power\n\tfrom .sumproduct import sumproduct\n\tfrom .ndarray import ndarray\n\tfrom .zeros import zeros\n\tfrom .shape import shape\n\tfrom .ostwo import ostwo\n\tfrom .nan import nan\n\tfrom .isnan import isnan\n\tfrom .logical_or import logical_or\n\tfrom .logical_and import logical_and\n\tfrom .all import all\n\tfrom .less import less\n\tfrom .equal import equal\n\tfrom .greater import greater\n\tfrom .greater_equal import greater_equal\n\tfrom .isfinite import isfinite\n\tfrom .isinf import isinf\n\n\tif ostwo:\n\t\tif not isinstance(edges, ndarray):\n\t\t\tedges = array(edges)\n\t\tif bins is None:\n\t\t\treturn\n", "def integral(bins, edges):\n\treturn np.sum(bins * np.diff(edges))\n\n", "def integral(bins, edges):\n\treturn np.sum(bins * (edges[1:] - edges[:-1]))\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n\tres = np.zeros_like(bins)\n\tres[1:] = np.cumsum(bins * np.diff(edges))\n\treturn res\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tassert len(bins) == len(edges) - 1\n\tdelta = np.diff(edges)\n\treturn np.sum(bins * delta)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tne = len(edges)\n\tnb = len(bins)\n\tassert ne-1 == nb\n\tintegral = 0.0\n\tfor i in range(nb):\n\t\tintegral += bins[i] * (edges[i+1] - edges[i])\n\treturn integral\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\treturn np.sum(bins * np.diff(edges))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tif (len(bins) != len(edges) - 1):\n\t\traise ValueError(\"Bins and edges must have the same length\")\n\tdelta = edges[1:] - edges[:-1]\n\treturn numpy.dot(delta, bins)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tif len(edges) == 1:\n\t\treturn bins\n\treturn bins * np.diff(edges)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tif bins.dtype != numpy.float64:\n\t\tbins = bins.astype(numpy.float64)\n\treturn numpy.sum(bins * (edges[1:] - edges[:-1]))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tfrom .histogram import histogram\n\tfrom .integration import integration\n\tfrom .meshgrid import meshgrid\n\tfrom .squeeze import squeeze\n\tfrom .transpose import transpose\n\tfrom .concatenate import concatenate\n\tfrom .power import power\n\tfrom .sumproduct import sumproduct\n\tfrom .ndarray import ndarray\n\tfrom .zeros import zeros\n\tfrom .shape import shape\n\tfrom .ostwo import ostwo\n\tfrom .nan import nan\n\tfrom .isnan import isnan\n\tfrom .logical_or import logical_or\n\tfrom .logical_and import logical_and\n\tfrom .all import all\n\tfrom .less import less\n\tfrom .equal import equal\n\tfrom .greater import greater\n\tfrom .greater_equal import greater_equal\n\tfrom .isfinite import isfinite\n\tfrom .isinf import isinf\n\n\tif ostwo:\n\t\tif not isinstance(edges, ndarray):\n\t\t\tedges = array(edges)\n\t\tif bins is None:\n\t\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\treturn np.sum(bins * np.diff(edges))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\treturn np.sum(bins * (edges[1:] - edges[:-1]))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "ques_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif not is_sequence(seq):\n\t\treturn False\n\tif is_source_seq(seq):\n\t\treturn False\n\treturn any(is_fill_request_seq(elem) for elem in iter(seq))\n\n", "def is_fill_request_seq(seq):\n\treturn is_fill_request(seq) or \\\n\t\t(isinstance(seq, Seq) and not isinstance(seq, Source) and \\\n\t\t\tany(is_fill_request(s) for s in seq))\n\n", "def is_fill_request_seq(seq):\n\treturn is_fill_request_seq_2(seq) and not is_source_seq(seq)\n\n", "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif seq.is_source():\n\t\treturn False\n\tfor elem in seq:\n\t\tif isinstance(elem, FillRequest):\n\t\t\treturn True\n\treturn False\n\n", "def is_fill_request_seq(seq):\n\treturn (\n\t\tis_fill_request(seq)\n\t\tor (\n\t\t\tis_sequence(seq)\n\t\t\tand not is_source(seq)\n\t\t\tand any(is_fill_request(element) for element in seq)\n\t\t)\n\t)\n\n\n", "def is_fill_request_seq(seq):\n\treturn (is_fill_request(seq) or\n\t\t(is_instance(seq, (list, tuple)) and\n\t\tany(is_fill_request(e) for e in seq) and\n\t\tnot is_source(seq)))\n\n", "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif isinstance(seq, Source):\n\t\treturn False\n\ttry:\n\t\tfor x in seq:\n\t\t\tif not is_fill_request_seq(x):\n\t\t\t\treturn False\n\t\t\treturn True\n\texcept TypeError:\n\t\treturn False\n\n", "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif isinstance(seq, Source):\n\t\treturn False\n\ttry:\n\t\tfor s in seq:\n\t\t\tif isinstance(s, FillRequest):\n\t\t\t\treturn True\n\texcept TypeError:\n\t\tpass\n\treturn False\n\n\n", "def is_fill_request_seq(seq):\n\treturn (is_fill_request(seq)\n\t\t\tor\n\t\t\t(is_fill_request_seq_or_source(seq)\n\t\t\tand\n\t\t\tnot is_source(seq))\n\t\t\t)\n\n", "def is_fill_request_seq(seq):\n\tif not is_fill_request(seq):\n\t\treturn False\n\tif is_source_seq(seq):\n\t\treturn False\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif not is_sequence(seq):\n\t\treturn False\n\tif is_source_seq(seq):\n\t\treturn False\n\treturn any(is_fill_request_seq(elem) for elem in iter(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn is_fill_request(seq) or \\\n\t\t(isinstance(seq, Seq) and not isinstance(seq, Source) and \\\n\t\t\tany(is_fill_request(s) for s in seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn is_fill_request_seq_2(seq) and not is_source_seq(seq)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif seq.is_source():\n\t\treturn False\n\tfor elem in seq:\n\t\tif isinstance(elem, FillRequest):\n\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn (\n\t\tis_fill_request(seq)\n\t\tor (\n\t\t\tis_sequence(seq)\n\t\t\tand not is_source(seq)\n\t\t\tand any(is_fill_request(element) for element in seq)\n\t\t)\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn (is_fill_request(seq) or\n\t\t(is_instance(seq, (list, tuple)) and\n\t\tany(is_fill_request(e) for e in seq) and\n\t\tnot is_source(seq)))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif isinstance(seq, Source):\n\t\treturn False\n\ttry:\n\t\tfor x in seq:\n\t\t\tif not is_fill_request_seq(x):\n\t\t\t\treturn False\n\t\t\treturn True\n\texcept TypeError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequest):\n\t\treturn True\n\tif isinstance(seq, Source):\n\t\treturn False\n\ttry:\n\t\tfor s in seq:\n\t\t\tif isinstance(s, FillRequest):\n\t\t\t\treturn True\n\texcept TypeError:\n\t\tpass\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn (is_fill_request(seq)\n\t\t\tor\n\t\t\t(is_fill_request_seq_or_source(seq)\n\t\t\tand\n\t\t\tnot is_source(seq))\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\tif not is_fill_request(seq):\n\t\treturn False\n\tif is_source_seq(seq):\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "ques_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(obj.fill) and callable(obj.request)\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\t\n\t\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "def is_fill_request_el(obj):\n\treturn is_fill_el(obj) and is_request_el(obj)\n\t\n", "def is_fill_request_el(obj):\n\treturn (hasattr(obj, 'fill') and callable(obj.fill)) and (hasattr(obj, 'request') and callable(obj.request))\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request') and callable(obj.fill) and callable(obj.request)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\t\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn is_fill_el(obj) and is_request_el(obj)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n\treturn (hasattr(obj, 'fill') and callable(obj.fill)) and (hasattr(obj, 'request') and callable(obj.request))\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "ques_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n\treturn hasattr(obj, \"run\") and hasattr(obj.run, \"__call__\")\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n", "def is_run_el(obj):\n\treturn (hasattr(obj, 'run') and callable(obj.run))\n\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n\n", "def is_run_el(obj):\n\treturn callable(getattr(obj, 'run', None)) and getattr(obj, 'run')\n\n\n", "def is_run_el(obj):\n\treturn is_method(obj, 'run')\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, \"run\") and hasattr(obj.run, \"__call__\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn (hasattr(obj, 'run') and callable(obj.run))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn callable(getattr(obj, 'run', None)) and getattr(obj, 'run')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n\treturn is_method(obj, 'run')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run') and callable(obj.run)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "ques_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "def is_fill_compute_el(obj):\n\tif not hasattr(obj, 'fill') or not hasattr(obj, 'compute'):\n\t\treturn False\n\treturn True\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "def is_fill_compute_el(obj):\n\treturn callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'compute', None))\n\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\tif not hasattr(obj, 'fill') or not hasattr(obj, 'compute'):\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn callable(getattr(obj, 'fill', None)) and callable(getattr(obj, 'compute', None))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"compute\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "ques_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n\td = {}\n\tfor k, v in d1.iteritems():\n\t\tif k not in d2:\n\t\t\td[k] = v\n\t\telif level != 0:\n\t\t\tif isinstance(v, dict) and isinstance(d2[k], dict):\n\t\t\t\tif level > 0:\n\t\t\t\t\tlevel -= 1\n\t\t\t\tv = difference(v, d2[k], level)\n\t\t\t\tlevel += 1\n\t\t\tif v:\n\t\t\t\td[k] = v\n\treturn d\n\n", "def difference(d1, d2, level=-1):\n\tresult = {}\n\tfor key, value in d1.iteritems():\n\t\tif key not in d2:\n\t\t\tresult[key] = value\n\t\telse:\n\t\t\tif level:\n\t\t\t\tif isinstance(value, dict) and isinstance(d2[key], dict):\n\t\t\t\t\tvalue = difference(value, d2[key], level - 1)\n\t\t\t\t\tif value:\n\t\t\t\t\t\tresult[key] = value\n\t\t\t\telse:\n\t\t\t\t\tif value != d2[key]:\n\t\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tif value != d2[key]:\n\t\t\t\t\tresult[key] = value\n\treturn result\n\n", "def difference(d1, d2, level=-1):\n\tdiff = {}\n\tif level == 0:\n\t\tif d1 != d2:\n\t\t\tdiff = d1\n\telse:\n\t\tfor key in d1.keys():\n\t\t\tif key in d2:\n\t\t\t\tif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n\t\t\t\t\tdiff[key] = difference(d1[key], d2[key], level-1)\n\t\t\t\t\tif not diff[key]:\n\t\t\t\t\t\tdel diff[key]\n\t\t\t\telif d1[key] != d2[key]:\n\t\t\t\t\tdiff[key] = d1[key]\n\t\t\telse:\n\t\t\t\tdiff[key] = d1[key]\n\treturn diff\n\n\n", "def difference(d1, d2, level=-1):\n\td = {}\n\tfor k in d1:\n\t\tif level == 0:\n\t\t\td[k] = d1[k]\n\t\telif k not in d2:\n\t\t\td[k] = d1[k]\n\t\telif not isinstance(d1[k], dict) or not isinstance(d2[k], dict):\n\t\t\tif d1[k] != d2[k]:\n\t\t\t\td[k] = d1[k]\n\t\telse:\n\t\t\td[k] = difference(d1[k], d2[k], level=(level - 1))\n\treturn d\n\n", "def difference(d1, d2, level=-1):\n\td = {}\n\tfor k in set(d1.keys()).difference(d2.keys()):\n\t\td[k] = d1[k]\n\tfor k in set(d2.keys()).difference(d1.keys()):\n\t\td[k] = d2[k]\n\tfor k in set(d1.keys()).intersection(d2.keys()):\n\t\tif d1[k] != d2[k]:\n\t\t\tif level != 0 and isinstance(d1[k], dict) and isinstance(d2[k], dict):\n\t\t\t\td[k] = difference(d1[k], d2[k], level=level-1)\n\t\t\telse:\n\t\t\t\td[k] = d1[k]\n\treturn d\n\n", "def difference(d1, d2, level=-1):\n\treturn _intersection(d1, d2, lambda a, b: a is not b, level)\n\n", "def difference(d1, d2, level=-1):\n\tif not isinstance(d1, dict):\n\t\traise TypeError(\"'d1' must be dict, not %s\" % type(d1).__name__)\n\tif not isinstance(d2, dict):\n\t\traise TypeError(\"'d2' must be dict, not %s\" % type(d2).__name__)\n\treturn _dict_difference(d1, d2, level)\n\n", "def difference(d1, d2, level=-1):\n\td = _difference(d1, d2, level)\n\treturn d\n\n\n", "def difference(d1, d2, level=-1):\n\treturn _difference(d1, d2, level)\n\n\n", "def difference(d1, d2, level=-1):\n\treturn _difference(d1, d2, level)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n\td = {}\n\tfor k, v in d1.iteritems():\n\t\tif k not in d2:\n\t\t\td[k] = v\n\t\telif level != 0:\n\t\t\tif isinstance(v, dict) and isinstance(d2[k], dict):\n\t\t\t\tif level > 0:\n\t\t\t\t\tlevel -= 1\n\t\t\t\tv = difference(v, d2[k], level)\n\t\t\t\tlevel += 1\n\t\t\tif v:\n\t\t\t\td[k] = v\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\tresult = {}\n\tfor key, value in d1.iteritems():\n\t\tif key not in d2:\n\t\t\tresult[key] = value\n\t\telse:\n\t\t\tif level:\n\t\t\t\tif isinstance(value, dict) and isinstance(d2[key], dict):\n\t\t\t\t\tvalue = difference(value, d2[key], level - 1)\n\t\t\t\t\tif value:\n\t\t\t\t\t\tresult[key] = value\n\t\t\t\telse:\n\t\t\t\t\tif value != d2[key]:\n\t\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tif value != d2[key]:\n\t\t\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\tdiff = {}\n\tif level == 0:\n\t\tif d1 != d2:\n\t\t\tdiff = d1\n\telse:\n\t\tfor key in d1.keys():\n\t\t\tif key in d2:\n\t\t\t\tif isinstance(d1[key], dict) and isinstance(d2[key], dict):\n\t\t\t\t\tdiff[key] = difference(d1[key], d2[key], level-1)\n\t\t\t\t\tif not diff[key]:\n\t\t\t\t\t\tdel diff[key]\n\t\t\t\telif d1[key] != d2[key]:\n\t\t\t\t\tdiff[key] = d1[key]\n\t\t\telse:\n\t\t\t\tdiff[key] = d1[key]\n\treturn diff\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\td = {}\n\tfor k in d1:\n\t\tif level == 0:\n\t\t\td[k] = d1[k]\n\t\telif k not in d2:\n\t\t\td[k] = d1[k]\n\t\telif not isinstance(d1[k], dict) or not isinstance(d2[k], dict):\n\t\t\tif d1[k] != d2[k]:\n\t\t\t\td[k] = d1[k]\n\t\telse:\n\t\t\td[k] = difference(d1[k], d2[k], level=(level - 1))\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\td = {}\n\tfor k in set(d1.keys()).difference(d2.keys()):\n\t\td[k] = d1[k]\n\tfor k in set(d2.keys()).difference(d1.keys()):\n\t\td[k] = d2[k]\n\tfor k in set(d1.keys()).intersection(d2.keys()):\n\t\tif d1[k] != d2[k]:\n\t\t\tif level != 0 and isinstance(d1[k], dict) and isinstance(d2[k], dict):\n\t\t\t\td[k] = difference(d1[k], d2[k], level=level-1)\n\t\t\telse:\n\t\t\t\td[k] = d1[k]\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\treturn _intersection(d1, d2, lambda a, b: a is not b, level)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\tif not isinstance(d1, dict):\n\t\traise TypeError(\"'d1' must be dict, not %s\" % type(d1).__name__)\n\tif not isinstance(d2, dict):\n\t\traise TypeError(\"'d2' must be dict, not %s\" % type(d2).__name__)\n\treturn _dict_difference(d1, d2, level)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\td = _difference(d1, d2, level)\n\treturn d\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\treturn _difference(d1, d2, level)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\treturn _difference(d1, d2, level)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "ques_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n\tself.__fill(coord, weight)\n\tself.__overflow += self.__ooverflow\n\tself.__ooverflow = 0\n\tself.__underflow += self.__ounderflow\n\tself.__ounderflow = 0\n\n", "def fill(self, coord, weight=1):\n\tself.__fill(coord, weight)\n\n", "def fill(self, coord, weight=1):\n\t\tfor i, x in enumerate(coord):\n\t\t\tif self.axis[i].isOverflow(x):\n\t\t\t\treturn\n\t\tself.rootn(weight)\n\t\tself.hist.Fill(self.to_root(coord), weight.value)\n\tdef fill_array(self, coord, weight=1):\n\t\t\"\"\"\n\t\tFill histogram at a Numpy array of coordinates with the given weight.\n\n", "def fill(self, coord, weight=1):\n\tself._h.Fill(coord, weight)\n\n\tdef fill_n(self, coords, weights=None):\n\t\t\"\"\"\n\t\tFill histogram with *coords*,*weights*\n\n", "def fill(self, coord, weight=1):\n\tpass\n\t\n", "def fill(self, coord, weight=1):\n\tif self.nDim != len(coord):\n\t\traise ValueError(\"number of coordinates is not correct\")\n\tif self.is_empty():\n\t\traise ValueError(\"histogram is empty\")\n\tidx = self.find_bin_index(coord)\n\tif idx is not None:\n\t\tself.array[idx] += weight\n\n\tdef fill_n(self, n, coord, weight=None):\n\t\t\"\"\"\n\t\tFill histogram for *n* times at *coord* with the given *weight*.\n\n", "def fill(self, coord, weight=1):\n\tself.hist.Fill(coord, weight)\n\n\tdef fill_n(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram at *coords* with *weights*.\n\n", "def fill(self, coord, weight=1):\n\tif not self.__is_init:\n\t\traise Exception(\"Histogram is not initialized\")\n\tif len(coord) != self.__dim:\n\t\traise Exception(\"Coordinates have wrong dimension\")\n\tif self.__dim == 1:\n\t\ti = int(round((coord[0] - self.__axis[0][0]) / self.__axis[0][2]))\n\t\tif i < 0 or i >= self.__axis[0][1]:\n\t\t\treturn\n\t\tself.__data[i] += weight\n\telif self.__dim == 2:\n\t\ti = int(round((coord[0] - self.__axis[0][0]) / self.__axis[0][2]))\n\t\tj = int(round((coord[1] - self.__axis[1][0]) / self.__axis[1][2]))\n\t\tif i < 0 or i >= self.__axis[0][1] or j < 0 or j >= self.__axis[1][1]:\n\t\t\treturn\n\t\tself.__data[i][\n", "def fill(self, coord, weight=1):\n\tpass\n\t\n", "def fill(self, coord, weight=1):\n\t\treturn _pyroot.TH1F.fill(self, coord, weight)\n\t\n\tdef findBin(self, x, y = 0, z = 0):\n\t\t\"\"\"\n\t\tReturn bin number corresponding to the specified x, y and z\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n\tself.__fill(coord, weight)\n\tself.__overflow += self.__ooverflow\n\tself.__ooverflow = 0\n\tself.__underflow += self.__ounderflow\n\tself.__ounderflow = 0\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tself.__fill(coord, weight)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\tfor i, x in enumerate(coord):\n\t\t\tif self.axis[i].isOverflow(x):\n\t\t\t\treturn\n\t\tself.rootn(weight)\n\t\tself.hist.Fill(self.to_root(coord), weight.value)\n\tdef fill_array(self, coord, weight=1):\n\t\t\"\"\"\n\t\tFill histogram at a Numpy array of coordinates with the given weight.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tself._h.Fill(coord, weight)\n\n\tdef fill_n(self, coords, weights=None):\n\t\t\"\"\"\n\t\tFill histogram with *coords*,*weights*\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tif self.nDim != len(coord):\n\t\traise ValueError(\"number of coordinates is not correct\")\n\tif self.is_empty():\n\t\traise ValueError(\"histogram is empty\")\n\tidx = self.find_bin_index(coord)\n\tif idx is not None:\n\t\tself.array[idx] += weight\n\n\tdef fill_n(self, n, coord, weight=None):\n\t\t\"\"\"\n\t\tFill histogram for *n* times at *coord* with the given *weight*.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tself.hist.Fill(coord, weight)\n\n\tdef fill_n(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram at *coords* with *weights*.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tif not self.__is_init:\n\t\traise Exception(\"Histogram is not initialized\")\n\tif len(coord) != self.__dim:\n\t\traise Exception(\"Coordinates have wrong dimension\")\n\tif self.__dim == 1:\n\t\ti = int(round((coord[0] - self.__axis[0][0]) / self.__axis[0][2]))\n\t\tif i < 0 or i >= self.__axis[0][1]:\n\t\t\treturn\n\t\tself.__data[i] += weight\n\telif self.__dim == 2:\n\t\ti = int(round((coord[0] - self.__axis[0][0]) / self.__axis[0][2]))\n\t\tj = int(round((coord[1] - self.__axis[1][0]) / self.__axis[1][2]))\n\t\tif i < 0 or i >= self.__axis[0][1] or j < 0 or j >= self.__axis[1][1]:\n\t\t\treturn\n\t\tself.__data[i][\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t\treturn _pyroot.TH1F.fill(self, coord, weight)\n\t\n\tdef findBin(self, x, y = 0, z = 0):\n\t\t\"\"\"\n\t\tReturn bin number corresponding to the specified x, y and z\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "ques_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n\terrors = {}\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors[str(key)] = 'expected string or bytes-like object'\n\t\telif not LABEL_KEY_RE.match(key):\n\t\t\terrors[key] = \"Label key '{}' does not match the regex {}\".format(key, LABEL_KEY_RE.pattern)\n\t\tif not isinstance(value, str):\n\t\t\terrors[str(value)] = 'expected string or bytes-like object'\n\t\telif not LABEL_VALUE_RE.match(value):\n\t\t\terrors[value] = \"Label value '{}' does not match the regex {}\".format(value, LABEL_VALUE_RE.pattern)\n\tif errors:\n\t\traise ValidationError(list(errors.items()))\n\n\n", "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, (str, bytes)):\n\t\t\terrors.append({key: 'expected string or bytes-like object'})\n\t\telif not re.match(LABEL_KEY_REGEX, key):\n\t\t\terrors.append({key: f\"Label key '{key}' does not match the regex '{LABEL_KEY_REGEX}'\"})\n\t\tif not isinstance(value, (str, bytes)):\n\t\t\terrors.append({value: 'expected string or bytes-like object'})\n\t\telif not re.match(LABEL_VALUE_REGEX, value):\n\t\t\terrors.append({value: f\"Label value '{value}' does not match the regex '{LABEL_VALUE_REGEX}'\"})\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "def _validate_labels(labels):\n\terrors = []\n\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors.append({key: \"Label key '{}' is not a string\".format(key)})\n\t\telse:\n\t\t\tmatch = _regex_key.match(key)\n\t\t\tif not match:\n\t\t\t\terrors.append(\n\t\t\t\t\t{\n\t\t\t\t\t\tkey: \"Label key '{}' does not match the regex {}\".format(\n\t\t\t\t\t\t\tkey, _regex_key.pattern\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\t\t\t\t)\n\n\t\tif not isinstance(value, str):\n\t\t\terrors.append(\n\t\t\t\t{key: \"Label value '{}' is not a string\".format(value)}\n\t\t\t)\n\t\telse:\n\t\t\tmatch = _regex_value.match(value)\n\t\t\tif not match:\n\t\t\t\terrors.append(\n\t\t\t\t\t{\n\n", "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\t\t\tcontinue\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({value: err.messages})\n\t\t\tcontinue\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors.append({key: \"expected string or bytes-like object\"})\n\t\tif not isinstance(value, str):\n\t\t\terrors.append({key: \"expected string or bytes-like object\"})\n\t\tif not _LABEL_KEY_RE.match(key):\n\t\t\terrors.append({key: f\"Label key '{key}' does not match the regex {_LABEL_KEY_RE.pattern!r}\"})\n\t\tif not _LABEL_VALUE_RE.match(value):\n\t\t\terrors.append({value: f\"Label value '{value}' does not match the regex {_LABEL_VALUE_RE.pattern!r}\"})\n\tif errors:\n\t\traise ValidationError(_human_readable_errors(errors))\n\n\n", "def _validate_labels(labels):\n\terrors = []\n\tfor (key, value) in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages[0]})\n\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\ttry:\n\t\t\t\t\t_validate_label_value(v)\n\t\t\t\texcept ValidationError as err:\n\t\t\t\t\terrors.append({v: err.messages[0]})\n\t\telse:\n\t\t\ttry:\n\t\t\t\t_validate_label_value(value)\n\t\t\texcept ValidationError as err:\n\t\t\t\terrors.append({value: err.messages[0]})\n\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({value: err.messages})\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise ValidationError(\"Labels must be a dictionary\")\n\n\terrors = []\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors.append(\n\t\t\t\t{\n\t\t\t\t\tkey: f\"expected string or bytes-like object (got {type(key).__name__})\"\n\t\t\t\t}\n\t\t\t)\n\t\telif not re.match(LABEL_KEY_RE, key):\n\t\t\terrors.append({key: f\"Label key '{key}' does not match the regex {LABEL_KEY_RE}\"})\n\n\t\tif not isinstance(value, str):\n\t\t\terrors.append(\n\t\t\t\t{\n\t\t\t\t\tvalue: f\"expected string or bytes-like object (got {type(value).__name__})\"\n\t\t\t\t}\n\t\t\t)\n\t\telif not re.match(LABEL_VALUE_RE, value):\n\t\t\terrors.append(\n\t\t\n", "def _validate_labels(labels):\n\terrors = {}\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.update(err.messages)\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.update(err.messages)\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n\terrors = {}\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors[str(key)] = 'expected string or bytes-like object'\n\t\telif not LABEL_KEY_RE.match(key):\n\t\t\terrors[key] = \"Label key '{}' does not match the regex {}\".format(key, LABEL_KEY_RE.pattern)\n\t\tif not isinstance(value, str):\n\t\t\terrors[str(value)] = 'expected string or bytes-like object'\n\t\telif not LABEL_VALUE_RE.match(value):\n\t\t\terrors[value] = \"Label value '{}' does not match the regex {}\".format(value, LABEL_VALUE_RE.pattern)\n\tif errors:\n\t\traise ValidationError(list(errors.items()))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, (str, bytes)):\n\t\t\terrors.append({key: 'expected string or bytes-like object'})\n\t\telif not re.match(LABEL_KEY_REGEX, key):\n\t\t\terrors.append({key: f\"Label key '{key}' does not match the regex '{LABEL_KEY_REGEX}'\"})\n\t\tif not isinstance(value, (str, bytes)):\n\t\t\terrors.append({value: 'expected string or bytes-like object'})\n\t\telif not re.match(LABEL_VALUE_REGEX, value):\n\t\t\terrors.append({value: f\"Label value '{value}' does not match the regex '{LABEL_VALUE_REGEX}'\"})\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = []\n\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors.append({key: \"Label key '{}' is not a string\".format(key)})\n\t\telse:\n\t\t\tmatch = _regex_key.match(key)\n\t\t\tif not match:\n\t\t\t\terrors.append(\n\t\t\t\t\t{\n\t\t\t\t\t\tkey: \"Label key '{}' does not match the regex {}\".format(\n\t\t\t\t\t\t\tkey, _regex_key.pattern\n\t\t\t\t\t\t)\n\t\t\t\t\t}\n\t\t\t\t)\n\n\t\tif not isinstance(value, str):\n\t\t\terrors.append(\n\t\t\t\t{key: \"Label value '{}' is not a string\".format(value)}\n\t\t\t)\n\t\telse:\n\t\t\tmatch = _regex_value.match(value)\n\t\t\tif not match:\n\t\t\t\terrors.append(\n\t\t\t\t\t{\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\t\t\tcontinue\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({value: err.messages})\n\t\t\tcontinue\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors.append({key: \"expected string or bytes-like object\"})\n\t\tif not isinstance(value, str):\n\t\t\terrors.append({key: \"expected string or bytes-like object\"})\n\t\tif not _LABEL_KEY_RE.match(key):\n\t\t\terrors.append({key: f\"Label key '{key}' does not match the regex {_LABEL_KEY_RE.pattern!r}\"})\n\t\tif not _LABEL_VALUE_RE.match(value):\n\t\t\terrors.append({value: f\"Label value '{value}' does not match the regex {_LABEL_VALUE_RE.pattern!r}\"})\n\tif errors:\n\t\traise ValidationError(_human_readable_errors(errors))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = []\n\tfor (key, value) in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages[0]})\n\n\t\tif isinstance(value, list):\n\t\t\tfor v in value:\n\t\t\t\ttry:\n\t\t\t\t\t_validate_label_value(v)\n\t\t\t\texcept ValidationError as err:\n\t\t\t\t\terrors.append({v: err.messages[0]})\n\t\telse:\n\t\t\ttry:\n\t\t\t\t_validate_label_value(value)\n\t\t\texcept ValidationError as err:\n\t\t\t\terrors.append({value: err.messages[0]})\n\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({value: err.messages})\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = []\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.append({key: err.messages})\n\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise ValidationError(\"Labels must be a dictionary\")\n\n\terrors = []\n\tfor key, value in labels.items():\n\t\tif not isinstance(key, str):\n\t\t\terrors.append(\n\t\t\t\t{\n\t\t\t\t\tkey: f\"expected string or bytes-like object (got {type(key).__name__})\"\n\t\t\t\t}\n\t\t\t)\n\t\telif not re.match(LABEL_KEY_RE, key):\n\t\t\terrors.append({key: f\"Label key '{key}' does not match the regex {LABEL_KEY_RE}\"})\n\n\t\tif not isinstance(value, str):\n\t\t\terrors.append(\n\t\t\t\t{\n\t\t\t\t\tvalue: f\"expected string or bytes-like object (got {type(value).__name__})\"\n\t\t\t\t}\n\t\t\t)\n\t\telif not re.match(LABEL_VALUE_RE, value):\n\t\t\terrors.append(\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\terrors = {}\n\tfor key, value in labels.items():\n\t\ttry:\n\t\t\t_validate_label_key(key)\n\t\texcept ValidationError as err:\n\t\t\terrors.update(err.messages)\n\t\ttry:\n\t\t\t_validate_label_value(value)\n\t\texcept ValidationError as err:\n\t\t\terrors.update(err.messages)\n\tif errors:\n\t\traise ValidationError(errors)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "ques_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(r'^[a-zA-Z0-9_\\-]+$')\n\treturn _resource_name_regex\n\n\n", "def _get_resource_name_regex():\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(\n\t\t\tr'''^(\n\t\t\t\t[a-zA-Z0-9_-]+\n\t\t\t)$''', re.VERBOSE | re.IGNORECASE)\n\treturn _resource_name_regex\n\n\n", "def _get_resource_name_regex():\n\tif _resource_name_regex is not None:\n\t\treturn _resource_name_regex\n\treturn re.compile(r\"^[a-zA-Z0-9_\\-\\.]{3,100}$\")\n\n\n", "def _get_resource_name_regex():\n\tif not hasattr(_get_resource_name_regex, \"regex\"):\n\t\t_get_resource_name_regex.regex = re.compile(\"^[a-zA-Z]+[a-zA-Z0-9_-]*$\")\n\treturn _get_resource_name_regex.regex\n\n\n", "def _get_resource_name_regex():\n\tglobal _resource_name_regex\n\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(\n\t\t\tr\"(?P<name>[a-zA-Z_][a-zA-Z0-9_]*(?:[.][a-zA-Z_][a-zA-Z0-9_]*)*)\"\n\t\t)\n\n\treturn _resource_name_regex\n\n\n", "def _get_resource_name_regex():\n\tif _RESOURCE_NAME_REGEX is None:\n\t\t_RESOURCE_NAME_REGEX = re.compile(r\"^[a-zA-Z0-9_\\-]{1,255}$\")\n\treturn _RESOURCE_NAME_REGEX\n\n\n", "def _get_resource_name_regex():\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(\n\t\t\tr\"\"\"\n\t\t\t^\n\t\t\t(?P<name>[a-zA-Z]([a-zA-Z0-9-]+)?[a-zA-Z0-9])\n\t\t\t$\n\t\t\t\"\"\",\n\t\t\tre.VERBOSE)\n\treturn _resource_name_regex\n\n\n", "def _get_resource_name_regex():\n\tif not hasattr(_get_resource_name_regex, \"re\"):\n\t\t_get_resource_name_regex.re = re.compile(\"^[a-zA-Z0-9_]+$\")\n\treturn _get_resource_name_regex.re\n\n\n", "def _get_resource_name_regex():\n\tif not hasattr(_get_resource_name_regex, 'pattern'):\n\t\t_get_resource_name_regex.pattern = re.compile('^[a-z][a-z0-9]*$')\n\treturn _get_resource_name_regex.pattern\n\n\n", "def _get_resource_name_regex():\n\tglobal _resource_name_regex\n\n\tif _resource_name_regex is not None:\n\t\treturn _resource_name_regex\n\n\t# inspired by https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/validation/validation.go\n\t_resource_name_regex = re.compile(\n\t\t\"^[a-z0-9A-Z][a-z0-9A-Z-]{1,63}$\",\n\t\tre.ASCII\n\t)\n\n\treturn _resource_name_regex\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(r'^[a-zA-Z0-9_\\-]+$')\n\treturn _resource_name_regex\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(\n\t\t\tr'''^(\n\t\t\t\t[a-zA-Z0-9_-]+\n\t\t\t)$''', re.VERBOSE | re.IGNORECASE)\n\treturn _resource_name_regex\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tif _resource_name_regex is not None:\n\t\treturn _resource_name_regex\n\treturn re.compile(r\"^[a-zA-Z0-9_\\-\\.]{3,100}$\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tif not hasattr(_get_resource_name_regex, \"regex\"):\n\t\t_get_resource_name_regex.regex = re.compile(\"^[a-zA-Z]+[a-zA-Z0-9_-]*$\")\n\treturn _get_resource_name_regex.regex\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tglobal _resource_name_regex\n\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(\n\t\t\tr\"(?P<name>[a-zA-Z_][a-zA-Z0-9_]*(?:[.][a-zA-Z_][a-zA-Z0-9_]*)*)\"\n\t\t)\n\n\treturn _resource_name_regex\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tif _RESOURCE_NAME_REGEX is None:\n\t\t_RESOURCE_NAME_REGEX = re.compile(r\"^[a-zA-Z0-9_\\-]{1,255}$\")\n\treturn _RESOURCE_NAME_REGEX\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tif _resource_name_regex is None:\n\t\t_resource_name_regex = re.compile(\n\t\t\tr\"\"\"\n\t\t\t^\n\t\t\t(?P<name>[a-zA-Z]([a-zA-Z0-9-]+)?[a-zA-Z0-9])\n\t\t\t$\n\t\t\t\"\"\",\n\t\t\tre.VERBOSE)\n\treturn _resource_name_regex\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tif not hasattr(_get_resource_name_regex, \"re\"):\n\t\t_get_resource_name_regex.re = re.compile(\"^[a-zA-Z0-9_]+$\")\n\treturn _get_resource_name_regex.re\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tif not hasattr(_get_resource_name_regex, 'pattern'):\n\t\t_get_resource_name_regex.pattern = re.compile('^[a-z][a-z0-9]*$')\n\treturn _get_resource_name_regex.pattern\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\tglobal _resource_name_regex\n\n\tif _resource_name_regex is not None:\n\t\treturn _resource_name_regex\n\n\t# inspired by https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/validation/validation.go\n\t_resource_name_regex = re.compile(\n\t\t\"^[a-z0-9A-Z][a-z0-9A-Z-]{1,63}$\",\n\t\tre.ASCII\n\t)\n\n\treturn _resource_name_regex\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n\tif not re.match(regular_expression, value):\n\t\traise ValidationError(\n\t\t\t_(\"%(value)s is not a valid %(name)s\"),\n\t\t\tparams={'value': value, 'name': name},\n\t\t)\n\n\n", "def validate_value(value):\n\tif not value_regex.match(value):\n\t\traise ValidationError(\n\t\t\t\"The given value is not conform to the following regular expression: \" +\n\t\t\tvalue_regex.pattern)\n\n\n", "def validate_value(value):\n\tif not VALID_VALUE_REGEX.match(value):\n\t\traise ValidationError(\"{} is not a valid value.\".format(value))\n\n", "def validate_value(value):\n\tif not VALID_VALUE.match(value):\n\t\traise ValidationError('The value %s does not conform to the grammar.' % value)\n\n\n", "def validate_value(value):\n\tif not re.match(value_regex, value):\n\t\traise ValidationError('Invalid value: ' + value)\n\n", "def validate_value(value):\n\tif value is None:\n\t\traise ValidationError('None values are not allowed.')\n\telif not _regex.match(value):\n\t\traise ValidationError('A valid id is composed of \n", "def validate_value(value):\n\tif not re.match(regexes[value.lower()], value):\n\t\traise ValidationError(\"Value %r is not conform to %r\" % (value, regexes[value.lower()]))\n\n", "def validate_value(value):\n\tregex = get_regex()\n\ttry:\n\t\tregex.match(value)\n\texcept:\n\t\traise ValidationError(\n\t\t\t_('The value is not correct.'),\n\t\t\tcode='invalid_value',\n\t\t\tparams={'value': value},\n\t\t)\n", "def validate_value(value):\n\tif value is None:\n\t\treturn\n\tif not re.match(regex_list[value[0]], value):\n\t\traise ValidationError('This value does not match the regular expression.')\n\n", "def validate_value(value):\n\tif not re.search(regex, value):\n\t\traise ValidationError(\"The value '%s' is not conform to the regular expression %s.\" % (value, regex))\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n\tif not re.match(regular_expression, value):\n\t\traise ValidationError(\n\t\t\t_(\"%(value)s is not a valid %(name)s\"),\n\t\t\tparams={'value': value, 'name': name},\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not value_regex.match(value):\n\t\traise ValidationError(\n\t\t\t\"The given value is not conform to the following regular expression: \" +\n\t\t\tvalue_regex.pattern)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not VALID_VALUE_REGEX.match(value):\n\t\traise ValidationError(\"{} is not a valid value.\".format(value))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not VALID_VALUE.match(value):\n\t\traise ValidationError('The value %s does not conform to the grammar.' % value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not re.match(value_regex, value):\n\t\traise ValidationError('Invalid value: ' + value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif value is None:\n\t\traise ValidationError('None values are not allowed.')\n\telif not _regex.match(value):\n\t\traise ValidationError('A valid id is composed of \n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not re.match(regexes[value.lower()], value):\n\t\traise ValidationError(\"Value %r is not conform to %r\" % (value, regexes[value.lower()]))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tregex = get_regex()\n\ttry:\n\t\tregex.match(value)\n\texcept:\n\t\traise ValidationError(\n\t\t\t_('The value is not correct.'),\n\t\t\tcode='invalid_value',\n\t\t\tparams={'value': value},\n\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif value is None:\n\t\treturn\n\tif not re.match(regex_list[value[0]], value):\n\t\traise ValidationError('This value does not match the regular expression.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not re.search(regex, value):\n\t\traise ValidationError(\"The value '%s' is not conform to the regular expression %s.\" % (value, regex))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n\tif not re.match(regex[key], key):\n\t\traise ValidationError('The key {0} is not conform to the regular expression {1}'.format(key, regex[key]))\n\n", "def validate_key(key):\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError('Key \"%s\" does not match the regular expression \"%s\"' % (key, KEY_REGEX))\n\n\n", "def validate_key(key):\n\tif not re.match(settings.KEY_REGEX, key):\n\t\traise ValidationError(\"Key '%s' is not valid. It must match the regular expression '%s'\" % (key, settings.KEY_REGEX))\n\n", "def validate_key(key):\n\tif not key:\n\t\traise ValidationError('Empty key')\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError('Invalid key: {0}'.format(key))\n\n", "def validate_key(key):\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError(\"Invalid key: \" + key)\n\n", "def validate_key(key):\n\tif not re.match(_KEY_REGEXP, key):\n\t\traise ValidationError('The given key is not conform to the regular expression.')\n\n", "def validate_key(key):\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError(\"%s is not a valid key\" % key)\n\n", "def validate_key(key):\n\tif not VALID_KEY_REGEX.match(key):\n\t\traise ValidationError('The given key \"%s\" is not valid.' % key)\n\n", "def validate_key(key):\n\tif not re.match(key_regex, key):\n\t\traise ValidationError('The given key is not conform to the regular expression.')\n\n\n", "def validate_key(key):\n\tkey_regex = re.compile(r\"^[a-zA-Z0-9_]*$\")\n\tif not key_regex.match(key):\n\t\traise ValidationError(\"Invalid key given: \" + key)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n\tif not re.match(regex[key], key):\n\t\traise ValidationError('The key {0} is not conform to the regular expression {1}'.format(key, regex[key]))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError('Key \"%s\" does not match the regular expression \"%s\"' % (key, KEY_REGEX))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not re.match(settings.KEY_REGEX, key):\n\t\traise ValidationError(\"Key '%s' is not valid. It must match the regular expression '%s'\" % (key, settings.KEY_REGEX))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not key:\n\t\traise ValidationError('Empty key')\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError('Invalid key: {0}'.format(key))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError(\"Invalid key: \" + key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not re.match(_KEY_REGEXP, key):\n\t\traise ValidationError('The given key is not conform to the regular expression.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not re.match(KEY_REGEX, key):\n\t\traise ValidationError(\"%s is not a valid key\" % key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not VALID_KEY_REGEX.match(key):\n\t\traise ValidationError('The given key \"%s\" is not valid.' % key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not re.match(key_regex, key):\n\t\traise ValidationError('The given key is not conform to the regular expression.')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tkey_regex = re.compile(r\"^[a-zA-Z0-9_]*$\")\n\tif not key_regex.match(key):\n\t\traise ValidationError(\"Invalid key given: \" + key)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "ques_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif first_level:\n\t\t\tobserver_schema[key] = value\n\t\telse:\n\t\t\tobserver_schema[key] = None\n\treturn observer_schema\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_list(value)\n\t\telif first_level:\n\t\t\tobserver_schema_dict[key] = value\n\t\telse:\n\t\t\tobserver_schema_dict[key] = None\n\n\treturn observer_schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = dict(manifest_dict)\n\tfor key, value in observer_schema.items():\n\t\tif not isinstance(value, dict) and not isinstance(value, list):\n\t\t\tobserver_schema[key] = None\n\n\tif first_level:\n\t\tfor key, value in observer_schema.items():\n\t\t\tif key in IDENTIFYING_FIELDS:\n\t\t\t\tobserver_schema[key] = value\n\n\treturn observer_schema\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_list(value)\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tif first_level:\n\t\t\t\tobserver_schema_dict[key] = value\n\t\t\telse:\n\t\t\t\tobserver_schema_dict[key] = None\n\treturn observer_schema_dict\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = generate_default_observer_schema_list(value)\n\t\telif first_level:\n\t\t\tnew_dict[key] = value\n\t\telse:\n\t\t\tnew_dict[key] = None\n\treturn new_dict\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key, value in manifest_dict.items():\n\t\tif (type(value) is dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif (type(value) is list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telse:\n\t\t\tif (first_level and key in IDENTIFYING_FIELDS):\n\t\t\t\tobserver_schema[key] = value\n\t\t\telse:\n\t\t\t\tobserver_schema[key] = None\n\treturn observer_schema\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key in manifest_dict.keys():\n\t\tvalue = manifest_dict[key]\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tobserver_schema[key] = None\n\tif first_level:\n\t\tobserver_schema.update(manifest_dict)\n\treturn observer_schema\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telse:\n\t\t\tif first_level and key in IDENTIFYING_FIELDS:\n\t\t\t\tobserver_schema[key] = value\n\t\t\telse:\n\t\t\t\tobserver_schema[key] = None\n\treturn observer_schema\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tschema_dict = {}\n\tfor key in manifest_dict.keys():\n\t\tif isinstance(manifest_dict[key], dict):\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key])\n\t\telif isinstance(manifest_dict[key], list):\n\t\t\tschema_dict[key] = generate_default_observer_schema_list(manifest_dict[key])\n\t\telse:\n\t\t\tschema_dict[key] = manifest_dict[key] if first_level else None\n\n\treturn schema_dict\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tif first_level:\n\t\treturn {\n\t\t\t\"kind\": manifest_dict[\"kind\"],\n\t\t\t\"apiVersion\": manifest_dict[\"apiVersion\"],\n\t\t\t\"metadata\": {\n\t\t\t\t\"name\": manifest_dict[\"metadata\"][\"name\"],\n\t\t\t\t\"namespace\": manifest_dict[\"metadata\"][\"namespace\"],\n\t\t\t\t\"labels\": manifest_dict[\"metadata\"][\"labels\"]\n\t\t\t}\n\t\t}\n\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) is list:\n\t\t\tnew_dict[key] = generate_default_observer_schema_list(value)\n\t\telse:\n\t\t\tnew_dict[key] = None\n\treturn new_dict\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif first_level:\n\t\t\tobserver_schema[key] = value\n\t\telse:\n\t\t\tobserver_schema[key] = None\n\treturn observer_schema\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_list(value)\n\t\telif first_level:\n\t\t\tobserver_schema_dict[key] = value\n\t\telse:\n\t\t\tobserver_schema_dict[key] = None\n\n\treturn observer_schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = dict(manifest_dict)\n\tfor key, value in observer_schema.items():\n\t\tif not isinstance(value, dict) and not isinstance(value, list):\n\t\t\tobserver_schema[key] = None\n\n\tif first_level:\n\t\tfor key, value in observer_schema.items():\n\t\t\tif key in IDENTIFYING_FIELDS:\n\t\t\t\tobserver_schema[key] = value\n\n\treturn observer_schema\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_list(value)\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tif first_level:\n\t\t\t\tobserver_schema_dict[key] = value\n\t\t\telse:\n\t\t\t\tobserver_schema_dict[key] = None\n\treturn observer_schema_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = generate_default_observer_schema_list(value)\n\t\telif first_level:\n\t\t\tnew_dict[key] = value\n\t\telse:\n\t\t\tnew_dict[key] = None\n\treturn new_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key, value in manifest_dict.items():\n\t\tif (type(value) is dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif (type(value) is list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telse:\n\t\t\tif (first_level and key in IDENTIFYING_FIELDS):\n\t\t\t\tobserver_schema[key] = value\n\t\t\telse:\n\t\t\t\tobserver_schema[key] = None\n\treturn observer_schema\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key in manifest_dict.keys():\n\t\tvalue = manifest_dict[key]\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tobserver_schema[key] = None\n\tif first_level:\n\t\tobserver_schema.update(manifest_dict)\n\treturn observer_schema\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tobserver_schema[key] = generate_default_observer_schema_list(value)\n\t\telse:\n\t\t\tif first_level and key in IDENTIFYING_FIELDS:\n\t\t\t\tobserver_schema[key] = value\n\t\t\telse:\n\t\t\t\tobserver_schema[key] = None\n\treturn observer_schema\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tschema_dict = {}\n\tfor key in manifest_dict.keys():\n\t\tif isinstance(manifest_dict[key], dict):\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key])\n\t\telif isinstance(manifest_dict[key], list):\n\t\t\tschema_dict[key] = generate_default_observer_schema_list(manifest_dict[key])\n\t\telse:\n\t\t\tschema_dict[key] = manifest_dict[key] if first_level else None\n\n\treturn schema_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tif first_level:\n\t\treturn {\n\t\t\t\"kind\": manifest_dict[\"kind\"],\n\t\t\t\"apiVersion\": manifest_dict[\"apiVersion\"],\n\t\t\t\"metadata\": {\n\t\t\t\t\"name\": manifest_dict[\"metadata\"][\"name\"],\n\t\t\t\t\"namespace\": manifest_dict[\"metadata\"][\"namespace\"],\n\t\t\t\t\"labels\": manifest_dict[\"metadata\"][\"labels\"]\n\t\t\t}\n\t\t}\n\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) is list:\n\t\t\tnew_dict[key] = generate_default_observer_schema_list(value)\n\t\telse:\n\t\t\tnew_dict[key] = None\n\treturn new_dict\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif \"type\" not in field:\n\t\t\traise ValueError(\"type not found in observer schema\")\n\t\tfield_type = field[\"type\"]\n\t\tif field_type == \"string\":\n\t\t\tif field[\"name\"] not in response:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"{} not found in response\".format(field[\"name\"])\n\t\t\t\t)\n\t\t\tlast_applied_manifest.append(response[field[\"name\"]])\n\t\telif field_type == \"array\":\n\t\t\tif field[\"name\"] not in response:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"{} not found in response\".format(field[\"name\"])\n\t\t\t\t)\n\t\t\tlast_applied_manifest.append([])\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[-1], field[\"items\"], response[field[\"name\"]]\n\t\t\t)\n\t\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, field in enumerate(observer_schema):\n\t\tif field is None:\n\t\t\tcontinue\n\n\t\tif isinstance(field, dict):\n\t\t\tlast_applied_manifest[i] = last_applied_manifest[i] or {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], field, response[i]\n\t\t\t)\n\t\telif isinstance(field, list):\n\t\t\tlast_applied_manifest[i] = last_applied_manifest[i] or []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[i], field, response[i]\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[i] = last_applied_manifest[i] or response[i]\n\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif isinstance(response, list):\n\t\tfor index, child_response in enumerate(response):\n\t\t\tchild_observer_schema = observer_schema[index]\n\t\t\tif isinstance(child_observer_schema, str):\n\t\t\t\t# the child is an observed field\n\t\t\t\ttry:\n\t\t\t\t\tlast_applied_manifest[index] = child_response\n\t\t\t\texcept IndexError:\n\t\t\t\t\tlast_applied_manifest.append(child_response)\n\t\t\telse:\n\t\t\t\t# the child is not an observed field\n\t\t\t\ttry:\n\t\t\t\t\tchild_last_applied_manifest = last_applied_manifest[index]\n\t\t\t\texcept IndexError:\n\t\t\t\t\tchild_last_applied_manifest = []\n\t\t\t\t\tlast_applied_manifest.append(child_last_applied_manifest)\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, item in enumerate(response):\n\t\tif i >= len(last_applied_manifest):\n\t\t\tlast_applied_manifest.append(dict())\n\t\tif isinstance(item, list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], item\n\t\t\t)\n\t\telif isinstance(item, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], item\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[i] = item\n\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor elem in response:\n\t\tfor field in observer_schema:\n\t\t\tif field in elem:\n\t\t\t\tif field not in last_applied_manifest[0]:\n\t\t\t\t\tlast_applied_manifest[0][field] = elem[field]\n\t\t\t\tif type(elem[field]) == dict:\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[0][field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\telem[field]\n\t\t\t\t\t)\n\t\t\t\telif type(elem[field]) == list:\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[0][field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\telem[field]\n\t\t\t\t\t)\n\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, response_item in enumerate(response):\n\t\tif isinstance(response_item, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], response_item\n\t\t\t)\n\t\telif isinstance(response_item, list) or isinstance(response_item, tuple):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], response_item\n\t\t\t)\n\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor (index, field) in enumerate(observer_schema):\n\t\tif isinstance(field, dict):\n\t\t\tif list(field.keys())[0] == \"map\":\n\t\t\t\tif len(last_applied_manifest) <= index:\n\t\t\t\t\tlast_applied_manifest.append([])\n\t\t\t\tif field[\"map\"][\"type\"] == \"list\":\n\t\t\t\t\tif (\n\t\t\t\t\t\tlen(last_applied_manifest[index]) == 0\n\t\t\t\t\t\tand len(response[index]) > 0\n\t\t\t\t\t):\n\t\t\t\t\t\tfor i in range(len(response[index])):\n\t\t\t\t\t\t\tlast_applied_manifest[index].append([])\n\t\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\t\tlast_applied_manifest[index][i],\n\t\t\t\t\t\t\t\tfield[\"map\"][\"value\"],\n\t\t\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif len(last_applied_manifest) == 0:\n\t\tfor item in response:\n\t\t\tif isinstance(item, dict):\n\t\t\t\tlast_applied_manifest.append(\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t{}, observer_schema, item\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest.append(item)\n\telse:\n\t\tfor item_l, item_o, item_r in zip(\n\t\t\tlast_applied_manifest, observer_schema, response\n\t\t):\n\t\t\tif isinstance(item_o, dict):\n\t\t\t\titem_l = update_last_applied_manifest_dict_from_resp(\n\t\t\t\t\titem_l, item_o, item_r\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\titem_l = item_r\n\treturn last_applied_manifest\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor index, field in enumerate(observer_schema):\n\t\tif field.is_dict():\n\t\t\tif len(last_applied_manifest) <= index:\n\t\t\t\tlast_applied_manifest.append({})\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[index], field, response[index]\n\t\t\t)\n\t\telif field.is_list():\n\t\t\tif len(last_applied_manifest) <= index:\n\t\t\t\tlast_applied_manifest.append([])\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[index], field, response[index]\n\t\t\t)\n\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, field in enumerate(observer_schema):\n\t\tif 'fields' in field:\n\t\t\tif field['name'] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field['name']] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field['name']], field['fields'], response[i]\n\t\t\t)\n\t\telif 'name' in field:\n\t\t\tif field['name'] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field['name']] = response[i]\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif \"type\" not in field:\n\t\t\traise ValueError(\"type not found in observer schema\")\n\t\tfield_type = field[\"type\"]\n\t\tif field_type == \"string\":\n\t\t\tif field[\"name\"] not in response:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"{} not found in response\".format(field[\"name\"])\n\t\t\t\t)\n\t\t\tlast_applied_manifest.append(response[field[\"name\"]])\n\t\telif field_type == \"array\":\n\t\t\tif field[\"name\"] not in response:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"{} not found in response\".format(field[\"name\"])\n\t\t\t\t)\n\t\t\tlast_applied_manifest.append([])\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[-1], field[\"items\"], response[field[\"name\"]]\n\t\t\t)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, field in enumerate(observer_schema):\n\t\tif field is None:\n\t\t\tcontinue\n\n\t\tif isinstance(field, dict):\n\t\t\tlast_applied_manifest[i] = last_applied_manifest[i] or {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], field, response[i]\n\t\t\t)\n\t\telif isinstance(field, list):\n\t\t\tlast_applied_manifest[i] = last_applied_manifest[i] or []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[i], field, response[i]\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[i] = last_applied_manifest[i] or response[i]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif isinstance(response, list):\n\t\tfor index, child_response in enumerate(response):\n\t\t\tchild_observer_schema = observer_schema[index]\n\t\t\tif isinstance(child_observer_schema, str):\n\t\t\t\t# the child is an observed field\n\t\t\t\ttry:\n\t\t\t\t\tlast_applied_manifest[index] = child_response\n\t\t\t\texcept IndexError:\n\t\t\t\t\tlast_applied_manifest.append(child_response)\n\t\t\telse:\n\t\t\t\t# the child is not an observed field\n\t\t\t\ttry:\n\t\t\t\t\tchild_last_applied_manifest = last_applied_manifest[index]\n\t\t\t\texcept IndexError:\n\t\t\t\t\tchild_last_applied_manifest = []\n\t\t\t\t\tlast_applied_manifest.append(child_last_applied_manifest)\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, item in enumerate(response):\n\t\tif i >= len(last_applied_manifest):\n\t\t\tlast_applied_manifest.append(dict())\n\t\tif isinstance(item, list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], item\n\t\t\t)\n\t\telif isinstance(item, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], item\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[i] = item\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor elem in response:\n\t\tfor field in observer_schema:\n\t\t\tif field in elem:\n\t\t\t\tif field not in last_applied_manifest[0]:\n\t\t\t\t\tlast_applied_manifest[0][field] = elem[field]\n\t\t\t\tif type(elem[field]) == dict:\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[0][field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\telem[field]\n\t\t\t\t\t)\n\t\t\t\telif type(elem[field]) == list:\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[0][field],\n\t\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\t\telem[field]\n\t\t\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, response_item in enumerate(response):\n\t\tif isinstance(response_item, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], response_item\n\t\t\t)\n\t\telif isinstance(response_item, list) or isinstance(response_item, tuple):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], response_item\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor (index, field) in enumerate(observer_schema):\n\t\tif isinstance(field, dict):\n\t\t\tif list(field.keys())[0] == \"map\":\n\t\t\t\tif len(last_applied_manifest) <= index:\n\t\t\t\t\tlast_applied_manifest.append([])\n\t\t\t\tif field[\"map\"][\"type\"] == \"list\":\n\t\t\t\t\tif (\n\t\t\t\t\t\tlen(last_applied_manifest[index]) == 0\n\t\t\t\t\t\tand len(response[index]) > 0\n\t\t\t\t\t):\n\t\t\t\t\t\tfor i in range(len(response[index])):\n\t\t\t\t\t\t\tlast_applied_manifest[index].append([])\n\t\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\t\tlast_applied_manifest[index][i],\n\t\t\t\t\t\t\t\tfield[\"map\"][\"value\"],\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif len(last_applied_manifest) == 0:\n\t\tfor item in response:\n\t\t\tif isinstance(item, dict):\n\t\t\t\tlast_applied_manifest.append(\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t{}, observer_schema, item\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest.append(item)\n\telse:\n\t\tfor item_l, item_o, item_r in zip(\n\t\t\tlast_applied_manifest, observer_schema, response\n\t\t):\n\t\t\tif isinstance(item_o, dict):\n\t\t\t\titem_l = update_last_applied_manifest_dict_from_resp(\n\t\t\t\t\titem_l, item_o, item_r\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\titem_l = item_r\n\treturn last_applied_manifest\n", "is_pass": true, "return_code": 0}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor index, field in enumerate(observer_schema):\n\t\tif field.is_dict():\n\t\t\tif len(last_applied_manifest) <= index:\n\t\t\t\tlast_applied_manifest.append({})\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[index], field, response[index]\n\t\t\t)\n\t\telif field.is_list():\n\t\t\tif len(last_applied_manifest) <= index:\n\t\t\t\tlast_applied_manifest.append([])\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[index], field, response[index]\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, field in enumerate(observer_schema):\n\t\tif 'fields' in field:\n\t\t\tif field['name'] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field['name']] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field['name']], field['fields'], response[i]\n\t\t\t)\n\t\telif 'name' in field:\n\t\t\tif field['name'] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field['name']] = response[i]\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor observed_field in observer_schema:\n\t\ttry:\n\t\t\tobserved_value = response[observed_field]\n\t\texcept KeyError:\n\t\t\traise KeyError(\n\t\t\t\t\"The observed field {} is not present in the Kubernetes response {}\".format(\n\t\t\t\t\tobserved_field, response\n\t\t\t\t)\n\t\t\t)\n\t\tif isinstance(observed_value, list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest.setdefault(observed_field, []),\n\t\t\t\tobserver_schema[observed_field],\n\t\t\t\tobserved_value,\n\t\t\t)\n\t\telif isinstance(observed_value, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest.setdefault(observed_field, {}),\n\t\t\t\tobserver_schema[observed_field],\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in observer_schema.items():\n\t\tif isinstance(value, dict):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\tif key in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], value, response[key]\n\t\t\t\t)\n\t\telif isinstance(value, list):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = []\n\t\t\tif key in response:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], value[0], response[key]\n\t\t\t\t)\n\t\telse:\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] =\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t# If the field is a dict, we recurse in the dict\n\tif isinstance(observer_schema[response['kind']], dict):\n\t\tfor sub_field in observer_schema[response['kind']]:\n\t\t\t# If the field is missing, we add it\n\t\t\tif not sub_field in last_applied_manifest[response['kind']]:\n\t\t\t\tlast_applied_manifest[response['kind']][sub_field] = {}\n\t\t\t# If the value is a dict, we recurse\n\t\t\tif isinstance(observer_schema[response['kind']][sub_field], dict):\n\t\t\t\tfor sub_sub_field in observer_schema[response['kind']][sub_field]:\n\t\t\t\t\t# If the field is missing, we add it\n\t\t\t\t\tif not sub_sub_field in last_applied_manifest[response['kind']][sub_field]:\n\t\t\t\t\t\tlast_applied_manifest[response['kind']][sub_field][sub_sub_field] =\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\ttry:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\texcept KeyError:\n\t\t\ttry:\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t    last_applied_manifest[field],\n\t\t\t\t    observer_schema[field],\n\t\t\t\t    response[field]\n\t\t\t\t)\n\t\t\texcept KeyError:\n\t\t\t\ttry:\n\t\t\t\t\tlast_applied_manifest[field] = []\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t    last_applied_manifest[field],\n\t\t\t\t\t    observer_schema[field],\n\t\t\t\t\t    response[field]\n\t\t\t\t\t)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field_name in observer_schema[\"observed_fields\"]:\n\t\tif field_name not in response:\n\t\t\traise KeyError(\n\t\t\t\t\"Field %s is not present in the Kubernetes response\" % field_name\n\t\t\t)\n\t\tif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = {}\n\t\tif isinstance(response[field_name], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observed_fields\"][field_name],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\telif isinstance(response[field_name], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observed_fields\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema[\"fields\"]:\n\t\tif isinstance(field, dict):\n\t\t\tfield_name = list(field.keys())[0]\n\t\t\tfield_value = list(field.values())[0]\n\t\t\tif not last_applied_manifest.get(field_name):\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field_value, response\n\t\t\t)\n\t\telif isinstance(field, list):\n\t\t\tfield_name = field[0]\n\t\t\tfield_value = field[1]\n\t\t\tif not last_applied_manifest.get(field_name):\n\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field_\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in observer_schema.items():\n\t\tif isinstance(value, dict):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[key], value, response[key]\n\t\t\t)\n\t\telif isinstance(value, list):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[key], value, response[key]\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[key] = response[key]\n\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor observed_field in observer_schema[\"observed_fields\"]:\n\t\tif observed_field[\"name\"] not in last_applied_manifest:\n\t\t\tlast_applied_manifest[observed_field[\"name\"]] = {}\n\t\t\tfor key in response[observed_field[\"name\"]]:\n\t\t\t\tif key in observed_field[\"keys\"]:\n\t\t\t\t\tlast_applied_manifest[observed_field[\"name\"]][key] = {}\n\t\t\t\t\tif type(response[observed_field[\"name\"]][key]) == dict:\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[observed_field[\"name\"]][key],\n\t\t\t\t\t\t\tobserved_field,\n\t\t\t\t\t\t\tresponse[observed_field[\"name\"]][key],\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\tupdate_last_applied_manifest_list\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field_name, field_observer in observer_schema[\"observed\"].items():\n\t\tif field_name in last_applied_manifest:\n\t\t\tcontinue\n\t\tif field_name not in response:\n\t\t\traise KeyError(\n\t\t\t\tf\"Failed to find '{field_name}' in Kubernetes response\"\n\t\t\t)\n\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\tif field_observer[\"type\"] == \"list\":\n\t\t\tlast_applied_manifest[field_name] = []\n\t\t\tfor list_item in response[field_name]:\n\t\t\t\tlast_applied_manifest[field_name].append(\n\t\t\t\t\t{\n\t\t\t\t\t\t\"metadata\": {\n\t\t\t\t\t\t\t\"name\": list_item[\"metadata\"][\"name\"],\n\t\t\t\t\t\t\t\"namespace\": list_item[\"metadata\"][\"namespace\"],\n\t\t\t\t\t\t}\n\t\t\t\t\t\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\tif observer_schema[field] is not None:\n\t\t\tif field not in response:\n\t\t\t\traise KeyError(\n\t\t\t\t\t\"Observed field {} is not present in the response.\".format(field)\n\t\t\t\t)\n\t\t\tif type(last_applied_manifest[field]) is list:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\tresponse[field],\n\t\t\t\t)\n\t\t\telif type(last_applied_manifest[field]) is dict:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\t\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor observed_field in observer_schema:\n\t\ttry:\n\t\t\tobserved_value = response[observed_field]\n\t\texcept KeyError:\n\t\t\traise KeyError(\n\t\t\t\t\"The observed field {} is not present in the Kubernetes response {}\".format(\n\t\t\t\t\tobserved_field, response\n\t\t\t\t)\n\t\t\t)\n\t\tif isinstance(observed_value, list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest.setdefault(observed_field, []),\n\t\t\t\tobserver_schema[observed_field],\n\t\t\t\tobserved_value,\n\t\t\t)\n\t\telif isinstance(observed_value, dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest.setdefault(observed_field, {}),\n\t\t\t\tobserver_schema[observed_field],\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in observer_schema.items():\n\t\tif isinstance(value, dict):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\tif key in response:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], value, response[key]\n\t\t\t\t)\n\t\telif isinstance(value, list):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = []\n\t\t\tif key in response:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], value[0], response[key]\n\t\t\t\t)\n\t\telse:\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\t# If the field is a dict, we recurse in the dict\n\tif isinstance(observer_schema[response['kind']], dict):\n\t\tfor sub_field in observer_schema[response['kind']]:\n\t\t\t# If the field is missing, we add it\n\t\t\tif not sub_field in last_applied_manifest[response['kind']]:\n\t\t\t\tlast_applied_manifest[response['kind']][sub_field] = {}\n\t\t\t# If the value is a dict, we recurse\n\t\t\tif isinstance(observer_schema[response['kind']][sub_field], dict):\n\t\t\t\tfor sub_sub_field in observer_schema[response['kind']][sub_field]:\n\t\t\t\t\t# If the field is missing, we add it\n\t\t\t\t\tif not sub_sub_field in last_applied_manifest[response['kind']][sub_field]:\n\t\t\t\t\t\tlast_applied_manifest[response['kind']][sub_field][sub_sub_field] =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\ttry:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\texcept KeyError:\n\t\t\ttry:\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t    last_applied_manifest[field],\n\t\t\t\t    observer_schema[field],\n\t\t\t\t    response[field]\n\t\t\t\t)\n\t\t\texcept KeyError:\n\t\t\t\ttry:\n\t\t\t\t\tlast_applied_manifest[field] = []\n\t\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\t    last_applied_manifest[field],\n\t\t\t\t\t    observer_schema[field],\n\t\t\t\t\t    response[field]\n\t\t\t\t\t)\n\t\t\t\texcept KeyError:\n\t\t\t\t\ttry:\n\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field_name in observer_schema[\"observed_fields\"]:\n\t\tif field_name not in response:\n\t\t\traise KeyError(\n\t\t\t\t\"Field %s is not present in the Kubernetes response\" % field_name\n\t\t\t)\n\t\tif field_name not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field_name] = {}\n\t\tif isinstance(response[field_name], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observed_fields\"][field_name],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\telif isinstance(response[field_name], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserver_schema[\"observed_fields\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema[\"fields\"]:\n\t\tif isinstance(field, dict):\n\t\t\tfield_name = list(field.keys())[0]\n\t\t\tfield_value = list(field.values())[0]\n\t\t\tif not last_applied_manifest.get(field_name):\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field_value, response\n\t\t\t)\n\t\telif isinstance(field, list):\n\t\t\tfield_name = field[0]\n\t\t\tfield_value = field[1]\n\t\t\tif not last_applied_manifest.get(field_name):\n\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name], field_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in observer_schema.items():\n\t\tif isinstance(value, dict):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[key], value, response[key]\n\t\t\t)\n\t\telif isinstance(value, list):\n\t\t\tif key not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[key] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[key], value, response[key]\n\t\t\t)\n\t\telse:\n\t\t\tlast_applied_manifest[key] = response[key]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor observed_field in observer_schema[\"observed_fields\"]:\n\t\tif observed_field[\"name\"] not in last_applied_manifest:\n\t\t\tlast_applied_manifest[observed_field[\"name\"]] = {}\n\t\t\tfor key in response[observed_field[\"name\"]]:\n\t\t\t\tif key in observed_field[\"keys\"]:\n\t\t\t\t\tlast_applied_manifest[observed_field[\"name\"]][key] = {}\n\t\t\t\t\tif type(response[observed_field[\"name\"]][key]) == dict:\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[observed_field[\"name\"]][key],\n\t\t\t\t\t\t\tobserved_field,\n\t\t\t\t\t\t\tresponse[observed_field[\"name\"]][key],\n\t\t\t\t\t\t)\n\t\t\t\t\telse:\n\t\t\t\t\t\tupdate_last_applied_manifest_list\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field_name, field_observer in observer_schema[\"observed\"].items():\n\t\tif field_name in last_applied_manifest:\n\t\t\tcontinue\n\t\tif field_name not in response:\n\t\t\traise KeyError(\n\t\t\t\tf\"Failed to find '{field_name}' in Kubernetes response\"\n\t\t\t)\n\t\tlast_applied_manifest[field_name] = response[field_name]\n\t\tif field_observer[\"type\"] == \"list\":\n\t\t\tlast_applied_manifest[field_name] = []\n\t\t\tfor list_item in response[field_name]:\n\t\t\t\tlast_applied_manifest[field_name].append(\n\t\t\t\t\t{\n\t\t\t\t\t\t\"metadata\": {\n\t\t\t\t\t\t\t\"name\": list_item[\"metadata\"][\"name\"],\n\t\t\t\t\t\t\t\"namespace\": list_item[\"metadata\"][\"namespace\"],\n\t\t\t\t\t\t}\n\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\tif observer_schema[field] is not None:\n\t\t\tif field not in response:\n\t\t\t\traise KeyError(\n\t\t\t\t\t\"Observed field {} is not present in the response.\".format(field)\n\t\t\t\t)\n\t\t\tif type(last_applied_manifest[field]) is list:\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\tobserver_schema[field],\n\t\t\t\t\tresponse[field],\n\t\t\t\t)\n\t\t\telif type(last_applied_manifest[field]) is dict:\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field],\n\t\t\t\t\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "ques_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif isinstance(resource, CustomResourceDefinition):\n\t\t\tfor scope in resource.spec.validation.openAPIV3Schema.properties.scope.enum:\n\t\t\t\tif scope == \"Namespaced\":\n\t\t\t\t\tapp.observers.append(\n\t\t\t\t\t\tNamespacedResourceObserver(\n\t\t\t\t\t\t\tmetadata=ObjectMeta(name=resource.spec.names.plural),\n\t\t\t\t\t\t\tapiVersion=resource.apiVersion,\n\t\t\t\t\t\t\tkind=resource.spec.names.kind,\n\t\t\t\t\t\t\tnamespace=app.metadata.namespace,\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\t\telif scope == \"Cluster\":\n\t\t\t\t\tapp.observers.append(\n\t\t\t\t\t\tClusterResourceObserver(\n\t\t\t\t\t\t\tmetadata=ObjectMeta(name=resource.spec.names.plural),\n\t\t\t\t\t\t\tapiVersion=resource\n", "def generate_default_observer_schema(app):\n\tfor resource in (\n\t\tresource\n\t\tfor resource in app.spec.manifest\n\t\tif resource.kind != \"Application\"\n\t):\n\t\tif not resource.observer:\n\t\t\tresource.observer = Observer(\n\t\t\t\tschema=ObserverSchema(\n\t\t\t\t\tselector=ObserverSelector(\n\t\t\t\t\t\tmatchLabels=resource.metadata.labels\n\t\t\t\t\t)\n\t\t\t\t),\n\t\t\t\tnamespace=app.metadata.namespace,\n\t\t\t)\n\n\n", "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest.resources:\n\t\tif resource.kind in KUBERNETES_API_RESOURCES:\n\t\t\tgenerate_default_observer_schema_for_resource(resource)\n\t\telse:\n\t\t\traise ValueError(\n\t\t\t\tf\"Resource {resource.kind} not supported by Krake. \"\n\t\t\t\tf\"Supported resources are {KUBERNETES_API_RESOURCES}\"\n\t\t\t)\n\n\n", "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif not resource.observer_schema and resource.kind in DEFAULT_OBSERVER_SCHEMAS:\n\t\t\tresource.observer_schema = DEFAULT_OBSERVER_SCHEMAS[resource.kind]\n\n\n", "def generate_default_observer_schema(app):\n\tfor resource in get_resources(app.spec.manifest):\n\t\tif resource.metadata.name not in app.spec.observer_schemas:\n\t\t\tapp.spec.observer_schemas[resource.metadata.name] = generate_observer_schema(\n\t\t\t\tresource\n\t\t\t)\n\n\n", "def generate_default_observer_schema(app):\n\tfor _, resource in app.spec.manifest.items():\n\t\tif resource.group not in app.spec.observers:\n\t\t\tapp.spec.observers[resource.group] = {}\n\t\tif resource.version not in app.spec.observers[resource.group]:\n\t\t\tapp.spec.observers[resource.group][resource.version] = {}\n\t\tif resource.kind not in app.spec.observers[resource.group][resource.version]:\n\t\t\tapp.spec.observers[resource.group][resource.version][resource.kind] = (\n\t\t\t\t{\"selector\": resource.metadata.name}\n\t\t\t)\n", "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif resource.type in observer_schemas:\n\t\t\tcontinue\n\n\t\tlogger.info(\n\t\t\tf\"No observer schema for {resource.type} defined, generating default schema\"\n\t\t)\n\n\t\tobserver_schemas[resource.type] = generate_default_observer_schema_for_type(\n\t\t\tresource.type\n\t\t)\n\n\n", "def generate_default_observer_schema(app):\n\tfor manifest in app.spec.manifest:\n\t\tif manifest.type not in DEFAULT_OBSERVER_SCHEMA:\n\t\t\tlogger.warning(\n\t\t\t\t\"No default observer schema available for Kubernetes resource of type %s\",\n\t\t\t\tmanifest.type,\n\t\t\t)\n\t\t\tcontinue\n\t\tif manifest.observer_schema:\n\t\t\tcontinue\n\t\tif manifest.type not in app.spec.observer_schema:\n\t\t\tapp.spec.observer_schema[manifest.type] = Schema()\n\t\tmanifest.observer_schema = (\n\t\t\tapp.spec.observer_schema[manifest.type].spec.schema\n\t\t)\n\n\n", "def generate_default_observer_schema(app):\n\tfor manifest in app.spec.manifest:\n\t\tif manifest.kind == \"KubernetesResource\":\n\t\t\thandle_kubernetes_resource(app, manifest.kubernetes_resource)\n\t\telif manifest.kind == \"KubernetesResourceList\":\n\t\t\tfor kubernetes_resource in manifest.kubernetes_resource_list:\n\t\t\t\thandle_kubernetes_resource(app, kubernetes_resource)\n\n\n", "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif resource.kind in DEFAULT_OBSERVER_SCHEMAS:\n\t\t\tschema = DEFAULT_OBSERVER_SCHEMAS[resource.kind]\n\t\t\tobserver = app.spec.observer.setdefault(resource.kind, {})\n\t\t\tfor key, value in schema.items():\n\t\t\t\tobserver.setdefault(key, value)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif isinstance(resource, CustomResourceDefinition):\n\t\t\tfor scope in resource.spec.validation.openAPIV3Schema.properties.scope.enum:\n\t\t\t\tif scope == \"Namespaced\":\n\t\t\t\t\tapp.observers.append(\n\t\t\t\t\t\tNamespacedResourceObserver(\n\t\t\t\t\t\t\tmetadata=ObjectMeta(name=resource.spec.names.plural),\n\t\t\t\t\t\t\tapiVersion=resource.apiVersion,\n\t\t\t\t\t\t\tkind=resource.spec.names.kind,\n\t\t\t\t\t\t\tnamespace=app.metadata.namespace,\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\t\t\t\telif scope == \"Cluster\":\n\t\t\t\t\tapp.observers.append(\n\t\t\t\t\t\tClusterResourceObserver(\n\t\t\t\t\t\t\tmetadata=ObjectMeta(name=resource.spec.names.plural),\n\t\t\t\t\t\t\tapiVersion=resource\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in (\n\t\tresource\n\t\tfor resource in app.spec.manifest\n\t\tif resource.kind != \"Application\"\n\t):\n\t\tif not resource.observer:\n\t\t\tresource.observer = Observer(\n\t\t\t\tschema=ObserverSchema(\n\t\t\t\t\tselector=ObserverSelector(\n\t\t\t\t\t\tmatchLabels=resource.metadata.labels\n\t\t\t\t\t)\n\t\t\t\t),\n\t\t\t\tnamespace=app.metadata.namespace,\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest.resources:\n\t\tif resource.kind in KUBERNETES_API_RESOURCES:\n\t\t\tgenerate_default_observer_schema_for_resource(resource)\n\t\telse:\n\t\t\traise ValueError(\n\t\t\t\tf\"Resource {resource.kind} not supported by Krake. \"\n\t\t\t\tf\"Supported resources are {KUBERNETES_API_RESOURCES}\"\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif not resource.observer_schema and resource.kind in DEFAULT_OBSERVER_SCHEMAS:\n\t\t\tresource.observer_schema = DEFAULT_OBSERVER_SCHEMAS[resource.kind]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in get_resources(app.spec.manifest):\n\t\tif resource.metadata.name not in app.spec.observer_schemas:\n\t\t\tapp.spec.observer_schemas[resource.metadata.name] = generate_observer_schema(\n\t\t\t\tresource\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor _, resource in app.spec.manifest.items():\n\t\tif resource.group not in app.spec.observers:\n\t\t\tapp.spec.observers[resource.group] = {}\n\t\tif resource.version not in app.spec.observers[resource.group]:\n\t\t\tapp.spec.observers[resource.group][resource.version] = {}\n\t\tif resource.kind not in app.spec.observers[resource.group][resource.version]:\n\t\t\tapp.spec.observers[resource.group][resource.version][resource.kind] = (\n\t\t\t\t{\"selector\": resource.metadata.name}\n\t\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif resource.type in observer_schemas:\n\t\t\tcontinue\n\n\t\tlogger.info(\n\t\t\tf\"No observer schema for {resource.type} defined, generating default schema\"\n\t\t)\n\n\t\tobserver_schemas[resource.type] = generate_default_observer_schema_for_type(\n\t\t\tresource.type\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor manifest in app.spec.manifest:\n\t\tif manifest.type not in DEFAULT_OBSERVER_SCHEMA:\n\t\t\tlogger.warning(\n\t\t\t\t\"No default observer schema available for Kubernetes resource of type %s\",\n\t\t\t\tmanifest.type,\n\t\t\t)\n\t\t\tcontinue\n\t\tif manifest.observer_schema:\n\t\t\tcontinue\n\t\tif manifest.type not in app.spec.observer_schema:\n\t\t\tapp.spec.observer_schema[manifest.type] = Schema()\n\t\tmanifest.observer_schema = (\n\t\t\tapp.spec.observer_schema[manifest.type].spec.schema\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor manifest in app.spec.manifest:\n\t\tif manifest.kind == \"KubernetesResource\":\n\t\t\thandle_kubernetes_resource(app, manifest.kubernetes_resource)\n\t\telif manifest.kind == \"KubernetesResourceList\":\n\t\t\tfor kubernetes_resource in manifest.kubernetes_resource_list:\n\t\t\t\thandle_kubernetes_resource(app, kubernetes_resource)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in app.spec.manifest:\n\t\tif resource.kind in DEFAULT_OBSERVER_SCHEMAS:\n\t\t\tschema = DEFAULT_OBSERVER_SCHEMAS[resource.kind]\n\t\t\tobserver = app.spec.observer.setdefault(resource.kind, {})\n\t\t\tfor key, value in schema.items():\n\t\t\t\tobserver.setdefault(key, value)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "ques_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()  # pragma: no cover\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\treturn self.format_sql(sql, params)\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\tpass\n\t\n\tdef format_placeholder(\n\t\tself,\n\t\tplaceholder: AnyStr,\n\t\tindex: int,\n\t) -> AnyStr:\n\t\"\"\"\n\tConvert an in-style placeholder to its out-style placeholder.\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\tif self._in_style is None:\n\t\t\traise ValueError(\"in-style is None.\")\n\n\t\tif self._out_style is None:\n\t\t\traise ValueError(\"out-style is None.\")\n\n\t\t(sql, params) = self._in_style.format(sql, params)\n\n\t\treturn (self._out_style.format(sql, params))\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\treturn self.sql_params.format(sql, params)\n\n\tdef format_params(self, sql: AnyStr, params: SQLParamsParams) -> Tuple[AnyStr, SQLParamsParams]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\treturn self._format(sql, params, None)\n\n\tdef _format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t\tout: OutParams,\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\tif out is None:\n\t\t\tout = {}\n\t\t\tif self._style is None:\n\t\t\t\treturn sql, params\n\t\t\tsql = self._style.format(sql, params, out)\n\t\t\tparams = out\n\t\telif self._style is not None:\n\t\t\tsql = self._style.format(sql, params, out)\n\t\treturn sql, params\n\n\tdef get_param(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t\tname: str,\n\t) -> Any:\n\t\t\"\"\"\n\t\tGet a parameter value.\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()  # pragma: no cover\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\treturn self.format_sql(sql, params)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\tpass\n\t\n\tdef format_placeholder(\n\t\tself,\n\t\tplaceholder: AnyStr,\n\t\tindex: int,\n\t) -> AnyStr:\n\t\"\"\"\n\tConvert an in-style placeholder to its out-style placeholder.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\traise NotImplementedError()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\tif self._in_style is None:\n\t\t\traise ValueError(\"in-style is None.\")\n\n\t\tif self._out_style is None:\n\t\t\traise ValueError(\"out-style is None.\")\n\n\t\t(sql, params) = self._in_style.format(sql, params)\n\n\t\treturn (self._out_style.format(sql, params))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\treturn self.sql_params.format(sql, params)\n\n\tdef format_params(self, sql: AnyStr, params: SQLParamsParams) -> Tuple[AnyStr, SQLParamsParams]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of\n\tthe in-style parameters.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\treturn self._format(sql, params, None)\n\n\tdef _format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t\tout: OutParams,\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\tif out is None:\n\t\t\tout = {}\n\t\t\tif self._style is None:\n\t\t\t\treturn sql, params\n\t\t\tsql = self._style.format(sql, params, out)\n\t\t\tparams = out\n\t\telif self._style is not None:\n\t\t\tsql = self._style.format(sql, params, out)\n\t\treturn sql, params\n\n\tdef get_param(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t\tname: str,\n\t) -> Any:\n\t\t\"\"\"\n\t\tGet a parameter value.\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "ques_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\traise NotImplementedError()\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t# Get the parameter style.\n\tparam_style = self._param_style\n\n\t# Get the parameter marker.\n\tparam_marker = self._param_marker\n\n\t# Get the parameter marker characters.\n\tparam_marker_characters = self._param_marker_characters\n\n\t# Get the named parameter prefix.\n\tnamed_parameter_prefix = self._named_parameter_prefix\n\n\t# Get the named parameter suffix.\n\tnamed_parameter_suffix = self._named_parameter_suffix\n\n\t# Get the named parameter pattern.\n\tnamed_parameter_pattern = self._named_parameter_pattern\n\n\t# Get the named parameter reverse pattern.\n\tnamed_parameter_reverse_pattern = self._named_parameter_reverse_pattern\n\n\t# Get the named parameter template.\n\tnamed_parameter_template = self._named_parameter_template\n\n\t# Get the named parameter reverse template.\n\tnamed_parameter_reverse_template = self._named_parameter_reverse_template\n\n\t# Get the ordinal parameter pattern.\n\tordinal_parameter_pattern = self._\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\tif self.in_style is SQLParams.ordinal:\n\t\t\tmany_params = list(many_params)\n\t\t\treturn self.formatmany_ordinal(sql, many_params)\n\t\telse:\n\t\t\treturn self.formatmany_named(sql, many_params)\n\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tassert self.__in_style is not None\n\tassert self.__out_style is not None\n\tassert self.__placeholder is not None\n\tassert self.__placeholder != self.__in_style.get_placeholder(1)\n\t\n\tif self.__out_style == self.__in_style:\n\t\treturn (sql, many_params)\n\t\n\tif self.__in_style.is_named_style():\n\t\tassert all(isinstance(params, Mapping) for params in many_params)\n\t\treturn (\n\t\t\tself.format(sql),\n\t\t\t[self.convert(params) for params in many_params],\n\t\t)\n\telse:\n\t\tassert all(isinstance(params, Sequence) for params in many_params)\n\t\treturn (\n\t\t\tself.format(sql),\n\t\t\t[self.convert(params) for params in many_params],\n\t\t)\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\traise NotImplementedError()\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif isinstance(sql, str):\n\t\tsql = sql.encode(self.charset)\n\tif isinstance(sql, bytes):\n\t\tsqlfragments = self.regex.split(sql)\n\t\tfor i in range(1, len(sqlfragments), 4):\n\t\t\tsqlfragments[i] = self.out_style.encode(self.charset)\n\t\tsql = b''.join(sqlfragments)\n\t\treturn sql, [params for params in many_params]\n\telse:\n\t\traise TypeError(\n\t\t\t\"sql must be a str or bytes object. not %s\" % type(sql).__name__\n\t\t)\n\n\t\t\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif self.in_style == self.out_style:\n\t\treturn sql, many_params\n\n\treturn self.convertmany(\n\t\tself._formatmany_params_in,\n\t\tself._formatmany_params_out,\n\t\tsql,\n\t\tmany_params,\n\t)\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t# If we have a list of parameters, then we have to format each one.\n\t# We store the formatted SQL and parameters in a list.\n\tformatted_sql = []\n\tformatted_params = []\n\tfor params in many_params:\n\t\t# Format the SQL and parameters.\n\t\tsql, params = self.format(sql, params)\n\t\t# Add them to the list.\n\t\tformatted_sql.append(sql)\n\t\tformatted_params.append(params)\n\t# If the SQL is a byte string, then we need to join the byte strings.\n\tif isinstance(sql, bytes):\n\t\t# Join the byte strings.\n\t\tformatted_sql = b''.join(formatted_sql)\n\t# Otherwise, the SQL is a string, so we need to join the strings.\n\telse:\n\t\t# Join the strings.\n\t\tformatted_sql = ''.join(formatted_sql)\n\t# Return the formatted SQL and parameters.\n\treturn formatted_sql, formatted_params\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif not isinstance(many_params, Iterable):\n\t\traise TypeError(\n\t\t\tf\"many_params={many_params!r} is not an iterable\"\n\t\t)\n\n\tif self.in_style is None:\n\t\treturn sql, many_params\n\n\tif self.in_style == SQLParams.IN_STYLE_NAMED:\n\t\tif isinstance(many_params, dict):\n\t\t\tmany_params = [many_params]\n\n\t\tif not all(isinstance(params, Mapping) for params in many_params):\n\t\t\traise TypeError(\n\t\t\t\tf\"many_params={many_params!r} contains a non-mapping\"\n\t\t\t)\n\n\t\tsql, many_params = self._formatmany_named(sql, many_params)\n\n\telif self.in_style == SQLParams.IN_STYLE_ORDINAL:\n\t\tif not all(isinstance(params, Sequence) for params in many_params):\n\t\t\traise TypeError(\n\t\t\t\tf\"many_\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\tif not self.in_style:\n\t\t\traise RuntimeError(\"in_style must be set to use formatmany()\")\n\t\tif not self.out_style:\n\t\t\traise RuntimeError(\"out_style must be set to use formatmany()\")\n\t\tif not isinstance(sql, (str, bytes)):\n\t\t\traise TypeError(\"sql must be str or bytes\")\n\t\tif not isinstance(many_params, Iterable):\n\t\t\traise TypeError(\"many_params must be Iterable\")\n\t\tmany_params = list(many_params)\n\t\tif many_params:\n\t\t\tif isinstance(many_params[0], Mapping):\n\t\t\t\tif self.in_style.is_ordinal_style:\n\t\t\t\t\traise TypeError(\"many_params[0] must be Sequence for in_style=%s\" % self.in_style)\n\t\t\telif isinstance(many_params[0], Sequence):\n\t\t\t\tif self.in_style.is_named_style:\n\t\t\t\t\traise Type\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t# Get the parameter style.\n\tparam_style = self._param_style\n\n\t# Get the parameter marker.\n\tparam_marker = self._param_marker\n\n\t# Get the parameter marker characters.\n\tparam_marker_characters = self._param_marker_characters\n\n\t# Get the named parameter prefix.\n\tnamed_parameter_prefix = self._named_parameter_prefix\n\n\t# Get the named parameter suffix.\n\tnamed_parameter_suffix = self._named_parameter_suffix\n\n\t# Get the named parameter pattern.\n\tnamed_parameter_pattern = self._named_parameter_pattern\n\n\t# Get the named parameter reverse pattern.\n\tnamed_parameter_reverse_pattern = self._named_parameter_reverse_pattern\n\n\t# Get the named parameter template.\n\tnamed_parameter_template = self._named_parameter_template\n\n\t# Get the named parameter reverse template.\n\tnamed_parameter_reverse_template = self._named_parameter_reverse_template\n\n\t# Get the ordinal parameter pattern.\n\tordinal_parameter_pattern = self._\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\tif self.in_style is SQLParams.ordinal:\n\t\t\tmany_params = list(many_params)\n\t\t\treturn self.formatmany_ordinal(sql, many_params)\n\t\telse:\n\t\t\treturn self.formatmany_named(sql, many_params)\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tassert self.__in_style is not None\n\tassert self.__out_style is not None\n\tassert self.__placeholder is not None\n\tassert self.__placeholder != self.__in_style.get_placeholder(1)\n\t\n\tif self.__out_style == self.__in_style:\n\t\treturn (sql, many_params)\n\t\n\tif self.__in_style.is_named_style():\n\t\tassert all(isinstance(params, Mapping) for params in many_params)\n\t\treturn (\n\t\t\tself.format(sql),\n\t\t\t[self.convert(params) for params in many_params],\n\t\t)\n\telse:\n\t\tassert all(isinstance(params, Sequence) for params in many_params)\n\t\treturn (\n\t\t\tself.format(sql),\n\t\t\t[self.convert(params) for params in many_params],\n\t\t)\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif isinstance(sql, str):\n\t\tsql = sql.encode(self.charset)\n\tif isinstance(sql, bytes):\n\t\tsqlfragments = self.regex.split(sql)\n\t\tfor i in range(1, len(sqlfragments), 4):\n\t\t\tsqlfragments[i] = self.out_style.encode(self.charset)\n\t\tsql = b''.join(sqlfragments)\n\t\treturn sql, [params for params in many_params]\n\telse:\n\t\traise TypeError(\n\t\t\t\"sql must be a str or bytes object. not %s\" % type(sql).__name__\n\t\t)\n\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif self.in_style == self.out_style:\n\t\treturn sql, many_params\n\n\treturn self.convertmany(\n\t\tself._formatmany_params_in,\n\t\tself._formatmany_params_out,\n\t\tsql,\n\t\tmany_params,\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t# If we have a list of parameters, then we have to format each one.\n\t# We store the formatted SQL and parameters in a list.\n\tformatted_sql = []\n\tformatted_params = []\n\tfor params in many_params:\n\t\t# Format the SQL and parameters.\n\t\tsql, params = self.format(sql, params)\n\t\t# Add them to the list.\n\t\tformatted_sql.append(sql)\n\t\tformatted_params.append(params)\n\t# If the SQL is a byte string, then we need to join the byte strings.\n\tif isinstance(sql, bytes):\n\t\t# Join the byte strings.\n\t\tformatted_sql = b''.join(formatted_sql)\n\t# Otherwise, the SQL is a string, so we need to join the strings.\n\telse:\n\t\t# Join the strings.\n\t\tformatted_sql = ''.join(formatted_sql)\n\t# Return the formatted SQL and parameters.\n\treturn formatted_sql, formatted_params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif not isinstance(many_params, Iterable):\n\t\traise TypeError(\n\t\t\tf\"many_params={many_params!r} is not an iterable\"\n\t\t)\n\n\tif self.in_style is None:\n\t\treturn sql, many_params\n\n\tif self.in_style == SQLParams.IN_STYLE_NAMED:\n\t\tif isinstance(many_params, dict):\n\t\t\tmany_params = [many_params]\n\n\t\tif not all(isinstance(params, Mapping) for params in many_params):\n\t\t\traise TypeError(\n\t\t\t\tf\"many_params={many_params!r} contains a non-mapping\"\n\t\t\t)\n\n\t\tsql, many_params = self._formatmany_named(sql, many_params)\n\n\telif self.in_style == SQLParams.IN_STYLE_ORDINAL:\n\t\tif not all(isinstance(params, Sequence) for params in many_params):\n\t\t\traise TypeError(\n\t\t\t\tf\"many_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\tif not self.in_style:\n\t\t\traise RuntimeError(\"in_style must be set to use formatmany()\")\n\t\tif not self.out_style:\n\t\t\traise RuntimeError(\"out_style must be set to use formatmany()\")\n\t\tif not isinstance(sql, (str, bytes)):\n\t\t\traise TypeError(\"sql must be str or bytes\")\n\t\tif not isinstance(many_params, Iterable):\n\t\t\traise TypeError(\"many_params must be Iterable\")\n\t\tmany_params = list(many_params)\n\t\tif many_params:\n\t\t\tif isinstance(many_params[0], Mapping):\n\t\t\t\tif self.in_style.is_ordinal_style:\n\t\t\t\t\traise TypeError(\"many_params[0] must be Sequence for in_style=%s\" % self.in_style)\n\t\t\telif isinstance(many_params[0], Sequence):\n\t\t\t\tif self.in_style.is_named_style:\n\t\t\t\t\traise Type\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "ques_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n\tdir_list = os.listdir(path)\n\n\tif '.ocfl' not in dir_list:\n\t\traise NotImplementedError('.ocfl directory must exist')\n\n\tif 'ocfl_object.json' not in os.listdir(os.path.join(path, '.ocfl')):\n\t\traise NotImplementedError('ocfl_object.json must exist')\n\n\treturn True\n", "def validate(self, path):\n\t\treturn self._validate(path)\n\n\t@_assert_not_frozen\n\tdef add_from_path(self, path, relative_path=None, **kwargs):\n\t\t\"\"\"\n\t\tAdd file or directory to OCFL at `relative_path`.\n\n", "def validate(self, path):\n\treturn self.validator.validate(path)\n\n\tdef validate_fast(self, path):\n\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\n", "def validate(self, path):\n\treturn self.validate_with_output(path) is None\n\n\tdef validate_with_output(self, path):\n\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\n", "def validate(self, path):\n\t# TODO\n\treturn True\n\n\tdef validate_structure(self, path):\n\t\t\"\"\"\n\t\tValidate OCFL object at path or pyfs root.\n\n", "def validate(self, path):\n\tpass\n\n", "def validate(self, path):\n\tif path is None:\n\t\tpath = self.root\n\tvalidator = self._pyfs.validate(path)\n\t# TODO: for now, we are ignoring warnings\n\treturn validator.is_valid() and validator.get_warnings() == []\n\n\tdef get_digests(self, path):\n\t\"\"\"\n\tCompute digests for all files in the OCFL object at path or pyfs root.\n\n", "def validate(self, path):\n\treturn self._impl.validate(path)\n\n", "def validate(self, path):\n\tif not path:\n\t\tpath = self._root\n\treturn self._validate(path)\n\n\tdef _validate(self, path):\n\t\t\"\"\"\n\t\tValidate OCFL object at path.\n\n", "def validate(self, path):\n\tif path is None:\n\t\treturn self.__validate()\n\telse:\n\t\treturn self.__validate(path)\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n\tdir_list = os.listdir(path)\n\n\tif '.ocfl' not in dir_list:\n\t\traise NotImplementedError('.ocfl directory must exist')\n\n\tif 'ocfl_object.json' not in os.listdir(os.path.join(path, '.ocfl')):\n\t\traise NotImplementedError('ocfl_object.json must exist')\n\n\treturn True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\treturn self._validate(path)\n\n\t@_assert_not_frozen\n\tdef add_from_path(self, path, relative_path=None, **kwargs):\n\t\t\"\"\"\n\t\tAdd file or directory to OCFL at `relative_path`.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\treturn self.validator.validate(path)\n\n\tdef validate_fast(self, path):\n\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\treturn self.validate_with_output(path) is None\n\n\tdef validate_with_output(self, path):\n\t\"\"\"\n\tValidate OCFL object at path or pyfs root.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t# TODO\n\treturn True\n\n\tdef validate_structure(self, path):\n\t\t\"\"\"\n\t\tValidate OCFL object at path or pyfs root.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tif path is None:\n\t\tpath = self.root\n\tvalidator = self._pyfs.validate(path)\n\t# TODO: for now, we are ignoring warnings\n\treturn validator.is_valid() and validator.get_warnings() == []\n\n\tdef get_digests(self, path):\n\t\"\"\"\n\tCompute digests for all files in the OCFL object at path or pyfs root.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\treturn self._impl.validate(path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tif not path:\n\t\tpath = self._root\n\treturn self._validate(path)\n\n\tdef _validate(self, path):\n\t\t\"\"\"\n\t\tValidate OCFL object at path.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tif path is None:\n\t\treturn self.__validate()\n\telse:\n\t\treturn self.__validate(path)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n\treturn '%s%s:%s' % (prefix, self.name, self.status)\n\n", "def status_str(self, prefix=''):\n\treturn '%s%s' % (prefix, self.status)\n\n\n", "def status_str(self, prefix=''):\n\treturn '%s%s: %s' % (prefix, self.__class__.__name__, self._status_str())\n\n\n\tdef _status_str(self):\n\t\t\"\"\"\n\t\tReturn string of validator status.\n\t\t\"\"\"\n\t\treturn '%s: %d' % (self._status, self.error_count)\n\n\n\tdef validate(self, *args):\n\t\t\"\"\"\n\t\tValidate file.\n\t\t\"\"\"\n\t\tself._status = Status.OK\n\n\n\tdef _set_status(self, status):\n\t\t\"\"\"\n\t\tSet validator status.\n\t\t\"\"\"\n\t\tself._status = status\n\n\n\tdef _set_error(self, msg):\n\t\t\"\"\"\n\t\tSet validator error.\n\t\t\"\"\"\n\t\tself.errors.append(msg)\n\t\tself.error_count += 1\n\t\tself._set_status(Status.ERROR)\n\n\n\tdef _set_warning(self, msg):\n\t\t\"\"\"\n\t\tSet validator warning.\n\t\t\"\"\"\n\t\tself.warnings.append\n", "def status_str(self, prefix=''):\n\tprefix = prefix + ' ' if prefix else ''\n\treturn '{}[{}] {}'.format(prefix, self.status, self.message)\n\n\n", "def status_str(self, prefix=''):\n\treturn prefix + ' ' + str(self.status)\n\n\n", "def status_str(self, prefix=''):\n\ts = ''\n\ttry:\n\t\ts = '%s %s:%d' % (prefix, self.location, self.line)\n\texcept:\n\t\tpass\n\treturn s\n\n\n", "def status_str(self, prefix=''):\n\treturn '%s%s: %s%s' % (prefix, self.name, self.status,\n\t\t\t\t\t\t   ' (%s)' % self.message if self.message else '')\n\n\n", "def status_str(self, prefix=''):\n\treturn prefix + self.status.name\n\n", "def status_str(self, prefix=''):\n\tif not self.valid:\n\t\treturn prefix + \"validator is down\"\n\treturn prefix + \"validator is up (uptime: %d sec)\" % (time.time() - self.uptime)\n\n", "def status_str(self, prefix=''):\n\tif self.status == self.STATUS_INVALID:\n\t\treturn prefix + 'invalid'\n\telif self.status == self.STATUS_VALID:\n\t\treturn prefix + 'valid'\n\telif self.status == self.STATUS_UNKNOWN:\n\t\treturn prefix + 'unknown'\n\telse:\n\t\tassert 0, 'invalid status'\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\treturn '%s%s:%s' % (prefix, self.name, self.status)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '%s%s' % (prefix, self.status)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '%s%s: %s' % (prefix, self.__class__.__name__, self._status_str())\n\n\n\tdef _status_str(self):\n\t\t\"\"\"\n\t\tReturn string of validator status.\n\t\t\"\"\"\n\t\treturn '%s: %d' % (self._status, self.error_count)\n\n\n\tdef validate(self, *args):\n\t\t\"\"\"\n\t\tValidate file.\n\t\t\"\"\"\n\t\tself._status = Status.OK\n\n\n\tdef _set_status(self, status):\n\t\t\"\"\"\n\t\tSet validator status.\n\t\t\"\"\"\n\t\tself._status = status\n\n\n\tdef _set_error(self, msg):\n\t\t\"\"\"\n\t\tSet validator error.\n\t\t\"\"\"\n\t\tself.errors.append(msg)\n\t\tself.error_count += 1\n\t\tself._set_status(Status.ERROR)\n\n\n\tdef _set_warning(self, msg):\n\t\t\"\"\"\n\t\tSet validator warning.\n\t\t\"\"\"\n\t\tself.warnings.append\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\tprefix = prefix + ' ' if prefix else ''\n\treturn '{}[{}] {}'.format(prefix, self.status, self.message)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn prefix + ' ' + str(self.status)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\ts = ''\n\ttry:\n\t\ts = '%s %s:%d' % (prefix, self.location, self.line)\n\texcept:\n\t\tpass\n\treturn s\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '%s%s: %s%s' % (prefix, self.name, self.status,\n\t\t\t\t\t\t   ' (%s)' % self.message if self.message else '')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn prefix + self.status.name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\tif not self.valid:\n\t\treturn prefix + \"validator is down\"\n\treturn prefix + \"validator is up (uptime: %d sec)\" % (time.time() - self.uptime)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\tif self.status == self.STATUS_INVALID:\n\t\treturn prefix + 'invalid'\n\telif self.status == self.STATUS_VALID:\n\t\treturn prefix + 'valid'\n\telif self.status == self.STATUS_UNKNOWN:\n\t\treturn prefix + 'unknown'\n\telse:\n\t\tassert 0, 'invalid status'\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n\treturn '%s%s, %s, %s, %s, %s\\n' % (prefix, self.filename, self.state, self.action, self.result, self.message)\n\n", "def status_str(self, prefix=''):\n\treturn '\\n'.join(self.status(prefix))\n\n\n", "def status_str(self, prefix=''):\n\treturn '\\n'.join(self.status_list(prefix))\n\n", "def status_str(self, prefix=''):\n\treturn '\\n'.join(\n\t\t['%s%s' % (prefix, line) for line in self._lines]\n\t)\n\n", "def status_str(self, prefix=''):\n\t\t\n\t\tlines = []\n\t\t\n\t\tfor entry in self.log:\n\t\t\tlines.append(prefix + entry)\n\t\t\t\n\t\treturn '\\n'.join(lines)\n\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn string representation of validation log.\n\t\t\"\"\"\n\t\treturn self.status_str()\n\t\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn number of entries in validation log.\n\t\t\"\"\"\n\t\treturn len(self.log)\n\n\tdef __iter__(self):\n\t\treturn iter(self.log)\n\n\tdef __getitem__(self, i):\n\t\treturn self.log[i]\n\t\n\tdef __contains__(self, obj):\n\t\treturn obj in self.log\n\t\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClear validation log.\n\t\t\"\"\"\n\t\tself.log = []\n\t\t\n\tdef append(self, entry):\n\t\t\"\"\"\n\t\tAdd an entry to validation log.\n\t\t\"\"\"\n\t\t\n", "def status_str(self, prefix=''):\n\treturn \"\\n\".join(map(lambda x: prefix + str(x), self.log))\n\n\n", "def status_str(self, prefix=''):\n\treturn prefix + ' \\n'.join([str(e) for e in self])\n\n\tdef as_csv(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn validation log as comma-separated values.\n\t\t\"\"\"\n\t\treturn prefix + '\\n'.join([e.as_csv() for e in self])\n\n\tdef __str__(self):\n\t\treturn self.status_str()\n\n\tdef to_json(self):\n\t\treturn [e.to_json() for e in self]\n\n\tdef to_dict(self):\n\t\treturn [e.to_dict() for e in self]\n\n\tdef to_excel(self, ws):\n\t\tws.title = 'Validation Log'\n\t\tws.append(['Type', 'Message', 'Severity', 'Row', 'Column'])\n\t\tfor e in self:\n\t\t\tws.append([e.etype, e.msg, e.severity, e.row, e.col])\n\n\tdef to_html(self, prefix=\"<br>\", suffix=\"<br>\"):\n\t\t\n", "def status_str(self, prefix=''):\n\ts = ''\n\tfor e in self.log:\n\t\ts += prefix + e + '\\n'\n\treturn s\n", "def status_str(self, prefix=''):\n\treturn '\\n'.join(\n\t\tself.format_log_entry(prefix + entry)\n\t\tfor entry in self.log)\n\n\n", "def status_str(self, prefix=''):\n\treturn '\\n'.join([prefix + msg for msg in self.msgs])\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\treturn '%s%s, %s, %s, %s, %s\\n' % (prefix, self.filename, self.state, self.action, self.result, self.message)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '\\n'.join(self.status(prefix))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '\\n'.join(self.status_list(prefix))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '\\n'.join(\n\t\t['%s%s' % (prefix, line) for line in self._lines]\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\t\n\t\tlines = []\n\t\t\n\t\tfor entry in self.log:\n\t\t\tlines.append(prefix + entry)\n\t\t\t\n\t\treturn '\\n'.join(lines)\n\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn string representation of validation log.\n\t\t\"\"\"\n\t\treturn self.status_str()\n\t\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturn number of entries in validation log.\n\t\t\"\"\"\n\t\treturn len(self.log)\n\n\tdef __iter__(self):\n\t\treturn iter(self.log)\n\n\tdef __getitem__(self, i):\n\t\treturn self.log[i]\n\t\n\tdef __contains__(self, obj):\n\t\treturn obj in self.log\n\t\n\tdef clear(self):\n\t\t\"\"\"\n\t\tClear validation log.\n\t\t\"\"\"\n\t\tself.log = []\n\t\t\n\tdef append(self, entry):\n\t\t\"\"\"\n\t\tAdd an entry to validation log.\n\t\t\"\"\"\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn \"\\n\".join(map(lambda x: prefix + str(x), self.log))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn prefix + ' \\n'.join([str(e) for e in self])\n\n\tdef as_csv(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn validation log as comma-separated values.\n\t\t\"\"\"\n\t\treturn prefix + '\\n'.join([e.as_csv() for e in self])\n\n\tdef __str__(self):\n\t\treturn self.status_str()\n\n\tdef to_json(self):\n\t\treturn [e.to_json() for e in self]\n\n\tdef to_dict(self):\n\t\treturn [e.to_dict() for e in self]\n\n\tdef to_excel(self, ws):\n\t\tws.title = 'Validation Log'\n\t\tws.append(['Type', 'Message', 'Severity', 'Row', 'Column'])\n\t\tfor e in self:\n\t\t\tws.append([e.etype, e.msg, e.severity, e.row, e.col])\n\n\tdef to_html(self, prefix=\"<br>\", suffix=\"<br>\"):\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\ts = ''\n\tfor e in self.log:\n\t\ts += prefix + e + '\\n'\n\treturn s\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '\\n'.join(\n\t\tself.format_log_entry(prefix + entry)\n\t\tfor entry in self.log)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '\\n'.join([prefix + msg for msg in self.msgs])\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "ques_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "ques_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n\tspec_version = self.spec_version\n\tif extract_spec_version:\n\t\ttry:\n\t\t\t# If a type value is present then extract the spec_version from it.\n\t\t\t# This is not a complete test, but should catch most cases.\n\t\t\ttype_value = inventory.get('type', None)\n\t\t\tif type_value is not None:\n\t\t\t\tspec_version = type_value.split('/')[1]\n\t\texcept Exception as e:\n\t\t\traise ValueError(\"Error extracting spec_version from type value: {}\".format(e))\n\n\t# If the spec_version is 1.0 then validate the inventory\n\t# using the OpenIOC schema.\n\tif spec_version == '1.0':\n\t\treturn self._validate_ioc(inventory)\n\n\t# If the spec_version is 1.1 then validate the inventory\n\t# using the STIX schema.\n\tif spec_version == '1.1':\n\t\treturn self._validate_stix(inventory)\n\n\traise ValueError(\"Unsupported spec_version {}\".\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tspec_version = self.get_inventory_spec_version(inventory)\n\t\telse:\n\t\t\tspec_version = self.spec_version\n\telse:\n\t\tspec_version = self.spec_version\n\n\tif spec_version == '0.1':\n\t\treturn self.validate_inventory_v01(inventory)\n\telif spec_version == '0.2':\n\t\treturn self.validate_inventory_v02(inventory)\n\telse:\n\t\traise UnsupportedInventorySpecError(spec_version)\n\n\treturn True\n\n\tdef validate_inventory_v01(self, inventory):\n\t\t\"\"\"\n\t\tValidate an inventory for version 0.1 of the spec.\n\n\t\tThis is a very loose validation, only checking that the inventory is\n\t\ta dict, and that it contains a dict called 'inventory'.\n\t\t\"\"\"\n\t\tif not isinstance(inventory, dict):\n\t\t\treturn False\n\n\t\tif not 'inventory' in inventory:\n\t\t\treturn False\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif self.spec_version is not None:\n\t\t\tif self.spec_version != inventory[\"metadata\"][\"spec_version\"]:\n\t\t\t\traise ValueError(\"spec_version in inventory file doesn't match spec_version in library\")\n\t\telse:\n\t\t\tself.spec_version = inventory[\"metadata\"][\"spec_version\"]\n\t\t\tself.version_info = self.get_version_info(self.spec_version)\n\t\t\tself.constants = self.version_info[\"constants\"]\n\n\tif not isinstance(inventory, dict):\n\t\traise TypeError(\"Expected a dictionary\")\n\n\tif \"metadata\" not in inventory:\n\t\traise ValueError(\"Inventory must have a 'metadata' key\")\n\n\tif \"spec_version\" not in inventory[\"metadata\"]:\n\t\traise ValueError(\"Inventory must have a 'spec_version' key in 'metadata'\")\n\n\tif \"version\" not in inventory[\"metadata\"]:\n\t\traise ValueError(\"Inventory must have a 'version' key in 'metadata'\")\n\n\n", "def validate(self, inventory, extract_spec_version=False):\n\t# Check for required fields\n\tif self.spec_version == \"0.9\":\n\t\trequired_fields = [\"name\", \"type\"]\n\telse:\n\t\trequired_fields = [\"name\", \"type\", \"spec_version\"]\n\tfor f in required_fields:\n\t\tif f not in inventory:\n\t\t\tself.errors.append(\"Missing required field '%s'\" % f)\n\t\t\treturn False\n\n\t# Check that spec_version is valid\n\tif extract_spec_version:\n\t\tversion = self.get_spec_version(inventory)\n\t\tif version:\n\t\t\tself.spec_version = version\n\t\t\tself.errors = []\n\t\telse:\n\t\t\treturn False\n\tif self.spec_version not in self.valid_spec_versions:\n\t\tself.errors.append(\"Invalid spec_version '%s'\" % self.spec_version)\n\t\treturn False\n\n\t# Check that type is valid\n\tif inventory[\"type\"] not in self.valid_types:\n\t\tself.errors.append(\"Invalid type '%s'\" % inventory\n", "def validate(self, inventory, extract_spec_version=False):\n\t\t# First extract the spec_version if we need to\n\t\tif extract_spec_version:\n\t\t\textracted_spec_version = _extract_spec_version(inventory)\n\t\t\tif extracted_spec_version is not None:\n\t\t\t\tself.spec_version = extracted_spec_version\n\t\t# First check to see if it is a valid JSON object\n\t\tif not _is_valid_json(inventory):\n\t\t\treturn False\n\t\t# Check the inventory_format_version\n\t\tif not _is_valid_format_version(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the type value\n\t\tif not _is_valid_type(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the metadata\n\t\tif not _is_valid_metadata(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the schema\n\t\tif not _is_valid_schema(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the data\n\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tself.extract_spec_version(inventory)\n\n\t# validate the inventory against the given spec_version\n\t# and return the list of errors\n\tvalidator = self.validator_cls(self.spec_version)\n\tself.errors = validator.validate(inventory)\n\n\treturn self.errors\n\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\ttry:\n\t\t\tself.spec_version = self.type_version(inventory['type'])\n\t\texcept (KeyError, ValueError):\n\t\t\tpass\n\t# Check the type value is correct\n\ttry:\n\t\ttype_value = inventory['type']\n\texcept KeyError:\n\t\t# No type value\n\t\treturn False\n\tif type_value != self.type_value():\n\t\treturn False\n\t# Check the required keys are present\n\tfor key in self.required_keys:\n\t\tif key not in inventory:\n\t\t\treturn False\n\t# Check the keys are valid\n\tfor key in inventory:\n\t\tif key not in self.valid_keys:\n\t\t\treturn False\n\t# Check the required values\n\tfor key in inventory:\n\t\tif key in self.required_values:\n\t\t\tif not inventory[key]:\n\t\t\t\treturn False\n\treturn True\n\n", "def validate(self, inventory, extract_spec_version=False):\n\t\tif extract_spec_version:\n\t\t\tself.inventory_spec_version = self.get_spec_version(inventory)\n\t\t\tif not self.inventory_spec_version:\n\t\t\t\tself.warnings.append(\"No inventory type: unable to determine inventory specification version\")\n\t\t\t\tself.inventory_spec_version = self.spec_version\n\t\telse:\n\t\t\tself.inventory_spec_version = self.spec_version\n\t\tself.validate_type(inventory)\n\t\tself.validate_spec_version(inventory)\n\t\tself.validate_metadata(inventory)\n\t\tself.validate_hosts(inventory)\n\t\tself.validate_groups(inventory)\n\t\tself.validate_vars(inventory)\n\t\tself.validate_children(inventory)\n\t\treturn self.is_valid()\n\n\tdef validate_type(self, inventory):\n\t\t\"\"\"\n\t\tValidate the type value.\n\t\t\"\"\"\n\t\tif 'type' not in inventory:\n\t\t\tself.warnings.append(\"No\n", "def validate(self, inventory, extract_spec_version=False):\n\tif not self.spec_version:\n\t\traise Exception(\"No specification version to validate against.\")\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] == 'inventory':\n\t\t\t\tself.spec_version = '1.0'\n\t\t\telif inventory['type'] == 'inventory/1.0':\n\t\t\t\tself.spec_version = '1.0'\n\t\t\telif inventory['type'] == 'inventory/2.0':\n\t\t\t\tself.spec_version = '2.0'\n\t\t\telse:\n\t\t\t\traise Exception(\"Unsupported inventory type: %s\" % inventory['type'])\n\t\telse:\n\t\t\traise Exception(\"No 'type' value found in inventory.\")\n\tif self.spec_version == '1.0':\n\t\tself.validate_inventory_1_0(inventory)\n\telif self.spec_version == '2.0':\n\t\tself.validate_inventory_2_0(inventory)\n\telse:\n\t\traise Exception\n", "def validate(self, inventory, extract_spec_version=False):\n\tif not self.validate_type(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_format(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_source(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_version(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_groups(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_hosts(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_children(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_vars(inventory) and extract_spec_version:\n\t\treturn False\n\treturn True\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tspec_version = self.spec_version\n\tif extract_spec_version:\n\t\ttry:\n\t\t\t# If a type value is present then extract the spec_version from it.\n\t\t\t# This is not a complete test, but should catch most cases.\n\t\t\ttype_value = inventory.get('type', None)\n\t\t\tif type_value is not None:\n\t\t\t\tspec_version = type_value.split('/')[1]\n\t\texcept Exception as e:\n\t\t\traise ValueError(\"Error extracting spec_version from type value: {}\".format(e))\n\n\t# If the spec_version is 1.0 then validate the inventory\n\t# using the OpenIOC schema.\n\tif spec_version == '1.0':\n\t\treturn self._validate_ioc(inventory)\n\n\t# If the spec_version is 1.1 then validate the inventory\n\t# using the STIX schema.\n\tif spec_version == '1.1':\n\t\treturn self._validate_stix(inventory)\n\n\traise ValueError(\"Unsupported spec_version {}\".\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tspec_version = self.get_inventory_spec_version(inventory)\n\t\telse:\n\t\t\tspec_version = self.spec_version\n\telse:\n\t\tspec_version = self.spec_version\n\n\tif spec_version == '0.1':\n\t\treturn self.validate_inventory_v01(inventory)\n\telif spec_version == '0.2':\n\t\treturn self.validate_inventory_v02(inventory)\n\telse:\n\t\traise UnsupportedInventorySpecError(spec_version)\n\n\treturn True\n\n\tdef validate_inventory_v01(self, inventory):\n\t\t\"\"\"\n\t\tValidate an inventory for version 0.1 of the spec.\n\n\t\tThis is a very loose validation, only checking that the inventory is\n\t\ta dict, and that it contains a dict called 'inventory'.\n\t\t\"\"\"\n\t\tif not isinstance(inventory, dict):\n\t\t\treturn False\n\n\t\tif not 'inventory' in inventory:\n\t\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif self.spec_version is not None:\n\t\t\tif self.spec_version != inventory[\"metadata\"][\"spec_version\"]:\n\t\t\t\traise ValueError(\"spec_version in inventory file doesn't match spec_version in library\")\n\t\telse:\n\t\t\tself.spec_version = inventory[\"metadata\"][\"spec_version\"]\n\t\t\tself.version_info = self.get_version_info(self.spec_version)\n\t\t\tself.constants = self.version_info[\"constants\"]\n\n\tif not isinstance(inventory, dict):\n\t\traise TypeError(\"Expected a dictionary\")\n\n\tif \"metadata\" not in inventory:\n\t\traise ValueError(\"Inventory must have a 'metadata' key\")\n\n\tif \"spec_version\" not in inventory[\"metadata\"]:\n\t\traise ValueError(\"Inventory must have a 'spec_version' key in 'metadata'\")\n\n\tif \"version\" not in inventory[\"metadata\"]:\n\t\traise ValueError(\"Inventory must have a 'version' key in 'metadata'\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t# Check for required fields\n\tif self.spec_version == \"0.9\":\n\t\trequired_fields = [\"name\", \"type\"]\n\telse:\n\t\trequired_fields = [\"name\", \"type\", \"spec_version\"]\n\tfor f in required_fields:\n\t\tif f not in inventory:\n\t\t\tself.errors.append(\"Missing required field '%s'\" % f)\n\t\t\treturn False\n\n\t# Check that spec_version is valid\n\tif extract_spec_version:\n\t\tversion = self.get_spec_version(inventory)\n\t\tif version:\n\t\t\tself.spec_version = version\n\t\t\tself.errors = []\n\t\telse:\n\t\t\treturn False\n\tif self.spec_version not in self.valid_spec_versions:\n\t\tself.errors.append(\"Invalid spec_version '%s'\" % self.spec_version)\n\t\treturn False\n\n\t# Check that type is valid\n\tif inventory[\"type\"] not in self.valid_types:\n\t\tself.errors.append(\"Invalid type '%s'\" % inventory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\t# First extract the spec_version if we need to\n\t\tif extract_spec_version:\n\t\t\textracted_spec_version = _extract_spec_version(inventory)\n\t\t\tif extracted_spec_version is not None:\n\t\t\t\tself.spec_version = extracted_spec_version\n\t\t# First check to see if it is a valid JSON object\n\t\tif not _is_valid_json(inventory):\n\t\t\treturn False\n\t\t# Check the inventory_format_version\n\t\tif not _is_valid_format_version(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the type value\n\t\tif not _is_valid_type(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the metadata\n\t\tif not _is_valid_metadata(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the schema\n\t\tif not _is_valid_schema(inventory, self.spec_version):\n\t\t\treturn False\n\t\t# Check the data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tself.extract_spec_version(inventory)\n\n\t# validate the inventory against the given spec_version\n\t# and return the list of errors\n\tvalidator = self.validator_cls(self.spec_version)\n\tself.errors = validator.validate(inventory)\n\n\treturn self.errors\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\ttry:\n\t\t\tself.spec_version = self.type_version(inventory['type'])\n\t\texcept (KeyError, ValueError):\n\t\t\tpass\n\t# Check the type value is correct\n\ttry:\n\t\ttype_value = inventory['type']\n\texcept KeyError:\n\t\t# No type value\n\t\treturn False\n\tif type_value != self.type_value():\n\t\treturn False\n\t# Check the required keys are present\n\tfor key in self.required_keys:\n\t\tif key not in inventory:\n\t\t\treturn False\n\t# Check the keys are valid\n\tfor key in inventory:\n\t\tif key not in self.valid_keys:\n\t\t\treturn False\n\t# Check the required values\n\tfor key in inventory:\n\t\tif key in self.required_values:\n\t\t\tif not inventory[key]:\n\t\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\tif extract_spec_version:\n\t\t\tself.inventory_spec_version = self.get_spec_version(inventory)\n\t\t\tif not self.inventory_spec_version:\n\t\t\t\tself.warnings.append(\"No inventory type: unable to determine inventory specification version\")\n\t\t\t\tself.inventory_spec_version = self.spec_version\n\t\telse:\n\t\t\tself.inventory_spec_version = self.spec_version\n\t\tself.validate_type(inventory)\n\t\tself.validate_spec_version(inventory)\n\t\tself.validate_metadata(inventory)\n\t\tself.validate_hosts(inventory)\n\t\tself.validate_groups(inventory)\n\t\tself.validate_vars(inventory)\n\t\tself.validate_children(inventory)\n\t\treturn self.is_valid()\n\n\tdef validate_type(self, inventory):\n\t\t\"\"\"\n\t\tValidate the type value.\n\t\t\"\"\"\n\t\tif 'type' not in inventory:\n\t\t\tself.warnings.append(\"No\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif not self.spec_version:\n\t\traise Exception(\"No specification version to validate against.\")\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] == 'inventory':\n\t\t\t\tself.spec_version = '1.0'\n\t\t\telif inventory['type'] == 'inventory/1.0':\n\t\t\t\tself.spec_version = '1.0'\n\t\t\telif inventory['type'] == 'inventory/2.0':\n\t\t\t\tself.spec_version = '2.0'\n\t\t\telse:\n\t\t\t\traise Exception(\"Unsupported inventory type: %s\" % inventory['type'])\n\t\telse:\n\t\t\traise Exception(\"No 'type' value found in inventory.\")\n\tif self.spec_version == '1.0':\n\t\tself.validate_inventory_1_0(inventory)\n\telif self.spec_version == '2.0':\n\t\tself.validate_inventory_2_0(inventory)\n\telse:\n\t\traise Exception\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif not self.validate_type(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_format(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_source(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_version(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_groups(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_hosts(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_children(inventory) and extract_spec_version:\n\t\treturn False\n\tif not self.validate_vars(inventory) and extract_spec_version:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "ques_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n\tdigests_needed = set()\n\tfor f in manifest_files:\n\t\tfor d in f.digests:\n\t\t\tdigests_needed.add(d)\n\tdigests_needed.difference_update(digests_used)\n\tif digests_needed:\n\t\tdigests_needed = sorted(digests_needed)\n\t\tdigests_needed = \", \".join(digests_needed)\n\t\traise ManifestInvalidDigest(digests_needed)\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tif f.use_digest():\n\t\t\tif not f.digests_available():\n\t\t\t\traise DigestException(\n\t\t\t\t\t\"Digests unavailable for %s. \"\n\t\t\t\t\t\"Try running egencache --update.\" % f.cpv)\n\t\t\tif not f.digests_used():\n\t\t\t\traise DigestException(\n\t\t\t\t\t\"Digests unused for %s. \"\n\t\t\t\t\t\"Try running eclean-dist --force.\" % f.cpv)\n\n\tdef _parse_manifest_contents(self, contents):\n\t\tmf = Manifest(contents=contents)\n\t\tmf._allow_missing_files = self.opts.get('allow_missing_manifest')\n\t\tmf.parse()\n\t\treturn mf.fhash\n\n\tdef _read_manifest(self, pkg):\n\t\t\"\"\"\n\t\tRead and parse the Manifest file.\n\t\t\"\"\"\n\t\treturn self._parse_manifest_contents(pkg.\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tif \"DIGEST\" not in f:\n\t\t\tcontinue\n\t\tdigests = set(f[\"DIGEST\"].keys())\n\t\tif not digests.issubset(digests_used):\n\t\t\twrite_msg(\"Warning: DIGEST values not used in \" + \\\n\t\t\t\t\"portage.manifest: %s\\n\" % \\\n\t\t\t\t(\", \".join(digests.difference(digests_used))))\n\t\t\tcontinue\n\t\tif \"size\" not in f:\n\t\t\twrite_msg(\"Warning: DIGEST values present but \" + \\\n\t\t\t\t\"size is missing in portage.manifest: %s\\n\" % \\\n\t\t\t\t(\", \".join(digests)))\n\n\t\tif \"size\" in f and f[\"size\"] == 0:\n\t\t\tif \"MD5\" in f[\"DIGEST\"]:\n\t\t\t\tif f[\"DIGEST\"][\"MD5\"] != \"d41d8cd98f00b204e9800\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor mf in manifest_files.values():\n\t\tif not mf.digests:\n\t\t\tcontinue\n\t\tfor d in mf.digests:\n\t\t\tif d != \"size\":\n\t\t\t\tdigests_used.add(d)\n\t\t\t\tif self.digests[d] is None:\n\t\t\t\t\tself.errors.append(\n\t\t\t\t\t\t\"manifest requires digest '%s' \"\n\t\t\t\t\t\t\"which is not available\" % d)\n\n\tif self.eapi_attrs.unpack_depend:\n\t\tdigests_used.add(\"size\")\n\n\tif \"size\" in digests_used and self.digests[\"size\"] is None:\n\t\tself.errors.append(\"dependency may require 'size' \"\n\t\t\t\"which is not available\")\n\n\tdigests_used.discard(\"size\")\n\tdigests_used.discard(\"CONTENTS\")\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor manifest_file in manifest_files:\n\t\tif manifest_file.digests is None:\n\t\t\tcontinue\n\t\tfor digest in manifest_file.digests:\n\t\t\tif digest not in digests_used:\n\t\t\t\tself._eapi_support_check_fail(\n\t\t\t\t\t(\"EAPI '%s' requires digest '%s', \" +\n\t\t\t\t\t\"which is not present in Manifest\") %\n\t\t\t\t\t(self.eapi, digest))\n\n\t\t\tdigests_used.remove(digest)\n\n\tif digests_used:\n\t\tself._eapi_support_check_fail(\n\t\t\t(\"EAPI '%s' requires digests: '%s', \" +\n\t\t\t\"which are not present in Manifest\") %\n\t\t\t(self.eapi, \", \".join(digests_used)))\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor filename, entries in manifest_files.items():\n\t\tif len(entries) == 0:\n\t\t\tself._eerror(\"manifest file '%s' contains no entries\" % \\\n\t\t\t\tfilename)\n\t\t\tcontinue\n\n\t\tfor entry in entries:\n\t\t\tif entry.type_name != \"AUX\":\n\t\t\t\tcontinue\n\t\t\tfor digest in entry.all_digests:\n\t\t\t\tif digest not in digests_used:\n\t\t\t\t\tself._eerror(\"digest '%s' for '%s' not used\" % \\\n\t\t\t\t\t\t(digest, filename))\n\n\t\t\tif entry.type_name == \"AUX\":\n\t\t\t\tif not entry.size:\n\t\t\t\t\tself._eerror(\"AUX entry for '%s' has no 'size' attribute\" % \\\n\t\t\t\t\t\tfilename)\n\t\t\t\tif not entry.digest:\n\t\t\t\t\tself._eerror(\"AUX entry for '%s' has no 'digest' attribute\"\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tif not f.get('digest'):\n\t\t\traise errors.MismatchedDigest(\n\t\t\t\t\"Missing digest for file %s\" % f['filename'])\n\t\tif not digests_used.get(f['digest']):\n\t\t\traise errors.MismatchedDigest(\n\t\t\t\t\"Digest %s is not used\" % f['digest'])\n\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tdigests_used_set = set(digests_used)\n\tfor mf in manifest_files:\n\t\tdigests = mf.get_digests()\n\t\tdigests_set = set(digests)\n\t\tif self._default_digest not in digests_set:\n\t\t\t# If the default digest is not present, it is a fatal error.\n\t\t\traise ManifestInvalidDigest(\n\t\t\t\t\"File '%s' does not have the digest '%s'\" % \\\n\t\t\t\t(mf.get_filename(), self._default_digest))\n\t\tif self._default_digest in digests_used_set:\n\t\t\t# The default digest is present and used.  It is a fatal error\n\t\t\t# if another digest is also used.\n\t\t\tif digests_used_set != set([self._default_digest]):\n\t\t\t\traise ManifestInvalidDigest(\n\t\t\t\t\t\"File '%s' has multiple digests but only one can be used\" % \\\n\t\t\t\t\tmf.get\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tfor attr in (\"digest\", \"digest_orig\"):\n\t\t\tdigest = getattr(f, attr)\n\t\t\tif digest is not None and digest not in digests_used:\n\t\t\t\tdigest_desc = \"digest\"\n\t\t\t\tif attr == \"digest_orig\":\n\t\t\t\t\tdigest_desc = \"original digest\"\n\t\t\t\tself.output(\n\t\t\t\t\t\"%s: %s %s not used\" % (f.relative_path, digest_desc,\n\t\t\t\t\tdigest),\n\t\t\t\t\tlevel=logging.WARN)\n\t\t\t\tdigests_used[digest] = False\n\n\tfor digest in digests_used:\n\t\tif digests_used[digest]:\n\t\t\tcontinue\n\t\tself.output(\n\t\t\t\"%s: digest %s not in manifest files\" % (self.cpv, digest),\n\t\t\tlevel=logging.WARN)\n\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor (f, digests) in manifest_files:\n\t\tif not digests:\n\t\t\traise ManifestInvalidDigest(self.mycpv,\n\t\t\t\t\"Manifest file '%s' does not have a digest\" % f)\n\t\telif not f in digests_used:\n\t\t\traise ManifestInvalidDigest(self.mycpv,\n\t\t\t\t\"Manifest file '%s' has digest '%s' which is not used\" % \\\n\t\t\t\t\t(f, digests))\n\n\treturn True\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tdigests_needed = set()\n\tfor f in manifest_files:\n\t\tfor d in f.digests:\n\t\t\tdigests_needed.add(d)\n\tdigests_needed.difference_update(digests_used)\n\tif digests_needed:\n\t\tdigests_needed = sorted(digests_needed)\n\t\tdigests_needed = \", \".join(digests_needed)\n\t\traise ManifestInvalidDigest(digests_needed)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tif f.use_digest():\n\t\t\tif not f.digests_available():\n\t\t\t\traise DigestException(\n\t\t\t\t\t\"Digests unavailable for %s. \"\n\t\t\t\t\t\"Try running egencache --update.\" % f.cpv)\n\t\t\tif not f.digests_used():\n\t\t\t\traise DigestException(\n\t\t\t\t\t\"Digests unused for %s. \"\n\t\t\t\t\t\"Try running eclean-dist --force.\" % f.cpv)\n\n\tdef _parse_manifest_contents(self, contents):\n\t\tmf = Manifest(contents=contents)\n\t\tmf._allow_missing_files = self.opts.get('allow_missing_manifest')\n\t\tmf.parse()\n\t\treturn mf.fhash\n\n\tdef _read_manifest(self, pkg):\n\t\t\"\"\"\n\t\tRead and parse the Manifest file.\n\t\t\"\"\"\n\t\treturn self._parse_manifest_contents(pkg.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tif \"DIGEST\" not in f:\n\t\t\tcontinue\n\t\tdigests = set(f[\"DIGEST\"].keys())\n\t\tif not digests.issubset(digests_used):\n\t\t\twrite_msg(\"Warning: DIGEST values not used in \" + \\\n\t\t\t\t\"portage.manifest: %s\\n\" % \\\n\t\t\t\t(\", \".join(digests.difference(digests_used))))\n\t\t\tcontinue\n\t\tif \"size\" not in f:\n\t\t\twrite_msg(\"Warning: DIGEST values present but \" + \\\n\t\t\t\t\"size is missing in portage.manifest: %s\\n\" % \\\n\t\t\t\t(\", \".join(digests)))\n\n\t\tif \"size\" in f and f[\"size\"] == 0:\n\t\t\tif \"MD5\" in f[\"DIGEST\"]:\n\t\t\t\tif f[\"DIGEST\"][\"MD5\"] != \"d41d8cd98f00b204e9800\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor mf in manifest_files.values():\n\t\tif not mf.digests:\n\t\t\tcontinue\n\t\tfor d in mf.digests:\n\t\t\tif d != \"size\":\n\t\t\t\tdigests_used.add(d)\n\t\t\t\tif self.digests[d] is None:\n\t\t\t\t\tself.errors.append(\n\t\t\t\t\t\t\"manifest requires digest '%s' \"\n\t\t\t\t\t\t\"which is not available\" % d)\n\n\tif self.eapi_attrs.unpack_depend:\n\t\tdigests_used.add(\"size\")\n\n\tif \"size\" in digests_used and self.digests[\"size\"] is None:\n\t\tself.errors.append(\"dependency may require 'size' \"\n\t\t\t\"which is not available\")\n\n\tdigests_used.discard(\"size\")\n\tdigests_used.discard(\"CONTENTS\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor manifest_file in manifest_files:\n\t\tif manifest_file.digests is None:\n\t\t\tcontinue\n\t\tfor digest in manifest_file.digests:\n\t\t\tif digest not in digests_used:\n\t\t\t\tself._eapi_support_check_fail(\n\t\t\t\t\t(\"EAPI '%s' requires digest '%s', \" +\n\t\t\t\t\t\"which is not present in Manifest\") %\n\t\t\t\t\t(self.eapi, digest))\n\n\t\t\tdigests_used.remove(digest)\n\n\tif digests_used:\n\t\tself._eapi_support_check_fail(\n\t\t\t(\"EAPI '%s' requires digests: '%s', \" +\n\t\t\t\"which are not present in Manifest\") %\n\t\t\t(self.eapi, \", \".join(digests_used)))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor filename, entries in manifest_files.items():\n\t\tif len(entries) == 0:\n\t\t\tself._eerror(\"manifest file '%s' contains no entries\" % \\\n\t\t\t\tfilename)\n\t\t\tcontinue\n\n\t\tfor entry in entries:\n\t\t\tif entry.type_name != \"AUX\":\n\t\t\t\tcontinue\n\t\t\tfor digest in entry.all_digests:\n\t\t\t\tif digest not in digests_used:\n\t\t\t\t\tself._eerror(\"digest '%s' for '%s' not used\" % \\\n\t\t\t\t\t\t(digest, filename))\n\n\t\t\tif entry.type_name == \"AUX\":\n\t\t\t\tif not entry.size:\n\t\t\t\t\tself._eerror(\"AUX entry for '%s' has no 'size' attribute\" % \\\n\t\t\t\t\t\tfilename)\n\t\t\t\tif not entry.digest:\n\t\t\t\t\tself._eerror(\"AUX entry for '%s' has no 'digest' attribute\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tif not f.get('digest'):\n\t\t\traise errors.MismatchedDigest(\n\t\t\t\t\"Missing digest for file %s\" % f['filename'])\n\t\tif not digests_used.get(f['digest']):\n\t\t\traise errors.MismatchedDigest(\n\t\t\t\t\"Digest %s is not used\" % f['digest'])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tdigests_used_set = set(digests_used)\n\tfor mf in manifest_files:\n\t\tdigests = mf.get_digests()\n\t\tdigests_set = set(digests)\n\t\tif self._default_digest not in digests_set:\n\t\t\t# If the default digest is not present, it is a fatal error.\n\t\t\traise ManifestInvalidDigest(\n\t\t\t\t\"File '%s' does not have the digest '%s'\" % \\\n\t\t\t\t(mf.get_filename(), self._default_digest))\n\t\tif self._default_digest in digests_used_set:\n\t\t\t# The default digest is present and used.  It is a fatal error\n\t\t\t# if another digest is also used.\n\t\t\tif digests_used_set != set([self._default_digest]):\n\t\t\t\traise ManifestInvalidDigest(\n\t\t\t\t\t\"File '%s' has multiple digests but only one can be used\" % \\\n\t\t\t\t\tmf.get\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor f in manifest_files:\n\t\tfor attr in (\"digest\", \"digest_orig\"):\n\t\t\tdigest = getattr(f, attr)\n\t\t\tif digest is not None and digest not in digests_used:\n\t\t\t\tdigest_desc = \"digest\"\n\t\t\t\tif attr == \"digest_orig\":\n\t\t\t\t\tdigest_desc = \"original digest\"\n\t\t\t\tself.output(\n\t\t\t\t\t\"%s: %s %s not used\" % (f.relative_path, digest_desc,\n\t\t\t\t\tdigest),\n\t\t\t\t\tlevel=logging.WARN)\n\t\t\t\tdigests_used[digest] = False\n\n\tfor digest in digests_used:\n\t\tif digests_used[digest]:\n\t\t\tcontinue\n\t\tself.output(\n\t\t\t\"%s: digest %s not in manifest files\" % (self.cpv, digest),\n\t\t\tlevel=logging.WARN)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor (f, digests) in manifest_files:\n\t\tif not digests:\n\t\t\traise ManifestInvalidDigest(self.mycpv,\n\t\t\t\t\"Manifest file '%s' does not have a digest\" % f)\n\t\telif not f in digests_used:\n\t\t\traise ManifestInvalidDigest(self.mycpv,\n\t\t\t\t\"Manifest file '%s' has digest '%s' which is not used\" % \\\n\t\t\t\t\t(f, digests))\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "ques_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n\tif self.version < prior.version:\n\t\traise ValueError(\"prior version must not be greater than current version\")\n\tif self.version == prior.version:\n\t\traise ValueError(\"prior version must not be equal to current version\")\n\tif self.version_date < prior.version_date:\n\t\traise ValueError(\"prior version must not have a later version date\")\n\tif self.version_date == prior.version_date:\n\t\traise ValueError(\"prior version must not have the same version date\")\n\tif self.n_nodes != prior.n_nodes:\n\t\traise ValueError(\"prior version must have the same number of nodes\")\n\tif self.n_arcs != prior.n_arcs:\n\t\traise ValueError(\"prior version must have the same number of arcs\")\n\tif self.n_layers != prior.n_layers:\n\t\traise ValueError(\"prior version must have the same number of layers\")\n\tif self.n_times != prior.n_times:\n\t\traise ValueError(\"prior version must have the same\n", "def validate_as_prior_version(self, prior):\n\tif not isinstance(prior, InventoryValidator):\n\t\traise TypeError(_(\"'prior' must be a valid InventoryValidator object\"))\n\t# Compare the number of items in the two inventories.\n\tif len(self.items) > len(prior.items):\n\t\traise InventoryError(_(\"The current inventory has more items than the prior inventory\"))\n\t# Check that all items in the prior inventory are in the current inventory.\n\tfor item in prior.items:\n\t\tif not item.in_inventory(self):\n\t\t\traise InventoryError(_(\"Item from the prior inventory '{}' is not in the current inventory\").format(item.name))\n\t# Now check that the current inventory items are consistent with the prior inventory.\n\tfor item in self.items:\n\t\tif not item.is_consistent_with(prior):\n\t\t\traise InventoryError(_(\"Item '{}' is not consistent with the prior inventory\").format(item.name))\n\treturn True\n\n\n", "def validate_as_prior_version(self, prior):\n\t# Initialize the list of\n\t# errors and warnings found.\n\terrors = list()\n\twarnings = list()\n\t# Check that the current\n\t# inventory is a valid\n\t# extension of prior.\n\tif self.year < prior.year:\n\t\terrors.append(\"year is not greater \"\n\t\t\t\t\"than prior year\")\n\tif self.year == prior.year:\n\t\tif self.quarter < prior.quarter:\n\t\t\terrors.append(\"quarter is not \"\n\t\t\t\t\t\"greater than \"\n\t\t\t\t\t\"prior quarter\")\n\t\tif self.quarter == prior.quarter:\n\t\t\terrors.append(\"year and quarter \"\n\t\t\t\t\t\"are equal to \"\n\t\t\t\t\t\"prior year and \"\n\t\t\t\t\t\"quarter\")\n\t# Check that the current\n\t# inventory has a valid\n\t# prior version.\n\tif self.prior_version != prior.version:\n\t\terrors.append(\"prior version is \"\n\t\t\t\t\"not equal to \"\n\t\t\t\t\"\n", "def validate_as_prior_version(self, prior):\n\tif not isinstance(prior, InventoryValidator):\n\t\traise ValueError(\"prior must be an InventoryValidator object\")\n\tif not isinstance(prior, self.__class__):\n\t\traise ValueError(\"prior must be an InventoryValidator object of the same class as the current object\")\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior\n", "def validate_as_prior_version(self, prior):\n\tif prior.validate_as_prior_version(self):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\t# end of InventoryValidator.validate_as_prior_version\n\t# -----------------------------------------------------\n\n\n\t# -----------------------------------------------------\n\t# validate_as_prior_version_of_another\n\t# -----------------------------------------------------\n\tdef validate_as_prior_version_of_another(self, later):\n\t\"\"\"\n\tCheck that self is a valid earlier version of later.\n\n", "def validate_as_prior_version(self, prior):\n\tfor field in self.required_fields:\n\t\tself_value = getattr(self, field)\n\t\tprior_value = getattr(prior, field)\n\t\tif self_value != prior_value:\n\t\t\traise ValueError(\"Incompatible values for field '%s': '%s' vs '%s'\" % (field, self_value, prior_value))\n\n\tfor field in self.optional_fields:\n\t\tself_value = getattr(self, field)\n\t\tprior_value = getattr(prior, field)\n\t\tif self_value != prior_value:\n\t\t\tif field == 'size':\n\t\t\t\tself.validate_size_as_prior_version(prior_value)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Incompatible values for field '%s': '%s' vs '%s'\" % (field, self_value, prior_value))\n\n\tfor field in self.required_list_fields:\n\t\tself_list = getattr(self, field)\n\t\tprior_list =\n", "def validate_as_prior_version(self, prior):\n\tprior.assert_is_valid()\n\t# Check that the prior inventory has a different id.\n\tif self.inventory_validator.id == prior.inventory_validator.id:\n\t\traise AssertionError(\"prior inventory has the same id\")\n\t# Check that the prior inventory has the same number of components.\n\tif len(self.inventory_validator.component_validators) != len(prior.inventory_validator.component_validators):\n\t\traise AssertionError(\"prior inventory has a different number of components\")\n\t# Check that each component in the prior inventory is a valid prior version of the corresponding component in the current inventory.\n\tfor c in self.inventory_validator.component_validators:\n\t\tprior_component = prior.inventory_validator.get_component_validator(c.id)\n\t\tc.validate_as_prior_version(prior_component)\n\t# Check that each component in the prior inventory has a different id.\n\tprior_component_ids = prior.inventory_validator.get_component_ids()\n\tfor\n", "def validate_as_prior_version(self, prior):\n\t#\n\t# Make sure that prior is a valid prior version of self.\n\t#\n\tif not self.is_prior_version(prior):\n\t\traise ValueError(\"prior is not a valid prior version of self\")\n\n", "def validate_as_prior_version(self, prior):\n\t#\n\t# Check that the prior inventory object is indeed prior to the current\n\t# inventory object.\n\t#\n\tif prior.get_last_update_time().get_time_object() > self._last_update_time.get_time_object():\n\t\traise Exception(\"The prior inventory object is not prior to the current inventory object.\")\n\t#\n\t# Check that the prior inventory object contains all the sites, sources,\n\t# and receivers in the current inventory object.\n\t#\n\tfor site_id in self._sites_by_id.keys():\n\t\tif not (prior.get_site(site_id)):\n\t\t\traise Exception(\"The prior inventory object does not contain a site with id %d.\" % site_id)\n\tfor source_id in self._sources_by_id.keys():\n\t\tif not (prior.get_source(source_id)):\n\t\t\traise Exception(\"The prior inventory object does not contain a source with id %d.\" % source_id)\n\tfor receiver_id in self._receivers_by_id.keys():\n\t\t\n", "def validate_as_prior_version(self, prior):\n\t#\tCheck that the current inventory is a valid prior version of the\n\t#\tprior inventory.\n\t#\t\t- Check that all the fields in the current inventory exists in\n\t#\t\t  the prior inventory.\n\t#\t\t- Check that all the fields in the prior inventory are present\n\t#\t\t  in the current inventory.\n\t#\t\t- Check that all the fields in the current inventory are\n\t#\t\t  identical to the corresponding fields in the prior inventory,\n\t#\t\t  except for the fields containing information about the\n\t#\t\t  inventory itself.\n\t#\t\t- Check that the inventory file version number is one less than\n\t#\t\t  the prior inventory file version number.\n\t#\t\t- Check that the inventory file version number is at least 1.\n\t#\t\t  (The inventory version number is initially set to 0 and\n\t#\t\t  incremented each time the inventory is updated.)\n\t#\t\t- Check that the revision number of the current inventory is\n\t#\t\t  one less than the revision number of the prior inventory.\n\t#\t\t- Check that the revision number\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n\tif self.version < prior.version:\n\t\traise ValueError(\"prior version must not be greater than current version\")\n\tif self.version == prior.version:\n\t\traise ValueError(\"prior version must not be equal to current version\")\n\tif self.version_date < prior.version_date:\n\t\traise ValueError(\"prior version must not have a later version date\")\n\tif self.version_date == prior.version_date:\n\t\traise ValueError(\"prior version must not have the same version date\")\n\tif self.n_nodes != prior.n_nodes:\n\t\traise ValueError(\"prior version must have the same number of nodes\")\n\tif self.n_arcs != prior.n_arcs:\n\t\traise ValueError(\"prior version must have the same number of arcs\")\n\tif self.n_layers != prior.n_layers:\n\t\traise ValueError(\"prior version must have the same number of layers\")\n\tif self.n_times != prior.n_times:\n\t\traise ValueError(\"prior version must have the same\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tif not isinstance(prior, InventoryValidator):\n\t\traise TypeError(_(\"'prior' must be a valid InventoryValidator object\"))\n\t# Compare the number of items in the two inventories.\n\tif len(self.items) > len(prior.items):\n\t\traise InventoryError(_(\"The current inventory has more items than the prior inventory\"))\n\t# Check that all items in the prior inventory are in the current inventory.\n\tfor item in prior.items:\n\t\tif not item.in_inventory(self):\n\t\t\traise InventoryError(_(\"Item from the prior inventory '{}' is not in the current inventory\").format(item.name))\n\t# Now check that the current inventory items are consistent with the prior inventory.\n\tfor item in self.items:\n\t\tif not item.is_consistent_with(prior):\n\t\t\traise InventoryError(_(\"Item '{}' is not consistent with the prior inventory\").format(item.name))\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t# Initialize the list of\n\t# errors and warnings found.\n\terrors = list()\n\twarnings = list()\n\t# Check that the current\n\t# inventory is a valid\n\t# extension of prior.\n\tif self.year < prior.year:\n\t\terrors.append(\"year is not greater \"\n\t\t\t\t\"than prior year\")\n\tif self.year == prior.year:\n\t\tif self.quarter < prior.quarter:\n\t\t\terrors.append(\"quarter is not \"\n\t\t\t\t\t\"greater than \"\n\t\t\t\t\t\"prior quarter\")\n\t\tif self.quarter == prior.quarter:\n\t\t\terrors.append(\"year and quarter \"\n\t\t\t\t\t\"are equal to \"\n\t\t\t\t\t\"prior year and \"\n\t\t\t\t\t\"quarter\")\n\t# Check that the current\n\t# inventory has a valid\n\t# prior version.\n\tif self.prior_version != prior.version:\n\t\terrors.append(\"prior version is \"\n\t\t\t\t\"not equal to \"\n\t\t\t\t\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tif not isinstance(prior, InventoryValidator):\n\t\traise ValueError(\"prior must be an InventoryValidator object\")\n\tif not isinstance(prior, self.__class__):\n\t\traise ValueError(\"prior must be an InventoryValidator object of the same class as the current object\")\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior does not have any new types\n\t# Check that prior does not have any new variables\n\t# Check that prior does not have any new stations\n\t# Check that prior\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tif prior.validate_as_prior_version(self):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\t# end of InventoryValidator.validate_as_prior_version\n\t# -----------------------------------------------------\n\n\n\t# -----------------------------------------------------\n\t# validate_as_prior_version_of_another\n\t# -----------------------------------------------------\n\tdef validate_as_prior_version_of_another(self, later):\n\t\"\"\"\n\tCheck that self is a valid earlier version of later.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tfor field in self.required_fields:\n\t\tself_value = getattr(self, field)\n\t\tprior_value = getattr(prior, field)\n\t\tif self_value != prior_value:\n\t\t\traise ValueError(\"Incompatible values for field '%s': '%s' vs '%s'\" % (field, self_value, prior_value))\n\n\tfor field in self.optional_fields:\n\t\tself_value = getattr(self, field)\n\t\tprior_value = getattr(prior, field)\n\t\tif self_value != prior_value:\n\t\t\tif field == 'size':\n\t\t\t\tself.validate_size_as_prior_version(prior_value)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Incompatible values for field '%s': '%s' vs '%s'\" % (field, self_value, prior_value))\n\n\tfor field in self.required_list_fields:\n\t\tself_list = getattr(self, field)\n\t\tprior_list =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tprior.assert_is_valid()\n\t# Check that the prior inventory has a different id.\n\tif self.inventory_validator.id == prior.inventory_validator.id:\n\t\traise AssertionError(\"prior inventory has the same id\")\n\t# Check that the prior inventory has the same number of components.\n\tif len(self.inventory_validator.component_validators) != len(prior.inventory_validator.component_validators):\n\t\traise AssertionError(\"prior inventory has a different number of components\")\n\t# Check that each component in the prior inventory is a valid prior version of the corresponding component in the current inventory.\n\tfor c in self.inventory_validator.component_validators:\n\t\tprior_component = prior.inventory_validator.get_component_validator(c.id)\n\t\tc.validate_as_prior_version(prior_component)\n\t# Check that each component in the prior inventory has a different id.\n\tprior_component_ids = prior.inventory_validator.get_component_ids()\n\tfor\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t#\n\t# Make sure that prior is a valid prior version of self.\n\t#\n\tif not self.is_prior_version(prior):\n\t\traise ValueError(\"prior is not a valid prior version of self\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t#\n\t# Check that the prior inventory object is indeed prior to the current\n\t# inventory object.\n\t#\n\tif prior.get_last_update_time().get_time_object() > self._last_update_time.get_time_object():\n\t\traise Exception(\"The prior inventory object is not prior to the current inventory object.\")\n\t#\n\t# Check that the prior inventory object contains all the sites, sources,\n\t# and receivers in the current inventory object.\n\t#\n\tfor site_id in self._sites_by_id.keys():\n\t\tif not (prior.get_site(site_id)):\n\t\t\traise Exception(\"The prior inventory object does not contain a site with id %d.\" % site_id)\n\tfor source_id in self._sources_by_id.keys():\n\t\tif not (prior.get_source(source_id)):\n\t\t\traise Exception(\"The prior inventory object does not contain a source with id %d.\" % source_id)\n\tfor receiver_id in self._receivers_by_id.keys():\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t#\tCheck that the current inventory is a valid prior version of the\n\t#\tprior inventory.\n\t#\t\t- Check that all the fields in the current inventory exists in\n\t#\t\t  the prior inventory.\n\t#\t\t- Check that all the fields in the prior inventory are present\n\t#\t\t  in the current inventory.\n\t#\t\t- Check that all the fields in the current inventory are\n\t#\t\t  identical to the corresponding fields in the prior inventory,\n\t#\t\t  except for the fields containing information about the\n\t#\t\t  inventory itself.\n\t#\t\t- Check that the inventory file version number is one less than\n\t#\t\t  the prior inventory file version number.\n\t#\t\t- Check that the inventory file version number is at least 1.\n\t#\t\t  (The inventory version number is initially set to 0 and\n\t#\t\t  incremented each time the inventory is updated.)\n\t#\t\t- Check that the revision number of the current inventory is\n\t#\t\t  one less than the revision number of the prior inventory.\n\t#\t\t- Check that the revision number\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "ques_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor content_file in inventory.get_all_content_files(version):\n\t\tfor logical_path in content_file.logical_paths:\n\t\t\tif logical_path not in logical_path_map:\n\t\t\t\tlogical_path_map[logical_path] = set()\n\t\t\tlogical_path_map[logical_path].add(content_file)\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\tpath_to_files = {}\n\tfor (logical_path, content_file) in inventory.get_version_content_files(version):\n\t\tpath_to_files.setdefault(logical_path, set()).add(content_file)\n\treturn path_to_files\n\n\n", "def get_logical_path_map(inventory, version):\n\tif inventory.version_has_logical_path_map(version):\n\t\treturn inventory.get_logical_path_map(version)\n\telse:\n\t\treturn _get_logical_path_map(inventory, version)\n\n\n", "def get_logical_path_map(inventory, version):\n\tresult = {}\n\tfor item in inventory.findall('item'):\n\t\tversion_specific_path = item.get('version_specific_path', None)\n\t\tif version_specific_path is None:\n\t\t\tlogging.warn('Inventory item %s has no version_specific_path' % item)\n\t\t\tcontinue\n\t\tfor version_info in item.findall('version_info'):\n\t\t\tif version_info.get('version') == version:\n\t\t\t\tlogical_path = version_info.get('logical_path', None)\n\t\t\t\tif logical_path is None:\n\t\t\t\t\tlogging.warn('Inventory item %s has no logical_path' % item)\n\t\t\t\t\tcontinue\n\t\t\t\tif logical_path in result:\n\t\t\t\t\tlogging.warn('logical_path %s appears twice in inventory for version %s' % (logical_path, version))\n\t\t\t\tresult[logical_path] = set(version_specific_path.split(';'))\n\treturn\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = collections.defaultdict(set)\n\tfor content_file in inventory.content_files:\n\t\tif content_file.version == version:\n\t\t\tlogical_path_map[content_file.logical_path_in_state].add(content_file)\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor i in inventory.get_files():\n\t\tcontent_files = set()\n\t\tfor f in i.find(version):\n\t\t\tif f.is_executable:\n\t\t\t\tfile_id = f.file_id.encode('utf-8')\n\t\t\t\tcontent_files.add(file_id)\n\t\t\telse:\n\t\t\t\tpath = f.path_utf8\n\t\t\t\tcontent_files.add(path)\n\t\tlogical_path_map[i.utf8] = content_files\n\treturn logical_path_map\n\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = collections.defaultdict(set)\n\tfor path_in_state, content_files in inventory.path_map.items():\n\t\tif content_files:\n\t\t\tlogical_path_map[path_in_state] = set(content_files)\n\t\telse:\n\t\t\tlogical_path_map[path_in_state] = None\n\tfor version in version.get_parent_versions():\n\t\tfor path_in_state, content_files in version.inventory.path_map.items():\n\t\t\tif content_files:\n\t\t\t\tlogical_path_map[path_in_state].update(content_files)\n\t\t\telse:\n\t\t\t\tlogical_path_map[path_in_state] = None\n\treturn dict(logical_path_map)\n\n", "def get_logical_path_map(inventory, version):\n\tresult = {}\n\tfor item in inventory.findall('content'):\n\t\tfile = item.attrib['file']\n\t\tif 'version' in item.attrib:\n\t\t\tif item.attrib['version'] != version:\n\t\t\t\tcontinue\n\t\tlogical_path_in_state = item.attrib['path'].strip('/')\n\t\tif logical_path_in_state not in result:\n\t\t\tresult[logical_path_in_state] = set()\n\t\tresult[logical_path_in_state].add(file)\n\treturn result\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = dict()\n\tfor lp in inventory.logical_paths:\n\t\tif lp.version == version:\n\t\t\tif lp.logical_path not in logical_path_map:\n\t\t\t\tlogical_path_map[lp.logical_path] = set()\n\t\t\tfor cf in lp.content_files:\n\t\t\t\tlogical_path_map[lp.logical_path].add(cf)\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = collections.defaultdict(set)\n\tfor logical_path, file_identifier in inventory.get_files(version).iteritems():\n\t\tphysical_path = inventory.get_file_physical_path(file_identifier)\n\t\tlogical_path_map[logical_path].add(physical_path)\n\treturn logical_path_map\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor content_file in inventory.get_all_content_files(version):\n\t\tfor logical_path in content_file.logical_paths:\n\t\t\tif logical_path not in logical_path_map:\n\t\t\t\tlogical_path_map[logical_path] = set()\n\t\t\tlogical_path_map[logical_path].add(content_file)\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tpath_to_files = {}\n\tfor (logical_path, content_file) in inventory.get_version_content_files(version):\n\t\tpath_to_files.setdefault(logical_path, set()).add(content_file)\n\treturn path_to_files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tif inventory.version_has_logical_path_map(version):\n\t\treturn inventory.get_logical_path_map(version)\n\telse:\n\t\treturn _get_logical_path_map(inventory, version)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tresult = {}\n\tfor item in inventory.findall('item'):\n\t\tversion_specific_path = item.get('version_specific_path', None)\n\t\tif version_specific_path is None:\n\t\t\tlogging.warn('Inventory item %s has no version_specific_path' % item)\n\t\t\tcontinue\n\t\tfor version_info in item.findall('version_info'):\n\t\t\tif version_info.get('version') == version:\n\t\t\t\tlogical_path = version_info.get('logical_path', None)\n\t\t\t\tif logical_path is None:\n\t\t\t\t\tlogging.warn('Inventory item %s has no logical_path' % item)\n\t\t\t\t\tcontinue\n\t\t\t\tif logical_path in result:\n\t\t\t\t\tlogging.warn('logical_path %s appears twice in inventory for version %s' % (logical_path, version))\n\t\t\t\tresult[logical_path] = set(version_specific_path.split(';'))\n\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = collections.defaultdict(set)\n\tfor content_file in inventory.content_files:\n\t\tif content_file.version == version:\n\t\t\tlogical_path_map[content_file.logical_path_in_state].add(content_file)\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor i in inventory.get_files():\n\t\tcontent_files = set()\n\t\tfor f in i.find(version):\n\t\t\tif f.is_executable:\n\t\t\t\tfile_id = f.file_id.encode('utf-8')\n\t\t\t\tcontent_files.add(file_id)\n\t\t\telse:\n\t\t\t\tpath = f.path_utf8\n\t\t\t\tcontent_files.add(path)\n\t\tlogical_path_map[i.utf8] = content_files\n\treturn logical_path_map\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = collections.defaultdict(set)\n\tfor path_in_state, content_files in inventory.path_map.items():\n\t\tif content_files:\n\t\t\tlogical_path_map[path_in_state] = set(content_files)\n\t\telse:\n\t\t\tlogical_path_map[path_in_state] = None\n\tfor version in version.get_parent_versions():\n\t\tfor path_in_state, content_files in version.inventory.path_map.items():\n\t\t\tif content_files:\n\t\t\t\tlogical_path_map[path_in_state].update(content_files)\n\t\t\telse:\n\t\t\t\tlogical_path_map[path_in_state] = None\n\treturn dict(logical_path_map)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tresult = {}\n\tfor item in inventory.findall('content'):\n\t\tfile = item.attrib['file']\n\t\tif 'version' in item.attrib:\n\t\t\tif item.attrib['version'] != version:\n\t\t\t\tcontinue\n\t\tlogical_path_in_state = item.attrib['path'].strip('/')\n\t\tif logical_path_in_state not in result:\n\t\t\tresult[logical_path_in_state] = set()\n\t\tresult[logical_path_in_state].add(file)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = dict()\n\tfor lp in inventory.logical_paths:\n\t\tif lp.version == version:\n\t\t\tif lp.logical_path not in logical_path_map:\n\t\t\t\tlogical_path_map[lp.logical_path] = set()\n\t\t\tfor cf in lp.content_files:\n\t\t\t\tlogical_path_map[lp.logical_path].add(cf)\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = collections.defaultdict(set)\n\tfor logical_path, file_identifier in inventory.get_files(version).iteritems():\n\t\tphysical_path = inventory.get_file_physical_path(file_identifier)\n\t\tlogical_path_map[logical_path].add(physical_path)\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "ques_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n\tfor f in fixity:\n\t\tif not 'name' in f:\n\t\t\traise ValueError(\n\t\t\t\t\"Missing 'name' field in fixity entry: %s\" % str(f)\n\t\t\t)\n\t\tif not 'digests' in f:\n\t\t\traise ValueError(\n\t\t\t\t\"Missing 'digests' field in fixity entry: %s\" % str(f)\n\t\t\t)\n\t\tif not 'size' in f:\n\t\t\traise ValueError(\n\t\t\t\t\"Missing 'size' field in fixity entry: %s\" % str(f)\n\t\t\t)\n\t\tif f['name'] not in manifest_files:\n\t\t\traise ValueError(\n\t\t\t\t\"Fixity entry for file not in manifest: %s\" % f['name']\n\t\t\t)\n\t\tfor d in f['digests']:\n\t\t\tif not 'hash' in d:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Missing 'hash' field in fixity.digests entry: %\n", "def validate_fixity(self, fixity, manifest_files):\n\t# Check for required fields\n\tif not isinstance(fixity, dict):\n\t\traise ValueError('Fixity block is not a dict')\n\tfor field in ('type', 'hash', 'algorithm'):\n\t\tif field not in fixity:\n\t\t\traise ValueError('Fixity block is missing \"{}\" field'.format(field))\n\n\t# Check type\n\tif fixity['type'] != 'fixity':\n\t\traise ValueError('Fixity block has unexpected type \"{}\"'.format(fixity['type']))\n\n\t# Check algorithm\n\tif fixity['algorithm'] != 'sha256':\n\t\traise ValueError('Fixity block has unexpected algorithm \"{}\"'.format(fixity['algorithm']))\n\n\t# Check for hash collisions\n\thashes = set()\n\tfor hash_ in fixity['hash']:\n\t\tif hash_['file'] in hashes:\n\t\t\traise ValueError('Fixity block has duplicate hash for file \"{}\"'.format(hash_['file']))\n\t\telse:\n\t\t\thashes.add(hash_['file'])\n\n\t# Check for missing\n", "def validate_fixity(self, fixity, manifest_files):\n\tif not isinstance(fixity, dict):\n\t\traise ValueError(\"Fixity must be a dictionary\")\n\n\tfor k, v in fixity.items():\n\t\tif k not in manifest_files:\n\t\t\traise ValueError(\"File {} not in manifest\".format(k))\n\n\t\tif 'sha1' not in v:\n\t\t\traise ValueError(\"No sha1 value for {}\".format(k))\n\n\t\tif 'size' not in v:\n\t\t\traise ValueError(\"No size value for {}\".format(k))\n\n\t\tif 'mtime' not in v:\n\t\t\traise ValueError(\"No mtime value for {}\".format(k))\n\n\t\tif 'atime' not in v:\n\t\t\traise ValueError(\"No atime value for {}\".format(k))\n\n\t\tif 'mode' not in v:\n\t\t\traise ValueError(\"No mode value for {}\".format(k))\n\n\t\tif 'type' not in v:\n\t\t\traise ValueError(\"No type value for {}\".format(k))\n\n\t\tif 'uid' not in v\n", "def validate_fixity(self, fixity, manifest_files):\n\tif 'checksum' not in fixity:\n\t\traise ValidationError('FIXITY block must contain a CHECKSUM field.',\n\t\t\t\t\t\t\t  'FIXITY')\n\tif 'date' not in fixity:\n\t\traise ValidationError('FIXITY block must contain a DATE field.',\n\t\t\t\t\t\t\t  'FIXITY')\n\t# check for valid date\n\ttry:\n\t\tfixity['date'] = datetime.datetime.strptime(fixity['date'],\n\t\t\t\t\t\t\t\t\t\t\t\t   '%Y-%m-%d')\n\texcept ValueError:\n\t\traise ValidationError('FIXITY block must contain a valid DATE in '\n\t\t\t\t\t\t\t  'YYYY-MM-DD format.', 'FIXITY')\n\tif 'files' not in fixity:\n\t\traise ValidationError('FIXITY block must contain a FILES field.',\n\t\t\t\t\t\t\t  'FIXITY')\n\tif type(fixity['files']) != list:\n\t\traise\n", "def validate_fixity(self, fixity, manifest_files):\n\t# Check that the fixity block is a list\n\tif type(fixity) != list:\n\t\tprint(\"Fixity block is not a list\")\n\t\treturn False\n\n\t# Check that the fixity block is not empty\n\tif not fixity:\n\t\tprint(\"Fixity block is empty\")\n\t\treturn False\n\n\t# Check that every element of the fixity block is a dictionary\n\tfor fixity_item in fixity:\n\t\tif type(fixity_item) != dict:\n\t\t\tprint(\"Fixity element is not a dictionary\")\n\t\t\treturn False\n\n\t# Check that every element of the fixity block has the required keys\n\tfor fixity_item in fixity:\n\t\tfor key in [\"filename\", \"hash\"]:\n\t\t\tif not key in fixity_item:\n\t\t\t\tprint(\"Fixity element does not have required key: \" + key)\n\t\t\t\treturn False\n\n\t# Check that every element of the fixity block has a filename that is present in the manifest\n\tfor fixity_item in fixity:\n\t\tif not fixity_item[\"\n", "def validate_fixity(self, fixity, manifest_files):\n\terrors = []\n\tif not isinstance(fixity, dict):\n\t\terrors.append(\"fixity: not a dictionary\")\n\t\treturn errors\n\tfor filename, checksum_list in fixity.items():\n\t\tif not isinstance(checksum_list, list):\n\t\t\terrors.append(\"fixity: {}: not a list\".format(filename))\n\t\t\tcontinue\n\t\tfor checksum in checksum_list:\n\t\t\tif not isinstance(checksum, dict):\n\t\t\t\terrors.append(\"fixity: {}: {}: not a dictionary\".format(filename, checksum))\n\t\t\t\tcontinue\n\t\t\tif not checksum.get(\"type\") or not checksum.get(\"value\"):\n\t\t\t\terrors.append(\"fixity: {}: {}: missing type or value\".format(filename, checksum))\n\t\t\t\tcontinue\n\t\t\tif checksum[\"type\"] not in (\"md5\", \"sha1\", \"sha256\", \"sha512\"):\n\t\t\t\terrors.append(\"fixity: {}: {}: unknown type\".format(\n", "def validate_fixity(self, fixity, manifest_files):\n\tfor file in fixity:\n\t\tif file not in manifest_files:\n\t\t\traise InventoryError('File \"{}\" in fixity block does not exist in manifest'.format(file))\n\t\tif not isinstance(fixity[file], dict):\n\t\t\traise InventoryError('Fixity for file \"{}\" is not a dictionary'.format(file))\n\t\tif 'md5' not in fixity[file]:\n\t\t\traise InventoryError('Fixity for file \"{}\" does not specify an md5'.format(file))\n\t\tif not isinstance(fixity[file]['md5'], str):\n\t\t\traise InventoryError('Fixity for file \"{}\" specifies an md5 that is not a string'.format(file))\n\n\tif len(fixity) != len(manifest_files):\n\t\traise InventoryError('Fixity block does not contain fixity for all files in manifest')\n\n\treturn fixity\n\n\t\n\tdef validate_inventory(self, inventory_dict):\n\t\"\"\"\n\tValidate the structure of an inventory.\n\n", "def validate_fixity(self, fixity, manifest_files):\n\tmanifest_files_set = set(manifest_files)\n\n\t# check fixity structure\n\tvalidate_fixity_structure(fixity)\n\n\t# check that all fixity files are present in the manifest\n\tfixity_files_set = set(fixity.keys())\n\tmissing_files = fixity_files_set - manifest_files_set\n\tif missing_files:\n\t\traise ValueError('Files referenced in fixity but missing from manifest: {}'.format(missing_files))\n\n\t# check that all manifest files have fixity\n\tmissing_files = manifest_files_set - fixity_files_set\n\tif missing_files:\n\t\traise ValueError('Files listed in manifest but missing from fixity: {}'.format(missing_files))\n\n\treturn True\n\n", "def validate_fixity(self, fixity, manifest_files):\n\t# Validate each file's fixity\n\tfor file_id in fixity:\n\t\tif file_id not in manifest_files:\n\t\t\traise InventoryError(\"Fixity for unknown file '{0}'\".format(file_id))\n\t\tfile = manifest_files[file_id]\n\t\tif 'size' not in file:\n\t\t\traise InventoryError(\"File '{0}' is missing size\".format(file_id))\n\t\tif 'fixity' not in file:\n\t\t\traise InventoryError(\"File '{0}' is missing fixity\".format(file_id))\n\t\tif file['fixity'] != fixity[file_id]:\n\t\t\traise InventoryError(\"Fixity for file '{0}' does not match manifest\".format(file_id))\n\n\t# Make sure all files have a fixity\n\tfor file_id in manifest_files:\n\t\tif file_id not in fixity:\n\t\t\traise InventoryError(\"File '{0}' is missing fixity\".format(file_id))\n\n\n", "def validate_fixity(self, fixity, manifest_files):\n\tif \"files\" not in fixity:\n\t\traise errors.InvalidInventory(\"missing fixity files\")\n\n\tfor f in fixity[\"files\"]:\n\t\tif f[\"path\"] not in manifest_files:\n\t\t\traise errors.InvalidInventory(\n\t\t\t    \"fixity file not in manifest: {}\".format(f[\"path\"]))\n\n\t\tif \"sums\" not in f:\n\t\t\traise errors.InvalidInventory(\"missing fixity sums\")\n\t\tif \"size\" not in f:\n\t\t\traise errors.InvalidInventory(\"missing fixity size\")\n\n\t\tif f[\"size\"] != manifest_files[f[\"path\"]][\"size\"]:\n\t\t\traise errors.InvalidInventory(\"fixity size mismatch\")\n\n\t\tif \"md5\" not in f[\"sums\"]:\n\t\t\traise errors.InvalidInventory(\"missing fixity md5\")\n\t\tif \"sha1\" not in f[\"sums\"]:\n\t\t\traise errors.InvalidInventory(\"missing fixity sha1\")\n\t\tif \"sha256\" not in f[\"sums\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tfor f in fixity:\n\t\tif not 'name' in f:\n\t\t\traise ValueError(\n\t\t\t\t\"Missing 'name' field in fixity entry: %s\" % str(f)\n\t\t\t)\n\t\tif not 'digests' in f:\n\t\t\traise ValueError(\n\t\t\t\t\"Missing 'digests' field in fixity entry: %s\" % str(f)\n\t\t\t)\n\t\tif not 'size' in f:\n\t\t\traise ValueError(\n\t\t\t\t\"Missing 'size' field in fixity entry: %s\" % str(f)\n\t\t\t)\n\t\tif f['name'] not in manifest_files:\n\t\t\traise ValueError(\n\t\t\t\t\"Fixity entry for file not in manifest: %s\" % f['name']\n\t\t\t)\n\t\tfor d in f['digests']:\n\t\t\tif not 'hash' in d:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Missing 'hash' field in fixity.digests entry: %\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t# Check for required fields\n\tif not isinstance(fixity, dict):\n\t\traise ValueError('Fixity block is not a dict')\n\tfor field in ('type', 'hash', 'algorithm'):\n\t\tif field not in fixity:\n\t\t\traise ValueError('Fixity block is missing \"{}\" field'.format(field))\n\n\t# Check type\n\tif fixity['type'] != 'fixity':\n\t\traise ValueError('Fixity block has unexpected type \"{}\"'.format(fixity['type']))\n\n\t# Check algorithm\n\tif fixity['algorithm'] != 'sha256':\n\t\traise ValueError('Fixity block has unexpected algorithm \"{}\"'.format(fixity['algorithm']))\n\n\t# Check for hash collisions\n\thashes = set()\n\tfor hash_ in fixity['hash']:\n\t\tif hash_['file'] in hashes:\n\t\t\traise ValueError('Fixity block has duplicate hash for file \"{}\"'.format(hash_['file']))\n\t\telse:\n\t\t\thashes.add(hash_['file'])\n\n\t# Check for missing\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tif not isinstance(fixity, dict):\n\t\traise ValueError(\"Fixity must be a dictionary\")\n\n\tfor k, v in fixity.items():\n\t\tif k not in manifest_files:\n\t\t\traise ValueError(\"File {} not in manifest\".format(k))\n\n\t\tif 'sha1' not in v:\n\t\t\traise ValueError(\"No sha1 value for {}\".format(k))\n\n\t\tif 'size' not in v:\n\t\t\traise ValueError(\"No size value for {}\".format(k))\n\n\t\tif 'mtime' not in v:\n\t\t\traise ValueError(\"No mtime value for {}\".format(k))\n\n\t\tif 'atime' not in v:\n\t\t\traise ValueError(\"No atime value for {}\".format(k))\n\n\t\tif 'mode' not in v:\n\t\t\traise ValueError(\"No mode value for {}\".format(k))\n\n\t\tif 'type' not in v:\n\t\t\traise ValueError(\"No type value for {}\".format(k))\n\n\t\tif 'uid' not in v\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tif 'checksum' not in fixity:\n\t\traise ValidationError('FIXITY block must contain a CHECKSUM field.',\n\t\t\t\t\t\t\t  'FIXITY')\n\tif 'date' not in fixity:\n\t\traise ValidationError('FIXITY block must contain a DATE field.',\n\t\t\t\t\t\t\t  'FIXITY')\n\t# check for valid date\n\ttry:\n\t\tfixity['date'] = datetime.datetime.strptime(fixity['date'],\n\t\t\t\t\t\t\t\t\t\t\t\t   '%Y-%m-%d')\n\texcept ValueError:\n\t\traise ValidationError('FIXITY block must contain a valid DATE in '\n\t\t\t\t\t\t\t  'YYYY-MM-DD format.', 'FIXITY')\n\tif 'files' not in fixity:\n\t\traise ValidationError('FIXITY block must contain a FILES field.',\n\t\t\t\t\t\t\t  'FIXITY')\n\tif type(fixity['files']) != list:\n\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t# Check that the fixity block is a list\n\tif type(fixity) != list:\n\t\tprint(\"Fixity block is not a list\")\n\t\treturn False\n\n\t# Check that the fixity block is not empty\n\tif not fixity:\n\t\tprint(\"Fixity block is empty\")\n\t\treturn False\n\n\t# Check that every element of the fixity block is a dictionary\n\tfor fixity_item in fixity:\n\t\tif type(fixity_item) != dict:\n\t\t\tprint(\"Fixity element is not a dictionary\")\n\t\t\treturn False\n\n\t# Check that every element of the fixity block has the required keys\n\tfor fixity_item in fixity:\n\t\tfor key in [\"filename\", \"hash\"]:\n\t\t\tif not key in fixity_item:\n\t\t\t\tprint(\"Fixity element does not have required key: \" + key)\n\t\t\t\treturn False\n\n\t# Check that every element of the fixity block has a filename that is present in the manifest\n\tfor fixity_item in fixity:\n\t\tif not fixity_item[\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\terrors = []\n\tif not isinstance(fixity, dict):\n\t\terrors.append(\"fixity: not a dictionary\")\n\t\treturn errors\n\tfor filename, checksum_list in fixity.items():\n\t\tif not isinstance(checksum_list, list):\n\t\t\terrors.append(\"fixity: {}: not a list\".format(filename))\n\t\t\tcontinue\n\t\tfor checksum in checksum_list:\n\t\t\tif not isinstance(checksum, dict):\n\t\t\t\terrors.append(\"fixity: {}: {}: not a dictionary\".format(filename, checksum))\n\t\t\t\tcontinue\n\t\t\tif not checksum.get(\"type\") or not checksum.get(\"value\"):\n\t\t\t\terrors.append(\"fixity: {}: {}: missing type or value\".format(filename, checksum))\n\t\t\t\tcontinue\n\t\t\tif checksum[\"type\"] not in (\"md5\", \"sha1\", \"sha256\", \"sha512\"):\n\t\t\t\terrors.append(\"fixity: {}: {}: unknown type\".format(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tfor file in fixity:\n\t\tif file not in manifest_files:\n\t\t\traise InventoryError('File \"{}\" in fixity block does not exist in manifest'.format(file))\n\t\tif not isinstance(fixity[file], dict):\n\t\t\traise InventoryError('Fixity for file \"{}\" is not a dictionary'.format(file))\n\t\tif 'md5' not in fixity[file]:\n\t\t\traise InventoryError('Fixity for file \"{}\" does not specify an md5'.format(file))\n\t\tif not isinstance(fixity[file]['md5'], str):\n\t\t\traise InventoryError('Fixity for file \"{}\" specifies an md5 that is not a string'.format(file))\n\n\tif len(fixity) != len(manifest_files):\n\t\traise InventoryError('Fixity block does not contain fixity for all files in manifest')\n\n\treturn fixity\n\n\t\n\tdef validate_inventory(self, inventory_dict):\n\t\"\"\"\n\tValidate the structure of an inventory.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tmanifest_files_set = set(manifest_files)\n\n\t# check fixity structure\n\tvalidate_fixity_structure(fixity)\n\n\t# check that all fixity files are present in the manifest\n\tfixity_files_set = set(fixity.keys())\n\tmissing_files = fixity_files_set - manifest_files_set\n\tif missing_files:\n\t\traise ValueError('Files referenced in fixity but missing from manifest: {}'.format(missing_files))\n\n\t# check that all manifest files have fixity\n\tmissing_files = manifest_files_set - fixity_files_set\n\tif missing_files:\n\t\traise ValueError('Files listed in manifest but missing from fixity: {}'.format(missing_files))\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t# Validate each file's fixity\n\tfor file_id in fixity:\n\t\tif file_id not in manifest_files:\n\t\t\traise InventoryError(\"Fixity for unknown file '{0}'\".format(file_id))\n\t\tfile = manifest_files[file_id]\n\t\tif 'size' not in file:\n\t\t\traise InventoryError(\"File '{0}' is missing size\".format(file_id))\n\t\tif 'fixity' not in file:\n\t\t\traise InventoryError(\"File '{0}' is missing fixity\".format(file_id))\n\t\tif file['fixity'] != fixity[file_id]:\n\t\t\traise InventoryError(\"Fixity for file '{0}' does not match manifest\".format(file_id))\n\n\t# Make sure all files have a fixity\n\tfor file_id in manifest_files:\n\t\tif file_id not in fixity:\n\t\t\traise InventoryError(\"File '{0}' is missing fixity\".format(file_id))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tif \"files\" not in fixity:\n\t\traise errors.InvalidInventory(\"missing fixity files\")\n\n\tfor f in fixity[\"files\"]:\n\t\tif f[\"path\"] not in manifest_files:\n\t\t\traise errors.InvalidInventory(\n\t\t\t    \"fixity file not in manifest: {}\".format(f[\"path\"]))\n\n\t\tif \"sums\" not in f:\n\t\t\traise errors.InvalidInventory(\"missing fixity sums\")\n\t\tif \"size\" not in f:\n\t\t\traise errors.InvalidInventory(\"missing fixity size\")\n\n\t\tif f[\"size\"] != manifest_files[f[\"path\"]][\"size\"]:\n\t\t\traise errors.InvalidInventory(\"fixity size mismatch\")\n\n\t\tif \"md5\" not in f[\"sums\"]:\n\t\t\traise errors.InvalidInventory(\"missing fixity md5\")\n\t\tif \"sha1\" not in f[\"sums\"]:\n\t\t\traise errors.InvalidInventory(\"missing fixity sha1\")\n\t\tif \"sha256\" not in f[\"sums\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "ques_id": "62b463153879012d19481498", "code": ["def files_list(path):\n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\n", "def files_list(path):\n\treturn sorted(os.listdir(path))\n\n", "def files_list(path):\n\treturn os.listdir(path)\n\n\n", "def files_list(path):\n\treturn [os.path.join(path, _) for _ in os.listdir(path)]\n\n\n", "def files_list(path):\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n", "def files_list(path):\n\treturn os.listdir(path)\n\n", "def files_list(path):\n\treturn [f for f in listdir(path) if isfile(join(path, f))]\n\n", "def files_list(path):\n\treturn [os.path.join(path, _) for _ in os.listdir(path)]\n\n\n", "def files_list(path):\n\tfor file in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, file)):\n\t\t\tyield file\n\n\n", "def files_list(path):\n\treturn [f for f in listdir(path) if isfile(join(path, f))]\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn sorted(os.listdir(path))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn os.listdir(path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn [os.path.join(path, _) for _ in os.listdir(path)]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn os.listdir(path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn [f for f in listdir(path) if isfile(join(path, f))]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn [os.path.join(path, _) for _ in os.listdir(path)]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\tfor file in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, file)):\n\t\t\tyield file\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn [f for f in listdir(path) if isfile(join(path, f))]\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "ques_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n\txml_files = {}\n\n\tfor xml in xmls:\n\t\tfilepath = os.path.join(source, xml)\n\t\txml_files[os.path.splitext(os.path.basename(filepath))[0]] = Package(xml=filepath, files=files)\n\n\treturn xml_files\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\tdata = {}\n\tfor xml in xmls:\n\t\tbasename = os.path.basename(xml)\n\t\tbasename = os.path.splitext(basename)[0]\n\t\tdata[basename] = Package(source, basename)\n\t\tdata[basename].xml = xml\n\n\tfor file in files:\n\t\tbasename = os.path.basename(file)\n\t\tbasename = os.path.splitext(basename)[0]\n\t\tif basename in data.keys():\n\t\t\tdata[basename].files.append(file)\n\n\treturn data\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\tidx = 0\n\tdict_of_files = {}\n\tfor xml_filename in xmls:\n\t\t# Extract XML basename and check if it is in the dict\n\t\tbasename = os.path.basename(xml_filename)\n\t\tif basename in dict_of_files:\n\t\t\t# Add file to Package object\n\t\t\tdict_of_files[basename].add_file(files[idx])\n\t\telse:\n\t\t\t# Create Package object\n\t\t\tdict_of_files[basename] = Package(source, xml_filename, files[idx])\n\t\tidx += 1\n\treturn dict_of_files\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t# pylint: disable=line-too-long\n\tdata = {}\n\tfor xml_filename in xmls:\n\t\tdata[xml_filename] = Package(source, xml_filename)\n\t\tfor file in files:\n\t\t\t# If the file is not the XML file and has the same basename as the XML file then add to package\n\t\t\tif xml_filename != file and file.split(\".\")[0] == xml_filename.split(\".\")[0]:\n\t\t\t\tdata[xml_filename].add_file(file)\n\treturn data\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t# Group files by XML filename\n\tfiles_by_xml_filename = {}\n\n\tfor xml_filename in xmls:\n\t\tfiles_by_xml_filename[xml_filename] = Package(source, xml_filename, {})\n\n\tfor f in files:\n\t\tif ('.' in f) and (f.rsplit('.', 1)[-1] not in _FILE_TYPES):\n\t\t\tcontinue\n\t\txml_filename = f.rsplit('.', 1)[0]\n\t\tif xml_filename not in files_by_xml_filename:\n\t\t\tfiles_by_xml_filename[xml_filename] = Package(source, xml_filename, {})\n\n\t\tfiles_by_xml_filename[xml_filename].files[f] = None\n\n\treturn files_by_xml_filename\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\tgrouped_files = {}\n\n\tfor xml in xmls:\n\t\tpackage = Package(source)\n\t\tpackage.xml = xml\n\n\t\tfor file in files:\n\t\t\tif file.startswith(xml[:-4]):\n\t\t\t\tpackage.files.append(file)\n\n\t\tgrouped_files[xml] = package\n\n\treturn grouped_files\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\n\t# Get XML filenames\n\txml_filenames = [os.path.basename(i) for i in xmls]\n\n\t# Group files by their XML basename\n\tgrouped_files = {}\n\tfor xml in xml_filenames:\n\t\tgrouped_files[xml] = []\n\t\tfor file in files:\n\t\t\tif xml.rsplit('.', 1)[0] in file:\n\t\t\t\tgrouped_files[xml].append(file)\n\n\t# Create dict to return\n\tdata = {}\n\n\tfor xml in xml_filenames:\n\t\txml_path = os.path.join(source, xml)\n\t\tdata[xml] = Package(xml_path, grouped_files[xml], source)\n\n\treturn data\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t\n\tsource = _ensure_trailing_slash(source)\n\txmls = _ensure_trailing_slash(xmls)\n\t\n\t# build mapping of xml filenames to files\n\txml_files = {}\n\tfor xml in xmls:\n\t\t\n\t\t# remove the prefix from the xml file\n\t\txml = xml.replace(source, '')\n\t\txml = xml.replace(xmls, '')\n\t\t\n\t\t# remove the xml extension\n\t\txml = xml.replace('.xml', '')\n\t\t\n\t\t# build list of files\n\t\tfiles_list = []\n\t\tfor file in files:\n\t\t\t\n\t\t\t# remove the prefix from the file\n\t\t\tfile = file.replace(source, '')\n\t\t\tfile = file.replace(xmls, '')\n\t\t\t\n\t\t\t# add the file to the list\n\t\t\tif xml in file:\n\t\t\t\tfiles_list.append(file)\n\t\t\n\t\t# add file list to the dictionary\n\t\txml_files\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t# xml_filename = os.path.splitext(os.path.basename(xml))[0]\n\t# xml_filename = xml_filename.replace(\"_\", \" \")\n\txml_filename = source.replace(\"_\", \" \")\n\tpackage = Package(xml_filename)\n\tfor file in files:\n\t\tif file.endswith(\"xml\") and file.startswith(source):\n\t\t\tpackage.xml = os.path.join(xmls, file)\n\t\telif file.startswith(\"ppt\") and file.endswith(\"pptx\"):\n\t\t\tpackage.pptx = os.path.join(xmls, file)\n\t\telif file.startswith(\"ppt\") and file.endswith(\"ppt\"):\n\t\t\tpackage.ppt = os.path.join(xmls, file)\n\t\telif file.startswith(\"ppt\") and file.endswith(\"pdf\"):\n\t\t\tpackage.pdf = os.path.join(xmls, file)\n\t\telif file.end\n", "def _group_files_by_xml_filename(source, xmls, files):\n\tgrouped_files = {}\n\tfor xml in xmls:\n\t\txml_filename = os.path.splitext(os.path.basename(xml))[0]\n\t\tgrouped_files[xml_filename] = Package(source, xml_filename)\n\t\t# group all the files by their xml filename\n\t\t# if the file is not an xml file, we assume it's a data file\n\t\tfor file in files:\n\t\t\tif xml_filename in file and os.path.splitext(file)[1] != \".xml\":\n\t\t\t\tgrouped_files[xml_filename].data_files.append(file)\n\n\treturn grouped_files\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\txml_files = {}\n\n\tfor xml in xmls:\n\t\tfilepath = os.path.join(source, xml)\n\t\txml_files[os.path.splitext(os.path.basename(filepath))[0]] = Package(xml=filepath, files=files)\n\n\treturn xml_files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\tdata = {}\n\tfor xml in xmls:\n\t\tbasename = os.path.basename(xml)\n\t\tbasename = os.path.splitext(basename)[0]\n\t\tdata[basename] = Package(source, basename)\n\t\tdata[basename].xml = xml\n\n\tfor file in files:\n\t\tbasename = os.path.basename(file)\n\t\tbasename = os.path.splitext(basename)[0]\n\t\tif basename in data.keys():\n\t\t\tdata[basename].files.append(file)\n\n\treturn data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\tidx = 0\n\tdict_of_files = {}\n\tfor xml_filename in xmls:\n\t\t# Extract XML basename and check if it is in the dict\n\t\tbasename = os.path.basename(xml_filename)\n\t\tif basename in dict_of_files:\n\t\t\t# Add file to Package object\n\t\t\tdict_of_files[basename].add_file(files[idx])\n\t\telse:\n\t\t\t# Create Package object\n\t\t\tdict_of_files[basename] = Package(source, xml_filename, files[idx])\n\t\tidx += 1\n\treturn dict_of_files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t# pylint: disable=line-too-long\n\tdata = {}\n\tfor xml_filename in xmls:\n\t\tdata[xml_filename] = Package(source, xml_filename)\n\t\tfor file in files:\n\t\t\t# If the file is not the XML file and has the same basename as the XML file then add to package\n\t\t\tif xml_filename != file and file.split(\".\")[0] == xml_filename.split(\".\")[0]:\n\t\t\t\tdata[xml_filename].add_file(file)\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t# Group files by XML filename\n\tfiles_by_xml_filename = {}\n\n\tfor xml_filename in xmls:\n\t\tfiles_by_xml_filename[xml_filename] = Package(source, xml_filename, {})\n\n\tfor f in files:\n\t\tif ('.' in f) and (f.rsplit('.', 1)[-1] not in _FILE_TYPES):\n\t\t\tcontinue\n\t\txml_filename = f.rsplit('.', 1)[0]\n\t\tif xml_filename not in files_by_xml_filename:\n\t\t\tfiles_by_xml_filename[xml_filename] = Package(source, xml_filename, {})\n\n\t\tfiles_by_xml_filename[xml_filename].files[f] = None\n\n\treturn files_by_xml_filename\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\tgrouped_files = {}\n\n\tfor xml in xmls:\n\t\tpackage = Package(source)\n\t\tpackage.xml = xml\n\n\t\tfor file in files:\n\t\t\tif file.startswith(xml[:-4]):\n\t\t\t\tpackage.files.append(file)\n\n\t\tgrouped_files[xml] = package\n\n\treturn grouped_files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\n\t# Get XML filenames\n\txml_filenames = [os.path.basename(i) for i in xmls]\n\n\t# Group files by their XML basename\n\tgrouped_files = {}\n\tfor xml in xml_filenames:\n\t\tgrouped_files[xml] = []\n\t\tfor file in files:\n\t\t\tif xml.rsplit('.', 1)[0] in file:\n\t\t\t\tgrouped_files[xml].append(file)\n\n\t# Create dict to return\n\tdata = {}\n\n\tfor xml in xml_filenames:\n\t\txml_path = os.path.join(source, xml)\n\t\tdata[xml] = Package(xml_path, grouped_files[xml], source)\n\n\treturn data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t\n\tsource = _ensure_trailing_slash(source)\n\txmls = _ensure_trailing_slash(xmls)\n\t\n\t# build mapping of xml filenames to files\n\txml_files = {}\n\tfor xml in xmls:\n\t\t\n\t\t# remove the prefix from the xml file\n\t\txml = xml.replace(source, '')\n\t\txml = xml.replace(xmls, '')\n\t\t\n\t\t# remove the xml extension\n\t\txml = xml.replace('.xml', '')\n\t\t\n\t\t# build list of files\n\t\tfiles_list = []\n\t\tfor file in files:\n\t\t\t\n\t\t\t# remove the prefix from the file\n\t\t\tfile = file.replace(source, '')\n\t\t\tfile = file.replace(xmls, '')\n\t\t\t\n\t\t\t# add the file to the list\n\t\t\tif xml in file:\n\t\t\t\tfiles_list.append(file)\n\t\t\n\t\t# add file list to the dictionary\n\t\txml_files\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t# xml_filename = os.path.splitext(os.path.basename(xml))[0]\n\t# xml_filename = xml_filename.replace(\"_\", \" \")\n\txml_filename = source.replace(\"_\", \" \")\n\tpackage = Package(xml_filename)\n\tfor file in files:\n\t\tif file.endswith(\"xml\") and file.startswith(source):\n\t\t\tpackage.xml = os.path.join(xmls, file)\n\t\telif file.startswith(\"ppt\") and file.endswith(\"pptx\"):\n\t\t\tpackage.pptx = os.path.join(xmls, file)\n\t\telif file.startswith(\"ppt\") and file.endswith(\"ppt\"):\n\t\t\tpackage.ppt = os.path.join(xmls, file)\n\t\telif file.startswith(\"ppt\") and file.endswith(\"pdf\"):\n\t\t\tpackage.pdf = os.path.join(xmls, file)\n\t\telif file.end\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\tgrouped_files = {}\n\tfor xml in xmls:\n\t\txml_filename = os.path.splitext(os.path.basename(xml))[0]\n\t\tgrouped_files[xml_filename] = Package(source, xml_filename)\n\t\t# group all the files by their xml filename\n\t\t# if the file is not an xml file, we assume it's a data file\n\t\tfor file in files:\n\t\t\tif xml_filename in file and os.path.splitext(file)[1] != \".xml\":\n\t\t\t\tgrouped_files[xml_filename].data_files.append(file)\n\n\treturn grouped_files\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "ques_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix)\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn (\n\t\tfile_path.name.startswith(prefix)\n\t\tand (\n\t\t\tfile_path.suffix in (\".xml\", \".pdf\", \".txt\", \".doc\", \".docx\", \".odt\")\n\t\t)\n\t\tand (file_path.name != \"mets.xml\")\n\t)\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\tprefix = prefix.lower()\n\tfile_path = file_path.lower()\n\treturn file_path.startswith(prefix) and file_path != prefix\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\tif file_path.startswith(prefix):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn True if file_path.startswith(prefix) else False\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn file_path.startswith(prefix)\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn file_path.startswith(prefix)\n\n", "def match_file_by_prefix(prefix, file_path):\n\tif file_path.startswith(prefix):\n\t\treturn True\n\treturn False\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn any(prefix in f for f in os.listdir(file_path))\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\tif file_path.find(prefix) >= 0:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn (\n\t\tfile_path.name.startswith(prefix)\n\t\tand (\n\t\t\tfile_path.suffix in (\".xml\", \".pdf\", \".txt\", \".doc\", \".docx\", \".odt\")\n\t\t)\n\t\tand (file_path.name != \"mets.xml\")\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tprefix = prefix.lower()\n\tfile_path = file_path.lower()\n\treturn file_path.startswith(prefix) and file_path != prefix\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tif file_path.startswith(prefix):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn True if file_path.startswith(prefix) else False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn file_path.startswith(prefix)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn file_path.startswith(prefix)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tif file_path.startswith(prefix):\n\t\treturn True\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn any(prefix in f for f in os.listdir(file_path))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tif file_path.find(prefix) >= 0:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "ques_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [x for x in files if x.endswith('.xml') and os.path.basename(x).startswith(prefix)]\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\tprefix = remove_extension(prefix)\n\treturn [f for f in files if get_filename(f).startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\tfilenames = []\n\tfor filename in files:\n\t\tif len(filename) > len(prefix):\n\t\t\tif filename[0:len(prefix)] == prefix:\n\t\t\t\tfilenames.append(filename)\n\treturn filenames\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [x for x in files if x.endswith('.xml') and os.path.basename(x).startswith(prefix)]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\tprefix = remove_extension(prefix)\n\treturn [f for f in files if get_filename(f).startswith(prefix)]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\tfilenames = []\n\tfor filename in files:\n\t\tif len(filename) > len(prefix):\n\t\t\tif filename[0:len(prefix)] == prefix:\n\t\t\t\tfilenames.append(filename)\n\treturn filenames\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if os.path.basename(f).startswith(prefix)]\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n\tdata = {}\n\tfor file in os.listdir(folder):\n\t\tif file.endswith(\".xml\"):\n\t\t\tfile_path = os.path.join(folder, file)\n\t\t\txml = _explore_xml(file_path)\n\t\t\tname = os.path.basename(file)[:-4]\n\t\t\tdata[name] = xml\n\n\treturn data\n\n\n", "def _explore_folder(folder):\n\tfiles = {}\n\tfor file in os.listdir(folder):\n\t\tif file.endswith(\".xml\"):\n\t\t\tbasename = os.path.splitext(file)[0]\n\t\t\tif basename not in files:\n\t\t\t\tfiles[basename] = [os.path.join(folder, file)]\n\t\t\telse:\n\t\t\t\tfiles[basename].append(os.path.join(folder, file))\n\tpackages = {}\n\tfor file, file_list in files.items():\n\t\tif len(file_list) < len(LANGUAGES):\n\t\t\tprint(\"{} has only {} files\".format(file, len(file_list)))\n\t\telse:\n\t\t\tpackages[file] = file_list\n\n\treturn packages\n\n", "def _explore_folder(folder):\n\tfrom os import listdir\n\tfrom os.path import isfile, join, splitext\n\n\tfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n\tgroups = {}\n\tfor file in files:\n\t\t(base, ext) = splitext(file)\n\t\tif ext == '.xml':\n\t\t\tif base in groups:\n\t\t\t\tgroups[base].append(file)\n\t\t\telse:\n\t\t\t\tgroups[base] = [file]\n\n\treturn groups\n\n\n", "def _explore_folder(folder):\n\tpackages = {}\n\tmodules = {}\n\n\tfor f in os.listdir(folder):\n\t\tif f.endswith(\".xml\"):\n\t\t\tname = f.split(\".\")[0]\n\t\t\tif name in packages:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tpackages[name] = []\n\t\t\t\tmodules[name] = []\n\t\t\tpackages[name].append(f)\n\t\t\tif \".\" in f:\n\t\t\t\tmodules[name].append(f.split(\".\")[1])\n\n\treturn packages, modules\n\n", "def _explore_folder(folder):\n\td = {}\n\tfor filename in os.listdir(folder):\n\t\tif filename.endswith('.xml'):\n\t\t\td.setdefault(filename, []).append(filename)\n\treturn d\n\n", "def _explore_folder(folder):\n\t# Get all xml files\n\tfiles = os.listdir(folder)\n\txmls = [file for file in files if file.endswith('.xml')]\n\n\t# Group files by xml name\n\tgroups = {}\n\tfor xml in xmls:\n\t\tbasename = os.path.splitext(xml)[0]\n\t\tothers = [file for file in files if file.startswith(basename)]\n\t\tgroups[basename] = others\n\n\t# Return data\n\tdata = {}\n\tfor xml_name, group in groups.items():\n\t\txml_file = os.path.join(folder, xml_name + '.xml')\n\t\txml = _open_xml(xml_file)\n\t\tothers = [os.path.join(folder, file) for file in group]\n\t\tdata[xml_name] = _get_package_data(xml, others)\n\n\treturn data\n\n", "def _explore_folder(folder):\n\tpackages = {}\n\t# Loop through files in folder\n\tfor f in os.listdir(folder):\n\t\t# Get file name and extension\n\t\tf_name, f_ext = os.path.splitext(f)\n\t\t# Get full path\n\t\tf_path = os.path.join(folder, f)\n\t\t# Check if file is a XML file\n\t\tif os.path.isfile(f_path) and f_ext == \".xml\":\n\t\t\t# Get package data\n\t\t\tpackage = _get_package_data(f_path)\n\t\t\t# Remove package data from dict if empty\n\t\t\tif not package:\n\t\t\t\tcontinue\n\t\t\t# Add package to dict\n\t\t\tif f_name not in packages:\n\t\t\t\tpackages[f_name] = package\n\treturn packages\n\n\n", "def _explore_folder(folder):\n\tpackages = {}\n\n\tfor f in os.listdir(folder):\n\t\t# Ignore non-XML files\n\t\tif not f.endswith('.xml'):\n\t\t\tcontinue\n\n\t\t# Get file path\n\t\tf = os.path.join(folder, f)\n\n\t\t# Read XML\n\t\ttree = ET.parse(f)\n\t\troot = tree.getroot()\n\n\t\t# Get package name\n\t\tpkg_name = root.attrib['name']\n\n\t\t# Get package version\n\t\tpkg_version = root.attrib['version']\n\n\t\t# Get package description\n\t\tpkg_description = root.attrib['description']\n\n\t\t# Get package title\n\t\tpkg_title = root.attrib['title']\n\n\t\t# Get package author\n\t\tpkg_author = root.attrib['author']\n\n\t\t# Get package license\n\t\tpkg_license = root.attrib['license']\n\n\t\t# Get package homepage\n\t\tpkg_homepage = root.attrib['homepage']\n\n\t\t# Get package date\n\t\tpkg_date\n", "def _explore_folder(folder):\n\timport os\n\timport sys\n\timport xml.etree.ElementTree as ET\n\tfrom xml.etree.ElementTree import ParseError\n\tret = {}\n\tfor file in os.listdir(folder):\n\t\tif file.endswith(\".xml\"):\n\t\t\ttry:\n\t\t\t\ttree = ET.parse(folder + file)\n\t\t\t\troot = tree.getroot()\n\t\t\t\tname = root.find(\"name\").text\n\t\t\t\tversion = root.find(\"version\").text\n\t\t\t\tversion_comparison = root.find(\"version_comparison\").text\n\t\t\t\tret[name] = {\"version\": version, \"version_comparison\": version_comparison}\n\t\t\texcept ParseError:\n\t\t\t\tprint(\"Parse error in \" + file + \".\")\n\treturn ret\n\n", "def _explore_folder(folder):\n\tfiles = _get_files(folder)\n\txml_files = _filter_xml_files(files)\n\txml_basenames = _get_xml_basenames(xml_files)\n\txml_basename_files = _group_files_by_basename(xml_files)\n\tdata = []\n\tfor xml_basename in xml_basenames:\n\t\tdata.append(_get_package_data(xml_basename, xml_basename_files[xml_basename]))\n\treturn data\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n\tdata = {}\n\tfor file in os.listdir(folder):\n\t\tif file.endswith(\".xml\"):\n\t\t\tfile_path = os.path.join(folder, file)\n\t\t\txml = _explore_xml(file_path)\n\t\t\tname = os.path.basename(file)[:-4]\n\t\t\tdata[name] = xml\n\n\treturn data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfiles = {}\n\tfor file in os.listdir(folder):\n\t\tif file.endswith(\".xml\"):\n\t\t\tbasename = os.path.splitext(file)[0]\n\t\t\tif basename not in files:\n\t\t\t\tfiles[basename] = [os.path.join(folder, file)]\n\t\t\telse:\n\t\t\t\tfiles[basename].append(os.path.join(folder, file))\n\tpackages = {}\n\tfor file, file_list in files.items():\n\t\tif len(file_list) < len(LANGUAGES):\n\t\t\tprint(\"{} has only {} files\".format(file, len(file_list)))\n\t\telse:\n\t\t\tpackages[file] = file_list\n\n\treturn packages\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfrom os import listdir\n\tfrom os.path import isfile, join, splitext\n\n\tfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n\tgroups = {}\n\tfor file in files:\n\t\t(base, ext) = splitext(file)\n\t\tif ext == '.xml':\n\t\t\tif base in groups:\n\t\t\t\tgroups[base].append(file)\n\t\t\telse:\n\t\t\t\tgroups[base] = [file]\n\n\treturn groups\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tpackages = {}\n\tmodules = {}\n\n\tfor f in os.listdir(folder):\n\t\tif f.endswith(\".xml\"):\n\t\t\tname = f.split(\".\")[0]\n\t\t\tif name in packages:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tpackages[name] = []\n\t\t\t\tmodules[name] = []\n\t\t\tpackages[name].append(f)\n\t\t\tif \".\" in f:\n\t\t\t\tmodules[name].append(f.split(\".\")[1])\n\n\treturn packages, modules\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\td = {}\n\tfor filename in os.listdir(folder):\n\t\tif filename.endswith('.xml'):\n\t\t\td.setdefault(filename, []).append(filename)\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t# Get all xml files\n\tfiles = os.listdir(folder)\n\txmls = [file for file in files if file.endswith('.xml')]\n\n\t# Group files by xml name\n\tgroups = {}\n\tfor xml in xmls:\n\t\tbasename = os.path.splitext(xml)[0]\n\t\tothers = [file for file in files if file.startswith(basename)]\n\t\tgroups[basename] = others\n\n\t# Return data\n\tdata = {}\n\tfor xml_name, group in groups.items():\n\t\txml_file = os.path.join(folder, xml_name + '.xml')\n\t\txml = _open_xml(xml_file)\n\t\tothers = [os.path.join(folder, file) for file in group]\n\t\tdata[xml_name] = _get_package_data(xml, others)\n\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tpackages = {}\n\t# Loop through files in folder\n\tfor f in os.listdir(folder):\n\t\t# Get file name and extension\n\t\tf_name, f_ext = os.path.splitext(f)\n\t\t# Get full path\n\t\tf_path = os.path.join(folder, f)\n\t\t# Check if file is a XML file\n\t\tif os.path.isfile(f_path) and f_ext == \".xml\":\n\t\t\t# Get package data\n\t\t\tpackage = _get_package_data(f_path)\n\t\t\t# Remove package data from dict if empty\n\t\t\tif not package:\n\t\t\t\tcontinue\n\t\t\t# Add package to dict\n\t\t\tif f_name not in packages:\n\t\t\t\tpackages[f_name] = package\n\treturn packages\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tpackages = {}\n\n\tfor f in os.listdir(folder):\n\t\t# Ignore non-XML files\n\t\tif not f.endswith('.xml'):\n\t\t\tcontinue\n\n\t\t# Get file path\n\t\tf = os.path.join(folder, f)\n\n\t\t# Read XML\n\t\ttree = ET.parse(f)\n\t\troot = tree.getroot()\n\n\t\t# Get package name\n\t\tpkg_name = root.attrib['name']\n\n\t\t# Get package version\n\t\tpkg_version = root.attrib['version']\n\n\t\t# Get package description\n\t\tpkg_description = root.attrib['description']\n\n\t\t# Get package title\n\t\tpkg_title = root.attrib['title']\n\n\t\t# Get package author\n\t\tpkg_author = root.attrib['author']\n\n\t\t# Get package license\n\t\tpkg_license = root.attrib['license']\n\n\t\t# Get package homepage\n\t\tpkg_homepage = root.attrib['homepage']\n\n\t\t# Get package date\n\t\tpkg_date\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _explore_folder(folder):\n\timport os\n\timport sys\n\timport xml.etree.ElementTree as ET\n\tfrom xml.etree.ElementTree import ParseError\n\tret = {}\n\tfor file in os.listdir(folder):\n\t\tif file.endswith(\".xml\"):\n\t\t\ttry:\n\t\t\t\ttree = ET.parse(folder + file)\n\t\t\t\troot = tree.getroot()\n\t\t\t\tname = root.find(\"name\").text\n\t\t\t\tversion = root.find(\"version\").text\n\t\t\t\tversion_comparison = root.find(\"version_comparison\").text\n\t\t\t\tret[name] = {\"version\": version, \"version_comparison\": version_comparison}\n\t\t\texcept ParseError:\n\t\t\t\tprint(\"Parse error in \" + file + \".\")\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfiles = _get_files(folder)\n\txml_files = _filter_xml_files(files)\n\txml_basenames = _get_xml_basenames(xml_files)\n\txml_basename_files = _group_files_by_basename(xml_files)\n\tdata = []\n\tfor xml_basename in xml_basenames:\n\t\tdata.append(_get_package_data(xml_basename, xml_basename_files[xml_basename]))\n\treturn data\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n\toutput = {\n\t\t\"type\": None,\n\t\t\"path\": file_path\n\t}\n\tif \"asset\" in prefix:\n\t\toutput[\"type\"] = \"asset\"\n\telif \"rendition\" in prefix:\n\t\toutput[\"type\"] = \"rendition\"\n\treturn output\n\n\n", "def _eval_file(prefix, file_path):\n\tif file_path.find('assets') > -1:\n\t\tpackages[prefix]['asset'] = file_path\n\telif file_path.find('renditions') > -1:\n\t\tpackages[prefix]['rendition'] = file_path\n\treturn packages\n\n", "def _eval_file(prefix, file_path):\n\tlogging.info('_eval_file')\n\tfile_info = {}\n\n\tfor asset in packages['assets']:\n\t\tif asset['filename'] == prefix:\n\t\t\tfile_info['type'] = 'asset'\n\t\t\tfile_info['file_url'] = file_path\n\t\t\tfile_info['prefix'] = prefix\n\t\t\treturn file_info\n\n\tfor rendition in packages['renditions']:\n\t\tif rendition['filename'] == prefix:\n\t\t\tfile_info['type'] = 'rendition'\n\t\t\tfile_info['file_url'] = file_path\n\t\t\tfile_info['prefix'] = prefix\n\t\t\treturn file_info\n\n\treturn file_info\n\n\n", "def _eval_file(prefix, file_path):\n\tif not file_path.endswith('xml'):\n\t\t# assets\n\t\t_assets[_file_name(file_path)] = file_path\n\telse:\n\t\t# renditions\n\t\t_renditions[_file_name(file_path)] = file_path\n\n\n", "def _eval_file(prefix, file_path):\n\tfile_name, file_extension = os.path.splitext(file_path)\n\n\tif file_extension == \".xml\":\n\t\tfile_extension = _eval_xml_extension(prefix, file_path)\n\n\t\tif file_extension == \"\":\n\t\t\treturn \"\"\n\n\tif file_extension == \".xml\":\n\t\tfile_extension = _eval_xml_extension(prefix, file_path)\n\n\t\tif file_extension == \"\":\n\t\t\treturn \"\"\n\n\tif file_extension == \".xml\":\n\t\tif _is_package(file_path):\n\t\t\treturn {\n\t\t\t\t\"type\": \"package\",\n\t\t\t\t\"path\": file_path\n\t\t\t}\n\n\tif file_extension == \".xml\":\n\t\tif _is_rendition(file_path):\n\t\t\treturn {\n\t\t\t\t\"type\": \"rendition\",\n\t\t\t\t\"path\": file_path\n\t\t\t}\n\n\treturn \"\"\n\n", "def _eval_file(prefix, file_path):\n\t# _file_type = ''\n\t# # if filename in args['asset_files']:\n\t# if prefix in args['asset_files']:\n\t# \t_file_type = 'asset'\n\t# elif prefix in args['rendition_files']:\n\t# \t_file_type = 'rendition'\n\t# else:\n\t# \t_file_type = 'notype'\n\t# \tpass\n\n\t# return {'file_type': _file_type, 'file_path': file_path}\n\n\treturn {'file_type': 'asset', 'file_path': file_path}\n\n\n", "def _eval_file(prefix, file_path):\n\tfile_type = None\n\tif prefix.endswith('-asset'):\n\t\tfile_type = 'asset'\n\telif prefix.endswith('-rendition'):\n\t\tfile_type = 'rendition'\n\telse:\n\t\tfile_type = 'other'\n\n\treturn {\n\t\t'file_type': file_type,\n\t\t'file_path': file_path\n\t}\n\n", "def _eval_file(prefix, file_path):\n\tdef __key(idx, key):\n\t\treturn '{0}-{1}-{2}'.format(prefix, idx, key)\n\n\tidx = 0\n\tfor file in file_path:\n\t\tpackages[__key(idx, 'file')] = os.path.join(file_folder, file)\n\t\tpackages[__key(idx, 'type')] = 'asset'\n\t\tidx += 1\n\n", "def _eval_file(prefix, file_path):\n\tif prefix == 'assets':\n\t\treturn _eval_assets_file(file_path)\n\telif prefix == 'renditions':\n\t\treturn _eval_renditions_file(file_path)\n\telse:\n\t\traise Exception('Package file type not found.')\n\n\n", "def _eval_file(prefix, file_path):\n\t_log_debug('_eval_file()')\n\tif prefix in files['assets']:\n\t\t_log_debug('\\tprefix in assets')\n\t\tpackages[prefix]['type'] = 'asset'\n\t\tpackages[prefix]['file'] = file_path\n\telif prefix in files['renditions']:\n\t\t_log_debug('\\tprefix in renditions')\n\t\tpackages[prefix]['type'] = 'rendition'\n\t\tpackages[prefix]['file'] = file_path\n\telse:\n\t\t_log_debug('\\tprefix not in assets or renditions')\n\t\tpackages[prefix]['type'] = None\n\t\tpackages[prefix]['file'] = None\n\n\t_log_debug('\\tpackages[{}] = {}'.format(prefix, packages[prefix]))\n\n\treturn packages\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n\toutput = {\n\t\t\"type\": None,\n\t\t\"path\": file_path\n\t}\n\tif \"asset\" in prefix:\n\t\toutput[\"type\"] = \"asset\"\n\telif \"rendition\" in prefix:\n\t\toutput[\"type\"] = \"rendition\"\n\treturn output\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tif file_path.find('assets') > -1:\n\t\tpackages[prefix]['asset'] = file_path\n\telif file_path.find('renditions') > -1:\n\t\tpackages[prefix]['rendition'] = file_path\n\treturn packages\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tlogging.info('_eval_file')\n\tfile_info = {}\n\n\tfor asset in packages['assets']:\n\t\tif asset['filename'] == prefix:\n\t\t\tfile_info['type'] = 'asset'\n\t\t\tfile_info['file_url'] = file_path\n\t\t\tfile_info['prefix'] = prefix\n\t\t\treturn file_info\n\n\tfor rendition in packages['renditions']:\n\t\tif rendition['filename'] == prefix:\n\t\t\tfile_info['type'] = 'rendition'\n\t\t\tfile_info['file_url'] = file_path\n\t\t\tfile_info['prefix'] = prefix\n\t\t\treturn file_info\n\n\treturn file_info\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tif not file_path.endswith('xml'):\n\t\t# assets\n\t\t_assets[_file_name(file_path)] = file_path\n\telse:\n\t\t# renditions\n\t\t_renditions[_file_name(file_path)] = file_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tfile_name, file_extension = os.path.splitext(file_path)\n\n\tif file_extension == \".xml\":\n\t\tfile_extension = _eval_xml_extension(prefix, file_path)\n\n\t\tif file_extension == \"\":\n\t\t\treturn \"\"\n\n\tif file_extension == \".xml\":\n\t\tfile_extension = _eval_xml_extension(prefix, file_path)\n\n\t\tif file_extension == \"\":\n\t\t\treturn \"\"\n\n\tif file_extension == \".xml\":\n\t\tif _is_package(file_path):\n\t\t\treturn {\n\t\t\t\t\"type\": \"package\",\n\t\t\t\t\"path\": file_path\n\t\t\t}\n\n\tif file_extension == \".xml\":\n\t\tif _is_rendition(file_path):\n\t\t\treturn {\n\t\t\t\t\"type\": \"rendition\",\n\t\t\t\t\"path\": file_path\n\t\t\t}\n\n\treturn \"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t# _file_type = ''\n\t# # if filename in args['asset_files']:\n\t# if prefix in args['asset_files']:\n\t# \t_file_type = 'asset'\n\t# elif prefix in args['rendition_files']:\n\t# \t_file_type = 'rendition'\n\t# else:\n\t# \t_file_type = 'notype'\n\t# \tpass\n\n\t# return {'file_type': _file_type, 'file_path': file_path}\n\n\treturn {'file_type': 'asset', 'file_path': file_path}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tfile_type = None\n\tif prefix.endswith('-asset'):\n\t\tfile_type = 'asset'\n\telif prefix.endswith('-rendition'):\n\t\tfile_type = 'rendition'\n\telse:\n\t\tfile_type = 'other'\n\n\treturn {\n\t\t'file_type': file_type,\n\t\t'file_path': file_path\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tdef __key(idx, key):\n\t\treturn '{0}-{1}-{2}'.format(prefix, idx, key)\n\n\tidx = 0\n\tfor file in file_path:\n\t\tpackages[__key(idx, 'file')] = os.path.join(file_folder, file)\n\t\tpackages[__key(idx, 'type')] = 'asset'\n\t\tidx += 1\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tif prefix == 'assets':\n\t\treturn _eval_assets_file(file_path)\n\telif prefix == 'renditions':\n\t\treturn _eval_renditions_file(file_path)\n\telse:\n\t\traise Exception('Package file type not found.')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t_log_debug('_eval_file()')\n\tif prefix in files['assets']:\n\t\t_log_debug('\\tprefix in assets')\n\t\tpackages[prefix]['type'] = 'asset'\n\t\tpackages[prefix]['file'] = file_path\n\telif prefix in files['renditions']:\n\t\t_log_debug('\\tprefix in renditions')\n\t\tpackages[prefix]['type'] = 'rendition'\n\t\tpackages[prefix]['file'] = file_path\n\telse:\n\t\t_log_debug('\\tprefix not in assets or renditions')\n\t\tpackages[prefix]['type'] = None\n\t\tpackages[prefix]['file'] = None\n\n\t_log_debug('\\tpackages[{}] = {}'.format(prefix, packages[prefix]))\n\n\treturn packages\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "ques_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n\tif not lang in self.renditions:\n\t\tself.renditions[lang] = []\n\tself.renditions[lang].append(file_path)\n\n\n", "def add_rendition(self, lang, file_path):\n\twith open(file_path) as f:\n\t\tfile_data = f.read()\n\t\tfile_name = os.path.basename(file_path)\n\n\t\tif not self.renditions:\n\t\t\tself.renditions = {}\n\n\t\tself.renditions[lang] = file_name\n\t\tself.blobs[file_name] = file_data\n\n\n", "def add_rendition(self, lang, file_path):\n\tself.renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\tif lang not in self.renditions:\n\t\tself.renditions[lang] = []\n\tself.renditions[lang].append(file_path)\n\n\n", "def add_rendition(self, lang, file_path):\n\twith open(file_path, 'rb') as f:\n\t\tdata = f.read()\n\t\t#print(data)\n\t\tfile_name = os.path.basename(file_path)\n\t\t#print(file_name)\n\t\trendition = {\n\t\t\t\"lang\": lang,\n\t\t\t\"file_name\": file_name,\n\t\t\t\"file\": base64.b64encode(data).decode(\"utf-8\"),\n\t\t\t\"mime_type\": mimetypes.guess_type(file_path)[0]\n\t\t}\n\t\tself.renditions.append(rendition)\n\t\t#print(self.renditions)\n\t\treturn self\n\n\tdef set_abstract(self, abstract, lang):\n\t\tself.abstracts.append({\n\t\t\t\"lang\": lang,\n\t\t\t\"value\": abstract\n\t\t})\n\t\treturn self\n\n\tdef add_author(self, name, affiliation, orcid):\n\t\tauthor = {\n\t\t\t\"name\": name,\n\t\t\t\"\n", "def add_rendition(self, lang, file_path):\n\tself.renditions[lang] = file_path\n\n\tdef add_author(self, name, email, affiliation):\n\n\t\tauthor = Author()\n\t\tauthor.name = name\n\t\tauthor.email = email\n\t\tauthor.affiliation = affiliation\n\n\t\tself.authors.append(author)\n\n\tdef add_keyword(self, lang, keyword):\n\t\tself.keywords[lang] = keyword\n\n\tdef add_reviewer(self, name, email, affiliation):\n\n\t\treviewer = Reviewer()\n\t\treviewer.name = name\n\t\treviewer.email = email\n\t\treviewer.affiliation = affiliation\n\n\t\tself.reviewers.append(reviewer)\n\n\tdef add_subject(self, subject):\n\t\tself.subjects.append(subject)\n\n\tdef add_contribution(self, lang, title, abstract, keywords):\n\t\tself.contribution[lang] = {\n\t\t\t\t\t\t\t\t\t\"title\": title,\n\t\t\t\t\t\t\t\t\t\"abstract\": abstract,\n\t\t\t\t\t\t\t\t\t\"\n", "def add_rendition(self, lang, file_path):\n\tif lang not in self._renditions:\n\t\tself._renditions[lang] = file_path\n\telse:\n\t\traise ValueError(\"Rendition already exists\")\n\n\tif \"original\" not in self._renditions:\n\t\tself._renditions[\"original\"] = file_path\n\n\tself.save()\n\n\treturn self._renditions[lang]\n\n\n", "def add_rendition(self, lang, file_path):\n\tpass\n\n", "def add_rendition(self, lang, file_path):\n\tpass\n\n\n", "def add_rendition(self, lang, file_path):\n\tif lang not in self.renditions:\n\t\tself.renditions[lang] = file_path\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n\tif not lang in self.renditions:\n\t\tself.renditions[lang] = []\n\tself.renditions[lang].append(file_path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\twith open(file_path) as f:\n\t\tfile_data = f.read()\n\t\tfile_name = os.path.basename(file_path)\n\n\t\tif not self.renditions:\n\t\t\tself.renditions = {}\n\n\t\tself.renditions[lang] = file_name\n\t\tself.blobs[file_name] = file_data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself.renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tif lang not in self.renditions:\n\t\tself.renditions[lang] = []\n\tself.renditions[lang].append(file_path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\twith open(file_path, 'rb') as f:\n\t\tdata = f.read()\n\t\t#print(data)\n\t\tfile_name = os.path.basename(file_path)\n\t\t#print(file_name)\n\t\trendition = {\n\t\t\t\"lang\": lang,\n\t\t\t\"file_name\": file_name,\n\t\t\t\"file\": base64.b64encode(data).decode(\"utf-8\"),\n\t\t\t\"mime_type\": mimetypes.guess_type(file_path)[0]\n\t\t}\n\t\tself.renditions.append(rendition)\n\t\t#print(self.renditions)\n\t\treturn self\n\n\tdef set_abstract(self, abstract, lang):\n\t\tself.abstracts.append({\n\t\t\t\"lang\": lang,\n\t\t\t\"value\": abstract\n\t\t})\n\t\treturn self\n\n\tdef add_author(self, name, affiliation, orcid):\n\t\tauthor = {\n\t\t\t\"name\": name,\n\t\t\t\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself.renditions[lang] = file_path\n\n\tdef add_author(self, name, email, affiliation):\n\n\t\tauthor = Author()\n\t\tauthor.name = name\n\t\tauthor.email = email\n\t\tauthor.affiliation = affiliation\n\n\t\tself.authors.append(author)\n\n\tdef add_keyword(self, lang, keyword):\n\t\tself.keywords[lang] = keyword\n\n\tdef add_reviewer(self, name, email, affiliation):\n\n\t\treviewer = Reviewer()\n\t\treviewer.name = name\n\t\treviewer.email = email\n\t\treviewer.affiliation = affiliation\n\n\t\tself.reviewers.append(reviewer)\n\n\tdef add_subject(self, subject):\n\t\tself.subjects.append(subject)\n\n\tdef add_contribution(self, lang, title, abstract, keywords):\n\t\tself.contribution[lang] = {\n\t\t\t\t\t\t\t\t\t\"title\": title,\n\t\t\t\t\t\t\t\t\t\"abstract\": abstract,\n\t\t\t\t\t\t\t\t\t\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tif lang not in self._renditions:\n\t\tself._renditions[lang] = file_path\n\telse:\n\t\traise ValueError(\"Rendition already exists\")\n\n\tif \"original\" not in self._renditions:\n\t\tself._renditions[\"original\"] = file_path\n\n\tself.save()\n\n\treturn self._renditions[lang]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tpass\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tif lang not in self.renditions:\n\t\tself.renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "ques_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n\tself.assets[basename] = file_path\n\n\tdef get_asset(self, basename):\n\t\treturn self.assets[basename]\n\n\n\tdef add_asset_file(self, file_path):\n\t\tbasename = os.path.basename(file_path)\n\t\tself.add_asset(basename, file_path)\n\n\tdef add_assets_from_directory(self, directory):\n\t\tfor file_path in os.listdir(directory):\n\t\t\tfile_path = os.path.join(directory, file_path)\n\t\t\tself.add_asset_file(file_path)\n\n\tdef get_substitutions(self):\n\t\treturn {\n\t\t\t'assets': self.assets\n\t\t}\n\n\tdef get_substitutions_from_string(self, string):\n\t\tsubstitutions = self.get_substitutions()\n\t\tsubstitutions.update(self.substitutions)\n\t\treturn string.format(**substitutions)\n\n\tdef get_sub\n", "def add_asset(self, basename, file_path):\n\tif basename not in self.assets:\n\t\tself.assets[basename] = file_path\n\n\treturn self.assets[basename]\n\n", "def add_asset(self, basename, file_path):\n\tself.assets[basename] = file_path\n\n", "def add_asset(self, basename, file_path):\n\tself.assets[basename] = file_path\n\n\t# TODO:\n\t# - check if file is there\n\t# - check if extension is valid\n\t# - check if there is already a file with that name\n\n\n", "def add_asset(self, basename, file_path):\n\tif not basename in self.assets:\n\t\tself.assets[basename] = {}\n\tself.assets[basename][os.path.splitext(file_path)[1]] = file_path\n\n\n", "def add_asset(self, basename, file_path):\n\tif basename not in self.assets:\n\t\tself.assets[basename] = {}\n\tself.assets[basename][file_path.suffix] = file_path\n\n\n", "def add_asset(self, basename, file_path):\n\tassets = self.document.get('assets', {})\n\tassets[basename] = file_path\n\tself.document['assets'] = assets\n\treturn True\n\n", "def add_asset(self, basename, file_path):\n\tif os.path.exists(file_path):\n\t\tself.assets[basename] = file_path\n\n\treturn self\n\n", "def add_asset(self, basename, file_path):\n\tif not basename in self.assets:\n\t\tself.assets[basename] = {}\n\tself.assets[basename][os.path.splitext(file_path)[1]] = file_path\n\tself.save()\n\n\treturn True\n\n", "def add_asset(self, basename, file_path):\n\tif basename not in self.assets:\n\t\tself.assets[basename] = []\n\n\tif file_path not in self.assets[basename]:\n\t\tself.assets[basename].append(file_path)\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n\tself.assets[basename] = file_path\n\n\tdef get_asset(self, basename):\n\t\treturn self.assets[basename]\n\n\n\tdef add_asset_file(self, file_path):\n\t\tbasename = os.path.basename(file_path)\n\t\tself.add_asset(basename, file_path)\n\n\tdef add_assets_from_directory(self, directory):\n\t\tfor file_path in os.listdir(directory):\n\t\t\tfile_path = os.path.join(directory, file_path)\n\t\t\tself.add_asset_file(file_path)\n\n\tdef get_substitutions(self):\n\t\treturn {\n\t\t\t'assets': self.assets\n\t\t}\n\n\tdef get_substitutions_from_string(self, string):\n\t\tsubstitutions = self.get_substitutions()\n\t\tsubstitutions.update(self.substitutions)\n\t\treturn string.format(**substitutions)\n\n\tdef get_sub\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tif basename not in self.assets:\n\t\tself.assets[basename] = file_path\n\n\treturn self.assets[basename]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself.assets[basename] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself.assets[basename] = file_path\n\n\t# TODO:\n\t# - check if file is there\n\t# - check if extension is valid\n\t# - check if there is already a file with that name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tif not basename in self.assets:\n\t\tself.assets[basename] = {}\n\tself.assets[basename][os.path.splitext(file_path)[1]] = file_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tif basename not in self.assets:\n\t\tself.assets[basename] = {}\n\tself.assets[basename][file_path.suffix] = file_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tassets = self.document.get('assets', {})\n\tassets[basename] = file_path\n\tself.document['assets'] = assets\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tif os.path.exists(file_path):\n\t\tself.assets[basename] = file_path\n\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tif not basename in self.assets:\n\t\tself.assets[basename] = {}\n\tself.assets[basename][os.path.splitext(file_path)[1]] = file_path\n\tself.save()\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tif basename not in self.assets:\n\t\tself.assets[basename] = []\n\n\tif file_path not in self.assets[basename]:\n\t\tself.assets[basename].append(file_path)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "ques_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n\tzipf = zipfile.ZipFile(zip_path)\n\txml_files = [x for x in zipf.namelist() if x.endswith('xml')]\n\txml_files_data = {}\n\tfor xml_file in xml_files:\n\t\twith zipf.open(xml_file, 'r') as f:\n\t\t\txml_file_name = os.path.basename(xml_file)\n\t\t\txml_file_data = f.read()\n\t\t\txml_files_data[xml_file_name] = xml_file_data\n\treturn xml_files_data\n\n\n", "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\treturn {\n\t\t\tbasename: zf.read(name)\n\t\t\tfor name in zf.namelist()\n\t\t\tfor basename in _get_package_basename(name)\n\t\t}\n\n\n", "def _explore_zipfile(zip_path):\n\tfiles = {}\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor name in zf.namelist():\n\t\t\tpath = os.path.normpath(name)\n\t\t\tif not os.path.basename(path):\n\t\t\t\tcontinue\n\t\t\tbasename = os.path.basename(path)\n\t\t\tbasename = os.path.splitext(basename)[0]\n\t\t\tif basename in files:\n\t\t\t\tcontinue\n\t\t\tfiles[basename] = {}\n\t\t\tfiles[basename]['path'] = path\n\t\t\tfiles[basename]['xml'] = _extract_xml(zf.open(path))\n\t\t\tfiles[basename]['xml_path'] = os.path.dirname(path)\n\t\t\tfiles[basename]['type'] = 'zip'\n\treturn files\n\n\n", "def _explore_zipfile(zip_path):\n\t# Getting zip file\n\t# zf = zipfile.ZipFile(zip_path, 'r')\n\t# Getting xmls from zip\n\t# xmls = [f for f in zf.namelist() if f.endswith('.xml')]\n\n\t# Getting data from xmls\n\t# data = [_get_xml_data(zf.open(f)) for f in xmls]\n\n\t# Grouping data by basename\n\t# data_by_basename = {}\n\t# for d in data:\n\t# \tbasename = d['basename']\n\t# \tif basename not in data_by_basename:\n\t# \t\tdata_by_basename[basename] = []\n\t# \tdata_by_basename[basename].append(d)\n\n\t# zf.close()\n\n\t# return data_by_basename\n\treturn {}\n\n\n", "def _explore_zipfile(zip_path):\n\t# TODO: make it work for both python2 and python3\n\t# TODO: handle multiple files with same basename\n\t# TODO: handle files without valid XML\n\t# TODO: handle files without valid XML (name, version, summary, description)\n\t# TODO: handle files without valid XML (group, license, url)\n\t# TODO: handle files without valid XML (author, email)\n\t# TODO: handle files without valid XML (icon)\n\t# TODO: handle files without valid XML (screenshots)\n\t# TODO: handle files without valid XML (release)\n\t# TODO: handle files without valid XML (compatibility)\n\n\tzip_file = zipfile.ZipFile(zip_path)\n\txml_files = zip_file.namelist()\n\n\tfiles_by_basename = defaultdict(list)\n\tfor xml_file in xml_files:\n\t\tbasename = os.path.basename(xml_file)\n\t\tfiles_by_basename[basename].append(xml_file)\n\n\tpackages = {}\n\tfor basename, files in files_by\n", "def _explore_zipfile(zip_path):\n\tzip_data = {}\n\twith ZipFile(zip_path, 'r') as zip_file:\n\t\tfor zip_file_name in zip_file.namelist():\n\t\t\tif zip_file_name.endswith('.xml'):\n\t\t\t\txml_name = os.path.basename(zip_file_name)\n\t\t\t\txml_data = zip_file.read(zip_file_name).decode('utf-8')\n\t\t\t\tzip_data[xml_name] = xml_data\n\treturn zip_data\n\n\n", "def _explore_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path)\n\tgrouped_files = _group_files(zip_file.namelist())\n\tpackages = _get_packages_data(zip_file, grouped_files)\n\treturn packages\n\n\n", "def _explore_zipfile(zip_path):\n\t# Open zip file\n\tzipfile = zipfile.ZipFile(zip_path)\n\n\t# Find XML files\n\txml_files = [filename for filename in zipfile.namelist() if filename.endswith('.xml')]\n\txml_basenames = [os.path.basename(filename).split('.')[0] for filename in xml_files]\n\n\t# Group files by their basename\n\tgrouped_files = {}\n\tfor xml_basename in xml_basenames:\n\t\tgrouped_files[xml_basename] = [filename for filename in xml_files if xml_basename in filename]\n\n\t# Get data from files\n\tdata = {}\n\tfor group in grouped_files.keys():\n\t\tdata[group] = _get_package_data(zipfile, grouped_files[group])\n\n\treturn data\n\n\n", "def _explore_zipfile(zip_path):\n\tif not os.path.exists(zip_path):\n\t\treturn None\n\n\twith zipfile.ZipFile(zip_path) as f:\n\t\tzipped_files = f.namelist()\n\t\textracted_files = {}\n\t\tfor zipped_file in zipped_files:\n\t\t\tif zipped_file.startswith('__MACOSX'):\n\t\t\t\tcontinue\n\t\t\tif zipped_file.endswith('/'):\n\t\t\t\tcontinue\n\t\t\tif not zipped_file.endswith('.xml'):\n\t\t\t\tcontinue\n\n\t\t\txml_basename = zipped_file.split('/')[-1].split('.')[0]\n\t\t\tif xml_basename not in extracted_files:\n\t\t\t\textracted_files[xml_basename] = []\n\t\t\textracted_files[xml_basename].append(zipped_file)\n\n\t\tzip_data = {}\n\t\tfor xml_basename, xml_files in extracted_files.items():\n\t\t\t\n", "def _explore_zipfile(zip_path):\n\tlogger.debug('_explore_zipfile: reading {0}'.format(zip_path))\n\t# read zip file and get only xml files\n\tzip_files = [f for f in zipfile.ZipFile(zip_path, 'r').namelist() if f.endswith('.xml')]\n\tlogger.debug('_explore_zipfile: {0} xml files in {1}'.format(len(zip_files), zip_path))\n\t# group files by their basename\n\tfiles = {}\n\tfor f in zip_files:\n\t\tbasename = os.path.basename(f)\n\t\tif basename not in files:\n\t\t\tfiles[basename] = [f]\n\t\telse:\n\t\t\tfiles[basename].append(f)\n\t# get data from files\n\tdata = {}\n\tfor f in files:\n\t\tlogger.debug('_explore_zipfile: getting data from {0}'.format(f))\n\t\tdata[f] = _get_data(_read_xml(zip_path, files[f\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n\tzipf = zipfile.ZipFile(zip_path)\n\txml_files = [x for x in zipf.namelist() if x.endswith('xml')]\n\txml_files_data = {}\n\tfor xml_file in xml_files:\n\t\twith zipf.open(xml_file, 'r') as f:\n\t\t\txml_file_name = os.path.basename(xml_file)\n\t\t\txml_file_data = f.read()\n\t\t\txml_files_data[xml_file_name] = xml_file_data\n\treturn xml_files_data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\treturn {\n\t\t\tbasename: zf.read(name)\n\t\t\tfor name in zf.namelist()\n\t\t\tfor basename in _get_package_basename(name)\n\t\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tfiles = {}\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor name in zf.namelist():\n\t\t\tpath = os.path.normpath(name)\n\t\t\tif not os.path.basename(path):\n\t\t\t\tcontinue\n\t\t\tbasename = os.path.basename(path)\n\t\t\tbasename = os.path.splitext(basename)[0]\n\t\t\tif basename in files:\n\t\t\t\tcontinue\n\t\t\tfiles[basename] = {}\n\t\t\tfiles[basename]['path'] = path\n\t\t\tfiles[basename]['xml'] = _extract_xml(zf.open(path))\n\t\t\tfiles[basename]['xml_path'] = os.path.dirname(path)\n\t\t\tfiles[basename]['type'] = 'zip'\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t# Getting zip file\n\t# zf = zipfile.ZipFile(zip_path, 'r')\n\t# Getting xmls from zip\n\t# xmls = [f for f in zf.namelist() if f.endswith('.xml')]\n\n\t# Getting data from xmls\n\t# data = [_get_xml_data(zf.open(f)) for f in xmls]\n\n\t# Grouping data by basename\n\t# data_by_basename = {}\n\t# for d in data:\n\t# \tbasename = d['basename']\n\t# \tif basename not in data_by_basename:\n\t# \t\tdata_by_basename[basename] = []\n\t# \tdata_by_basename[basename].append(d)\n\n\t# zf.close()\n\n\t# return data_by_basename\n\treturn {}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t# TODO: make it work for both python2 and python3\n\t# TODO: handle multiple files with same basename\n\t# TODO: handle files without valid XML\n\t# TODO: handle files without valid XML (name, version, summary, description)\n\t# TODO: handle files without valid XML (group, license, url)\n\t# TODO: handle files without valid XML (author, email)\n\t# TODO: handle files without valid XML (icon)\n\t# TODO: handle files without valid XML (screenshots)\n\t# TODO: handle files without valid XML (release)\n\t# TODO: handle files without valid XML (compatibility)\n\n\tzip_file = zipfile.ZipFile(zip_path)\n\txml_files = zip_file.namelist()\n\n\tfiles_by_basename = defaultdict(list)\n\tfor xml_file in xml_files:\n\t\tbasename = os.path.basename(xml_file)\n\t\tfiles_by_basename[basename].append(xml_file)\n\n\tpackages = {}\n\tfor basename, files in files_by\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tzip_data = {}\n\twith ZipFile(zip_path, 'r') as zip_file:\n\t\tfor zip_file_name in zip_file.namelist():\n\t\t\tif zip_file_name.endswith('.xml'):\n\t\t\t\txml_name = os.path.basename(zip_file_name)\n\t\t\t\txml_data = zip_file.read(zip_file_name).decode('utf-8')\n\t\t\t\tzip_data[xml_name] = xml_data\n\treturn zip_data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path)\n\tgrouped_files = _group_files(zip_file.namelist())\n\tpackages = _get_packages_data(zip_file, grouped_files)\n\treturn packages\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\t# Open zip file\n\tzipfile = zipfile.ZipFile(zip_path)\n\n\t# Find XML files\n\txml_files = [filename for filename in zipfile.namelist() if filename.endswith('.xml')]\n\txml_basenames = [os.path.basename(filename).split('.')[0] for filename in xml_files]\n\n\t# Group files by their basename\n\tgrouped_files = {}\n\tfor xml_basename in xml_basenames:\n\t\tgrouped_files[xml_basename] = [filename for filename in xml_files if xml_basename in filename]\n\n\t# Get data from files\n\tdata = {}\n\tfor group in grouped_files.keys():\n\t\tdata[group] = _get_package_data(zipfile, grouped_files[group])\n\n\treturn data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tif not os.path.exists(zip_path):\n\t\treturn None\n\n\twith zipfile.ZipFile(zip_path) as f:\n\t\tzipped_files = f.namelist()\n\t\textracted_files = {}\n\t\tfor zipped_file in zipped_files:\n\t\t\tif zipped_file.startswith('__MACOSX'):\n\t\t\t\tcontinue\n\t\t\tif zipped_file.endswith('/'):\n\t\t\t\tcontinue\n\t\t\tif not zipped_file.endswith('.xml'):\n\t\t\t\tcontinue\n\n\t\t\txml_basename = zipped_file.split('/')[-1].split('.')[0]\n\t\t\tif xml_basename not in extracted_files:\n\t\t\t\textracted_files[xml_basename] = []\n\t\t\textracted_files[xml_basename].append(zipped_file)\n\n\t\tzip_data = {}\n\t\tfor xml_basename, xml_files in extracted_files.items():\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tlogger.debug('_explore_zipfile: reading {0}'.format(zip_path))\n\t# read zip file and get only xml files\n\tzip_files = [f for f in zipfile.ZipFile(zip_path, 'r').namelist() if f.endswith('.xml')]\n\tlogger.debug('_explore_zipfile: {0} xml files in {1}'.format(len(zip_files), zip_path))\n\t# group files by their basename\n\tfiles = {}\n\tfor f in zip_files:\n\t\tbasename = os.path.basename(f)\n\t\tif basename not in files:\n\t\t\tfiles[basename] = [f]\n\t\telse:\n\t\t\tfiles[basename].append(f)\n\t# get data from files\n\tdata = {}\n\tfor f in files:\n\t\tlogger.debug('_explore_zipfile: getting data from {0}'.format(f))\n\t\tdata[f] = _get_data(_read_xml(zip_path, files[f\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "ques_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor info in zf.infolist():\n\t\t\tfiles.append(info.filename)\n\treturn files\n\n\n", "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\tfor file in zip_file.namelist():\n\t\t\tfiles.append(file)\n\treturn files\n\n\n", "def files_list_from_zipfile(zip_path):\n\tzip_path = os.path.abspath(zip_path)\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, mode='r') as f:\n\t\treturn f.namelist()\n\n\n", "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor info in zf.infolist():\n\t\t\tif info.filename.startswith('__MACOSX'):\n\t\t\t\tcontinue\n\t\t\tfiles.append(info.filename)\n\treturn files\n\n\n", "def files_list_from_zipfile(zip_path):\n\tif not zipfile.is_zipfile(zip_path):\n\t\traise ValueError(\"Invalid zip file: %s\" % (zip_path, ))\n\n\tzip_file = zipfile.ZipFile(zip_path)\n\tfiles = zip_file.namelist()\n\tzip_file.close()\n\n\treturn files\n\n", "def files_list_from_zipfile(zip_path):\n\twith ZipFile(zip_path, 'r') as zip_file:\n\t\treturn zip_file.namelist()\n\n\n", "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\treturn zip_file.namelist()\n\n\n", "def files_list_from_zipfile(zip_path):\n\twith ZipFile(zip_path) as zip_file:\n\t\treturn zip_file.namelist()\n\n\n", "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as z:\n\t\tfiles = z.namelist()\n\treturn files\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor info in zf.infolist():\n\t\t\tfiles.append(info.filename)\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\tfor file in zip_file.namelist():\n\t\t\tfiles.append(file)\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tzip_path = os.path.abspath(zip_path)\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, mode='r') as f:\n\t\treturn f.namelist()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfor info in zf.infolist():\n\t\t\tif info.filename.startswith('__MACOSX'):\n\t\t\t\tcontinue\n\t\t\tfiles.append(info.filename)\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tif not zipfile.is_zipfile(zip_path):\n\t\traise ValueError(\"Invalid zip file: %s\" % (zip_path, ))\n\n\tzip_file = zipfile.ZipFile(zip_path)\n\tfiles = zip_file.namelist()\n\tzip_file.close()\n\n\treturn files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\twith ZipFile(zip_path, 'r') as zip_file:\n\t\treturn zip_file.namelist()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\treturn zip_file.namelist()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\twith ZipFile(zip_path) as zip_file:\n\t\treturn zip_file.namelist()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path, 'r') as z:\n\t\tfiles = z.namelist()\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "ques_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=\"', 'w-st=\"')\n\n", "def fix_namespace_prefix_w(content):\n\treturn re.sub(r'(<w:)([^<>]+?)([ =>])', r'\\1w-\\2\\3', content)\n\n", "def fix_namespace_prefix_w(content):\n\tcontent = re.sub(r'(<w:st=\"[^\"]+\")', r'\\1 w:', content)\n\tcontent = re.sub(r'(<w:st=\"[^\"]+\")', r'\\1 w:', content)\n\treturn content\n\n", "def fix_namespace_prefix_w(content):\n\tcontent = re.sub(r'(?<=w:st=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsid=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidR=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRDefault=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRPr=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRDefault=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidDel=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidP=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRDefault=)\\\"', '-', content)\n\tcontent = re\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=\"', 'w-st=\"')\n\n", "def fix_namespace_prefix_w(content):\n\tcontent = re.sub(r'(?s)(<w:st[^>]*?)(st=\"[^\"]+?\")', r'\\1w-\\2', content)\n\treturn content\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=\"', 'w-st=\"')\n\n\n", "def fix_namespace_prefix_w(content):\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n\n", "def fix_namespace_prefix_w(content):\n\tcontent=content.replace(u'w:st=\"',u'w-st=\"')\n\tcontent=content.replace(u'w:val=\"',u'w-val=\"')\n\tcontent=content.replace(u'w:rsidRDefault=\"',u'w-rsidRDefault=\"')\n\tcontent=content.replace(u'w:rsidR=\"',u'w-rsidR=\"')\n\tcontent=content.replace(u'w:rsidRPr=\"',u'w-rsidRPr=\"')\n\tcontent=content.replace(u'w:rsidDel=\"',u'w-rsidDel=\"')\n\tcontent=content.replace(u'w:rsidP=\"',u'w-rsidP=\"')\n\tcontent=content.replace(u'w:rsidRFonts=\"',u'w-rsidRFonts=\"')\n\tcontent=content.replace(u'w:rsidSect=\"',u'w-rsidSect=\"')\n\tcontent=content.replace(u'w:rsidTr=\"',u'w-rs\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace(' w:st=\"', ' w-st=\"')\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=\"', 'w-st=\"')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn re.sub(r'(<w:)([^<>]+?)([ =>])', r'\\1w-\\2\\3', content)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\tcontent = re.sub(r'(<w:st=\"[^\"]+\")', r'\\1 w:', content)\n\tcontent = re.sub(r'(<w:st=\"[^\"]+\")', r'\\1 w:', content)\n\treturn content\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\tcontent = re.sub(r'(?<=w:st=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsid=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidR=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRDefault=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRPr=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRDefault=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidDel=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidP=)\\\"', '-', content)\n\tcontent = re.sub(r'(?<=w:rsidRDefault=)\\\"', '-', content)\n\tcontent = re\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=\"', 'w-st=\"')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\tcontent = re.sub(r'(?s)(<w:st[^>]*?)(st=\"[^\"]+?\")', r'\\1w-\\2', content)\n\treturn content\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=\"', 'w-st=\"')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\tcontent = content.replace('w:st=\"', 'w-st=\"')\n\treturn content\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\tcontent=content.replace(u'w:st=\"',u'w-st=\"')\n\tcontent=content.replace(u'w:val=\"',u'w-val=\"')\n\tcontent=content.replace(u'w:rsidRDefault=\"',u'w-rsidRDefault=\"')\n\tcontent=content.replace(u'w:rsidR=\"',u'w-rsidR=\"')\n\tcontent=content.replace(u'w:rsidRPr=\"',u'w-rsidRPr=\"')\n\tcontent=content.replace(u'w:rsidDel=\"',u'w-rsidDel=\"')\n\tcontent=content.replace(u'w:rsidP=\"',u'w-rsidP=\"')\n\tcontent=content.replace(u'w:rsidRFonts=\"',u'w-rsidRFonts=\"')\n\tcontent=content.replace(u'w:rsidSect=\"',u'w-rsidSect=\"')\n\tcontent=content.replace(u'w:rsidTr=\"',u'w-rs\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace(' w:st=\"', ' w-st=\"')\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "ques_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0].text\n\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor p in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(p)[0]\n\t\t\tif pubdate:\n\t\t\t\treturn pubdate\n\t\texcept:\n\t\t\tpass\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(pubdate_xpath)[0].strip()\n\t\texcept IndexError:\n\t\t\tpass\n\treturn ''\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif len(pubdate) > 0:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.find(pubdate_xpath)\n\t\tif pubdate is not None:\n\t\t\tpubdate = pubdate.text\n\t\t\treturn pubdate\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn \"\"\n\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tdate = node.xpath(pubdate_xpath)\n\t\tif len(date) > 0:\n\t\t\treturn date[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor p in pubdate_xpaths:\n\t\tpubdate = node.xpath(p)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0].text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor p in pubdate_xpaths:\n\t\ttry:\n\t\t\tpubdate = node.xpath(p)[0]\n\t\t\tif pubdate:\n\t\t\t\treturn pubdate\n\t\texcept:\n\t\t\tpass\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\ttry:\n\t\t\treturn node.xpath(pubdate_xpath)[0].strip()\n\t\texcept IndexError:\n\t\t\tpass\n\treturn ''\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif len(pubdate) > 0:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.find(pubdate_xpath)\n\t\tif pubdate is not None:\n\t\t\tpubdate = pubdate.text\n\t\t\treturn pubdate\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn \"\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tdate = node.xpath(pubdate_xpath)\n\t\tif len(date) > 0:\n\t\t\treturn date[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor p in pubdate_xpaths:\n\t\tpubdate = node.xpath(p)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "ques_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n\tif issue is None:\n\t\treturn (None, None)\n\n\tif issue.text is not None:\n\t\tm = re.search(r'(?P<number>\\d+)\\((?P<suppl>\\d+)\\)', issue.text)\n\t\tif m is not None:\n\t\t\treturn m.group('number', 'suppl')\n\t\telse:\n\t\t\treturn (issue.text, None)\n\telse:\n\t\treturn (None, None)\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t# number\n\tnumber = None\n\tnumber_value = None\n\tif 'number' in issue:\n\t\tnumber = issue['number']\n\t\tif number is not None:\n\t\t\tnumber_value = number.get_value()\n\t\t\tif number_value is None:\n\t\t\t\tnumber_value = number.get_text()\n\n\t# suppl\n\tsuppl = None\n\tsuppl_value = None\n\tif 'suppl' in issue:\n\t\tsuppl = issue['suppl']\n\t\tif suppl is not None:\n\t\t\tsuppl_value = suppl.get_value()\n\t\t\tif suppl_value is None:\n\t\t\t\tsuppl_value = suppl.get_text()\n\n\tif number_value is not None and suppl_value is not None:\n\t\tif number_value.strip() == '':\n\t\t\tnumber_value = None\n\t\tif suppl_value.strip() == '':\n\t\t\tsuppl_value = None\n\n\tif number_value is None and suppl_value is not None:\n\t\tnumber_value =\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t# number\n\tnumber = _extract_number_from_issue_element(issue)\n\tif number:\n\t\tnumber = number[0]\n\n\t# suppl\n\tsuppl = _extract_suppl_from_issue_element(issue)\n\tif suppl:\n\t\tsuppl = suppl[0]\n\n\treturn number, suppl\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsuppl = None\n\tfor child in issue.children:\n\t\tif isinstance(child, lxml.etree._Element):\n\t\t\tif child.tag == 'volume':\n\t\t\t\tnumber = child.text\n\t\t\telif child.tag == 'supplement':\n\t\t\t\tsuppl = child.text\n\t\t\telse:\n\t\t\t\traise ValueError('Unexpected child %s of issue' % child.tag)\n\treturn number, suppl\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber_value = None\n\tsuppl_value = None\n\n\t# Try to extract number value from the text contents of issue\n\tnumber_match = re.search(r'([0-9]+)', issue.text)\n\tif number_match:\n\t\tnumber_value = number_match.group(0)\n\n\t# Try to extract suppl value from the text contents of issue\n\tsuppl_match = re.search(r'\\((.+)\\)', issue.text)\n\tif suppl_match:\n\t\tsuppl_value = suppl_match.group(1)\n\n\treturn (number_value, suppl_value)\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tassert issue is not None\n\n\tissue_type = issue.attrib.get('publication_type', None)\n\tif issue_type is not None:\n\t\tissue_type = issue_type.lower()\n\n\tnumber = None\n\tsuppl = None\n\n\tif issue_type == 'supplement':\n\t\tsuppl = ''\n\telif issue_type == 'retraction':\n\t\tsuppl = 'ret'\n\telif issue_type == 'correction':\n\t\tsuppl = 'corr'\n\n\tif issue_type is None or issue_type == 'regular':\n\t\tnumber = issue.text\n\n\tif number is None:\n\t\tnumber = ''\n\n\treturn (number, suppl)\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_text = issue.text or \"\".join([x.text or \"\" for x in issue.contents])\n\t# get the number\n\tnumber = re.search(\"\\d+\", issue_text)\n\tif number:\n\t\tnumber = number.group(0)\n\telse:\n\t\tnumber = \"\"\n\n\t# get the suppl\n\tsuppl = re.search(\"suppl\\s*([IVXLC]+|\\d+)\", issue_text, re.IGNORECASE)\n\tif suppl:\n\t\tsuppl = suppl.group(1)\n\telse:\n\t\tsuppl = \"\"\n\treturn (number, suppl)\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_elements = issue.get_children()\n\tissue_number = None\n\tsuppl = None\n\tfor element in issue_elements:\n\t\tif element.get_name() == \"issue\":\n\t\t\tissue_number = element.get_value()\n\t\telif element.get_name() == \"supplement\":\n\t\t\tsuppl = element.get_value()\n\treturn issue_number, suppl\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t# print(issue)\n\tnumber, suppl = None, None\n\tif issue.tag == 'issue':\n\t\tnumber = issue.get('number')\n\t\tsuppl = issue.get('supplement')\n\t\tif number is None:\n\t\t\tnumber = issue.get('volume')\n\telif issue.tag == 'volume':\n\t\tnumber = issue.text\n\t\tsuppl = issue.get('supplement')\n\telse:\n\t\traise NotImplementedError('Unknown issue tag: %s' % issue.tag)\n\treturn number, suppl\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_text = issue.text\n\tfor match in re.finditer(r'\\(?(no\\.?\\s*|n\\.?\\s*|num\\.?\\s*|n\u00fam\\.?\\s*)?(\\d+)\\)?', issue_text, re.I):\n\t\treturn (match.group(2), None)\n\tfor match in re.finditer(r'\\(?(no\\.?\\s*|n\\.?\\s*|num\\.?\\s*|n\u00fam\\.?\\s*)?(\\d+)(?:[^\\d]*(?:sup\\.?\\s*|suppl\\.?\\s*|supplement\\.?\\s*|suppl\u00e9ment\\.?\\s*|s\\.?\\s*|sppl\\.?\\s*))?(\\d*)\\)?', issue_text, re.I):\n\t\treturn (match.group(2), match.group(3))\n\treturn (None, None)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tif issue is None:\n\t\treturn (None, None)\n\n\tif issue.text is not None:\n\t\tm = re.search(r'(?P<number>\\d+)\\((?P<suppl>\\d+)\\)', issue.text)\n\t\tif m is not None:\n\t\t\treturn m.group('number', 'suppl')\n\t\telse:\n\t\t\treturn (issue.text, None)\n\telse:\n\t\treturn (None, None)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t# number\n\tnumber = None\n\tnumber_value = None\n\tif 'number' in issue:\n\t\tnumber = issue['number']\n\t\tif number is not None:\n\t\t\tnumber_value = number.get_value()\n\t\t\tif number_value is None:\n\t\t\t\tnumber_value = number.get_text()\n\n\t# suppl\n\tsuppl = None\n\tsuppl_value = None\n\tif 'suppl' in issue:\n\t\tsuppl = issue['suppl']\n\t\tif suppl is not None:\n\t\t\tsuppl_value = suppl.get_value()\n\t\t\tif suppl_value is None:\n\t\t\t\tsuppl_value = suppl.get_text()\n\n\tif number_value is not None and suppl_value is not None:\n\t\tif number_value.strip() == '':\n\t\t\tnumber_value = None\n\t\tif suppl_value.strip() == '':\n\t\t\tsuppl_value = None\n\n\tif number_value is None and suppl_value is not None:\n\t\tnumber_value =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t# number\n\tnumber = _extract_number_from_issue_element(issue)\n\tif number:\n\t\tnumber = number[0]\n\n\t# suppl\n\tsuppl = _extract_suppl_from_issue_element(issue)\n\tif suppl:\n\t\tsuppl = suppl[0]\n\n\treturn number, suppl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsuppl = None\n\tfor child in issue.children:\n\t\tif isinstance(child, lxml.etree._Element):\n\t\t\tif child.tag == 'volume':\n\t\t\t\tnumber = child.text\n\t\t\telif child.tag == 'supplement':\n\t\t\t\tsuppl = child.text\n\t\t\telse:\n\t\t\t\traise ValueError('Unexpected child %s of issue' % child.tag)\n\treturn number, suppl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber_value = None\n\tsuppl_value = None\n\n\t# Try to extract number value from the text contents of issue\n\tnumber_match = re.search(r'([0-9]+)', issue.text)\n\tif number_match:\n\t\tnumber_value = number_match.group(0)\n\n\t# Try to extract suppl value from the text contents of issue\n\tsuppl_match = re.search(r'\\((.+)\\)', issue.text)\n\tif suppl_match:\n\t\tsuppl_value = suppl_match.group(1)\n\n\treturn (number_value, suppl_value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tassert issue is not None\n\n\tissue_type = issue.attrib.get('publication_type', None)\n\tif issue_type is not None:\n\t\tissue_type = issue_type.lower()\n\n\tnumber = None\n\tsuppl = None\n\n\tif issue_type == 'supplement':\n\t\tsuppl = ''\n\telif issue_type == 'retraction':\n\t\tsuppl = 'ret'\n\telif issue_type == 'correction':\n\t\tsuppl = 'corr'\n\n\tif issue_type is None or issue_type == 'regular':\n\t\tnumber = issue.text\n\n\tif number is None:\n\t\tnumber = ''\n\n\treturn (number, suppl)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_text = issue.text or \"\".join([x.text or \"\" for x in issue.contents])\n\t# get the number\n\tnumber = re.search(\"\\d+\", issue_text)\n\tif number:\n\t\tnumber = number.group(0)\n\telse:\n\t\tnumber = \"\"\n\n\t# get the suppl\n\tsuppl = re.search(\"suppl\\s*([IVXLC]+|\\d+)\", issue_text, re.IGNORECASE)\n\tif suppl:\n\t\tsuppl = suppl.group(1)\n\telse:\n\t\tsuppl = \"\"\n\treturn (number, suppl)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_elements = issue.get_children()\n\tissue_number = None\n\tsuppl = None\n\tfor element in issue_elements:\n\t\tif element.get_name() == \"issue\":\n\t\t\tissue_number = element.get_value()\n\t\telif element.get_name() == \"supplement\":\n\t\t\tsuppl = element.get_value()\n\treturn issue_number, suppl\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t# print(issue)\n\tnumber, suppl = None, None\n\tif issue.tag == 'issue':\n\t\tnumber = issue.get('number')\n\t\tsuppl = issue.get('supplement')\n\t\tif number is None:\n\t\t\tnumber = issue.get('volume')\n\telif issue.tag == 'volume':\n\t\tnumber = issue.text\n\t\tsuppl = issue.get('supplement')\n\telse:\n\t\traise NotImplementedError('Unknown issue tag: %s' % issue.tag)\n\treturn number, suppl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_text = issue.text\n\tfor match in re.finditer(r'\\(?(no\\.?\\s*|n\\.?\\s*|num\\.?\\s*|n\u00fam\\.?\\s*)?(\\d+)\\)?', issue_text, re.I):\n\t\treturn (match.group(2), None)\n\tfor match in re.finditer(r'\\(?(no\\.?\\s*|n\\.?\\s*|num\\.?\\s*|n\u00fam\\.?\\s*)?(\\d+)(?:[^\\d]*(?:sup\\.?\\s*|suppl\\.?\\s*|supplement\\.?\\s*|suppl\u00e9ment\\.?\\s*|s\\.?\\s*|sppl\\.?\\s*))?(\\d*)\\)?', issue_text, re.I):\n\t\treturn (match.group(2), match.group(3))\n\treturn (None, None)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "ques_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn '<%s %s>' % (\n\t\t\tself.__class__.__name__,\n\t\t\t', '.join(['%s=\"%s\"' % (k, v) for k, v in self.__dict__.items()]),\n\t\t)\n\ts = ''\n\tfor i in range(indent):\n\t\ts += '\\t'\n\ts += '<%s ' % self.__class__.__name__\n\tfor k, v in self.__dict__.items():\n\t\tif k == 'parent':\n\t\t\tcontinue\n\t\tif isinstance(v, list):\n\t\t\ts += '%s=[' % k\n\t\t\tfor i in v:\n\t\t\t\tif not isinstance(i, Pretty):\n\t\t\t\t\ts += '%s, ' % i\n\t\t\t\telse:\n\t\t\t\t\ts += '%s, ' % i.pretty(indent + 1)\n\t\t\ts += '] '\n\t\telif not isinstance(v, Pretty):\n", "def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn '{0}{1} {2} {3}'.format(indent*' ',\n\t\t\t\t\t\t\t\t\t   self.name,\n\t\t\t\t\t\t\t\t\t   self.short,\n\t\t\t\t\t\t\t\t\t   self.long)\n\telse:\n\t\treturn '{0}{1}'.format(indent*' ', self.name)\n\n", "def pretty(self, indent=0, debug=False):\n\treturn '{}{}'.format(' ' * indent, self.__repr__(debug))\n\n\n", "def pretty(self, indent=0, debug=False):\n\tl = []\n\tindent += 1\n\tfor k in self.keys():\n\t\ttry:\n\t\t\tl.append('\\n%s%s: %s' % (' '*indent, k, str(self[k]).decode('utf-8')))\n\t\texcept UnicodeEncodeError:\n\t\t\tl.append('\\n%s%s: %s' % (' '*indent, k, str(self[k])))\n\tif debug:\n\t\treturn '\\n%s%s' % (' '*indent, str(self.__class__)) + ''.join(l)\n\telse:\n\t\treturn ''.join(l)\n\n\n", "def pretty(self, indent=0, debug=False):\n\treturn self.get_name()\n\n", "def pretty(self, indent=0, debug=False):\n\trv = \" \"*indent + \"Function \"\n\tif debug:\n\t\trv += \"(%s:%s)\" % (self.filename, self.lineno)\n\trv += \"%s(%s)\\n\" % (self.name, \", \".join([str(p) for p in self.params]))\n\trv += \" \"*(indent+2) + \"{\\n\"\n\trv += \"\".join([s.pretty(indent+4, debug) for s in self.body])\n\trv += \" \"*(indent+2) + \"}\\n\"\n\treturn rv\n\n\n", "def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn '<%s %s>' % (self.__class__.__name__, self._attrs)\n\n\treturn self.format(indent=indent)\n\n\n", "def pretty(self, indent=0, debug=False):\n\tif self.is_leaf():\n\t\treturn \"   \" * indent + \"Leaf: \" + str(self.value)\n\telse:\n\t\tif debug:\n\t\t\treturn \"   \" * indent + \"Node: \" + str(self.value) + \": \" + str(self.children)\n\t\telse:\n\t\t\treturn \"   \" * indent + \"Node: \" + str(self.value) + \"\\n\" + \"\\n\".join([child.pretty(indent + 1, debug) for child in self.children])\n\n", "def pretty(self, indent=0, debug=False):\n\tif self.node_type == Node.NODE_TYPE_TEXT:\n\t\treturn self.node_value\n\tr = []\n\tr.append('%s<%s' % (indent * ' ', self.node_name))\n\tfor attr in self.attributes:\n\t\tr.append(' %s=\"%s\"' % (attr, self.attributes[attr]))\n\tif self.children:\n\t\tr.append('>')\n\t\tif debug:\n\t\t\tr.append('\\n')\n\t\tfor child in self.children:\n\t\t\tr.append('%s' % child.pretty(indent + 1, debug))\n\t\tr.append('%s</%s>' % (indent * ' ', self.node_name))\n\telse:\n\t\tr.append('/>')\n\tif debug:\n\t\tr.append('\\n')\n\treturn ''.join(r)\n\n\n", "def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn self.__str__()\n\telse:\n\t\treturn self._get_pretty(indent)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn '<%s %s>' % (\n\t\t\tself.__class__.__name__,\n\t\t\t', '.join(['%s=\"%s\"' % (k, v) for k, v in self.__dict__.items()]),\n\t\t)\n\ts = ''\n\tfor i in range(indent):\n\t\ts += '\\t'\n\ts += '<%s ' % self.__class__.__name__\n\tfor k, v in self.__dict__.items():\n\t\tif k == 'parent':\n\t\t\tcontinue\n\t\tif isinstance(v, list):\n\t\t\ts += '%s=[' % k\n\t\t\tfor i in v:\n\t\t\t\tif not isinstance(i, Pretty):\n\t\t\t\t\ts += '%s, ' % i\n\t\t\t\telse:\n\t\t\t\t\ts += '%s, ' % i.pretty(indent + 1)\n\t\t\ts += '] '\n\t\telif not isinstance(v, Pretty):\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn '{0}{1} {2} {3}'.format(indent*' ',\n\t\t\t\t\t\t\t\t\t   self.name,\n\t\t\t\t\t\t\t\t\t   self.short,\n\t\t\t\t\t\t\t\t\t   self.long)\n\telse:\n\t\treturn '{0}{1}'.format(indent*' ', self.name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\treturn '{}{}'.format(' ' * indent, self.__repr__(debug))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tl = []\n\tindent += 1\n\tfor k in self.keys():\n\t\ttry:\n\t\t\tl.append('\\n%s%s: %s' % (' '*indent, k, str(self[k]).decode('utf-8')))\n\t\texcept UnicodeEncodeError:\n\t\t\tl.append('\\n%s%s: %s' % (' '*indent, k, str(self[k])))\n\tif debug:\n\t\treturn '\\n%s%s' % (' '*indent, str(self.__class__)) + ''.join(l)\n\telse:\n\t\treturn ''.join(l)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\treturn self.get_name()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\trv = \" \"*indent + \"Function \"\n\tif debug:\n\t\trv += \"(%s:%s)\" % (self.filename, self.lineno)\n\trv += \"%s(%s)\\n\" % (self.name, \", \".join([str(p) for p in self.params]))\n\trv += \" \"*(indent+2) + \"{\\n\"\n\trv += \"\".join([s.pretty(indent+4, debug) for s in self.body])\n\trv += \" \"*(indent+2) + \"}\\n\"\n\treturn rv\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn '<%s %s>' % (self.__class__.__name__, self._attrs)\n\n\treturn self.format(indent=indent)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tif self.is_leaf():\n\t\treturn \"   \" * indent + \"Leaf: \" + str(self.value)\n\telse:\n\t\tif debug:\n\t\t\treturn \"   \" * indent + \"Node: \" + str(self.value) + \": \" + str(self.children)\n\t\telse:\n\t\t\treturn \"   \" * indent + \"Node: \" + str(self.value) + \"\\n\" + \"\\n\".join([child.pretty(indent + 1, debug) for child in self.children])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tif self.node_type == Node.NODE_TYPE_TEXT:\n\t\treturn self.node_value\n\tr = []\n\tr.append('%s<%s' % (indent * ' ', self.node_name))\n\tfor attr in self.attributes:\n\t\tr.append(' %s=\"%s\"' % (attr, self.attributes[attr]))\n\tif self.children:\n\t\tr.append('>')\n\t\tif debug:\n\t\t\tr.append('\\n')\n\t\tfor child in self.children:\n\t\t\tr.append('%s' % child.pretty(indent + 1, debug))\n\t\tr.append('%s</%s>' % (indent * ' ', self.node_name))\n\telse:\n\t\tr.append('/>')\n\tif debug:\n\t\tr.append('\\n')\n\treturn ''.join(r)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tif debug:\n\t\treturn self.__str__()\n\telse:\n\t\treturn self._get_pretty(indent)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "ques_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n\treturn [self._absorb(e) for e in args]\n\n", "def absorb(self, args):\n\tres = []\n\tfor a in args:\n\t\tif a.op == self.op == '&':\n\t\t\tif a in self.args:\n\t\t\t\tres.append(a)\n\t\t\t\tcontinue\n\t\telif a.op == self.op == '|':\n\t\t\tif not a in self.args:\n\t\t\t\tres.append(a)\n\t\t\t\tcontinue\n\t\telif a.op == '~' and a.args[0] in self.args:\n\t\t\tcontinue\n\t\tres.append(a)\n\treturn res\n\n", "def absorb(self, args):\n\tif len(args) < 2:\n\t\treturn args\n\tresult = []\n\tfor i in range(len(args)):\n\t\tfor j in range(len(args)):\n\t\t\tif i != j and args[i] == ~args[j]:\n\t\t\t\tif self.op == '&':\n\t\t\t\t\tresult.append(args[i] & args[j])\n\t\t\t\telif self.op == '|':\n\t\t\t\t\tresult.append(args[i] | args[j])\n\treturn result\n\n", "def absorb(self, args):\n\tif len(args) == 1:\n\t\treturn args\n\tnew_args = []\n\tfor (i, arg) in enumerate(args):\n\t\tif not isinstance(arg, Boolean):\n\t\t\tcontinue\n\t\tfor (j, other) in enumerate(args):\n\t\t\tif i == j:\n\t\t\t\tcontinue\n\t\t\t# Absorption\n\t\t\tif arg.op == '&' and other.op == '|':\n\t\t\t\tif arg.args[0] in other.args:\n\t\t\t\t\treturn self.absorb(new_args + [arg.args[0]])\n\t\t\t\tif arg.args[1] in other.args:\n\t\t\t\t\treturn self.absorb(new_args + [arg.args[1]])\n\t\t\telif arg.op == '|' and other.op == '&':\n\t\t\t\tif arg.args[0] in other.args:\n\t\t\t\t\treturn self.absorb(new_args + [arg.args[0]])\n\t\t\n", "def absorb(self, args):\n\t# TODO: Use `itertools` to make this pretty.\n\tif not args:\n\t\treturn args\n\tif len(args) == 1:\n\t\treturn args\n\t# If we have two elements, just check if 'absorption' or 'negative absorption'\n\t# applies.\n\telif len(args) == 2:\n\t\texprs = [args[0], args[1]]\n\t\tif isinstance(args[0], (ast.And, ast.Or)) and isinstance(args[1], (ast.And, ast.Or)):\n\t\t\tif args[0].op == args[1].op:\n\t\t\t\t# Absorption\n\t\t\t\tif all(arg in args[0].args for arg in args[1].args):\n\t\t\t\t\treturn [args[0]]\n\t\t\telse:\n\t\t\t\t# Negative absorption\n\t\t\t\tif all(arg in args[0].args for arg in args[1].args):\n\t\t\t\t\treturn [ast.And(args[0].args) if isinstance(args[0\n", "def absorb(self, args):\n\ta = []\n\tfor i, v in enumerate(args):\n\t\tif v.is_And:\n\t\t\tb = []\n\t\t\tfor w in v.args:\n\t\t\t\tif w.is_Or:\n\t\t\t\t\tb.extend(w.args)\n\t\t\t\telse:\n\t\t\t\t\tb.append(w)\n\t\t\tv = v.func(*b)\n\t\telif v.is_Or:\n\t\t\tb = []\n\t\t\tfor w in v.args:\n\t\t\t\tif w.is_And:\n\t\t\t\t\tb.extend(w.args)\n\t\t\t\telse:\n\t\t\t\t\tb.append(w)\n\t\t\tv = v.func(*b)\n\t\tif v.is_Not and i > 0:\n\t\t\tw = args[i - 1]\n\t\t\tif w.is_And:\n\t\t\t\tb = []\n\t\t\t\tfor x in w.args:\n\t\t\t\t\tif x.is_\n", "def absorb(self, args):\n\tnew_args = []\n\tfor arg in args:\n\t\tif isinstance(arg, bool):\n\t\t\tif arg is True:\n\t\t\t\treturn [True]\n\t\t\telse:\n\t\t\t\tcontinue\n\t\telif isinstance(arg, Not):\n\t\t\tif arg.expr is False:\n\t\t\t\treturn [False]\n\t\t\telse:\n\t\t\t\tcontinue\n\t\telif isinstance(arg, And):\n\t\t\tif all(self.is_in(arg.exprs[0], expr) for expr in args):\n\t\t\t\treturn [arg.exprs[0]]\n\t\t\telif all(self.is_in(expr, arg.exprs[0]) for expr in args):\n\t\t\t\treturn [arg.exprs[0]]\n\t\telif isinstance(arg, Or):\n\t\t\tif all(self.is_in(arg.exprs[0], expr) for expr in args):\n\t\t\t\treturn [arg.exprs[0]]\n\t\t\telif all(self.is\n", "def absorb(self, args):\n\tdef absorb_test(a, b):\n\t\treturn a.operand == b.operand and a.operator == b.operator\n\t\n\tdef absorb_result(a, b):\n\t\treturn a\n\t\n\tdef negate_absorb_test(a, b):\n\t\treturn a.operand == ~b.operand and a.operator == b.operator\n\t\n\tdef negate_absorb_result(a, b):\n\t\treturn b\n\t\n\treturn self.absorb_or_negate_absorb(args, absorb_test, absorb_result, negate_absorb_test, negate_absorb_result)\n\n", "def absorb(self, args):\n\ti = 0\n\twhile i < len(args) - 1:\n\t\tif args[i] == args[i + 1]:\n\t\t\targs.pop(i + 1)\n\t\t\tcontinue\n\t\tif args[i].is_op(op_not):\n\t\t\tif args[i + 1].is_op(op_and):\n\t\t\t\tj = 0\n\t\t\t\twhile j < len(args[i + 1].args):\n\t\t\t\t\tif args[i].args[0] == args[i + 1].args[j]:\n\t\t\t\t\t\targs[i + 1].args.pop(j)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif args[i].args[0].is_op(op_not) and args[i].args[0].args[0] == args[i + 1].args[j]:\n\t\t\t\t\t\targs[i + 1].args.pop(j)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tj += 1\n\t\t\t\tif not args[i + 1\n", "def absorb(self, args):\n\treturn [absorb_one(f, args) for f in args]\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n\treturn [self._absorb(e) for e in args]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tres = []\n\tfor a in args:\n\t\tif a.op == self.op == '&':\n\t\t\tif a in self.args:\n\t\t\t\tres.append(a)\n\t\t\t\tcontinue\n\t\telif a.op == self.op == '|':\n\t\t\tif not a in self.args:\n\t\t\t\tres.append(a)\n\t\t\t\tcontinue\n\t\telif a.op == '~' and a.args[0] in self.args:\n\t\t\tcontinue\n\t\tres.append(a)\n\treturn res\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tif len(args) < 2:\n\t\treturn args\n\tresult = []\n\tfor i in range(len(args)):\n\t\tfor j in range(len(args)):\n\t\t\tif i != j and args[i] == ~args[j]:\n\t\t\t\tif self.op == '&':\n\t\t\t\t\tresult.append(args[i] & args[j])\n\t\t\t\telif self.op == '|':\n\t\t\t\t\tresult.append(args[i] | args[j])\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tif len(args) == 1:\n\t\treturn args\n\tnew_args = []\n\tfor (i, arg) in enumerate(args):\n\t\tif not isinstance(arg, Boolean):\n\t\t\tcontinue\n\t\tfor (j, other) in enumerate(args):\n\t\t\tif i == j:\n\t\t\t\tcontinue\n\t\t\t# Absorption\n\t\t\tif arg.op == '&' and other.op == '|':\n\t\t\t\tif arg.args[0] in other.args:\n\t\t\t\t\treturn self.absorb(new_args + [arg.args[0]])\n\t\t\t\tif arg.args[1] in other.args:\n\t\t\t\t\treturn self.absorb(new_args + [arg.args[1]])\n\t\t\telif arg.op == '|' and other.op == '&':\n\t\t\t\tif arg.args[0] in other.args:\n\t\t\t\t\treturn self.absorb(new_args + [arg.args[0]])\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t# TODO: Use `itertools` to make this pretty.\n\tif not args:\n\t\treturn args\n\tif len(args) == 1:\n\t\treturn args\n\t# If we have two elements, just check if 'absorption' or 'negative absorption'\n\t# applies.\n\telif len(args) == 2:\n\t\texprs = [args[0], args[1]]\n\t\tif isinstance(args[0], (ast.And, ast.Or)) and isinstance(args[1], (ast.And, ast.Or)):\n\t\t\tif args[0].op == args[1].op:\n\t\t\t\t# Absorption\n\t\t\t\tif all(arg in args[0].args for arg in args[1].args):\n\t\t\t\t\treturn [args[0]]\n\t\t\telse:\n\t\t\t\t# Negative absorption\n\t\t\t\tif all(arg in args[0].args for arg in args[1].args):\n\t\t\t\t\treturn [ast.And(args[0].args) if isinstance(args[0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\ta = []\n\tfor i, v in enumerate(args):\n\t\tif v.is_And:\n\t\t\tb = []\n\t\t\tfor w in v.args:\n\t\t\t\tif w.is_Or:\n\t\t\t\t\tb.extend(w.args)\n\t\t\t\telse:\n\t\t\t\t\tb.append(w)\n\t\t\tv = v.func(*b)\n\t\telif v.is_Or:\n\t\t\tb = []\n\t\t\tfor w in v.args:\n\t\t\t\tif w.is_And:\n\t\t\t\t\tb.extend(w.args)\n\t\t\t\telse:\n\t\t\t\t\tb.append(w)\n\t\t\tv = v.func(*b)\n\t\tif v.is_Not and i > 0:\n\t\t\tw = args[i - 1]\n\t\t\tif w.is_And:\n\t\t\t\tb = []\n\t\t\t\tfor x in w.args:\n\t\t\t\t\tif x.is_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tnew_args = []\n\tfor arg in args:\n\t\tif isinstance(arg, bool):\n\t\t\tif arg is True:\n\t\t\t\treturn [True]\n\t\t\telse:\n\t\t\t\tcontinue\n\t\telif isinstance(arg, Not):\n\t\t\tif arg.expr is False:\n\t\t\t\treturn [False]\n\t\t\telse:\n\t\t\t\tcontinue\n\t\telif isinstance(arg, And):\n\t\t\tif all(self.is_in(arg.exprs[0], expr) for expr in args):\n\t\t\t\treturn [arg.exprs[0]]\n\t\t\telif all(self.is_in(expr, arg.exprs[0]) for expr in args):\n\t\t\t\treturn [arg.exprs[0]]\n\t\telif isinstance(arg, Or):\n\t\t\tif all(self.is_in(arg.exprs[0], expr) for expr in args):\n\t\t\t\treturn [arg.exprs[0]]\n\t\t\telif all(self.is\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tdef absorb_test(a, b):\n\t\treturn a.operand == b.operand and a.operator == b.operator\n\t\n\tdef absorb_result(a, b):\n\t\treturn a\n\t\n\tdef negate_absorb_test(a, b):\n\t\treturn a.operand == ~b.operand and a.operator == b.operator\n\t\n\tdef negate_absorb_result(a, b):\n\t\treturn b\n\t\n\treturn self.absorb_or_negate_absorb(args, absorb_test, absorb_result, negate_absorb_test, negate_absorb_result)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\ti = 0\n\twhile i < len(args) - 1:\n\t\tif args[i] == args[i + 1]:\n\t\t\targs.pop(i + 1)\n\t\t\tcontinue\n\t\tif args[i].is_op(op_not):\n\t\t\tif args[i + 1].is_op(op_and):\n\t\t\t\tj = 0\n\t\t\t\twhile j < len(args[i + 1].args):\n\t\t\t\t\tif args[i].args[0] == args[i + 1].args[j]:\n\t\t\t\t\t\targs[i + 1].args.pop(j)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tif args[i].args[0].is_op(op_not) and args[i].args[0].args[0] == args[i + 1].args[j]:\n\t\t\t\t\t\targs[i + 1].args.pop(j)\n\t\t\t\t\t\tcontinue\n\t\t\t\t\tj += 1\n\t\t\t\tif not args[i + 1\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\treturn [absorb_one(f, args) for f in args]\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "ques_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n\t\tdef hook_decorator(func):\n\t\t\treturn self.add_handler(hook, func)\n\t\treturn hook_decorator\n\n\tdef add_handler(self, hook, func):\n\t\t\"\"\"\n\t\tAdd a new handler to the registry.\n\n", "def on(self, hook):\n\tdef decorator(f):\n\t\tself.register(hook, f)\n\t\treturn f\n\treturn decorator\n\n", "def on(self, hook):\n\treturn lambda func: self.register(hook, func)\n\n", "def on(self, hook):\n\tdef decorator(fn):\n\t\tself.registry[hook].append(fn)\n\t\treturn fn\n\treturn decorator\n\n", "def on(self, hook):\n\tdef decorator(func):\n\t\tself._handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n", "def on(self, hook):\n\treturn partial(self.add, hook)\n\n", "def on(self, hook):\n\t\tif not isinstance(hook, HookType):\n\t\t\traise TypeError('\"hook\" must be a HookType')\n\t\tdef decorator(func):\n\t\t\tself.register(hook, func)\n\t\t\treturn func\n\t\treturn decorator\n\t\n\tdef off(self, hook, func):\n\t\t\"\"\"\n\t\tUnregisters a handler from the registry.\n\n", "def on(self, hook):\n\tdef decorator(func):\n\t\tself.add(hook, func)\n\t\treturn func\n\treturn decorator\n\n", "def on(self, hook):\n\tdef decorate(func):\n\t\tself.register(hook, func)\n\t\treturn func\n\treturn decorate\n\n\n", "def on(self, hook):\n\tdef decorator(func):\n\t\t# Register the listener\n\t\tself.handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n\t\tdef hook_decorator(func):\n\t\t\treturn self.add_handler(hook, func)\n\t\treturn hook_decorator\n\n\tdef add_handler(self, hook, func):\n\t\t\"\"\"\n\t\tAdd a new handler to the registry.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorator(f):\n\t\tself.register(hook, f)\n\t\treturn f\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\treturn lambda func: self.register(hook, func)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorator(fn):\n\t\tself.registry[hook].append(fn)\n\t\treturn fn\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\tdef decorator(func):\n\t\tself._handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\treturn partial(self.add, hook)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\tif not isinstance(hook, HookType):\n\t\t\traise TypeError('\"hook\" must be a HookType')\n\t\tdef decorator(func):\n\t\t\tself.register(hook, func)\n\t\t\treturn func\n\t\treturn decorator\n\t\n\tdef off(self, hook, func):\n\t\t\"\"\"\n\t\tUnregisters a handler from the registry.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorator(func):\n\t\tself.add(hook, func)\n\t\treturn func\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorate(func):\n\t\tself.register(hook, func)\n\t\treturn func\n\treturn decorate\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorator(func):\n\t\t# Register the listener\n\t\tself.handlers[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "ques_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"user\": user,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"api_version\": \"v1\",\n\t\t\"api_base_url\": \"http://localhost:5000\",\n\t\t\"api_base_path\": \"/api\",\n\t\t\"api_static_path\": \"\",\n\t\t\"api_static_file\": \"\",\n\t\t\"api_static_file_key\": \"\",\n\t\t\"api_static_file_key_hash\": \"\",\n\t\t\"api_static_file_key_hash_algorithm\": \"\",\n\t\t\"api_static_file_key_hash_salt\": \"\",\n\t\t\"api_static_file_key_hash_iterations\": 0,\n\t\t\"api_static_file_key_hash_length\": 0,\n\t}\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"auth\": {\n\t\t\t\"type\": \"static\",\n\t\t\t\"users\": {\n\t\t\t\tuser: {\n\t\t\t\t\t\"password\": \"test123\"\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"database\": {\n\t\t\t\"type\": \"etcd\",\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t}\n\t}\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"authentication\": {\n\t\t\t\"static\": {\n\t\t\t\t\"user\": user\n\t\t\t}\n\t\t},\n\t\t\"api\": {\n\t\t\t\"port\": 4000\n\t\t}\n\t}\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t'etcd_host': etcd_host,\n\t\t'etcd_port': etcd_port,\n\t\t'auth': {\n\t\t\t'static': {\n\t\t\t\t'user': user,\n\t\t\t\t'password': 'secret'\n\t\t\t}\n\t\t}\n\t}\n\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"static\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"users\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"name\": user,\n\t\t\t\t\t\t\"password\": \"password\"\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t},\n\t\t\"logging\": {\n\t\t\t\"type\": \"console\"\n\t\t}\n\t}\n\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"user\": user,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"etcd_ca\": \"\",\n\t\t\"etcd_cert\": \"\",\n\t\t\"etcd_key\": \"\",\n\t\t\"etcd_user\": \"\",\n\t\t\"etcd_password\": \"\"\n\t}\n\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"users\": [\n\t\t\t{\"user\": user, \"password\": \"pass\"}\n\t\t],\n\t\t\"database\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"key_prefix\": \"test\",\n\t\t\"secret_key\": \"secret\",\n\t\t\"auth\": {\n\t\t\t\"type\": \"static\"\n\t\t},\n\t\t\"ssl\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\"\n\t\t}\n\t}\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t'etcd_host': etcd_host,\n\t\t'etcd_port': etcd_port,\n\t\t'credentials': {\n\t\t\t'user': user,\n\t\t\t'password': \"test\",\n\t\t},\n\t\t'network_port': 8080,\n\t\t'network_host': '0.0.0.0',\n\t\t'network_threaded': True,\n\t\t'network_debug': True,\n\t\t'network_thread_count': 16,\n\t\t'network_use_reloader': False,\n\t\t'database_name': 'test_db',\n\t\t'database_host': '0.0.0.0',\n\t\t'database_port': 27017,\n\t\t'database_user': '',\n\t\t'database_password': '',\n\t}\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"authentication\": {\n\t\t\t\"mode\": \"static\",\n\t\t\t\"static\": {\n\t\t\t\t\"user\": user\n\t\t\t}\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"file\": \"logs/test.log\"\n\t\t},\n\t\t\"database\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t}\n\t}\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig[\"auth\"] = {\n\t\t\"static\": {\n\t\t\t\"user\": user,\n\t\t\t\"password\": \"password\"\n\t\t}\n\t}\n\tconfig[\"db\"] = {\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t}\n\t}\n\treturn config\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"user\": user,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"api_version\": \"v1\",\n\t\t\"api_base_url\": \"http://localhost:5000\",\n\t\t\"api_base_path\": \"/api\",\n\t\t\"api_static_path\": \"\",\n\t\t\"api_static_file\": \"\",\n\t\t\"api_static_file_key\": \"\",\n\t\t\"api_static_file_key_hash\": \"\",\n\t\t\"api_static_file_key_hash_algorithm\": \"\",\n\t\t\"api_static_file_key_hash_salt\": \"\",\n\t\t\"api_static_file_key_hash_iterations\": 0,\n\t\t\"api_static_file_key_hash_length\": 0,\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"auth\": {\n\t\t\t\"type\": \"static\",\n\t\t\t\"users\": {\n\t\t\t\tuser: {\n\t\t\t\t\t\"password\": \"test123\"\n\t\t\t\t}\n\t\t\t}\n\t\t},\n\t\t\"database\": {\n\t\t\t\"type\": \"etcd\",\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t}\n\t}\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"authentication\": {\n\t\t\t\"static\": {\n\t\t\t\t\"user\": user\n\t\t\t}\n\t\t},\n\t\t\"api\": {\n\t\t\t\"port\": 4000\n\t\t}\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t'etcd_host': etcd_host,\n\t\t'etcd_port': etcd_port,\n\t\t'auth': {\n\t\t\t'static': {\n\t\t\t\t'user': user,\n\t\t\t\t'password': 'secret'\n\t\t\t}\n\t\t}\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": True,\n\t\t\t\"static\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"users\": [\n\t\t\t\t\t{\n\t\t\t\t\t\t\"name\": user,\n\t\t\t\t\t\t\"password\": \"password\"\n\t\t\t\t\t}\n\t\t\t\t]\n\t\t\t}\n\t\t},\n\t\t\"logging\": {\n\t\t\t\"type\": \"console\"\n\t\t}\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"user\": user,\n\t\t\"etcd_host\": etcd_host,\n\t\t\"etcd_port\": etcd_port,\n\t\t\"etcd_ca\": \"\",\n\t\t\"etcd_cert\": \"\",\n\t\t\"etcd_key\": \"\",\n\t\t\"etcd_user\": \"\",\n\t\t\"etcd_password\": \"\"\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"users\": [\n\t\t\t{\"user\": user, \"password\": \"pass\"}\n\t\t],\n\t\t\"database\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"key_prefix\": \"test\",\n\t\t\"secret_key\": \"secret\",\n\t\t\"auth\": {\n\t\t\t\"type\": \"static\"\n\t\t},\n\t\t\"ssl\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\"\n\t\t}\n\t}\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t'etcd_host': etcd_host,\n\t\t'etcd_port': etcd_port,\n\t\t'credentials': {\n\t\t\t'user': user,\n\t\t\t'password': \"test\",\n\t\t},\n\t\t'network_port': 8080,\n\t\t'network_host': '0.0.0.0',\n\t\t'network_threaded': True,\n\t\t'network_debug': True,\n\t\t'network_thread_count': 16,\n\t\t'network_use_reloader': False,\n\t\t'database_name': 'test_db',\n\t\t'database_host': '0.0.0.0',\n\t\t'database_port': 27017,\n\t\t'database_user': '',\n\t\t'database_password': '',\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"authentication\": {\n\t\t\t\"mode\": \"static\",\n\t\t\t\"static\": {\n\t\t\t\t\"user\": user\n\t\t\t}\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"file\": \"logs/test.log\"\n\t\t},\n\t\t\"database\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t}\n\t}\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig[\"auth\"] = {\n\t\t\"static\": {\n\t\t\t\"user\": user,\n\t\t\t\"password\": \"password\"\n\t\t}\n\t}\n\tconfig[\"db\"] = {\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t}\n\t}\n\treturn config\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n\tself._validate(dt)\n\tif dt.tzinfo is self:\n\t\treturn dt\n\tdt = dt.astimezone(self)\n\t# If this is the first occurrence of the ambiguous datetime,\n\t# shift it back one minute and return the result. Otherwise,\n\t# return the result unchanged.\n\t#\n\t# NOTE: This is only ever a problem for the ambiguous datetime\n\t# immediately after a DST transition (because the timezone rules\n\t# are always written such that the DST transition happens *at*\n\t# the ambiguous time, e.g. 01:30:00 occurs twice, once as DST\n\t# and once as standard time).\n\t#\n\t# So, for a DST transition from 01:59:59 DST to 01:00:00\n\t# standard time, the first occurrence of 01:30:00 will be\n\t# interpreted as 01:30:00 standard time, and the second\n\t# occurrence will be interpreted as 01:30:00 DST.\n\t#\n\t# In the case of a DST transition from 02:59:59 standard time\n\n", "def _fromutc(self, dt):\n\t\tif dt.tzinfo is self:\n\t\t\treturn dt\n\n\t\tassert dt.tzinfo is not None\n\n\t\t# Convert dt to UTC.\n\t\tdt = dt.astimezone(UTC)\n\n\t\t# Find the offset of the current datetime.\n\t\toffset, is_dst, _ = self._utcoffset(dt)\n\n\t\tif not is_dst:\n\t\t\t# No DST, so we can just return the datetime as-is.\n\t\t\treturn dt + offset\n\n\t\t# We're in DST, so we need to find the first occurrence\n\t\t# of the datetime in local time.\n\t\tdt = dt + offset\n\t\tstart_of_year = self._find_start_of_year(dt.year)\n\t\tfor dt_offset in (0, 1, -1):\n\t\t\tcandidate = dt + timedelta(days=dt_offset)\n\t\t\tis_dst_candidate, _ = self._isdst(candidate, start_of_year)\n\n\t\t\n", "def _fromutc(self, dt):\n\tif self._tzinfo is None:\n\t\traise ValueError('fromutc: dt.tzinfo is None')\n\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc: dt.tzinfo is None')\n\n\tif not dt.tzinfo.utcoffset(dt) == dt.tzinfo.dst(dt):\n\t\traise ValueError('fromutc: dt.tzinfo is not UTC')\n\n\tdtoff = dt.utcoffset()\n\tif dtoff is None:\n\t\traise ValueError('fromutc: dt.tzinfo is not UTC')\n\n\t# calculate the offset from UTC to the new timezone\n\t# by subtracting the utcoffset from the datetime\n\t# (which will be zero for UTC)\n\tdt = dt - dtoff\n\n\t# now check to see if the datetime is ambiguous and\n\t# in a \"fold\".  if it is, we can't do the conversion\n\t# because there's no way to know which occurrence\n\t# of the ambiguous datetime we have\n\tif\n", "def _fromutc(self, dt):\n\tdt = self._tzinfo.fromutc(dt)\n\tif self._tzinfo.is_ambiguous(dt):\n\t\tdt = dt.replace(fold=1)\n\treturn dt\n\n\n", "def _fromutc(self, dt):\n\tif self._tzinfo._cache is None:\n\t\t# If the timezone isn't cached, it's impossible to know\n\t\t# whether the datetime is ambiguous or not.\n\t\treturn self.normalize(dt)\n\t\n\tdst_offset = self._tzinfo._dst_offset_cache\n\tutc_offset = self._tzinfo._utc_offset_cache\n\t\n\t# Calculate the offset of the datetime, in minutes\n\tdt_offset = (dt.hour * 60 + dt.minute) * 60 + dt.second\n\tdt_offset = dt_offset - (utc_offset + dst_offset)\n\t\n\t# If the datetime is in a fold, return the first occurrence of the\n\t# datetime\n\tif dt.fold and dt_offset < dst_offset:\n\t\tdst_offset = 0\n\t\n\treturn dt.replace(tzinfo=self._tzinfo._tzinfo, fold=dt.fold) + datetime.timedelta(0, dst_offset)\n\n\n", "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"dt parameter must be timezone-aware\")\n\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tif dt.tzinfo.utcoffset(dt) != self.utcoffset(dt):\n\t\traise ValueError(\"Can't convert between %s and %s timezones\" % (dt.tzinfo, self))\n\n\tnewdt = dt.replace(tzinfo=self)\n\n\tif newdt.tzinfo is None:\n\t\traise ValueError(\"%r.replace() returned an naive datetime\" % dt)\n\n\t# We may have a fold here.\n\tif dt.fold:\n\t\tnewdt = newdt.replace(fold=1)\n\n\treturn newdt\n\n\n", "def _fromutc(self, dt):\n\t# First, we need to know whether the datetime is actually ambiguous\n\t# in the given timezone.\n\ttzinfo = dt.tzinfo\n\toffset = tzinfo.utcoffset(dt)\n\tfold = tzinfo.dst(dt)\n\tif fold is None:\n\t\t# The datetime is not ambiguous, so we can return it directly\n\t\t# without further ado.\n\t\treturn dt\n\tif not offset:\n\t\t# The datetime is in a \"fold\" state, so we can return it\n\t\t# directly without further ado.\n\t\treturn dt\n\t# The datetime is ambiguous, so we need to find the right\n\t# occurrence of the ambiguous datetime.\n\t#\n\t# We do this by finding the nearest unambiguous datetime, then\n\t# adding the offset of the datetime we actually want.\n\t#\n\t# First, we get the nearest unambiguous datetime.\n\t#\n\t# To do this, we calculate the offset of the datetime we want\n\t# (i.e. the offset at the start of the transition period), then\n\n", "def _fromutc(self, dt):\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\t# Convert dt to UTC, then use self._utc_transition_times\n\t# to calculate the new UTC time.\n\tdt = self._utc_transition_times[bisect.bisect_right(self._utc_transition_times, dt.astimezone(self.utc).replace(tzinfo=None))-1]\n\tdt = dt.replace(tzinfo=self.utc)\n\tdt = dt.astimezone(self)\n\n\tif self._fold and self.is_ambiguous(dt):\n\t\tdt = self.normalize(dt)\n\n\treturn dt\n\n\n", "def _fromutc(self, dt):\n\tif dt.tzinfo is self._tz:\n\t\treturn dt\n\telif dt.tzinfo is None:\n\t\traise ValueError('dt.tzinfo is None')\n\n\ttry:\n\t\tdst_change = self._tz._utc_transition_times[0]\n\texcept IndexError:\n\t\t# No transitions, so we're in standard time.\n\t\treturn self._fromutc_std(dt)\n\n\tif dt < self._tz._utc_transition_times[-1]:\n\t\t# The time is unambiguous, so we can return the local time.\n\t\treturn self._fromutc_std(dt)\n\n\t# We have to determine whether the local time is ambiguous or not.\n\t# This is a little trickier than it seems, because we have to take\n\t# into account the fact that we might be in a fold.\n\n\t# We're going to search for the local time in the transitions,\n\t# starting from the most recent transition. If we find it, then\n\t# we've got an ambiguous time, and we'll use the standard algorithm\n\t\n", "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\tother_self = dt.tzinfo\n\n\tif not timezones.utc.normalize(dt).tzinfo:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\n\t# We need to know if we're in a fold.\n\tfold = 0\n\tif dt.fold:\n\t\t# We have a fold, so we need to work out if we're in the first\n\t\t# or second occurrence.\n\t\tif dt.replace(tzinfo=None) < dt:\n\t\t\t# We're in the first occurrence.\n\t\t\tfold = -1\n\t\telse:\n\t\t\t# We're in the second occurrence.\n\t\t\tfold = 1\n\n\t# We need to calculate the offset relative to UTC.\n\tutc_offset = dt.utcoffset()\n\tif utc_offset is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n\tself._validate(dt)\n\tif dt.tzinfo is self:\n\t\treturn dt\n\tdt = dt.astimezone(self)\n\t# If this is the first occurrence of the ambiguous datetime,\n\t# shift it back one minute and return the result. Otherwise,\n\t# return the result unchanged.\n\t#\n\t# NOTE: This is only ever a problem for the ambiguous datetime\n\t# immediately after a DST transition (because the timezone rules\n\t# are always written such that the DST transition happens *at*\n\t# the ambiguous time, e.g. 01:30:00 occurs twice, once as DST\n\t# and once as standard time).\n\t#\n\t# So, for a DST transition from 01:59:59 DST to 01:00:00\n\t# standard time, the first occurrence of 01:30:00 will be\n\t# interpreted as 01:30:00 standard time, and the second\n\t# occurrence will be interpreted as 01:30:00 DST.\n\t#\n\t# In the case of a DST transition from 02:59:59 standard time\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t\tif dt.tzinfo is self:\n\t\t\treturn dt\n\n\t\tassert dt.tzinfo is not None\n\n\t\t# Convert dt to UTC.\n\t\tdt = dt.astimezone(UTC)\n\n\t\t# Find the offset of the current datetime.\n\t\toffset, is_dst, _ = self._utcoffset(dt)\n\n\t\tif not is_dst:\n\t\t\t# No DST, so we can just return the datetime as-is.\n\t\t\treturn dt + offset\n\n\t\t# We're in DST, so we need to find the first occurrence\n\t\t# of the datetime in local time.\n\t\tdt = dt + offset\n\t\tstart_of_year = self._find_start_of_year(dt.year)\n\t\tfor dt_offset in (0, 1, -1):\n\t\t\tcandidate = dt + timedelta(days=dt_offset)\n\t\t\tis_dst_candidate, _ = self._isdst(candidate, start_of_year)\n\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif self._tzinfo is None:\n\t\traise ValueError('fromutc: dt.tzinfo is None')\n\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc: dt.tzinfo is None')\n\n\tif not dt.tzinfo.utcoffset(dt) == dt.tzinfo.dst(dt):\n\t\traise ValueError('fromutc: dt.tzinfo is not UTC')\n\n\tdtoff = dt.utcoffset()\n\tif dtoff is None:\n\t\traise ValueError('fromutc: dt.tzinfo is not UTC')\n\n\t# calculate the offset from UTC to the new timezone\n\t# by subtracting the utcoffset from the datetime\n\t# (which will be zero for UTC)\n\tdt = dt - dtoff\n\n\t# now check to see if the datetime is ambiguous and\n\t# in a \"fold\".  if it is, we can't do the conversion\n\t# because there's no way to know which occurrence\n\t# of the ambiguous datetime we have\n\tif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tdt = self._tzinfo.fromutc(dt)\n\tif self._tzinfo.is_ambiguous(dt):\n\t\tdt = dt.replace(fold=1)\n\treturn dt\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif self._tzinfo._cache is None:\n\t\t# If the timezone isn't cached, it's impossible to know\n\t\t# whether the datetime is ambiguous or not.\n\t\treturn self.normalize(dt)\n\t\n\tdst_offset = self._tzinfo._dst_offset_cache\n\tutc_offset = self._tzinfo._utc_offset_cache\n\t\n\t# Calculate the offset of the datetime, in minutes\n\tdt_offset = (dt.hour * 60 + dt.minute) * 60 + dt.second\n\tdt_offset = dt_offset - (utc_offset + dst_offset)\n\t\n\t# If the datetime is in a fold, return the first occurrence of the\n\t# datetime\n\tif dt.fold and dt_offset < dst_offset:\n\t\tdst_offset = 0\n\t\n\treturn dt.replace(tzinfo=self._tzinfo._tzinfo, fold=dt.fold) + datetime.timedelta(0, dst_offset)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"dt parameter must be timezone-aware\")\n\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tif dt.tzinfo.utcoffset(dt) != self.utcoffset(dt):\n\t\traise ValueError(\"Can't convert between %s and %s timezones\" % (dt.tzinfo, self))\n\n\tnewdt = dt.replace(tzinfo=self)\n\n\tif newdt.tzinfo is None:\n\t\traise ValueError(\"%r.replace() returned an naive datetime\" % dt)\n\n\t# We may have a fold here.\n\tif dt.fold:\n\t\tnewdt = newdt.replace(fold=1)\n\n\treturn newdt\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t# First, we need to know whether the datetime is actually ambiguous\n\t# in the given timezone.\n\ttzinfo = dt.tzinfo\n\toffset = tzinfo.utcoffset(dt)\n\tfold = tzinfo.dst(dt)\n\tif fold is None:\n\t\t# The datetime is not ambiguous, so we can return it directly\n\t\t# without further ado.\n\t\treturn dt\n\tif not offset:\n\t\t# The datetime is in a \"fold\" state, so we can return it\n\t\t# directly without further ado.\n\t\treturn dt\n\t# The datetime is ambiguous, so we need to find the right\n\t# occurrence of the ambiguous datetime.\n\t#\n\t# We do this by finding the nearest unambiguous datetime, then\n\t# adding the offset of the datetime we actually want.\n\t#\n\t# First, we get the nearest unambiguous datetime.\n\t#\n\t# To do this, we calculate the offset of the datetime we want\n\t# (i.e. the offset at the start of the transition period), then\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\t# Convert dt to UTC, then use self._utc_transition_times\n\t# to calculate the new UTC time.\n\tdt = self._utc_transition_times[bisect.bisect_right(self._utc_transition_times, dt.astimezone(self.utc).replace(tzinfo=None))-1]\n\tdt = dt.replace(tzinfo=self.utc)\n\tdt = dt.astimezone(self)\n\n\tif self._fold and self.is_ambiguous(dt):\n\t\tdt = self.normalize(dt)\n\n\treturn dt\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif dt.tzinfo is self._tz:\n\t\treturn dt\n\telif dt.tzinfo is None:\n\t\traise ValueError('dt.tzinfo is None')\n\n\ttry:\n\t\tdst_change = self._tz._utc_transition_times[0]\n\texcept IndexError:\n\t\t# No transitions, so we're in standard time.\n\t\treturn self._fromutc_std(dt)\n\n\tif dt < self._tz._utc_transition_times[-1]:\n\t\t# The time is unambiguous, so we can return the local time.\n\t\treturn self._fromutc_std(dt)\n\n\t# We have to determine whether the local time is ambiguous or not.\n\t# This is a little trickier than it seems, because we have to take\n\t# into account the fact that we might be in a fold.\n\n\t# We're going to search for the local time in the transitions,\n\t# starting from the most recent transition. If we find it, then\n\t# we've got an ambiguous time, and we'll use the standard algorithm\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\tother_self = dt.tzinfo\n\n\tif not timezones.utc.normalize(dt).tzinfo:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\n\t# We need to know if we're in a fold.\n\tfold = 0\n\tif dt.fold:\n\t\t# We have a fold, so we need to work out if we're in the first\n\t\t# or second occurrence.\n\t\tif dt.replace(tzinfo=None) < dt:\n\t\t\t# We're in the first occurrence.\n\t\t\tfold = -1\n\t\telse:\n\t\t\t# We're in the second occurrence.\n\t\t\tfold = 1\n\n\t# We need to calculate the offset relative to UTC.\n\tutc_offset = dt.utcoffset()\n\tif utc_offset is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "ques_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n\tndays = self._days\n\tnhours = self._hours\n\tnmins = self._minutes\n\tnsecs = self._seconds\n\tnleaps = self._leapseconds\n\tnmonths = self._months\n\tnyears = self._years\n\n\tif nmonths == 12:\n\t\tnyears += 1\n\t\tnmonths = 0\n\n\tif nmonths == -12:\n\t\tnyears -= 1\n\t\tnmonths = 0\n\n\tif nmins == 60:\n\t\tnhours += 1\n\t\tnmins = 0\n\n\tif nmins == -60:\n\t\tnhours -= 1\n\t\tnmins = 0\n\n\tif nhours == 24:\n\t\tndays += 1\n\t\tnhours = 0\n\n\tif nhours == -24:\n\t\tndays -= 1\n\t\tnhours = 0\n\n\tif nsecs == 60:\n\t\tnmins += 1\n\t\tnsecs = 0\n\n\tif nsecs == -60:\n\t\tnmins -= 1\n\t\tnsecs = 0\n\n\tif nleaps == 60:\n\t\tn\n", "def normalized(self):\n\tpass\n\t\n", "def normalized(self):\n\tpass\n\t\n", "def normalized(self):\n\tif self._norm is not None:\n\t\treturn self._norm\n\n\t# First, we must normalize the relative attributes.\n\tyears = self._years\n\tmonths = self._months\n\tdays = self._days\n\tleapdays = self._leapdays\n\thours = self._hours\n\tminutes = self._minutes\n\tseconds = self._seconds\n\tmicroseconds = self._microseconds\n\n\tyears += months // 12\n\tmonths = months % 12\n\n\tdays += hours // 24\n\thours = hours % 24\n\n\tminutes += seconds // 60\n\tseconds = seconds % 60\n\n\tmicroseconds += seconds * 1000000\n\tseconds = 0\n\n\tminutes += microseconds // 60000000\n\tmicroseconds = microseconds % 60000000\n\n\thours += minutes // 60\n\tminutes = minutes % 60\n\n\tdays += hours // 24\n\thours = hours % 24\n\n\t# Now, we can normalize the absolute attributes.\n\tif self._day is not None:\n\t\tif self._day > 0:\n\t\t\twhile days < self._day:\n\t\t\t\tdays = days + 1\n\t\t\t\n", "def normalized(self):\n\tdays, hours, minutes, seconds, microseconds = self.get_parts()\n\thours += days * 24\n\tminutes += hours * 60\n\tseconds += minutes * 60\n\tmicroseconds += seconds * 1000000\n\treturn relativedelta(\n\t\tdays=0,\n\t\thours=0,\n\t\tminutes=0,\n\t\tseconds=0,\n\t\tmicroseconds=microseconds,\n\t)\n\n\n", "def normalized(self):\n\t\n\t\n\n", "def normalized(self):\n\tyears = self.years\n\tmonths = self.months\n\tdays = self.days\n\tleapdays = self.leapdays\n\thours = self.hours\n\tminutes = self.minutes\n\tseconds = self.seconds\n\tmicroseconds = self.microseconds\n\tif months == 12:\n\t\tyears += 1\n\t\tmonths = 0\n\telif months == -12:\n\t\tyears -= 1\n\t\tmonths = 0\n\tif days < 0:\n\t\textra_months = math.floor(days / DAYSPERMONTH)\n\t\tmonths += extra_months\n\t\tdays -= extra_months * DAYSPERMONTH\n\telif days > DAYSPERMONTH:\n\t\textra_months = math.floor(days / DAYSPERMONTH)\n\t\tmonths += extra_months\n\t\tdays -= extra_months * DAYSPERMONTH\n\tif days < 0 and months > 0:\n\t\tmonths -= 1\n\t\tdays += DAYSPERMONTH\n\telif days > DAYSPERMONTH and months < 0:\n\t\tmonths += 1\n\t\tdays\n", "def normalized(self):\n\treturn self._fix(normalize=1)\n\n", "def normalized(self):\n\treturn self.__class__(\n\t\tyears=self._years,\n\t\tmonths=self._months,\n\t\tdays=self._days,\n\t\tleapdays=self._leapdays,\n\t\thours=self._hours,\n\t\tminutes=self._minutes,\n\t\tseconds=self._seconds,\n\t\tmicroseconds=self._microseconds,\n\t\tyear=self._year,\n\t\tmonth=self._month,\n\t\tday=self._day,\n\t\tweekday=self._weekday,\n\t\thour=self._hour,\n\t\tminute=self._minute,\n\t\tsecond=self._second,\n\t\tmicrosecond=self._microsecond,\n\t\tyear_day=self._year_day,\n\t\tmonth_day=self._month_day,\n\t\tn_leapdays=self._n_leapdays,\n\t\tday_counter=self._day_counter,\n\t\tweekday_counter=self._weekday_counter,\n\t\thour_counter=self._hour_counter,\n\t\tminute_counter=self._\n", "def normalized(self):\n\tpass\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n\tndays = self._days\n\tnhours = self._hours\n\tnmins = self._minutes\n\tnsecs = self._seconds\n\tnleaps = self._leapseconds\n\tnmonths = self._months\n\tnyears = self._years\n\n\tif nmonths == 12:\n\t\tnyears += 1\n\t\tnmonths = 0\n\n\tif nmonths == -12:\n\t\tnyears -= 1\n\t\tnmonths = 0\n\n\tif nmins == 60:\n\t\tnhours += 1\n\t\tnmins = 0\n\n\tif nmins == -60:\n\t\tnhours -= 1\n\t\tnmins = 0\n\n\tif nhours == 24:\n\t\tndays += 1\n\t\tnhours = 0\n\n\tif nhours == -24:\n\t\tndays -= 1\n\t\tnhours = 0\n\n\tif nsecs == 60:\n\t\tnmins += 1\n\t\tnsecs = 0\n\n\tif nsecs == -60:\n\t\tnmins -= 1\n\t\tnsecs = 0\n\n\tif nleaps == 60:\n\t\tn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tif self._norm is not None:\n\t\treturn self._norm\n\n\t# First, we must normalize the relative attributes.\n\tyears = self._years\n\tmonths = self._months\n\tdays = self._days\n\tleapdays = self._leapdays\n\thours = self._hours\n\tminutes = self._minutes\n\tseconds = self._seconds\n\tmicroseconds = self._microseconds\n\n\tyears += months // 12\n\tmonths = months % 12\n\n\tdays += hours // 24\n\thours = hours % 24\n\n\tminutes += seconds // 60\n\tseconds = seconds % 60\n\n\tmicroseconds += seconds * 1000000\n\tseconds = 0\n\n\tminutes += microseconds // 60000000\n\tmicroseconds = microseconds % 60000000\n\n\thours += minutes // 60\n\tminutes = minutes % 60\n\n\tdays += hours // 24\n\thours = hours % 24\n\n\t# Now, we can normalize the absolute attributes.\n\tif self._day is not None:\n\t\tif self._day > 0:\n\t\t\twhile days < self._day:\n\t\t\t\tdays = days + 1\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tdays, hours, minutes, seconds, microseconds = self.get_parts()\n\thours += days * 24\n\tminutes += hours * 60\n\tseconds += minutes * 60\n\tmicroseconds += seconds * 1000000\n\treturn relativedelta(\n\t\tdays=0,\n\t\thours=0,\n\t\tminutes=0,\n\t\tseconds=0,\n\t\tmicroseconds=microseconds,\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\t\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tyears = self.years\n\tmonths = self.months\n\tdays = self.days\n\tleapdays = self.leapdays\n\thours = self.hours\n\tminutes = self.minutes\n\tseconds = self.seconds\n\tmicroseconds = self.microseconds\n\tif months == 12:\n\t\tyears += 1\n\t\tmonths = 0\n\telif months == -12:\n\t\tyears -= 1\n\t\tmonths = 0\n\tif days < 0:\n\t\textra_months = math.floor(days / DAYSPERMONTH)\n\t\tmonths += extra_months\n\t\tdays -= extra_months * DAYSPERMONTH\n\telif days > DAYSPERMONTH:\n\t\textra_months = math.floor(days / DAYSPERMONTH)\n\t\tmonths += extra_months\n\t\tdays -= extra_months * DAYSPERMONTH\n\tif days < 0 and months > 0:\n\t\tmonths -= 1\n\t\tdays += DAYSPERMONTH\n\telif days > DAYSPERMONTH and months < 0:\n\t\tmonths += 1\n\t\tdays\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\treturn self._fix(normalize=1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\treturn self.__class__(\n\t\tyears=self._years,\n\t\tmonths=self._months,\n\t\tdays=self._days,\n\t\tleapdays=self._leapdays,\n\t\thours=self._hours,\n\t\tminutes=self._minutes,\n\t\tseconds=self._seconds,\n\t\tmicroseconds=self._microseconds,\n\t\tyear=self._year,\n\t\tmonth=self._month,\n\t\tday=self._day,\n\t\tweekday=self._weekday,\n\t\thour=self._hour,\n\t\tminute=self._minute,\n\t\tsecond=self._second,\n\t\tmicrosecond=self._microsecond,\n\t\tyear_day=self._year_day,\n\t\tmonth_day=self._month_day,\n\t\tn_leapdays=self._n_leapdays,\n\t\tday_counter=self._day_counter,\n\t\tweekday_counter=self._weekday_counter,\n\t\thour_counter=self._hour_counter,\n\t\tminute_counter=self._\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "ques_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n\tif not PY2:\n\t\treturn namefunc\n\n\tdef newnamefunc(self, dt):\n\t\tname = namefunc(self, dt)\n\t\tif isinstance(name, text_type):\n\t\t\tname = name.encode('ascii')\n\t\treturn name\n\treturn newnamefunc\n\n\n", "def tzname_in_python2(namefunc):\n\tif python2:\n\t\t@wraps(namefunc)\n\t\tdef newfunc(*args, **kwargs):\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\treturn newfunc\n\telse:\n\t\treturn namefunc\n\n", "def tzname_in_python2(namefunc):\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif not isinstance(result, str):\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\n\n", "def tzname_in_python2(namefunc):\n\tif sys.version_info.major == 2:\n\t\tdef tzname_in_python2_decorator(self, *args, **kwargs):\n\t\t\tname = namefunc(self, *args, **kwargs)\n\t\t\treturn name.encode('utf-8')\n\t\treturn tzname_in_python2_decorator\n\treturn namefunc\n\n\n", "def tzname_in_python2(namefunc):\n\t@functools.wraps(namefunc)\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif isinstance(result, text_type):\n\t\t\treturn result.encode('ascii')\n\t\treturn result\n\treturn wrapper\n\n\n", "def tzname_in_python2(namefunc):\n\timport sys\n\tif sys.version_info[0] == 2:\n\t\tdef tzname_in_python2_wrapper(*args, **kwargs):\n\t\t\treturn [str(i) for i in namefunc(*args, **kwargs)]\n\t\treturn tzname_in_python2_wrapper\n\treturn namefunc\n\n", "def tzname_in_python2(namefunc):\n\tdef tzname_in_python2_decorator(self, dt):\n\t\ttzname = namefunc(self, dt)\n\t\tif isinstance(tzname, six.text_type):\n\t\t\ttzname = tzname.encode(\"utf-8\")\n\t\treturn tzname\n\treturn tzname_in_python2_decorator\n\n\n", "def tzname_in_python2(namefunc):\n\tdef wrapper(self, dt):\n\t\tif not PYTHON3:\n\t\t\treturn (namefunc(self, dt)[0] or '').encode('utf-8'), \\\n\t\t\t\t(namefunc(self, dt)[1] or '').encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(self, dt)\n\treturn wrapper\n\n", "def tzname_in_python2(namefunc):\n\tdef newfunc(dt):\n\t\trv = namefunc(dt)\n\t\tif not isinstance(rv, str):\n\t\t\treturn rv.encode('utf-8')\n\t\treturn rv\n\treturn newfunc\n\n", "def tzname_in_python2(namefunc):\n\tdef wrapper(*args, **kwargs):\n\t\tret = namefunc(*args, **kwargs)\n\t\tif sys.version_info[0] < 3:\n\t\t\tret = [x.encode('utf-8') for x in ret]\n\t\treturn ret\n\treturn wrapper\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n\tif not PY2:\n\t\treturn namefunc\n\n\tdef newnamefunc(self, dt):\n\t\tname = namefunc(self, dt)\n\t\tif isinstance(name, text_type):\n\t\t\tname = name.encode('ascii')\n\t\treturn name\n\treturn newnamefunc\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\tif python2:\n\t\t@wraps(namefunc)\n\t\tdef newfunc(*args, **kwargs):\n\t\t\treturn namefunc(*args, **kwargs).encode('utf-8')\n\t\treturn newfunc\n\telse:\n\t\treturn namefunc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif not isinstance(result, str):\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\tif sys.version_info.major == 2:\n\t\tdef tzname_in_python2_decorator(self, *args, **kwargs):\n\t\t\tname = namefunc(self, *args, **kwargs)\n\t\t\treturn name.encode('utf-8')\n\t\treturn tzname_in_python2_decorator\n\treturn namefunc\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t@functools.wraps(namefunc)\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif isinstance(result, text_type):\n\t\t\treturn result.encode('ascii')\n\t\treturn result\n\treturn wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\timport sys\n\tif sys.version_info[0] == 2:\n\t\tdef tzname_in_python2_wrapper(*args, **kwargs):\n\t\t\treturn [str(i) for i in namefunc(*args, **kwargs)]\n\t\treturn tzname_in_python2_wrapper\n\treturn namefunc\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\tdef tzname_in_python2_decorator(self, dt):\n\t\ttzname = namefunc(self, dt)\n\t\tif isinstance(tzname, six.text_type):\n\t\t\ttzname = tzname.encode(\"utf-8\")\n\t\treturn tzname\n\treturn tzname_in_python2_decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\tdef wrapper(self, dt):\n\t\tif not PYTHON3:\n\t\t\treturn (namefunc(self, dt)[0] or '').encode('utf-8'), \\\n\t\t\t\t(namefunc(self, dt)[1] or '').encode('utf-8')\n\t\telse:\n\t\t\treturn namefunc(self, dt)\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\tdef newfunc(dt):\n\t\trv = namefunc(dt)\n\t\tif not isinstance(rv, str):\n\t\t\treturn rv.encode('utf-8')\n\t\treturn rv\n\treturn newfunc\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\tdef wrapper(*args, **kwargs):\n\t\tret = namefunc(*args, **kwargs)\n\t\tif sys.version_info[0] < 3:\n\t\t\tret = [x.encode('utf-8') for x in ret]\n\t\treturn ret\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "ques_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution('pyshark').version\n\texcept Exception:\n\t\treturn \"UNKNOWN\"\n\n\n", "def get_versions():\n\tversion_file = os.path.join(os.path.dirname(__file__), 'VERSION')\n\ttry:\n\t\treturn open(version_file).read().strip()\n\texcept IOError:\n\t\treturn '0.0.0'\n\n\n", "def get_versions():\n\tversion_info = {}\n\ttag_commit = None\n\tbranch = None\n\ttry:\n\t\timport git\n\t\trepo = git.Repo(os.path.dirname(os.path.abspath(__file__)))\n\t\tversion_info['commit'] = repo.head.commit.hexsha\n\t\tbranch = repo.active_branch.name\n\t\t# Get the tag that points to this commit\n\t\tfor tag in repo.tags:\n\t\t\tif tag.commit == repo.head.commit:\n\t\t\t\ttag_commit = tag\n\t\t\t\tbreak\n\texcept Exception:\n\t\t# Version information from git not available\n\t\tpass\n\t# Return a version string of the form:\n\t# <tag>/<branch>-<commit>\n\tif tag_commit:\n\t\tversion_info['tag'] = tag_commit.name\n\telse:\n\t\tversion_info['tag'] = 'unknown'\n\tif branch:\n\t\tversion_info['branch'] = branch\n\telse:\n\t\tversion_info['branch\n", "def get_versions():\n\tfrom ._version import get_versions\n\ttry:\n\t\treturn get_versions()\n\texcept ImportError:\n\t\treturn {'version': 'unknown'}\n", "def get_versions():\n\ttry:\n\t\tfrom ._version import get_versions\n\t\treturn get_versions()\n\texcept ImportError:\n\t\treturn dict(version=VERSION, dirty=False, error=True)\n\n", "def get_versions():\n\tdefault = 'Unknown'\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution('django-cron').version\n\texcept Exception:\n\t\treturn default\n\n", "def get_versions():\n\ttry:\n\t\tfrom . import version\n\t\treturn version.get_versions()\n\texcept ImportError:\n\t\treturn {'version': 'unknown', 'full-revisionid': 'unknown', 'error': str(sys.exc_info()[1])}\n\texcept AttributeError:\n\t\treturn {'version': 'unknown', 'full-revisionid': 'unknown', 'error': str(sys.exc_info()[1])}\n\texcept CommandError:\n\t\treturn {'version': 'unknown', 'full-revisionid': 'unknown', 'error': str(sys.exc_info()[1])}\n\n", "def get_versions():\n\tv = {}\n\ttry:\n\t\tfrom subprocess import Popen, PIPE\n\t\tp = Popen([\"git\", \"rev-parse\", \"HEAD\"], stdout=PIPE)\n\t\tv[\"full-revisionid\"] = p.communicate()[0].strip().decode(\"ascii\")\n\t\tp = Popen(\"git config --get remote.origin.url\".split(\" \"), stdout=PIPE)\n\t\tv[\"full-repository\"] = p.communicate()[0].strip().decode(\"ascii\")\n\t\tp = Popen(\"git config --get branch.master.merge\".split(\" \"), stdout=PIPE)\n\t\tv[\"branch\"] = p.communicate()[0].strip().decode(\"ascii\")\n\t\tv[\"revisionid\"] = v[\"full-revisionid\"][:7]\n\t\tv[\"repository\"] = \"/\".join(v[\"full-repository\"].split(\"/\")[-2:])\n\t\tv[\"dirty\"] = False\n\texcept:\n", "def get_versions():\n\ttry:\n\t\treturn json.load(open(os.path.join(os.path.dirname(__file__), 'version.json')))\n\texcept IOError as e:\n\t\tif e.errno != errno.ENOENT:\n\t\t\traise e\n\t\treturn {\n\t\t\t'__version__': 'unknown',\n\t\t\t'__git_revision__': 'unknown',\n\t\t}\n", "def get_versions():\n\ttry:\n\t\tfrom distutils.dist import DistributionNotFound\n\t\tfrom distutils.version import LooseVersion\n\texcept ImportError:\n\t\treturn {\n\t\t\t'__version__': \"unknown\",\n\t\t\t'__github_version__': \"unknown\"\n\t\t}\n\n\ttry:\n\t\tfrom ._version import version as __version__\n\texcept ImportError:\n\t\t__version__ = \"unknown\"\n\ttry:\n\t\tfrom ._github_version import version as __github_version__\n\texcept (ImportError, DistributionNotFound):\n\t\t__github_version__ = \"unknown\"\n\n\ttry:\n\t\tif LooseVersion(__version__) < LooseVersion(__github_version__):\n\t\t\t__version__ = __github_version__\n\texcept TypeError:\n\t\tpass\n\n\treturn {\n\t\t'__version__': __version__,\n\t\t'__github_version__': __github_version__,\n\t\t'__title__': \"pre-commit-hooks\",\n\t\t'__description__': \"A collection of pre-commit hooks\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution('pyshark').version\n\texcept Exception:\n\t\treturn \"UNKNOWN\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\tversion_file = os.path.join(os.path.dirname(__file__), 'VERSION')\n\ttry:\n\t\treturn open(version_file).read().strip()\n\texcept IOError:\n\t\treturn '0.0.0'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\tversion_info = {}\n\ttag_commit = None\n\tbranch = None\n\ttry:\n\t\timport git\n\t\trepo = git.Repo(os.path.dirname(os.path.abspath(__file__)))\n\t\tversion_info['commit'] = repo.head.commit.hexsha\n\t\tbranch = repo.active_branch.name\n\t\t# Get the tag that points to this commit\n\t\tfor tag in repo.tags:\n\t\t\tif tag.commit == repo.head.commit:\n\t\t\t\ttag_commit = tag\n\t\t\t\tbreak\n\texcept Exception:\n\t\t# Version information from git not available\n\t\tpass\n\t# Return a version string of the form:\n\t# <tag>/<branch>-<commit>\n\tif tag_commit:\n\t\tversion_info['tag'] = tag_commit.name\n\telse:\n\t\tversion_info['tag'] = 'unknown'\n\tif branch:\n\t\tversion_info['branch'] = branch\n\telse:\n\t\tversion_info['branch\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\tfrom ._version import get_versions\n\ttry:\n\t\treturn get_versions()\n\texcept ImportError:\n\t\treturn {'version': 'unknown'}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\tfrom ._version import get_versions\n\t\treturn get_versions()\n\texcept ImportError:\n\t\treturn dict(version=VERSION, dirty=False, error=True)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\tdefault = 'Unknown'\n\ttry:\n\t\timport pkg_resources\n\t\treturn pkg_resources.get_distribution('django-cron').version\n\texcept Exception:\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\tfrom . import version\n\t\treturn version.get_versions()\n\texcept ImportError:\n\t\treturn {'version': 'unknown', 'full-revisionid': 'unknown', 'error': str(sys.exc_info()[1])}\n\texcept AttributeError:\n\t\treturn {'version': 'unknown', 'full-revisionid': 'unknown', 'error': str(sys.exc_info()[1])}\n\texcept CommandError:\n\t\treturn {'version': 'unknown', 'full-revisionid': 'unknown', 'error': str(sys.exc_info()[1])}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\tv = {}\n\ttry:\n\t\tfrom subprocess import Popen, PIPE\n\t\tp = Popen([\"git\", \"rev-parse\", \"HEAD\"], stdout=PIPE)\n\t\tv[\"full-revisionid\"] = p.communicate()[0].strip().decode(\"ascii\")\n\t\tp = Popen(\"git config --get remote.origin.url\".split(\" \"), stdout=PIPE)\n\t\tv[\"full-repository\"] = p.communicate()[0].strip().decode(\"ascii\")\n\t\tp = Popen(\"git config --get branch.master.merge\".split(\" \"), stdout=PIPE)\n\t\tv[\"branch\"] = p.communicate()[0].strip().decode(\"ascii\")\n\t\tv[\"revisionid\"] = v[\"full-revisionid\"][:7]\n\t\tv[\"repository\"] = \"/\".join(v[\"full-repository\"].split(\"/\")[-2:])\n\t\tv[\"dirty\"] = False\n\texcept:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\treturn json.load(open(os.path.join(os.path.dirname(__file__), 'version.json')))\n\texcept IOError as e:\n\t\tif e.errno != errno.ENOENT:\n\t\t\traise e\n\t\treturn {\n\t\t\t'__version__': 'unknown',\n\t\t\t'__git_revision__': 'unknown',\n\t\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\tfrom distutils.dist import DistributionNotFound\n\t\tfrom distutils.version import LooseVersion\n\texcept ImportError:\n\t\treturn {\n\t\t\t'__version__': \"unknown\",\n\t\t\t'__github_version__': \"unknown\"\n\t\t}\n\n\ttry:\n\t\tfrom ._version import version as __version__\n\texcept ImportError:\n\t\t__version__ = \"unknown\"\n\ttry:\n\t\tfrom ._github_version import version as __github_version__\n\texcept (ImportError, DistributionNotFound):\n\t\t__github_version__ = \"unknown\"\n\n\ttry:\n\t\tif LooseVersion(__version__) < LooseVersion(__github_version__):\n\t\t\t__version__ = __github_version__\n\texcept TypeError:\n\t\tpass\n\n\treturn {\n\t\t'__version__': __version__,\n\t\t'__github_version__': __github_version__,\n\t\t'__title__': \"pre-commit-hooks\",\n\t\t'__description__': \"A collection of pre-commit hooks\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "ques_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n\t\n\t# Check style\n\tif type(style) is str:\n\t\tstyle = style.lower()\n\t\tif style == \"pep440\":\n\t\t\tstyle = \"PEP440\"\n\t\telif style == \"pep440-pre\":\n\t\t\tstyle = \"PEP440-pre\"\n\tif style not in STYLES:\n\t\traise ValueError(\"Invalid style: %r\" % style)\n\t\n\t# Render the version\n\tversion = STYLES[style].render(pieces)\n\t\n\t# Done\n\treturn version\n\n", "def render(pieces, style):\n\t\n\tif style == Style.NUMERIC:\n\t\treturn '.'.join(str(piece) for piece in pieces)\n\telif style == Style.ARABIC:\n\t\treturn '_'.join(RomanNumeral.convert(piece) for piece in pieces)\n\telif style == Style.ROMAN:\n\t\treturn '.'.join(RomanNumeral.convert(piece) for piece in pieces)\n\telif style == Style.ALPHABET:\n\t\treturn '.'.join(Alphabet.convert(piece) for piece in pieces)\n\telif style == Style.CUSTOM:\n\t\treturn '.'.join(Custom.convert(piece) for piece in pieces)\n\telse:\n\t\traise ValueError('Unknown style: %s' % style)\n\n", "def render(pieces, style):\n\t# Make sure we have a list of pieces\n\tif type(pieces) not in (tuple, list):\n\t\tpieces = [pieces]\n\t# Make sure we have a valid style\n\tstyle = style.lower()\n\tif style not in (\"x.y.z\", \"x.y\", \"x\"):\n\t\tstyle = \"x.y.z\"\n\t# Build the string\n\tif style == \"x.y.z\":\n\t\treturn \"{}.{}.{}\".format(*pieces)\n\telif style == \"x.y\":\n\t\treturn \"{}.{}\".format(*pieces[:2])\n\telif style == \"x\":\n\t\treturn \"{}\".format(pieces[0])\n", "def render(pieces, style):\n\tif style == 'all':\n\t\treturn '.'.join(pieces)\n\telif style == 'major':\n\t\treturn pieces[0]\n\telif style == 'minor':\n\t\treturn '%s.%s' % (pieces[0], pieces[1])\n\telif style == 'patch':\n\t\treturn '%s.%s.%s' % (pieces[0], pieces[1], pieces[2])\n\telif style == 'build':\n\t\treturn '%s.%s.%s.%s' % (pieces[0], pieces[1], pieces[2], pieces[3])\n\telif style == 'extension':\n\t\treturn '%s.%s.%s.%s-%s' % (pieces[0], pieces[1], pieces[2], pieces[3], pieces[4])\n\telif style == 'build-extension':\n\t\treturn '%s.%s.%s.%s-%s' % (pieces[0], pieces[1], pieces[2], pieces[3], pieces[4])\n\telse:\n", "def render(pieces, style):\n\tif style == 'long':\n\t\treturn '.'.join(pieces)\n\telif style == 'short':\n\t\treturn '.'.join(pieces[:2])\n\telse:\n\t\traise ValueError(f'Unknown style: {style}')\n\n\n", "def render(pieces, style):\n\tif style == \"dotted\":\n\t\treturn \".\".join(pieces)\n\telif style == \"dotted-sp\":\n\t\treturn \".\".join(pieces) + \" \"\n\telif style == \"underscore\":\n\t\treturn \"_\".join(pieces)\n\telif style == \"underscore-sp\":\n\t\treturn \"_\".join(pieces) + \" \"\n\telif style == \"dash\":\n\t\treturn \"-\".join(pieces)\n\telif style == \"dash-sp\":\n\t\treturn \"-\".join(pieces) + \" \"\n\telse:\n\t\traise ValueError(\"unknown style: \" + style)\n\n", "def render(pieces, style):\n\tif style == \"plain\":\n\t\treturn render_plain(pieces)\n\telif style == \"html\":\n\t\treturn render_html(pieces)\n\telif style == \"html-no-links\":\n\t\treturn render_html_no_links(pieces)\n\telif style == \"markdown\":\n\t\treturn render_markdown(pieces)\n\telif style == \"markdown-no-links\":\n\t\treturn render_markdown_no_links(pieces)\n\telse:\n\t\traise ValueError(\"unknown style: \" + repr(style))\n\n\n", "def render(pieces, style):\n\t# TODO: Implement this.\n\treturn pieces\n\n", "def render(pieces, style):\n\tif style == \"dot\":\n\t\treturn \".\".join(str(p) for p in pieces)\n\tif style == \"dash\":\n\t\treturn \"-\".join(str(p) for p in pieces)\n\tif style == \"tuple\":\n\t\treturn \"(%s)\" % \", \".join(str(p) for p in pieces)\n\tif style == \"tuple-dash\":\n\t\treturn \"(%s)\" % \" - \".join(str(p) for p in pieces)\n\tif style == \"tuple-dot\":\n\t\treturn \"(%s)\" % \" . \".join(str(p) for p in pieces)\n\traise ValueError(\"invalid style\")\n\t\n", "def render(pieces, style):\n\tif len(pieces) == 3:\n\t\treturn render_major_minor_patch(pieces, style)\n\telif len(pieces) == 4:\n\t\treturn render_major_minor_patch_extra(pieces, style)\n\telse:\n\t\treturn None\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n\t\n\t# Check style\n\tif type(style) is str:\n\t\tstyle = style.lower()\n\t\tif style == \"pep440\":\n\t\t\tstyle = \"PEP440\"\n\t\telif style == \"pep440-pre\":\n\t\t\tstyle = \"PEP440-pre\"\n\tif style not in STYLES:\n\t\traise ValueError(\"Invalid style: %r\" % style)\n\t\n\t# Render the version\n\tversion = STYLES[style].render(pieces)\n\t\n\t# Done\n\treturn version\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t\n\tif style == Style.NUMERIC:\n\t\treturn '.'.join(str(piece) for piece in pieces)\n\telif style == Style.ARABIC:\n\t\treturn '_'.join(RomanNumeral.convert(piece) for piece in pieces)\n\telif style == Style.ROMAN:\n\t\treturn '.'.join(RomanNumeral.convert(piece) for piece in pieces)\n\telif style == Style.ALPHABET:\n\t\treturn '.'.join(Alphabet.convert(piece) for piece in pieces)\n\telif style == Style.CUSTOM:\n\t\treturn '.'.join(Custom.convert(piece) for piece in pieces)\n\telse:\n\t\traise ValueError('Unknown style: %s' % style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t# Make sure we have a list of pieces\n\tif type(pieces) not in (tuple, list):\n\t\tpieces = [pieces]\n\t# Make sure we have a valid style\n\tstyle = style.lower()\n\tif style not in (\"x.y.z\", \"x.y\", \"x\"):\n\t\tstyle = \"x.y.z\"\n\t# Build the string\n\tif style == \"x.y.z\":\n\t\treturn \"{}.{}.{}\".format(*pieces)\n\telif style == \"x.y\":\n\t\treturn \"{}.{}\".format(*pieces[:2])\n\telif style == \"x\":\n\t\treturn \"{}\".format(pieces[0])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == 'all':\n\t\treturn '.'.join(pieces)\n\telif style == 'major':\n\t\treturn pieces[0]\n\telif style == 'minor':\n\t\treturn '%s.%s' % (pieces[0], pieces[1])\n\telif style == 'patch':\n\t\treturn '%s.%s.%s' % (pieces[0], pieces[1], pieces[2])\n\telif style == 'build':\n\t\treturn '%s.%s.%s.%s' % (pieces[0], pieces[1], pieces[2], pieces[3])\n\telif style == 'extension':\n\t\treturn '%s.%s.%s.%s-%s' % (pieces[0], pieces[1], pieces[2], pieces[3], pieces[4])\n\telif style == 'build-extension':\n\t\treturn '%s.%s.%s.%s-%s' % (pieces[0], pieces[1], pieces[2], pieces[3], pieces[4])\n\telse:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == 'long':\n\t\treturn '.'.join(pieces)\n\telif style == 'short':\n\t\treturn '.'.join(pieces[:2])\n\telse:\n\t\traise ValueError(f'Unknown style: {style}')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == \"dotted\":\n\t\treturn \".\".join(pieces)\n\telif style == \"dotted-sp\":\n\t\treturn \".\".join(pieces) + \" \"\n\telif style == \"underscore\":\n\t\treturn \"_\".join(pieces)\n\telif style == \"underscore-sp\":\n\t\treturn \"_\".join(pieces) + \" \"\n\telif style == \"dash\":\n\t\treturn \"-\".join(pieces)\n\telif style == \"dash-sp\":\n\t\treturn \"-\".join(pieces) + \" \"\n\telse:\n\t\traise ValueError(\"unknown style: \" + style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == \"plain\":\n\t\treturn render_plain(pieces)\n\telif style == \"html\":\n\t\treturn render_html(pieces)\n\telif style == \"html-no-links\":\n\t\treturn render_html_no_links(pieces)\n\telif style == \"markdown\":\n\t\treturn render_markdown(pieces)\n\telif style == \"markdown-no-links\":\n\t\treturn render_markdown_no_links(pieces)\n\telse:\n\t\traise ValueError(\"unknown style: \" + repr(style))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t# TODO: Implement this.\n\treturn pieces\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == \"dot\":\n\t\treturn \".\".join(str(p) for p in pieces)\n\tif style == \"dash\":\n\t\treturn \"-\".join(str(p) for p in pieces)\n\tif style == \"tuple\":\n\t\treturn \"(%s)\" % \", \".join(str(p) for p in pieces)\n\tif style == \"tuple-dash\":\n\t\treturn \"(%s)\" % \" - \".join(str(p) for p in pieces)\n\tif style == \"tuple-dot\":\n\t\treturn \"(%s)\" % \" . \".join(str(p) for p in pieces)\n\traise ValueError(\"invalid style\")\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif len(pieces) == 3:\n\t\treturn render_major_minor_patch(pieces, style)\n\telif len(pieces) == 4:\n\t\treturn render_major_minor_patch_extra(pieces, style)\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "ques_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n\terr = pieces['err']\n\tif err == '+':\n\t\treturn '.'\n\treturn '+'\n\n", "def plus_or_dot(pieces):\n\tif pieces[-1] == '+':\n\t\treturn '.'\n\treturn '+'\n\n", "def plus_or_dot(pieces):\n\tif '+' in pieces:\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "def plus_or_dot(pieces):\n\tfor piece in pieces:\n\t\tif piece == '+':\n\t\t\treturn '.'\n\treturn '+'\n\n", "def plus_or_dot(pieces):\n\treturn '+' if '+' not in pieces else '.'\n\n", "def plus_or_dot(pieces):\n\tif pieces[-1] == '+':\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "def plus_or_dot(pieces):\n\tif '+' in pieces:\n\t\treturn '.'\n\treturn '+'\n\n", "def plus_or_dot(pieces):\n\tif '+' in pieces:\n\t\treturn '.'\n\treturn '+'\n\n", "def plus_or_dot(pieces):\n\tif len(pieces) == 0 or pieces[-1] != \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "def plus_or_dot(pieces):\n\tif \"+\" in pieces:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n\terr = pieces['err']\n\tif err == '+':\n\t\treturn '.'\n\treturn '+'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tif pieces[-1] == '+':\n\t\treturn '.'\n\treturn '+'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tif '+' in pieces:\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n\tfor piece in pieces:\n\t\tif piece == '+':\n\t\t\treturn '.'\n\treturn '+'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n\treturn '+' if '+' not in pieces else '.'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n\tif pieces[-1] == '+':\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tif '+' in pieces:\n\t\treturn '.'\n\treturn '+'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n\tif '+' in pieces:\n\t\treturn '.'\n\treturn '+'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n\tif len(pieces) == 0 or pieces[-1] != \"+\":\n\t\treturn \"+\"\n\telse:\n\t\treturn \".\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tif \"+\" in pieces:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tpopen_kwargs = {'stdout': subprocess.PIPE,\n\t\t\t\t\t'stderr': subprocess.PIPE,\n\t\t\t\t\t'cwd': cwd,\n\t\t\t\t\t'env': env}\n\tfor args in commands:\n\t\t# NOTE: shell=True is required here to get the PATH searched!\n\t\tprocess = subprocess.Popen(args, shell=True, **popen_kwargs)\n\t\tout, err = process.communicate()\n\t\tif process.returncode != 0:\n\t\t\traise Exception('Command %s failed: %s' % (args, err))\n\t\tif verbose:\n\t\t\tif out:\n\t\t\t\tprint(out)\n\t\t\tif not hide_stderr and err:\n\t\t\t\tprint(err)\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tpopen_kwargs = dict(\n\t\tstdout=PIPE,\n\t\tstderr=(DEVNULL if hide_stderr else PIPE),\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\tif os.name == 'nt':\n\t\tpopen_kwargs['creationflags'] = 0x08000000  # CREATE_NO_WINDOW\n\n\tfor cmd in commands:\n\t\ttry:\n\t\t\tif verbose:\n\t\t\t\tprint('Running command: %s' % cmd)\n\t\t\tp = Popen(cmd, **popen_kwargs)\n\t\texcept Exception as e:\n\t\t\tprint('Failed to run command: %s' % cmd)\n\t\t\traise e\n\t\telse:\n\t\t\tbreak\n\telse:\n\t\traise Exception('Failed to run command: %s' % commands)\n\n\tout, err = p.communicate()\n\tif p.returncode != 0:\n\t\tif verbose and err:\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t\n\tif env is None:\n\t\tenv = {}\n\t\n\tfor command in commands:\n\t\ttry:\n\t\t\tcommand_env = os.environ.copy()\n\t\t\tcommand_env.update(env)\n\t\t\tif verbose:\n\t\t\t\tprint 'Running command: \"'+command+'\"'\n\t\t\tif sys.platform == 'win32':\n\t\t\t\t# Use shell=True on Windows, so shell-specific commands like \"dir\" work.\n\t\t\t\tshell = True\n\t\t\telse:\n\t\t\t\tshell = False\n\t\t\tif hide_stderr:\n\t\t\t\tstderr = open(os.devnull, 'w')\n\t\t\telse:\n\t\t\t\tstderr = None\n\t\t\tsubprocess.check_output(command, stderr=stderr, env=command_env, shell=shell, cwd=cwd)\n\t\texcept:\n\t\t\tprint 'Failed command:\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\t# If commands is a string then we just call it as is\n\t\tif isinstance(commands, str):\n\t\t\tstdout = check_output(commands, cwd=cwd, stderr=STDOUT, env=env)\n\t\t\tstdout = stdout.decode()\n\t\t\tif verbose:\n\t\t\t\tprint(stdout)\n\t\t\treturn stdout\n\n\t\t# If commands is a list then we assume the first one is the command,\n\t\t# and the rest are arguments.\n\t\tif isinstance(commands, list):\n\t\t\targs = commands[1:] + args\n\t\t\tcommands = commands[0]\n\t\t\tif verbose:\n\t\t\t\tprint('Executing:', commands, args)\n\t\t\tp = Popen([commands] + args, cwd=cwd, stdout=PIPE, stderr=(PIPE if hide_stderr else None), env=env)\n\t\t\toutput, stderr = p.communicate()\n\t\t\tif p.return\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tif args is None:\n\t\targs = []\n\n\tif isinstance(args, str):\n\t\targs = [args]\n\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\tif cwd and not os.path.exists(cwd):\n\t\tos.makedirs(cwd)\n\n\tif env is None:\n\t\tenv = os.environ\n\n\tfor command in commands:\n\t\tcmd = command + \" \" + \" \".join(args)\n\t\tprint(\"Running:\", cmd)\n\t\tif verbose:\n\t\t\tsubprocess.check_call(cmd, cwd=cwd, env=env, shell=True)\n\t\telse:\n\t\t\twith open(os.devnull, \"w\") as fnull:\n\t\t\t\tif hide_stderr:\n\t\t\t\t\tstderr = fnull\n\t\t\t\telse:\n\t\t\t\t\tstderr = None\n\t\t\t\tsubprocess\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tif not env:\n\t\tenv = os.environ.copy()\n\n\t# Find the first command that works\n\tfor command in commands:\n\t\ttry:\n\t\t\tsubprocess.check_call(\n\t\t\t\t[command] + args,\n\t\t\t\tcwd=cwd,\n\t\t\t\tstdout=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tenv=env\n\t\t\t)\n\t\t\treturn\n\t\texcept (OSError, subprocess.CalledProcessError):\n\t\t\tpass\n\n\t# Report failure\n\tif verbose:\n\t\tprint('Failed to execute one of the following commands:')\n\t\tfor command in commands:\n\t\t\tprint('  ' + command)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# Make sure its a list\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t# Merge the command and args\n\tcommands = [commands[0] + \" \" + args] + commands[1:]\n\tif env is None:\n\t\tenv = os.environ.copy()\n\tenv['PATH'] = os.environ['PATH']\n\t# Call the command(s)\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running: \" + command)\n\t\tif cwd:\n\t\t\tif hide_stderr:\n\t\t\t\tp = subprocess.Popen(command, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\t\telse:\n\t\t\t\tp = subprocess.Popen(command, cwd=cwd, env=env)\n\t\telse:\n\t\t\tif hide_stderr:\n\t\t\t\tp = subprocess.Popen(command, env=env, std\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# If we have a list of commands, we need to combine them into a single string\n\tif isinstance(commands, (list, tuple)):\n\t\tcommands = \" \".join(commands)\n\n\t# If we have a list of arguments, we need to combine them into a single string\n\tif isinstance(args, (list, tuple)):\n\t\targs = \" \".join(args)\n\n\t# Make sure we have a string\n\tassert isinstance(commands, str) and isinstance(args, str)\n\n\t# Construct the command string\n\tcommand = '%s %s' % (commands, args)\n\n\tif verbose:\n\t\tprint(command)\n\n\t# Execute the command\n\tsubprocess.check_call(command, cwd=cwd, shell=True, stderr=(subprocess.STDOUT if hide_stderr else None), env=env)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tp = None\n\tfor c in commands:\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = Popen(c, cwd=cwd, stdout=PIPE, stderr=(PIPE if hide_stderr else None), shell=True, env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = sys.exc_info()[1]\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint(\"unable to run command %s\" % c)\n\t\t\t\traise\n\tif p is None:\n\t\tif verbose:\n\t\t\tprint(\"no command %s found\" % commands[0])\n\t\treturn None, None\n\tstdout = p.communicate()[0].strip()\n\treturn stdout.decode('utf-8'), p.\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tstdout_text = None\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running command: {0} {1}\".format(command, args))\n\t\ttry:\n\t\t\tstdout_text = subprocess.check_output(\n\t\t\t\t[command] + args,\n\t\t\t\tstderr=subprocess.STDOUT,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env\n\t\t\t)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tif verbose or not hide_stderr:\n\t\t\t\tprint(\"Command failed: {0} {1}\".format(command, args))\n\t\t\t\tprint(e.output)\n\t\t\t\tprint(\"\")\n\t\t\tcontinue\n\t\telse:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Command succeeded: {0} {1}\".format(command, args))\n\t\t\tbreak\n\treturn stdout\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tpopen_kwargs = {'stdout': subprocess.PIPE,\n\t\t\t\t\t'stderr': subprocess.PIPE,\n\t\t\t\t\t'cwd': cwd,\n\t\t\t\t\t'env': env}\n\tfor args in commands:\n\t\t# NOTE: shell=True is required here to get the PATH searched!\n\t\tprocess = subprocess.Popen(args, shell=True, **popen_kwargs)\n\t\tout, err = process.communicate()\n\t\tif process.returncode != 0:\n\t\t\traise Exception('Command %s failed: %s' % (args, err))\n\t\tif verbose:\n\t\t\tif out:\n\t\t\t\tprint(out)\n\t\t\tif not hide_stderr and err:\n\t\t\t\tprint(err)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tpopen_kwargs = dict(\n\t\tstdout=PIPE,\n\t\tstderr=(DEVNULL if hide_stderr else PIPE),\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\tif os.name == 'nt':\n\t\tpopen_kwargs['creationflags'] = 0x08000000  # CREATE_NO_WINDOW\n\n\tfor cmd in commands:\n\t\ttry:\n\t\t\tif verbose:\n\t\t\t\tprint('Running command: %s' % cmd)\n\t\t\tp = Popen(cmd, **popen_kwargs)\n\t\texcept Exception as e:\n\t\t\tprint('Failed to run command: %s' % cmd)\n\t\t\traise e\n\t\telse:\n\t\t\tbreak\n\telse:\n\t\traise Exception('Failed to run command: %s' % commands)\n\n\tout, err = p.communicate()\n\tif p.returncode != 0:\n\t\tif verbose and err:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t\n\tif env is None:\n\t\tenv = {}\n\t\n\tfor command in commands:\n\t\ttry:\n\t\t\tcommand_env = os.environ.copy()\n\t\t\tcommand_env.update(env)\n\t\t\tif verbose:\n\t\t\t\tprint 'Running command: \"'+command+'\"'\n\t\t\tif sys.platform == 'win32':\n\t\t\t\t# Use shell=True on Windows, so shell-specific commands like \"dir\" work.\n\t\t\t\tshell = True\n\t\t\telse:\n\t\t\t\tshell = False\n\t\t\tif hide_stderr:\n\t\t\t\tstderr = open(os.devnull, 'w')\n\t\t\telse:\n\t\t\t\tstderr = None\n\t\t\tsubprocess.check_output(command, stderr=stderr, env=command_env, shell=shell, cwd=cwd)\n\t\texcept:\n\t\t\tprint 'Failed command:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\t# If commands is a string then we just call it as is\n\t\tif isinstance(commands, str):\n\t\t\tstdout = check_output(commands, cwd=cwd, stderr=STDOUT, env=env)\n\t\t\tstdout = stdout.decode()\n\t\t\tif verbose:\n\t\t\t\tprint(stdout)\n\t\t\treturn stdout\n\n\t\t# If commands is a list then we assume the first one is the command,\n\t\t# and the rest are arguments.\n\t\tif isinstance(commands, list):\n\t\t\targs = commands[1:] + args\n\t\t\tcommands = commands[0]\n\t\t\tif verbose:\n\t\t\t\tprint('Executing:', commands, args)\n\t\t\tp = Popen([commands] + args, cwd=cwd, stdout=PIPE, stderr=(PIPE if hide_stderr else None), env=env)\n\t\t\toutput, stderr = p.communicate()\n\t\t\tif p.return\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tif args is None:\n\t\targs = []\n\n\tif isinstance(args, str):\n\t\targs = [args]\n\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\tif cwd and not os.path.exists(cwd):\n\t\tos.makedirs(cwd)\n\n\tif env is None:\n\t\tenv = os.environ\n\n\tfor command in commands:\n\t\tcmd = command + \" \" + \" \".join(args)\n\t\tprint(\"Running:\", cmd)\n\t\tif verbose:\n\t\t\tsubprocess.check_call(cmd, cwd=cwd, env=env, shell=True)\n\t\telse:\n\t\t\twith open(os.devnull, \"w\") as fnull:\n\t\t\t\tif hide_stderr:\n\t\t\t\t\tstderr = fnull\n\t\t\t\telse:\n\t\t\t\t\tstderr = None\n\t\t\t\tsubprocess\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tif not env:\n\t\tenv = os.environ.copy()\n\n\t# Find the first command that works\n\tfor command in commands:\n\t\ttry:\n\t\t\tsubprocess.check_call(\n\t\t\t\t[command] + args,\n\t\t\t\tcwd=cwd,\n\t\t\t\tstdout=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tenv=env\n\t\t\t)\n\t\t\treturn\n\t\texcept (OSError, subprocess.CalledProcessError):\n\t\t\tpass\n\n\t# Report failure\n\tif verbose:\n\t\tprint('Failed to execute one of the following commands:')\n\t\tfor command in commands:\n\t\t\tprint('  ' + command)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# Make sure its a list\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t# Merge the command and args\n\tcommands = [commands[0] + \" \" + args] + commands[1:]\n\tif env is None:\n\t\tenv = os.environ.copy()\n\tenv['PATH'] = os.environ['PATH']\n\t# Call the command(s)\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running: \" + command)\n\t\tif cwd:\n\t\t\tif hide_stderr:\n\t\t\t\tp = subprocess.Popen(command, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\t\t\telse:\n\t\t\t\tp = subprocess.Popen(command, cwd=cwd, env=env)\n\t\telse:\n\t\t\tif hide_stderr:\n\t\t\t\tp = subprocess.Popen(command, env=env, std\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# If we have a list of commands, we need to combine them into a single string\n\tif isinstance(commands, (list, tuple)):\n\t\tcommands = \" \".join(commands)\n\n\t# If we have a list of arguments, we need to combine them into a single string\n\tif isinstance(args, (list, tuple)):\n\t\targs = \" \".join(args)\n\n\t# Make sure we have a string\n\tassert isinstance(commands, str) and isinstance(args, str)\n\n\t# Construct the command string\n\tcommand = '%s %s' % (commands, args)\n\n\tif verbose:\n\t\tprint(command)\n\n\t# Execute the command\n\tsubprocess.check_call(command, cwd=cwd, shell=True, stderr=(subprocess.STDOUT if hide_stderr else None), env=env)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tp = None\n\tfor c in commands:\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = Popen(c, cwd=cwd, stdout=PIPE, stderr=(PIPE if hide_stderr else None), shell=True, env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = sys.exc_info()[1]\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint(\"unable to run command %s\" % c)\n\t\t\t\traise\n\tif p is None:\n\t\tif verbose:\n\t\t\tprint(\"no command %s found\" % commands[0])\n\t\treturn None, None\n\tstdout = p.communicate()[0].strip()\n\treturn stdout.decode('utf-8'), p.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tstdout_text = None\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running command: {0} {1}\".format(command, args))\n\t\ttry:\n\t\t\tstdout_text = subprocess.check_output(\n\t\t\t\t[command] + args,\n\t\t\t\tstderr=subprocess.STDOUT,\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env\n\t\t\t)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tif verbose or not hide_stderr:\n\t\t\t\tprint(\"Command failed: {0} {1}\".format(command, args))\n\t\t\t\tprint(e.output)\n\t\t\t\tprint(\"\")\n\t\t\tcontinue\n\t\telse:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Command succeeded: {0} {1}\".format(command, args))\n\t\t\tbreak\n\treturn stdout\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "ques_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n\tconfig = VersioneerConfig()\n\n\t# parse the \"versioneer.cfg\" file, if it exists\n\ttry:\n\t\tconf_fp = open(\"versioneer.cfg\")\n\texcept IOError:\n\t\tpass\n\telse:\n\t\tconfig_parser = ConfigParser.RawConfigParser()\n\t\tconfig_parser.readfp(conf_fp)\n\t\tconf_fp.close()\n\n\t\t# [versioneer]\n\t\tif config_parser.has_section(\"versioneer\"):\n\t\t\tif config_parser.has_option(\"versioneer\", \"VCS\"):\n\t\t\t\tconfig.VCS = config_parser.get(\"versioneer\", \"VCS\")\n\t\t\tif config_parser.has_option(\"versioneer\", \"style\"):\n\t\t\t\tconfig.style = config_parser.get(\"versioneer\", \"style\")\n\t\t\tif config_parser.has_option(\"versioneer\", \"versionfile_source\"):\n\t\t\t\tconfig.versionfile_source = config_parser.get(\"versioneer\", \"version\n", "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# set the version identifier from VCS state\n\tif state.get(\"version_pep440_git\", False):\n\t\tversion = version_pep440_git(state[\"this_version\"])\n\telse:\n\t\tversion = version_pep440_prerelease(state[\"this_version\"])\n\tconfig.version = version\n\tstate[\"version\"] = version\n\n\t# set the version_name from VCS state\n\tversion_name = state[\"version_name_template\"].format(**state)\n\tconfig.version_name = version_name\n\tstate[\"version_name\"] = version_name\n\n\t# set the package_name from VCS state\n\tpackage_name = state[\"package_name_template\"].format(**state)\n\tconfig.package_name = package_name\n\tstate[\"package_name\"] = package_name\n\n\t# set the package_namespace from VCS state\n\tpackage_namespace = state[\"package_namespace_template\"].format(**state)\n\tconfig.package_namespace = package_namespace\n\tstate[\"\n", "def get_config():\n\t# if we're not in a git repo, bail\n\tif not os.path.isdir(\".git\"):\n\t\treturn None\n\n\t# if we are in a git repo, we expect to be able to execute git\n\tif not shutil.which(\"git\"):\n\t\tsys.exit(\"git executable not found; cannot use _version.py versioning\")\n\n\tvc = VersioneerConfig()\n\n\t# read in config\n\ttry:\n\t\twith open(vc.config_filename) as f:\n\t\t\texec(compile(f.read(), vc.config_filename, \"exec\"), vc.__dict__)\n\texcept IOError:\n\t\tpass  # that's ok, use defaults\n\n\t# sanity-check some of the values\n\tif not isinstance(vc.VCS, str):\n\t\tsys.exit(\n\t\t\t\"%s must contain a string naming the version-control system in use\"\n\t\t\t% vc.config_filename\n\t\t)\n\tvc.VCS = vc.VCS.lower()\n\tif vc.VCS not in (\"\n", "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# set defaults\n\tconfig.VCS = \"git\"\n\tconfig.versionfile_source = \"versioneer.py\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"versioneer-\"\n\n\t# allow setup.cfg to override these\n\tparser = ConfigParser.ConfigParser()\n\tparser.read(\"setup.cfg\")\n\tif parser.has_section(\"versioneer\"):\n\t\tif parser.has_option(\"versioneer\", \"VCS\"):\n\t\t\tconfig.VCS = parser.get(\"versioneer\", \"VCS\")\n\t\tif parser.has_option(\"versioneer\", \"versionfile_source\"):\n\t\t\tconfig.versionfile_source = parser.get(\"versioneer\", \"versionfile_source\")\n\t\tif parser.has_option(\"versioneer\", \"tag_prefix\"):\n\t\t\tconfig.tag_prefix = parser.get(\"versioneer\", \"tag_prefix\")\n\t\tif parser.has_option(\"versioneer\", \"parentdir_prefix\"):\n\t\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.get_version_from_tag = get_version_from_tag\n\tconfig.tag_prefix = \"v\"\n\tconfig.parentdir_prefix = \"versioneer-\"\n\tconfig.VCS = \"git\"\n\treturn config\n\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = \"git\"\n\n\t# compute the current version string\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--dirty\", \"--always\"]\n\tdescribe_version = subprocess.check_output(cmd).strip()\n\tconfig.version = describe_version.decode('utf-8')\n\n\t# compute the version string without dirty flag\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--always\"]\n\tdescribe_version = subprocess.check_output(cmd).strip()\n\tconfig.full_version = describe_version.decode('utf-8')\n\n\t# compute the short version string\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--abbrev=0\"]\n\tdescribe_version = subprocess.check_output(cmd).strip()\n\tconfig.version_prefix = describe_version.decode('utf-8')\n\n\t# compute the long version string\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--long\", \"\n", "def get_config():\n\tconfig = VersioneerConfig()\n\n\tif not os.path.exists(config.root):\n\t\traise VersioneerConfigError(\"root directory '%s' does not exist\" % config.root)\n\n\t# if --version is given, we can skip a lot of the work, since we only\n\t# need the version.\n\tif config.version:\n\t\treturn config\n\n\t# if we're invoked as \"versioneer.py\", use the CWD as the root directory,\n\t# otherwise use the directory containing versioneer.py as the root.\n\tif config.prog == \"versioneer.py\":\n\t\tconfig.root = os.getcwd()\n\telse:\n\t\tconfig.root = os.path.abspath(os.path.dirname(config.prog))\n\n\t# read config from {root}/pyproject.toml\n\tconfig.read_pyproject()\n\n\t# read config from {root}/_version.py\n\tconfig.read_module()\n\n\t# read config from {root}/setup.cfg\n\tconfig.read_setupcfg()\n\n\t# read config\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.root = _get_root()\n\tconfig.verbose = _get_verbose()\n\tconfig.VCS = _get_vcs(config)\n\tconfig.versionfile_source = _get_versionfile_source(config)\n\tconfig.versionfile_build = _get_versionfile_build(config)\n\tconfig.tag_prefix = _get_tag_prefix(config)\n\tconfig.parentdir_prefix = _get_parentdir_prefix(config)\n\tconfig.version_prefix = _get_version_prefix(config)\n\tconfig.run_tests = _get_run_tests(config)\n\treturn config\n\n", "def get_config():\n\t# load values from setup.cfg, if it exists\n\tconfig = VersioneerConfig()\n\tconfig_vars = vars(config)\n\tconfig_file = os.environ.get('VERSIO\n", "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# load the config from the src tree's _version.py file\n\tif not config.VCS:\n\t\tversioneer_py = os.path.join(config.root, config.versionfile_source)\n\t\tif not os.path.exists(versioneer_py):\n\t\t\tprint(\"%s not found\" % versioneer_py)\n\t\t\tsys.exit(1)\n\t\ttry:\n\t\t\texec(compile(open(versioneer_py, \"rb\").read(), versioneer_py, 'exec'), {}, config.__dict__)\n\t\texcept Exception as e:\n\t\t\tprint(\"error: %s while reading %s\" % (e, versioneer_py))\n\t\t\tsys.exit(1)\n\n\t# sanity-check the result\n\tif not config.VCS:\n\t\tprint(\"no VCS defined\")\n\t\tsys.exit(1)\n\tif not config.style:\n\t\tprint(\"no style defined\")\n\t\tsys.exit(1)\n\n\tif config.VCS\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# parse the \"versioneer.cfg\" file, if it exists\n\ttry:\n\t\tconf_fp = open(\"versioneer.cfg\")\n\texcept IOError:\n\t\tpass\n\telse:\n\t\tconfig_parser = ConfigParser.RawConfigParser()\n\t\tconfig_parser.readfp(conf_fp)\n\t\tconf_fp.close()\n\n\t\t# [versioneer]\n\t\tif config_parser.has_section(\"versioneer\"):\n\t\t\tif config_parser.has_option(\"versioneer\", \"VCS\"):\n\t\t\t\tconfig.VCS = config_parser.get(\"versioneer\", \"VCS\")\n\t\t\tif config_parser.has_option(\"versioneer\", \"style\"):\n\t\t\t\tconfig.style = config_parser.get(\"versioneer\", \"style\")\n\t\t\tif config_parser.has_option(\"versioneer\", \"versionfile_source\"):\n\t\t\t\tconfig.versionfile_source = config_parser.get(\"versioneer\", \"version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# set the version identifier from VCS state\n\tif state.get(\"version_pep440_git\", False):\n\t\tversion = version_pep440_git(state[\"this_version\"])\n\telse:\n\t\tversion = version_pep440_prerelease(state[\"this_version\"])\n\tconfig.version = version\n\tstate[\"version\"] = version\n\n\t# set the version_name from VCS state\n\tversion_name = state[\"version_name_template\"].format(**state)\n\tconfig.version_name = version_name\n\tstate[\"version_name\"] = version_name\n\n\t# set the package_name from VCS state\n\tpackage_name = state[\"package_name_template\"].format(**state)\n\tconfig.package_name = package_name\n\tstate[\"package_name\"] = package_name\n\n\t# set the package_namespace from VCS state\n\tpackage_namespace = state[\"package_namespace_template\"].format(**state)\n\tconfig.package_namespace = package_namespace\n\tstate[\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t# if we're not in a git repo, bail\n\tif not os.path.isdir(\".git\"):\n\t\treturn None\n\n\t# if we are in a git repo, we expect to be able to execute git\n\tif not shutil.which(\"git\"):\n\t\tsys.exit(\"git executable not found; cannot use _version.py versioning\")\n\n\tvc = VersioneerConfig()\n\n\t# read in config\n\ttry:\n\t\twith open(vc.config_filename) as f:\n\t\t\texec(compile(f.read(), vc.config_filename, \"exec\"), vc.__dict__)\n\texcept IOError:\n\t\tpass  # that's ok, use defaults\n\n\t# sanity-check some of the values\n\tif not isinstance(vc.VCS, str):\n\t\tsys.exit(\n\t\t\t\"%s must contain a string naming the version-control system in use\"\n\t\t\t% vc.config_filename\n\t\t)\n\tvc.VCS = vc.VCS.lower()\n\tif vc.VCS not in (\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# set defaults\n\tconfig.VCS = \"git\"\n\tconfig.versionfile_source = \"versioneer.py\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"versioneer-\"\n\n\t# allow setup.cfg to override these\n\tparser = ConfigParser.ConfigParser()\n\tparser.read(\"setup.cfg\")\n\tif parser.has_section(\"versioneer\"):\n\t\tif parser.has_option(\"versioneer\", \"VCS\"):\n\t\t\tconfig.VCS = parser.get(\"versioneer\", \"VCS\")\n\t\tif parser.has_option(\"versioneer\", \"versionfile_source\"):\n\t\t\tconfig.versionfile_source = parser.get(\"versioneer\", \"versionfile_source\")\n\t\tif parser.has_option(\"versioneer\", \"tag_prefix\"):\n\t\t\tconfig.tag_prefix = parser.get(\"versioneer\", \"tag_prefix\")\n\t\tif parser.has_option(\"versioneer\", \"parentdir_prefix\"):\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.get_version_from_tag = get_version_from_tag\n\tconfig.tag_prefix = \"v\"\n\tconfig.parentdir_prefix = \"versioneer-\"\n\tconfig.VCS = \"git\"\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = \"git\"\n\n\t# compute the current version string\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--dirty\", \"--always\"]\n\tdescribe_version = subprocess.check_output(cmd).strip()\n\tconfig.version = describe_version.decode('utf-8')\n\n\t# compute the version string without dirty flag\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--always\"]\n\tdescribe_version = subprocess.check_output(cmd).strip()\n\tconfig.full_version = describe_version.decode('utf-8')\n\n\t# compute the short version string\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--abbrev=0\"]\n\tdescribe_version = subprocess.check_output(cmd).strip()\n\tconfig.version_prefix = describe_version.decode('utf-8')\n\n\t# compute the long version string\n\tcmd = [config.VCS, \"describe\", \"--tags\", \"--long\", \"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\n\tif not os.path.exists(config.root):\n\t\traise VersioneerConfigError(\"root directory '%s' does not exist\" % config.root)\n\n\t# if --version is given, we can skip a lot of the work, since we only\n\t# need the version.\n\tif config.version:\n\t\treturn config\n\n\t# if we're invoked as \"versioneer.py\", use the CWD as the root directory,\n\t# otherwise use the directory containing versioneer.py as the root.\n\tif config.prog == \"versioneer.py\":\n\t\tconfig.root = os.getcwd()\n\telse:\n\t\tconfig.root = os.path.abspath(os.path.dirname(config.prog))\n\n\t# read config from {root}/pyproject.toml\n\tconfig.read_pyproject()\n\n\t# read config from {root}/_version.py\n\tconfig.read_module()\n\n\t# read config from {root}/setup.cfg\n\tconfig.read_setupcfg()\n\n\t# read config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.root = _get_root()\n\tconfig.verbose = _get_verbose()\n\tconfig.VCS = _get_vcs(config)\n\tconfig.versionfile_source = _get_versionfile_source(config)\n\tconfig.versionfile_build = _get_versionfile_build(config)\n\tconfig.tag_prefix = _get_tag_prefix(config)\n\tconfig.parentdir_prefix = _get_parentdir_prefix(config)\n\tconfig.version_prefix = _get_version_prefix(config)\n\tconfig.run_tests = _get_run_tests(config)\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\t# load values from setup.cfg, if it exists\n\tconfig = VersioneerConfig()\n\tconfig_vars = vars(config)\n\tconfig_file = os.environ.get('VERSIO\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# load the config from the src tree's _version.py file\n\tif not config.VCS:\n\t\tversioneer_py = os.path.join(config.root, config.versionfile_source)\n\t\tif not os.path.exists(versioneer_py):\n\t\t\tprint(\"%s not found\" % versioneer_py)\n\t\t\tsys.exit(1)\n\t\ttry:\n\t\t\texec(compile(open(versioneer_py, \"rb\").read(), versioneer_py, 'exec'), {}, config.__dict__)\n\t\texcept Exception as e:\n\t\t\tprint(\"error: %s while reading %s\" % (e, versioneer_py))\n\t\t\tsys.exit(1)\n\n\t# sanity-check the result\n\tif not config.VCS:\n\t\tprint(\"no VCS defined\")\n\t\tsys.exit(1)\n\tif not config.style:\n\t\tprint(\"no style defined\")\n\t\tsys.exit(1)\n\n\tif config.VCS\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "ques_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):\n\tdef decorator(cls):\n\t\tcls._vcs_handlers[vcs] = method\n\t\treturn cls\n\treturn decorator\n\n\n", "def register_vcs_handler(vcs, method):\n\tvcs.handler = method\n\treturn method\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\tf.vcs_handler = vcs\n\t\tf.vcs_method = method\n\t\treturn f\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\tdef register_vcs(func):\n\t\tfunc.vcs = vcs\n\n\t\t# Register on the global handler registry.\n\t\t_vcs_handlers.setdefault(vcs, []).append(func)\n\n\t\treturn func\n\n\treturn register_vcs\n\n\n", "def register_vcs_handler(vcs, method):\n\tvcs_handlers[vcs] = method\n\treturn method\n\n", "def register_vcs_handler(vcs, method):\n\tdef wrapper(func):\n\t\tglobal VCS_HANDLERS\n\t\tVCS_HANDLERS[vcs] = (func, method)\n\t\treturn func\n\treturn wrapper\n\n\n", "def register_vcs_handler(vcs, method):\n\tdef _register_vcs_handler(func):\n\t\t_vcs_handlers[vcs] = func\n\t\treturn func\n\treturn _register_vcs_handler\n\n", "def register_vcs_handler(vcs, method):\n\tdef wrapper(method):\n\t\tvcs_handlers[vcs] = method\n\t\treturn method\n\treturn wrapper\n\n\n", "def register_vcs_handler(vcs, method):\n\t\n\tdef wrapper(func):\n\t\t\n\t\tif not hasattr(func, 'vcs_handlers'):\n\t\t\tfunc.vcs_handlers = list()\n\t\t\n\t\tfunc.vcs_handlers.append((vcs, method))\n\t\t\n\t\treturn func\n\t\n\treturn wrapper\n\n\n", "def register_vcs_handler(vcs, method):\n\tif not isinstance(vcs, (list, tuple)):\n\t\tvcs = [vcs]\n\tdef register(func):\n\t\tfor v in vcs:\n\t\t\t_vcs_handlers[v][method] = func\n\t\treturn func\n\treturn register\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(cls):\n\t\tcls._vcs_handlers[vcs] = method\n\t\treturn cls\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tvcs.handler = method\n\treturn method\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\tf.vcs_handler = vcs\n\t\tf.vcs_method = method\n\t\treturn f\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef register_vcs(func):\n\t\tfunc.vcs = vcs\n\n\t\t# Register on the global handler registry.\n\t\t_vcs_handlers.setdefault(vcs, []).append(func)\n\n\t\treturn func\n\n\treturn register_vcs\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tvcs_handlers[vcs] = method\n\treturn method\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef wrapper(func):\n\t\tglobal VCS_HANDLERS\n\t\tVCS_HANDLERS[vcs] = (func, method)\n\t\treturn func\n\treturn wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef _register_vcs_handler(func):\n\t\t_vcs_handlers[vcs] = func\n\t\treturn func\n\treturn _register_vcs_handler\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef wrapper(method):\n\t\tvcs_handlers[vcs] = method\n\t\treturn method\n\treturn wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\t\n\tdef wrapper(func):\n\t\t\n\t\tif not hasattr(func, 'vcs_handlers'):\n\t\t\tfunc.vcs_handlers = list()\n\t\t\n\t\tfunc.vcs_handlers.append((vcs, method))\n\t\t\n\t\treturn func\n\t\n\treturn wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tif not isinstance(vcs, (list, tuple)):\n\t\tvcs = [vcs]\n\tdef register(func):\n\t\tfor v in vcs:\n\t\t\t_vcs_handlers[v][method] = func\n\t\treturn func\n\treturn register\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "ques_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif validate_objects:\n\t\t\tnum_objects, good_objects, bad_objects = self.validate_objects(check_digests=check_digests, show_warnings=show_warnings)\n\t\telse:\n\t\t\tnum_objects, good_objects, bad_objects = 0, 0, 0\n\n\t\tnum_dirs, good_dirs, bad_dirs = self.validate_dirs()\n\n\t\t#print(\"Num objects: %d, good objects: %d, bad objects: %d\" % (num_objects, good_objects, bad_objects))\n\t\t#print(\"Num dirs: %d, good dirs: %d, bad dirs: %d\" % (num_dirs, good_dirs, bad_dirs))\n\n\t\treturn num_objects, good_objects, bad_objects, num_dirs, good_dirs, bad_dirs\n\n\tdef validate_objects(self, check_digests=True, show_warnings=False):\n\t\t\"\"\"\n\t\tValidate objects on storage root.\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor f in self.list_objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj = self.get_object(f)\n\t\t\tif validate_objects:\n\t\t\t\tobj.validate()\n\t\t\t\tif check_digests:\n\t\t\t\t\tobj.check_digests()\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint('%s is invalid: %s' % (f, e))\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tself.log.debug(\"Validating storage root hierarchy\")\n\n\t# Get all storage root objects\n\tobjects = self.get_objects(storage_roots=True, validate_objects=validate_objects)\n\tnum_objects = len(objects)\n\n\tgood_objects = 0\n\tfor obj in objects:\n\t\tif check_digests and obj.digest is not None:\n\t\t\tif self.check_digest(obj.digest):\n\t\t\t\tgood_objects += 1\n\t\t\telse:\n\t\t\t\tself.log.warning(\"Failed to validate object digest for %s\", obj.digest)\n\t\t\t\tif show_warnings:\n\t\t\t\t\tself.log.warning(obj.debug_output())\n\t\telse:\n\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n\tdef validate_objects(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t\"\"\"\n\t\tValidate storage root objects.\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\twarnings = 0\n\t\t# iterate root hierarchy\n\t\tfor dirpath, dirnames, filenames in os.walk(self._storagedir):\n\t\t\t# check each file in this directory\n\t\t\tfor filename in filenames:\n\t\t\t\tnum_objects = num_objects + 1\n\t\t\t\t# get path to the file\n\t\t\t\tfilepath = os.path.join(dirpath, filename)\n\t\t\t\t# get the object\n\t\t\t\ttry:\n\t\t\t\t\tobj = self.get_object(filepath)\n\t\t\t\texcept:\n\t\t\t\t\tprint(\"Could not get object for path: %s\" % filepath)\n\t\t\t\t\tcontinue\n\t\t\t\t# check object if requested\n\t\t\t\tif validate_objects:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tobj.validate_object(check_digests=check_digests)\n\t\t\t\t\texcept:\n\t\t\t\t\t\t\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor storage_root in self.storage_roots.values():\n\t\tfor subdir in storage_root.subdirs.values():\n\t\t\tfor object in subdir.objects.values():\n\t\t\t\tnum_objects += 1\n\t\t\t\tif validate_objects:\n\t\t\t\t\tif object.validate(show_warnings=show_warnings):\n\t\t\t\t\t\tgood_objects += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.warning(\"Object %s failed validation\" % (object.key))\n\t\t\t\tif check_digests:\n\t\t\t\t\tobject.check_digests()\n\treturn (num_objects, good_objects)\n\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tself._validate_hierarchy(self.root, validate_objects, check_digests, show_warnings)\n\n\t# Validate objects\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.objects.values():\n\t\tnum_objects += 1\n\t\tif obj.validate(check_digests):\n\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tif validate_objects:\n\t\t\tfor object_info in self.list_objects():\n\t\t\t\tnum_objects += 1\n\t\t\t\ttry:\n\t\t\t\t\tself._validate_object(object_info, check_digests)\n\t\t\t\t\tgood_objects += 1\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"Invalid object: %s: %s\", object_info.object_path, e)\n\t\t\t\t\telse:\n\t\t\t\t\t\traise\n\t\treturn num_objects, good_objects\n\n\tdef _validate_object(self, object_info, check_digests):\n\t\t\"\"\"\n\t\tValidate an object.\n\t\t\"\"\"\n\t\tassert isinstance(object_info, ObjectInfo)\n\n\t\t# check that the object exists\n\t\tobject_path = object_info.object_path\n\t\tif not os.path.exists(object_\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\ttry:\n\t\t# TODO: check .digest files for each object\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tbad_objects = 0\n\t\tfor dirpath, dirnames, filenames in os.walk(self.storage_root):\n\t\t\tfor filename in filenames:\n\t\t\t\tif filename.endswith('.digest'):\n\t\t\t\t\tnum_objects += 1\n\t\t\t\t\tdigest_file_path = os.path.join(dirpath, filename)\n\t\t\t\t\tdigest_file_name = os.path.basename(digest_file_path)\n\t\t\t\t\tdigest_file_name_without_ext = digest_file_name[:-7]\n\t\t\t\t\tobject_file_path = os.path.join(dirpath, digest_file_name_without_ext)\n\t\t\t\t\tif not os.path.isfile(object_file_path):\n\t\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\t\tprint('\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\t\n\t\t# validate storage root\n\t\tif not os.path.isdir(self.storage_root):\n\t\t\t# we can't continue without this\n\t\t\traise StorageError(\"storage root not found\", self.storage_root)\n\t\t\n\t\t# validate hash directories\n\t\tfor hash_dir in os.listdir(self.storage_root):\n\t\t\tif len(hash_dir) != 2:\n\t\t\t\t# not a valid hash directory\n\t\t\t\tcontinue\n\t\t\t\n\t\t\thash_dir_path = os.path.join(self.storage_root, hash_dir)\n\t\t\t\n\t\t\t# check for invalid characters\n\t\t\tif not re.match(\"^[0-9a-fA-F]+$\", hash_dir):\n\t\t\t\traise StorageError(\"invalid hash directory\", hash_dir_path)\n\t\t\t\n\t\t\t# validate objects\n\t\t\tif validate_objects:\n\t\t\t\tfor hash_file in os.list\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif validate_objects:\n\t\t\tnum_objects, good_objects, bad_objects = self.validate_objects(check_digests=check_digests, show_warnings=show_warnings)\n\t\telse:\n\t\t\tnum_objects, good_objects, bad_objects = 0, 0, 0\n\n\t\tnum_dirs, good_dirs, bad_dirs = self.validate_dirs()\n\n\t\t#print(\"Num objects: %d, good objects: %d, bad objects: %d\" % (num_objects, good_objects, bad_objects))\n\t\t#print(\"Num dirs: %d, good dirs: %d, bad dirs: %d\" % (num_dirs, good_dirs, bad_dirs))\n\n\t\treturn num_objects, good_objects, bad_objects, num_dirs, good_dirs, bad_dirs\n\n\tdef validate_objects(self, check_digests=True, show_warnings=False):\n\t\t\"\"\"\n\t\tValidate objects on storage root.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor f in self.list_objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj = self.get_object(f)\n\t\t\tif validate_objects:\n\t\t\t\tobj.validate()\n\t\t\t\tif check_digests:\n\t\t\t\t\tobj.check_digests()\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint('%s is invalid: %s' % (f, e))\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tself.log.debug(\"Validating storage root hierarchy\")\n\n\t# Get all storage root objects\n\tobjects = self.get_objects(storage_roots=True, validate_objects=validate_objects)\n\tnum_objects = len(objects)\n\n\tgood_objects = 0\n\tfor obj in objects:\n\t\tif check_digests and obj.digest is not None:\n\t\t\tif self.check_digest(obj.digest):\n\t\t\t\tgood_objects += 1\n\t\t\telse:\n\t\t\t\tself.log.warning(\"Failed to validate object digest for %s\", obj.digest)\n\t\t\t\tif show_warnings:\n\t\t\t\t\tself.log.warning(obj.debug_output())\n\t\telse:\n\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n\tdef validate_objects(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t\"\"\"\n\t\tValidate storage root objects.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\twarnings = 0\n\t\t# iterate root hierarchy\n\t\tfor dirpath, dirnames, filenames in os.walk(self._storagedir):\n\t\t\t# check each file in this directory\n\t\t\tfor filename in filenames:\n\t\t\t\tnum_objects = num_objects + 1\n\t\t\t\t# get path to the file\n\t\t\t\tfilepath = os.path.join(dirpath, filename)\n\t\t\t\t# get the object\n\t\t\t\ttry:\n\t\t\t\t\tobj = self.get_object(filepath)\n\t\t\t\texcept:\n\t\t\t\t\tprint(\"Could not get object for path: %s\" % filepath)\n\t\t\t\t\tcontinue\n\t\t\t\t# check object if requested\n\t\t\t\tif validate_objects:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tobj.validate_object(check_digests=check_digests)\n\t\t\t\t\texcept:\n\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor storage_root in self.storage_roots.values():\n\t\tfor subdir in storage_root.subdirs.values():\n\t\t\tfor object in subdir.objects.values():\n\t\t\t\tnum_objects += 1\n\t\t\t\tif validate_objects:\n\t\t\t\t\tif object.validate(show_warnings=show_warnings):\n\t\t\t\t\t\tgood_objects += 1\n\t\t\t\t\telse:\n\t\t\t\t\t\tlogger.warning(\"Object %s failed validation\" % (object.key))\n\t\t\t\tif check_digests:\n\t\t\t\t\tobject.check_digests()\n\treturn (num_objects, good_objects)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tself._validate_hierarchy(self.root, validate_objects, check_digests, show_warnings)\n\n\t# Validate objects\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.objects.values():\n\t\tnum_objects += 1\n\t\tif obj.validate(check_digests):\n\t\t\tgood_objects += 1\n\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tif validate_objects:\n\t\t\tfor object_info in self.list_objects():\n\t\t\t\tnum_objects += 1\n\t\t\t\ttry:\n\t\t\t\t\tself._validate_object(object_info, check_digests)\n\t\t\t\t\tgood_objects += 1\n\t\t\t\texcept Exception as e:\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"Invalid object: %s: %s\", object_info.object_path, e)\n\t\t\t\t\telse:\n\t\t\t\t\t\traise\n\t\treturn num_objects, good_objects\n\n\tdef _validate_object(self, object_info, check_digests):\n\t\t\"\"\"\n\t\tValidate an object.\n\t\t\"\"\"\n\t\tassert isinstance(object_info, ObjectInfo)\n\n\t\t# check that the object exists\n\t\tobject_path = object_info.object_path\n\t\tif not os.path.exists(object_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot itself?\n\t\t# TODO: should this be done here, or in the StorageRoot\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\ttry:\n\t\t# TODO: check .digest files for each object\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tbad_objects = 0\n\t\tfor dirpath, dirnames, filenames in os.walk(self.storage_root):\n\t\t\tfor filename in filenames:\n\t\t\t\tif filename.endswith('.digest'):\n\t\t\t\t\tnum_objects += 1\n\t\t\t\t\tdigest_file_path = os.path.join(dirpath, filename)\n\t\t\t\t\tdigest_file_name = os.path.basename(digest_file_path)\n\t\t\t\t\tdigest_file_name_without_ext = digest_file_name[:-7]\n\t\t\t\t\tobject_file_path = os.path.join(dirpath, digest_file_name_without_ext)\n\t\t\t\t\tif not os.path.isfile(object_file_path):\n\t\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\t\tprint('\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\t\n\t\t# validate storage root\n\t\tif not os.path.isdir(self.storage_root):\n\t\t\t# we can't continue without this\n\t\t\traise StorageError(\"storage root not found\", self.storage_root)\n\t\t\n\t\t# validate hash directories\n\t\tfor hash_dir in os.listdir(self.storage_root):\n\t\t\tif len(hash_dir) != 2:\n\t\t\t\t# not a valid hash directory\n\t\t\t\tcontinue\n\t\t\t\n\t\t\thash_dir_path = os.path.join(self.storage_root, hash_dir)\n\t\t\t\n\t\t\t# check for invalid characters\n\t\t\tif not re.match(\"^[0-9a-fA-F]+$\", hash_dir):\n\t\t\t\traise StorageError(\"invalid hash directory\", hash_dir_path)\n\t\t\t\n\t\t\t# validate objects\n\t\t\tif validate_objects:\n\t\t\t\tfor hash_file in os.list\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "ques_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n\tself.create_dirs()\n\tself.create_info_files()\n\n\t# Write an empty inventory file\n\tself.inventory.write('')\n\n\t# Create an empty staging area\n\tself.stage_dir.touch()\n\n\t# Create an empty staging area inventory\n\tself.stage_inventory.write('')\n\n\t# Create an empty staging area inventory\n\tself.stage_info.write('')\n\n\t# Create an empty staging area inventory\n\tself.stage_head.write('')\n\n\n", "def initialize(self):\n\tself.log.debug(\"Initializing OCFL storage root...\")\n\tif self.exists():\n\t\traise OcflStorageRootError(\"OCFL storage root already exists\")\n\tself.log.debug(\"Creating OCFL storage root...\")\n\tself._mkdir(self.base_path)\n\tself.log.debug(\"Creating OCFL inventory root...\")\n\tself._mkdir(self.inventory_path)\n\tself.log.debug(\"Creating OCFL object root...\")\n\tself._mkdir(self.object_path)\n\tself.log.debug(\"Writing OCFL storage layout...\")\n\tself._write_layout()\n\tself.log.debug(\"Writing OCFL inventory version...\")\n\tself._write_inventory_version()\n\n", "def initialize(self):\n\tself.create()\n\tself.initialize()\n\n", "def initialize(self):\n\tself.obj_path_and_version = None\n\tself.obj_path = None\n\tself.obj_version = None\n\tself.state_path = None\n\tself.inv_path = None\n\tself.inventory = None\n\tself.state = None\n\tself.object_version = None\n\tself.object_path = None\n\tself.obj_dir = None\n\tself.state_dir = None\n\tself.inv_dir = None\n\tself.storage_dir = None\n\tself.storage_root = None\n\tself.storage_dirs = None\n\tself.storage_dirs_with_obj = None\n\tself.storage_dirs_with_obj_with_digest = None\n\tself.digest_alg = None\n\tself.digest_alg_inv = None\n\tself.digest_alg_obj = None\n\tself.digest_alg_state = None\n\tself.digest_alg_file = None\n\tself.version_num = None\n\tself.obj_content_dir = None\n\tself.obj_content_dir_with\n", "def initialize(self):\n\t# Create the storage root directory\n\tos.mkdir(self.path)\n\t# Create the inventory directory\n\tos.mkdir(os.path.join(self.path, INVENTORY))\n\t# Create the inventory file\n\tinventory_path = os.path.join(self.path, INVENTORY, INVENTORY_FILE)\n\tinventory = OCFLInventory()\n\tinventory.write(inventory_path)\n\t# Create the first content directory\n\tos.mkdir(os.path.join(self.path, CONTENT, CONTENT_DIR))\n\t# Create the first version directory\n\tos.mkdir(os.path.join(self.path, CONTENT, CONTENT_DIR, VERSION_DIR))\n\t# Create the first version file\n\tversion_path = os.path.join(self.path, CONTENT, CONTENT_DIR, VERSION_DIR, VERSION_FILE)\n\tversion = OCFLVersion()\n\tversion.write(version_path)\n\t# Store the first version to the inventory\n\titem = OCFLItem()\n\titem.add_version(CONTENT\n", "def initialize(self):\n\tif not os.path.isfile(self.ocflRoot + '/ocfl_object_1.0/inventory.json'):\n\t\tself.createStorage()\n\t\tself.createInventory()\n\t\tself.createVersion()\n\t\tself.createObject()\n\t\tself.createManifest()\n\t\tself.createHead()\n\t\tself.createContent()\n\t\tself.createFixity()\n\t\tself.updateInventory()\n\t\tself.updateHead()\n\t\tself.createVersionInfo()\n\n\n", "def initialize(self):\n\tif self.exists():\n\t\traise OCFLStorageRootError(\"OCFL storage root already exists\")\n\tself.create()\n\tself.__init_ocfl_storage_root()\n\tself.__write_inventory()\n\n\t# Create the root object\n\troot = self.new_object()\n\troot.set_id(\"ocfl\")\n\troot.set_label(\"OCFL Storage Root\")\n\troot.set_type(\"ocfl\")\n\troot.write()\n\n\t# Create the root object inventory\n\troot_inv = self.new_inventory(root)\n\troot_inv.set_type(\"ocfl\")\n\troot_inv.set_version(\"ocfl_1.0\")\n\troot_inv.set_message(\"Initialized OCFL Storage Root\")\n\troot_inv.set_date(datetime.now())\n\troot_inv.write()\n\n\t# Create the root inventory\n\troot_inv = self.new_inventory()\n\troot_inv.set_type(\"ocfl\")\n\troot_inv.set_version(\"ocfl_1.0\")\n\troot_inv.\n", "def initialize(self):\n\tself._test_init()\n\t\n\t# Create OCFL storage root\n\tos.makedirs(self.ocfl_path)\n\tos.makedirs(self.inventory_path)\n\t\n\t# Create initial inventory\n\tinventory = {\n\t\t\"id\": \"inventory.json\",\n\t\t\"spec_version\": \"1.0\",\n\t\t\"inventory_root\": self.ocfl_path\n\t}\n\t\n\twith open(self.inventory_path_file, \"w\") as f:\n\t\tjson.dump(inventory, f, indent=3)\n\t\n\t# Create initial content directory\n\tself.mkdir(\"content\")\n\t\n\t# Create initial version directory\n\tv1_path = self.path(\"v1\")\n\tos.makedirs(v1_path)\n\t\n\t# Create initial version inventory\n\tobject_id = \"{}-v1\".format(self.uuid)\n\tinventory = {\n\t\t\"id\": \"v1/inventory.json\",\n\t\t\"spec_version\": \"1.0\",\n\t\t\"object_id\n", "def initialize(self):\n\t# Validate the OCFL storage root path.\n\tif not self._validate_path(self.path):\n\t\traise OCFLInvalidStorageRootException(self.path)\n\n\t# Create the OCFL storage root directory tree.\n\tself._create_tree()\n\n\t# Create the inventory file.\n\tself._create_inventory()\n\n\t# Create the inventory-fetch file.\n\tself._create_inventory_fetch()\n\n\t# Create the inventory-tag file.\n\tself._create_inventory_tag()\n\n\t# Create the inventory-tag-list file.\n\tself._create_inventory_tag_list()\n\n\t# Create the inventory-tag-manifest file.\n\tself._create_inventory_tag_manifest()\n\n\t# Create the inventory-tag-tag file.\n\tself._create_inventory_tag_tag()\n\n\t# Write the OCFL manifest file.\n\tself._write_manifest()\n\n\t# Write the OCFL manifest-fetch file.\n\tself._write_manifest_fetch()\n\n\t# Write the OCFL manifest-tag file.\n\tself._write_manifest\n", "def initialize(self):\n\tself.storage.create_storage_root()\n\t\n\t# create the root inventory\n\tinventory = ocfl.inventory.Inventory()\n\tinventory.version = self.version\n\tinventory.head = self.version\n\tinventory.set_id()\n\tinventory.set_created()\n\tinventory.serialize(self.storage.inventory_path)\n\t\n\t# create the root content directory\n\tself.storage.create_version_directory(self.version)\n\tself.storage.create_object_directory(self.version)\n\t\n\t# create the root state file\n\tstate = ocfl.state.State()\n\tstate.version = self.version\n\tstate.set_created()\n\tstate.serialize(self.storage.state_path)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n\tself.create_dirs()\n\tself.create_info_files()\n\n\t# Write an empty inventory file\n\tself.inventory.write('')\n\n\t# Create an empty staging area\n\tself.stage_dir.touch()\n\n\t# Create an empty staging area inventory\n\tself.stage_inventory.write('')\n\n\t# Create an empty staging area inventory\n\tself.stage_info.write('')\n\n\t# Create an empty staging area inventory\n\tself.stage_head.write('')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself.log.debug(\"Initializing OCFL storage root...\")\n\tif self.exists():\n\t\traise OcflStorageRootError(\"OCFL storage root already exists\")\n\tself.log.debug(\"Creating OCFL storage root...\")\n\tself._mkdir(self.base_path)\n\tself.log.debug(\"Creating OCFL inventory root...\")\n\tself._mkdir(self.inventory_path)\n\tself.log.debug(\"Creating OCFL object root...\")\n\tself._mkdir(self.object_path)\n\tself.log.debug(\"Writing OCFL storage layout...\")\n\tself._write_layout()\n\tself.log.debug(\"Writing OCFL inventory version...\")\n\tself._write_inventory_version()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself.create()\n\tself.initialize()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself.obj_path_and_version = None\n\tself.obj_path = None\n\tself.obj_version = None\n\tself.state_path = None\n\tself.inv_path = None\n\tself.inventory = None\n\tself.state = None\n\tself.object_version = None\n\tself.object_path = None\n\tself.obj_dir = None\n\tself.state_dir = None\n\tself.inv_dir = None\n\tself.storage_dir = None\n\tself.storage_root = None\n\tself.storage_dirs = None\n\tself.storage_dirs_with_obj = None\n\tself.storage_dirs_with_obj_with_digest = None\n\tself.digest_alg = None\n\tself.digest_alg_inv = None\n\tself.digest_alg_obj = None\n\tself.digest_alg_state = None\n\tself.digest_alg_file = None\n\tself.version_num = None\n\tself.obj_content_dir = None\n\tself.obj_content_dir_with\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t# Create the storage root directory\n\tos.mkdir(self.path)\n\t# Create the inventory directory\n\tos.mkdir(os.path.join(self.path, INVENTORY))\n\t# Create the inventory file\n\tinventory_path = os.path.join(self.path, INVENTORY, INVENTORY_FILE)\n\tinventory = OCFLInventory()\n\tinventory.write(inventory_path)\n\t# Create the first content directory\n\tos.mkdir(os.path.join(self.path, CONTENT, CONTENT_DIR))\n\t# Create the first version directory\n\tos.mkdir(os.path.join(self.path, CONTENT, CONTENT_DIR, VERSION_DIR))\n\t# Create the first version file\n\tversion_path = os.path.join(self.path, CONTENT, CONTENT_DIR, VERSION_DIR, VERSION_FILE)\n\tversion = OCFLVersion()\n\tversion.write(version_path)\n\t# Store the first version to the inventory\n\titem = OCFLItem()\n\titem.add_version(CONTENT\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tif not os.path.isfile(self.ocflRoot + '/ocfl_object_1.0/inventory.json'):\n\t\tself.createStorage()\n\t\tself.createInventory()\n\t\tself.createVersion()\n\t\tself.createObject()\n\t\tself.createManifest()\n\t\tself.createHead()\n\t\tself.createContent()\n\t\tself.createFixity()\n\t\tself.updateInventory()\n\t\tself.updateHead()\n\t\tself.createVersionInfo()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tif self.exists():\n\t\traise OCFLStorageRootError(\"OCFL storage root already exists\")\n\tself.create()\n\tself.__init_ocfl_storage_root()\n\tself.__write_inventory()\n\n\t# Create the root object\n\troot = self.new_object()\n\troot.set_id(\"ocfl\")\n\troot.set_label(\"OCFL Storage Root\")\n\troot.set_type(\"ocfl\")\n\troot.write()\n\n\t# Create the root object inventory\n\troot_inv = self.new_inventory(root)\n\troot_inv.set_type(\"ocfl\")\n\troot_inv.set_version(\"ocfl_1.0\")\n\troot_inv.set_message(\"Initialized OCFL Storage Root\")\n\troot_inv.set_date(datetime.now())\n\troot_inv.write()\n\n\t# Create the root inventory\n\troot_inv = self.new_inventory()\n\troot_inv.set_type(\"ocfl\")\n\troot_inv.set_version(\"ocfl_1.0\")\n\troot_inv.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself._test_init()\n\t\n\t# Create OCFL storage root\n\tos.makedirs(self.ocfl_path)\n\tos.makedirs(self.inventory_path)\n\t\n\t# Create initial inventory\n\tinventory = {\n\t\t\"id\": \"inventory.json\",\n\t\t\"spec_version\": \"1.0\",\n\t\t\"inventory_root\": self.ocfl_path\n\t}\n\t\n\twith open(self.inventory_path_file, \"w\") as f:\n\t\tjson.dump(inventory, f, indent=3)\n\t\n\t# Create initial content directory\n\tself.mkdir(\"content\")\n\t\n\t# Create initial version directory\n\tv1_path = self.path(\"v1\")\n\tos.makedirs(v1_path)\n\t\n\t# Create initial version inventory\n\tobject_id = \"{}-v1\".format(self.uuid)\n\tinventory = {\n\t\t\"id\": \"v1/inventory.json\",\n\t\t\"spec_version\": \"1.0\",\n\t\t\"object_id\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t# Validate the OCFL storage root path.\n\tif not self._validate_path(self.path):\n\t\traise OCFLInvalidStorageRootException(self.path)\n\n\t# Create the OCFL storage root directory tree.\n\tself._create_tree()\n\n\t# Create the inventory file.\n\tself._create_inventory()\n\n\t# Create the inventory-fetch file.\n\tself._create_inventory_fetch()\n\n\t# Create the inventory-tag file.\n\tself._create_inventory_tag()\n\n\t# Create the inventory-tag-list file.\n\tself._create_inventory_tag_list()\n\n\t# Create the inventory-tag-manifest file.\n\tself._create_inventory_tag_manifest()\n\n\t# Create the inventory-tag-tag file.\n\tself._create_inventory_tag_tag()\n\n\t# Write the OCFL manifest file.\n\tself._write_manifest()\n\n\t# Write the OCFL manifest-fetch file.\n\tself._write_manifest_fetch()\n\n\t# Write the OCFL manifest-tag file.\n\tself._write_manifest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself.storage.create_storage_root()\n\t\n\t# create the root inventory\n\tinventory = ocfl.inventory.Inventory()\n\tinventory.version = self.version\n\tinventory.head = self.version\n\tinventory.set_id()\n\tinventory.set_created()\n\tinventory.serialize(self.storage.inventory_path)\n\t\n\t# create the root content directory\n\tself.storage.create_version_directory(self.version)\n\tself.storage.create_object_directory(self.version)\n\t\n\t# create the root state file\n\tstate = ocfl.state.State()\n\tstate.version = self.version\n\tstate.set_created()\n\tstate.serialize(self.storage.state_path)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "ques_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n\t# Convert to list of ints.\n\tversion_list = list(map(int, version.split('.')))\n\t# Iterate backwards through list.\n\tfor i in range(len(version_list)-1, -1, -1):\n\t\t# If the digit is less than 9, increment by 1 and return.\n\t\tif version_list[i] < 9:\n\t\t\tversion_list[i] += 1\n\t\t\treturn '.'.join(list(map(str, version_list)))\n\t\t# Else, set it to 0 and continue iterating.\n\t\tversion_list[i] = 0\n\t# If we've reached this point, we need to add a new digit.\n\treturn '0.0.0.1'\n\n", "def next_version(version):\n\tparts = [int(v) for v in version.split('.')]\n\tparts[-1] += 1\n\treturn '.'.join(str(v) for v in parts)\n\n\n", "def next_version(version):\n\tversion = version.split('.')\n\tif len(version) == 1:\n\t\tversion = [int(version[0]), 0]\n\telse:\n\t\tversion = [int(version[0]), int(version[1])]\n\n\tversion[1] += 1\n\tif version[1] >= 100:\n\t\tversion[0] += 1\n\t\tversion[1] = 0\n\n\treturn str(version[0]) + '.' + str(version[1]).zfill(2)\n\n", "def next_version(version):\n\tsuffix = version[-4:]\n\tif suffix[0] == '0':\n\t\t# zero-prefixed\n\t\tprefix = version[:-4]\n\t\treturn prefix + str(int(suffix) + 1).zfill(4)\n\telse:\n\t\t# non-zero prefixed\n\t\treturn str(int(suffix) + 1)\n\n", "def next_version(version):\n\tif not version:\n\t\treturn '1.0.0'\n\n\t# split into parts\n\tparts = version.split('.')\n\t# need to handle 0.9.9, 1.0.0, 1.0.1, 1.0.10, 1.1.0, 1.10.0, 1.10.1\n\tif parts[2] == '9':\n\t\tif parts[1] == '9':\n\t\t\tif parts[0] == '9':\n\t\t\t\traise Exception('Maximum version reached')\n\t\t\telse:\n\t\t\t\tparts[0] = str(int(parts[0]) + 1)\n\t\t\t\tparts[1] = '0'\n\t\t\t\tparts[2] = '0'\n\t\telse:\n\t\t\tparts[1] = str(int(parts[1]) + 1)\n\t\t\tparts[2] = '0'\n\telse:\n\t\tparts[2] = str(int(parts[2]) + 1)\n\n\treturn '.'.join(parts)\n", "def next_version(version):\n\tversion_list = version.split('.')\n\tif len(version_list) == 1:\n\t\treturn str(int(version) + 1)\n\telse:\n\t\t# Existing version has more than one component\n\t\t# so we will not zero-prefix new version\n\t\treturn '.'.join(\n\t\t\t[str(int(version_list[i]) + (1 if i == 0 else 0)) for i in range(len(version_list))]\n\t\t\t)\n\n", "def next_version(version):\n\ttry:\n\t\tprefixed = version.startswith('0')\n\t\tif prefixed:\n\t\t\tversion = version[1:]\n\t\tversion_numbers = map(int, version.split('.'))\n\t\tversion_numbers[-1] += 1\n\t\tversion = '.'.join(map(str, version_numbers))\n\t\tif prefixed:\n\t\t\tversion = '0' + version\n\t\treturn version\n\texcept:\n\t\traise Exception('Could not determine next version.')\n", "def next_version(version):\n\tif not version:\n\t\treturn \"1.0\"\n\telse:\n\t\tversion = version.split('.')\n\t\tif len(version) == 1:\n\t\t\treturn str(int(version[0]) + 1)\n\t\telse:\n\t\t\tif version[1]:\n\t\t\t\treturn version[0] + '.' + str(int(version[1]) + 1)\n\t\t\telse:\n\t\t\t\treturn str(int(version[0]) + 1)\n\n", "def next_version(version):\n\tif version == \"\":\n\t\treturn \"1\"\n\t\n\tif version[0] == \"0\":\n\t\treturn str(int(version) + 1)\n\telse:\n\t\t# Split into numeric and non-numeric components\n\t\tnum = []\n\t\tnonnum = []\n\t\tfor c in version:\n\t\t\tif c.isdigit():\n\t\t\t\tnum.append(c)\n\t\t\telse:\n\t\t\t\tnonnum.append(c)\n\t\tnum = int(\"\".join(num))\n\t\tnonnum = \"\".join(nonnum)\n\t\t\n\t\treturn \"{0}{1}\".format(num+1, nonnum)\n\t\n\n", "def next_version(version):\n\tif version[0] == '0':\n\t\treturn '0' + str(int(version[1:]) + 1).rjust(len(version) - 1, '0')\n\telse:\n\t\treturn str(int(version) + 1)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n\t# Convert to list of ints.\n\tversion_list = list(map(int, version.split('.')))\n\t# Iterate backwards through list.\n\tfor i in range(len(version_list)-1, -1, -1):\n\t\t# If the digit is less than 9, increment by 1 and return.\n\t\tif version_list[i] < 9:\n\t\t\tversion_list[i] += 1\n\t\t\treturn '.'.join(list(map(str, version_list)))\n\t\t# Else, set it to 0 and continue iterating.\n\t\tversion_list[i] = 0\n\t# If we've reached this point, we need to add a new digit.\n\treturn '0.0.0.1'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tparts = [int(v) for v in version.split('.')]\n\tparts[-1] += 1\n\treturn '.'.join(str(v) for v in parts)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tversion = version.split('.')\n\tif len(version) == 1:\n\t\tversion = [int(version[0]), 0]\n\telse:\n\t\tversion = [int(version[0]), int(version[1])]\n\n\tversion[1] += 1\n\tif version[1] >= 100:\n\t\tversion[0] += 1\n\t\tversion[1] = 0\n\n\treturn str(version[0]) + '.' + str(version[1]).zfill(2)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tsuffix = version[-4:]\n\tif suffix[0] == '0':\n\t\t# zero-prefixed\n\t\tprefix = version[:-4]\n\t\treturn prefix + str(int(suffix) + 1).zfill(4)\n\telse:\n\t\t# non-zero prefixed\n\t\treturn str(int(suffix) + 1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tif not version:\n\t\treturn '1.0.0'\n\n\t# split into parts\n\tparts = version.split('.')\n\t# need to handle 0.9.9, 1.0.0, 1.0.1, 1.0.10, 1.1.0, 1.10.0, 1.10.1\n\tif parts[2] == '9':\n\t\tif parts[1] == '9':\n\t\t\tif parts[0] == '9':\n\t\t\t\traise Exception('Maximum version reached')\n\t\t\telse:\n\t\t\t\tparts[0] = str(int(parts[0]) + 1)\n\t\t\t\tparts[1] = '0'\n\t\t\t\tparts[2] = '0'\n\t\telse:\n\t\t\tparts[1] = str(int(parts[1]) + 1)\n\t\t\tparts[2] = '0'\n\telse:\n\t\tparts[2] = str(int(parts[2]) + 1)\n\n\treturn '.'.join(parts)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tversion_list = version.split('.')\n\tif len(version_list) == 1:\n\t\treturn str(int(version) + 1)\n\telse:\n\t\t# Existing version has more than one component\n\t\t# so we will not zero-prefix new version\n\t\treturn '.'.join(\n\t\t\t[str(int(version_list[i]) + (1 if i == 0 else 0)) for i in range(len(version_list))]\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\ttry:\n\t\tprefixed = version.startswith('0')\n\t\tif prefixed:\n\t\t\tversion = version[1:]\n\t\tversion_numbers = map(int, version.split('.'))\n\t\tversion_numbers[-1] += 1\n\t\tversion = '.'.join(map(str, version_numbers))\n\t\tif prefixed:\n\t\t\tversion = '0' + version\n\t\treturn version\n\texcept:\n\t\traise Exception('Could not determine next version.')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tif not version:\n\t\treturn \"1.0\"\n\telse:\n\t\tversion = version.split('.')\n\t\tif len(version) == 1:\n\t\t\treturn str(int(version[0]) + 1)\n\t\telse:\n\t\t\tif version[1]:\n\t\t\t\treturn version[0] + '.' + str(int(version[1]) + 1)\n\t\t\telse:\n\t\t\t\treturn str(int(version[0]) + 1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tif version == \"\":\n\t\treturn \"1\"\n\t\n\tif version[0] == \"0\":\n\t\treturn str(int(version) + 1)\n\telse:\n\t\t# Split into numeric and non-numeric components\n\t\tnum = []\n\t\tnonnum = []\n\t\tfor c in version:\n\t\t\tif c.isdigit():\n\t\t\t\tnum.append(c)\n\t\t\telse:\n\t\t\t\tnonnum.append(c)\n\t\tnum = int(\"\".join(num))\n\t\tnonnum = \"\".join(nonnum)\n\t\t\n\t\treturn \"{0}{1}\".format(num+1, nonnum)\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tif version[0] == '0':\n\t\treturn '0' + str(int(version[1:]) + 1).rjust(len(version) - 1, '0')\n\telse:\n\t\treturn str(int(version) + 1)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "ques_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n\tprevious_inventory = None\n\tversion_inventories = {}\n\tfor version_dir in version_dirs:\n\t\tversion_number = int(version_dir.split(\".\")[1])\n\t\tversion_inventory = self.version_inventory(version_dir)\n\t\tversion_inventories[version_number] = version_inventory\n\t\tif self.inventories_are_the_same(version_inventory, previous_inventory):\n\t\t\traise ValidationError(\"version inventory is unchanged from previous version\")\n\t\tprevious_inventory = version_inventory\n\n\treturn version_inventories\n\n\n", "def validate_version_inventories(self, version_dirs):\n\t# Set of content digests for which the root inventory is different from\n\t# the version inventory.\n\troot_content_digests = set()\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.get_inventory(version_dir)\n\t\tif not version_inventory:\n\t\t\t# No inventory.\n\t\t\tself.add_validation_error(_('The version directory is missing a version inventory file.'))\n\t\t\tcontinue\n\t\tversion_index = int(version_dir[-1])\n\t\tif version_index == 0:\n\t\t\t# Root inventory.\n\t\t\troot_inventory = version_inventory\n\t\t\troot_content_digests = set(root_inventory.content_digests)\n\t\telse:\n\t\t\t# Check that the version inventory is the same as the root inventory\n\t\t\t# with the exception of the content digests of any files that have\n\t\t\t# changed.\n\t\t\tif not self.compare_inventories(root_inventory, version_inventory, root_content_digests):\n\t\t\t\n", "def validate_version_inventories(self, version_dirs):\n\tself.log.info('Validating version inventories')\n\tself.digests = {}\n\tfor dir in version_dirs:\n\t\tinv = self.inventories(dir)\n\t\tif inv is None:\n\t\t\tself.log.warning('No inventory found in version directory %s', dir)\n\t\telse:\n\t\t\tself.validate_inventory(inv)\n\treturn True\n\n\tdef validate_inventory(self, inv):\n\t\t\"\"\"\n\t\tValidate an inventory against the root inventory.\n\t\t\"\"\"\n\t\tself.log.info('Validating inventory %s', inv)\n\t\txml = self.read(inv, 'inventory.xml')\n\t\tif xml is None:\n\t\t\tself.log.error('Unable to read inventory.xml from %s', inv)\n\t\t\treturn False\n\t\troot = ET.fromstring(xml)\n\t\tif root.tag != 'inventory':\n\t\t\tself.log.error('Version inventory %s is not an inventory', inv)\n\t\t\treturn False\n\t\tif root.get('format') !=\n", "def validate_version_inventories(self, version_dirs):\n\tself.version_content_digests = {}\n\tself.version_inventory_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir[1])\n\t\tinventory_path = os.path.join(version_dir, 'inventory')\n\t\tif os.path.exists(inventory_path):\n\t\t\ttry:\n\t\t\t\tinventory = Inventory(inventory_path)\n\t\t\t\tself.version_inventory_digests[version] = inventory.get_digest()\n\t\t\t\tself.version_content_digests[version] = inventory.get_content_digests()\n\t\t\texcept:\n\t\t\t\tself.add_error(self.validate_version_inventories.__name__, 'Failed to load inventory for version %d' % version)\n\t\telse:\n\t\t\tself.add_error(self.validate_version_inventories.__name__, 'No inventory for version %d' % version)\n\n\tdef validate_content(self, version_dirs, content_dir):\n", "def validate_version_inventories(self, version_dirs):\n\tversion_inventory_files = []\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_file = os.path.join(version_dir, INVENTORY_FILENAME)\n\t\tif not os.path.isfile(version_inventory_file):\n\t\t\tself.add_error(\"Version %s is missing an inventory\" % version_dir)\n\t\t\tcontinue\n\n\t\tversion_inventory_files.append(version_inventory_file)\n\t\tself.inventories[version_dir] = Inventory(version_inventory_file)\n\n\t\t# check root inventory digests\n\t\troot_inventory = self.inventories['root']\n\t\tfor filename, digest in self.inventories[version_dir].digests.items():\n\t\t\tif filename in root_inventory.digests:\n\t\t\t\tif digest != root_inventory.digests[filename]:\n\t\t\t\t\tself.digest_mismatches.append(version_dir)\n\n\tif len(version_inventory_files) > 0:\n\t\tself.validate_inventories(\n", "def validate_version_inventories(self, version_dirs):\n\tversion_inventories = {}\n\tversion_content_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir.split('.')[1])\n\t\tinventory_path = os.path.join(version_dir, 'inventory.xml')\n\t\tif not os.path.exists(inventory_path):\n\t\t\t# raise an exception\n\t\t\traise ValidationError(inventory_path)\n\t\tversion_inventories[version] = inventory_path\n\t\tversion_content_digests[version] = self.get_content_digests(inventory_path)\n\n\t# Check each version's inventory to see if it matches the root inventory\n\t# up to that point\n\troot_inventory = self.get_root_inventory()\n\tfor version, inventory_path in version_inventories.items():\n\t\troot_inventory_up_to_version = root_inventory[0:version]\n\t\tversion_inventory = self.get_inventory_contents(inventory_path)\n\t\tif version_inventory != root_inventory_up_to\n", "def validate_version_inventories(self, version_dirs):\n\tif not version_dirs:\n\t\treturn\n\t# Get the digest of the root inventory.\n\twith open(os.path.join(self.basedir, 'inventory.xml'), 'rb') as f:\n\t\troot_digest = hash_base64(f.read())\n\t# Get the digests for the version inventories.\n\tversion_digests = []\n\tcontent_digests = []\n\tfor v in version_dirs:\n\t\tf = os.path.join(self.basedir, v, 'inventory.xml')\n\t\twith open(f, 'rb') as f:\n\t\t\tdigest = hash_base64(f.read())\n\t\t\tversion_digests.append(digest)\n\t\t\tif digest != root_digest:\n\t\t\t\tcontent_digests.append(digest)\n\t# The version digests should all be different.\n\tif len(set(version_digests)) != len(version_digests):\n\t\traise Invalid('version inventories are not all different')\n\tself.content_digests = content_\n", "def validate_version_inventories(self, version_dirs):\n\tcount = 0\n\tfor version_dir in version_dirs:\n\t\tcount += 1\n\t\tversion_dir_path = self.dir_path / version_dir\n\t\tversion_inventory_path = version_dir_path / \"inventory\"\n\t\tif not version_inventory_path.exists():\n\t\t\tself.add_error(\"inventory file missing\", version_inventory_path)\n\t\t\treturn\n\t\twith open(version_inventory_path) as f:\n\t\t\tversion_inventory = json.load(f)\n\t\tif not self.validate_inventory(version_inventory):\n\t\t\treturn\n\t\tif count == 1:\n\t\t\t# if this is the first inventory, it should be the same as the root inventory\n\t\t\tif version_inventory != self.root_inventory:\n\t\t\t\tself.add_error(\"first inventory does not match root\", version_inventory_path)\n\t\telse:\n\t\t\t# validate the diffs between this inventory and the previous one\n\t\t\tif not self.validate_inventory_diffs(version_inventory_path\n", "def validate_version_inventories(self, version_dirs):\n\t# This is a list of tuples of (version_dir, inventory_digest)\n\tversion_inventory_digests = []\n\n\t# This is a list of tuples of (version_dir, content_digest)\n\tversion_content_digests = []\n\n\t# This is a list of tuples of (version_dir, content_digest)\n\t# where content_digest is different from the root's\n\tversion_content_digests_diff = []\n\n\t# This is a list of tuples of (version_dir,\n\t# inventory_digest, content_digest) where content_digest is\n\t# different from the root's\n\tversion_inventory_content_digests_diff = []\n\n\tfor version_dir in version_dirs:\n\t\tversion_dir_path = os.path.join(self.repository.abspath, version_dir)\n\t\tversion_inventory_path = os.path.join(version_dir_path,\n\t\t\t\t\t\t\t\t\t\t\t  'inventory')\n\t\tversion_inventory_digest = self.repos\n", "def validate_version_inventories(self, version_dirs):\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir)\n\t\tinventory_filename = os.path.join(version_dir, 'inventory.xml')\n\t\ttry:\n\t\t\ttree = xml.etree.ElementTree.parse(inventory_filename)\n\t\texcept xml.etree.ElementTree.ParseError as e:\n\t\t\tself.logger.error(\n\t\t\t\t\"%s: inventory %s is not valid XML: %s\",\n\t\t\t\tself.repo_dir, inventory_filename, e,\n\t\t\t)\n\t\t\tself.errors += 1\n\t\t\tcontinue\n\t\tinventory = tree.getroot()\n\t\tif inventory.tag != 'inventory':\n\t\t\tself.logger.error(\n\t\t\t\t\"%s: inventory %s does not have root <inventory> tag\",\n\t\t\t\tself.repo_dir, inventory_filename\n\t\t\t)\n\t\t\tself.errors += 1\n\t\t\tcontinue\n\t\tinventory.attrib.setdefault('format', '1\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n\tprevious_inventory = None\n\tversion_inventories = {}\n\tfor version_dir in version_dirs:\n\t\tversion_number = int(version_dir.split(\".\")[1])\n\t\tversion_inventory = self.version_inventory(version_dir)\n\t\tversion_inventories[version_number] = version_inventory\n\t\tif self.inventories_are_the_same(version_inventory, previous_inventory):\n\t\t\traise ValidationError(\"version inventory is unchanged from previous version\")\n\t\tprevious_inventory = version_inventory\n\n\treturn version_inventories\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t# Set of content digests for which the root inventory is different from\n\t# the version inventory.\n\troot_content_digests = set()\n\tfor version_dir in version_dirs:\n\t\tversion_inventory = self.get_inventory(version_dir)\n\t\tif not version_inventory:\n\t\t\t# No inventory.\n\t\t\tself.add_validation_error(_('The version directory is missing a version inventory file.'))\n\t\t\tcontinue\n\t\tversion_index = int(version_dir[-1])\n\t\tif version_index == 0:\n\t\t\t# Root inventory.\n\t\t\troot_inventory = version_inventory\n\t\t\troot_content_digests = set(root_inventory.content_digests)\n\t\telse:\n\t\t\t# Check that the version inventory is the same as the root inventory\n\t\t\t# with the exception of the content digests of any files that have\n\t\t\t# changed.\n\t\t\tif not self.compare_inventories(root_inventory, version_inventory, root_content_digests):\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tself.log.info('Validating version inventories')\n\tself.digests = {}\n\tfor dir in version_dirs:\n\t\tinv = self.inventories(dir)\n\t\tif inv is None:\n\t\t\tself.log.warning('No inventory found in version directory %s', dir)\n\t\telse:\n\t\t\tself.validate_inventory(inv)\n\treturn True\n\n\tdef validate_inventory(self, inv):\n\t\t\"\"\"\n\t\tValidate an inventory against the root inventory.\n\t\t\"\"\"\n\t\tself.log.info('Validating inventory %s', inv)\n\t\txml = self.read(inv, 'inventory.xml')\n\t\tif xml is None:\n\t\t\tself.log.error('Unable to read inventory.xml from %s', inv)\n\t\t\treturn False\n\t\troot = ET.fromstring(xml)\n\t\tif root.tag != 'inventory':\n\t\t\tself.log.error('Version inventory %s is not an inventory', inv)\n\t\t\treturn False\n\t\tif root.get('format') !=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tself.version_content_digests = {}\n\tself.version_inventory_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir[1])\n\t\tinventory_path = os.path.join(version_dir, 'inventory')\n\t\tif os.path.exists(inventory_path):\n\t\t\ttry:\n\t\t\t\tinventory = Inventory(inventory_path)\n\t\t\t\tself.version_inventory_digests[version] = inventory.get_digest()\n\t\t\t\tself.version_content_digests[version] = inventory.get_content_digests()\n\t\t\texcept:\n\t\t\t\tself.add_error(self.validate_version_inventories.__name__, 'Failed to load inventory for version %d' % version)\n\t\telse:\n\t\t\tself.add_error(self.validate_version_inventories.__name__, 'No inventory for version %d' % version)\n\n\tdef validate_content(self, version_dirs, content_dir):\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tversion_inventory_files = []\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_file = os.path.join(version_dir, INVENTORY_FILENAME)\n\t\tif not os.path.isfile(version_inventory_file):\n\t\t\tself.add_error(\"Version %s is missing an inventory\" % version_dir)\n\t\t\tcontinue\n\n\t\tversion_inventory_files.append(version_inventory_file)\n\t\tself.inventories[version_dir] = Inventory(version_inventory_file)\n\n\t\t# check root inventory digests\n\t\troot_inventory = self.inventories['root']\n\t\tfor filename, digest in self.inventories[version_dir].digests.items():\n\t\t\tif filename in root_inventory.digests:\n\t\t\t\tif digest != root_inventory.digests[filename]:\n\t\t\t\t\tself.digest_mismatches.append(version_dir)\n\n\tif len(version_inventory_files) > 0:\n\t\tself.validate_inventories(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tversion_inventories = {}\n\tversion_content_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir.split('.')[1])\n\t\tinventory_path = os.path.join(version_dir, 'inventory.xml')\n\t\tif not os.path.exists(inventory_path):\n\t\t\t# raise an exception\n\t\t\traise ValidationError(inventory_path)\n\t\tversion_inventories[version] = inventory_path\n\t\tversion_content_digests[version] = self.get_content_digests(inventory_path)\n\n\t# Check each version's inventory to see if it matches the root inventory\n\t# up to that point\n\troot_inventory = self.get_root_inventory()\n\tfor version, inventory_path in version_inventories.items():\n\t\troot_inventory_up_to_version = root_inventory[0:version]\n\t\tversion_inventory = self.get_inventory_contents(inventory_path)\n\t\tif version_inventory != root_inventory_up_to\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tif not version_dirs:\n\t\treturn\n\t# Get the digest of the root inventory.\n\twith open(os.path.join(self.basedir, 'inventory.xml'), 'rb') as f:\n\t\troot_digest = hash_base64(f.read())\n\t# Get the digests for the version inventories.\n\tversion_digests = []\n\tcontent_digests = []\n\tfor v in version_dirs:\n\t\tf = os.path.join(self.basedir, v, 'inventory.xml')\n\t\twith open(f, 'rb') as f:\n\t\t\tdigest = hash_base64(f.read())\n\t\t\tversion_digests.append(digest)\n\t\t\tif digest != root_digest:\n\t\t\t\tcontent_digests.append(digest)\n\t# The version digests should all be different.\n\tif len(set(version_digests)) != len(version_digests):\n\t\traise Invalid('version inventories are not all different')\n\tself.content_digests = content_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tcount = 0\n\tfor version_dir in version_dirs:\n\t\tcount += 1\n\t\tversion_dir_path = self.dir_path / version_dir\n\t\tversion_inventory_path = version_dir_path / \"inventory\"\n\t\tif not version_inventory_path.exists():\n\t\t\tself.add_error(\"inventory file missing\", version_inventory_path)\n\t\t\treturn\n\t\twith open(version_inventory_path) as f:\n\t\t\tversion_inventory = json.load(f)\n\t\tif not self.validate_inventory(version_inventory):\n\t\t\treturn\n\t\tif count == 1:\n\t\t\t# if this is the first inventory, it should be the same as the root inventory\n\t\t\tif version_inventory != self.root_inventory:\n\t\t\t\tself.add_error(\"first inventory does not match root\", version_inventory_path)\n\t\telse:\n\t\t\t# validate the diffs between this inventory and the previous one\n\t\t\tif not self.validate_inventory_diffs(version_inventory_path\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t# This is a list of tuples of (version_dir, inventory_digest)\n\tversion_inventory_digests = []\n\n\t# This is a list of tuples of (version_dir, content_digest)\n\tversion_content_digests = []\n\n\t# This is a list of tuples of (version_dir, content_digest)\n\t# where content_digest is different from the root's\n\tversion_content_digests_diff = []\n\n\t# This is a list of tuples of (version_dir,\n\t# inventory_digest, content_digest) where content_digest is\n\t# different from the root's\n\tversion_inventory_content_digests_diff = []\n\n\tfor version_dir in version_dirs:\n\t\tversion_dir_path = os.path.join(self.repository.abspath, version_dir)\n\t\tversion_inventory_path = os.path.join(version_dir_path,\n\t\t\t\t\t\t\t\t\t\t\t  'inventory')\n\t\tversion_inventory_digest = self.repos\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir)\n\t\tinventory_filename = os.path.join(version_dir, 'inventory.xml')\n\t\ttry:\n\t\t\ttree = xml.etree.ElementTree.parse(inventory_filename)\n\t\texcept xml.etree.ElementTree.ParseError as e:\n\t\t\tself.logger.error(\n\t\t\t\t\"%s: inventory %s is not valid XML: %s\",\n\t\t\t\tself.repo_dir, inventory_filename, e,\n\t\t\t)\n\t\t\tself.errors += 1\n\t\t\tcontinue\n\t\tinventory = tree.getroot()\n\t\tif inventory.tag != 'inventory':\n\t\t\tself.logger.error(\n\t\t\t\t\"%s: inventory %s does not have root <inventory> tag\",\n\t\t\t\tself.repo_dir, inventory_filename\n\t\t\t)\n\t\t\tself.errors += 1\n\t\t\tcontinue\n\t\tinventory.attrib.setdefault('format', '1\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "ques_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n\tpath = os.path.abspath(path)\n\tif not os.path.exists(path):\n\t\treturn 'Path does not exist'\n\tif not os.path.isdir(path):\n\t\treturn 'Path is not a directory'\n\tif os.path.isfile(os.path.join(path, 'ocfl_object_1.0')):\n\t\t# Looks like an OCFL Object\n\t\treturn 'object'\n\tnamaste_files = glob.glob(os.path.join(path, '0=*'))\n\tif namaste_files:\n\t\tfor namaste_file in namaste_files:\n\t\t\tif os.path.isfile(namaste_file):\n\t\t\t\tif os.path.basename(namaste_file) == '0=inventory.txt':\n\t\t\t\t\treturn 'file'\n\t\t\t\telse:\n\t\t\t\t\treturn 'Looks like an OCFL Object Version'\n\t\t\tif os.path.isdir(namaste_file):\n\t\t\t\treturn '\n", "def find_path_type(path):\n\tfiles = glob('{}/0=*'.format(path))\n\tif len(files) == 0:\n\t\treturn 'file'\n\telif len(files) == 1:\n\t\treturn 'object'\n\telif len(files) == 2:\n\t\treturn 'root'\n\telse:\n\t\treturn 'unexpected'\n\n", "def find_path_type(path):\n\tpath = Path(path)\n\n\tif not path.is_dir():\n\t\treturn 'Not a directory'\n\n\t# Get the Namaste files\n\tnamaste_paths = list(path.glob(\"0=*\"))\n\tif not namaste_paths:\n\t\treturn 'No Namaste files found'\n\n\t# Is it a Storage Root?\n\tif len(namaste_paths) == 1:\n\t\tif path.name.startswith(\".\"):\n\t\t\treturn 'root'\n\t\telse:\n\t\t\treturn 'Object ID doesn\\'t start with \".\"'\n\n\t# Is it an Object?\n\tif len(namaste_paths) == 2:\n\t\tfor namaste_path in namaste_paths:\n\t\t\t# Check the file name\n\t\t\tif not namaste_path.name.startswith(\"0=\"):\n\t\t\t\treturn 'Namaste file name doesn\\'t start with \"0=\"'\n\n\t\t\t# Check the file contents\n\t\t\twith open(namaste_path, 'r') as f\n", "def find_path_type(path):\n\t# TODO: check for valid version number\n\tn_path = os.path.join(path, \"0=ocfl_object_1.0\")\n\tif os.path.isdir(n_path):\n\t\treturn \"object\"\n\tn_path = os.path.join(path, \"0=ocfl_root_1.0\")\n\tif os.path.isdir(n_path):\n\t\treturn \"root\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\treturn \"unknown\"\n", "def find_path_type(path):\n\tlogging.debug(\"find_path_type(%s)\" % path)\n\n\tif not os.path.exists(path):\n\t\treturn \"Path %s does not exist\" % path\n\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\n\t# find all the 0=* files in the directory\n\tfiles = [f for f in os.listdir(path) if f.startswith(\"0=\")]\n\n\tif not files:\n\t\treturn \"Path %s does not contain any instances of '0='\" % path\n\n\t# check to see if it looks like an OCFL Object\n\tif len(files) > 1:\n\t\treturn \"Path %s contains multiple instances of '0='\" % path\n\n\tif len(files) == 1:\n\t\tif files[0] != \"0=ocfl_object_1.0\":\n\t\t\treturn \"Path %s contains a '0=' file which is not '0=ocfl_object_1.0'\" % path\n\t\treturn \"object\"\n\n\t# check to see if it looks like a Storage Root\n\tfor\n", "def find_path_type(path):\n\tif not os.path.isdir(path):\n\t\treturn 'not a directory'\n\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\")):\n\t\treturn 'root'\n\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\", \"inventory.json\")):\n\t\treturn 'object'\n\n\t# check if it's a file\n\treturn 'file'\n\n", "def find_path_type(path):\n\tif not os.path.isdir(path):\n\t\treturn 'not_dir'\n\tif os.path.islink(path):\n\t\treturn 'link'\n\tif os.path.exists(os.path.join(path, 'OCFL_ROOT_INFO.txt')):\n\t\treturn 'root'\n\tif os.path.exists(os.path.join(path, 'inventory.json')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, '0=ocfl_object_1.0')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, 'inventory.xml')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, '0=ocfl_object_1.0')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, '0=content')):\n\t\treturn 'object'\n\tif os.path.exists(os\n", "def find_path_type(path):\n\tif not os.path.exists(path):\n\t\treturn \"path doesn't exist\"\n\tif os.path.isdir(path):\n\t\t# Check for Storage Root\n\t\tfor file in os.listdir(path):\n\t\t\tif file.startswith(\"0=\"):\n\t\t\t\treturn \"root\"\n\t\t# Check for Object\n\t\tfor file in os.listdir(path):\n\t\t\tif file.startswith(\"0=\"):\n\t\t\t\treturn \"object\"\n\t\treturn \"directory\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\treturn \"unknown path type\"\n\n\n", "def find_path_type(path):\n\tpath = os.path.abspath(path)\n\tif not os.path.isdir(path):\n\t\treturn \"Path is not a directory\"\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\")):\n\t\treturn \"Path is not a directory\"\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\", \"inventory.json\")):\n\t\treturn \"Path is not an OCFL Object\"\n\treturn \"object\"\n\n\n", "def find_path_type(path):\n\ttry:\n\t\tnamaste_files = [f for f in os.listdir(path) if re.match(r'0=[^/]*$', f)]\n\texcept OSError as e:\n\t\treturn \"OSError on {0}: {1}\".format(path, e)\n\tif not len(namaste_files):\n\t\treturn \"no namaste files found in {0}\".format(path)\n\tif len(namaste_files) > 1:\n\t\treturn \"multiple namaste files found in {0}\".format(path)\n\tnamaste_file = namaste_files[0]\n\tif re.match(r'0=.*/inventory\\.xml$', namaste_file):\n\t\treturn 'root'\n\tif re.match(r'0=.*/inventory\\.json$', namaste_file):\n\t\treturn 'object'\n\tif re.match(r'0=inventory\\.xml$', namaste_file):\n\t\treturn 'file'\n\treturn \"unknown namaste file {0}\".format(nam\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n\tpath = os.path.abspath(path)\n\tif not os.path.exists(path):\n\t\treturn 'Path does not exist'\n\tif not os.path.isdir(path):\n\t\treturn 'Path is not a directory'\n\tif os.path.isfile(os.path.join(path, 'ocfl_object_1.0')):\n\t\t# Looks like an OCFL Object\n\t\treturn 'object'\n\tnamaste_files = glob.glob(os.path.join(path, '0=*'))\n\tif namaste_files:\n\t\tfor namaste_file in namaste_files:\n\t\t\tif os.path.isfile(namaste_file):\n\t\t\t\tif os.path.basename(namaste_file) == '0=inventory.txt':\n\t\t\t\t\treturn 'file'\n\t\t\t\telse:\n\t\t\t\t\treturn 'Looks like an OCFL Object Version'\n\t\t\tif os.path.isdir(namaste_file):\n\t\t\t\treturn '\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tfiles = glob('{}/0=*'.format(path))\n\tif len(files) == 0:\n\t\treturn 'file'\n\telif len(files) == 1:\n\t\treturn 'object'\n\telif len(files) == 2:\n\t\treturn 'root'\n\telse:\n\t\treturn 'unexpected'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tpath = Path(path)\n\n\tif not path.is_dir():\n\t\treturn 'Not a directory'\n\n\t# Get the Namaste files\n\tnamaste_paths = list(path.glob(\"0=*\"))\n\tif not namaste_paths:\n\t\treturn 'No Namaste files found'\n\n\t# Is it a Storage Root?\n\tif len(namaste_paths) == 1:\n\t\tif path.name.startswith(\".\"):\n\t\t\treturn 'root'\n\t\telse:\n\t\t\treturn 'Object ID doesn\\'t start with \".\"'\n\n\t# Is it an Object?\n\tif len(namaste_paths) == 2:\n\t\tfor namaste_path in namaste_paths:\n\t\t\t# Check the file name\n\t\t\tif not namaste_path.name.startswith(\"0=\"):\n\t\t\t\treturn 'Namaste file name doesn\\'t start with \"0=\"'\n\n\t\t\t# Check the file contents\n\t\t\twith open(namaste_path, 'r') as f\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\t# TODO: check for valid version number\n\tn_path = os.path.join(path, \"0=ocfl_object_1.0\")\n\tif os.path.isdir(n_path):\n\t\treturn \"object\"\n\tn_path = os.path.join(path, \"0=ocfl_root_1.0\")\n\tif os.path.isdir(n_path):\n\t\treturn \"root\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\treturn \"unknown\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tlogging.debug(\"find_path_type(%s)\" % path)\n\n\tif not os.path.exists(path):\n\t\treturn \"Path %s does not exist\" % path\n\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\n\t# find all the 0=* files in the directory\n\tfiles = [f for f in os.listdir(path) if f.startswith(\"0=\")]\n\n\tif not files:\n\t\treturn \"Path %s does not contain any instances of '0='\" % path\n\n\t# check to see if it looks like an OCFL Object\n\tif len(files) > 1:\n\t\treturn \"Path %s contains multiple instances of '0='\" % path\n\n\tif len(files) == 1:\n\t\tif files[0] != \"0=ocfl_object_1.0\":\n\t\t\treturn \"Path %s contains a '0=' file which is not '0=ocfl_object_1.0'\" % path\n\t\treturn \"object\"\n\n\t# check to see if it looks like a Storage Root\n\tfor\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif not os.path.isdir(path):\n\t\treturn 'not a directory'\n\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\")):\n\t\treturn 'root'\n\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\", \"inventory.json\")):\n\t\treturn 'object'\n\n\t# check if it's a file\n\treturn 'file'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif not os.path.isdir(path):\n\t\treturn 'not_dir'\n\tif os.path.islink(path):\n\t\treturn 'link'\n\tif os.path.exists(os.path.join(path, 'OCFL_ROOT_INFO.txt')):\n\t\treturn 'root'\n\tif os.path.exists(os.path.join(path, 'inventory.json')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, '0=ocfl_object_1.0')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, 'inventory.xml')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, '0=ocfl_object_1.0')):\n\t\treturn 'object'\n\tif os.path.exists(os.path.join(path, '0=content')):\n\t\treturn 'object'\n\tif os.path.exists(os\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif not os.path.exists(path):\n\t\treturn \"path doesn't exist\"\n\tif os.path.isdir(path):\n\t\t# Check for Storage Root\n\t\tfor file in os.listdir(path):\n\t\t\tif file.startswith(\"0=\"):\n\t\t\t\treturn \"root\"\n\t\t# Check for Object\n\t\tfor file in os.listdir(path):\n\t\t\tif file.startswith(\"0=\"):\n\t\t\t\treturn \"object\"\n\t\treturn \"directory\"\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\treturn \"unknown path type\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tpath = os.path.abspath(path)\n\tif not os.path.isdir(path):\n\t\treturn \"Path is not a directory\"\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\")):\n\t\treturn \"Path is not a directory\"\n\tif not os.path.exists(os.path.join(path, \"0=ocfl_object_1.0\", \"inventory.json\")):\n\t\treturn \"Path is not an OCFL Object\"\n\treturn \"object\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\ttry:\n\t\tnamaste_files = [f for f in os.listdir(path) if re.match(r'0=[^/]*$', f)]\n\texcept OSError as e:\n\t\treturn \"OSError on {0}: {1}\".format(path, e)\n\tif not len(namaste_files):\n\t\treturn \"no namaste files found in {0}\".format(path)\n\tif len(namaste_files) > 1:\n\t\treturn \"multiple namaste files found in {0}\".format(path)\n\tnamaste_file = namaste_files[0]\n\tif re.match(r'0=.*/inventory\\.xml$', namaste_file):\n\t\treturn 'root'\n\tif re.match(r'0=.*/inventory\\.json$', namaste_file):\n\t\treturn 'object'\n\tif re.match(r'0=inventory\\.xml$', namaste_file):\n\t\treturn 'file'\n\treturn \"unknown namaste file {0}\".format(nam\n", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "ques_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n\t# Check whether we should show votes\n\tif self.config['options']['showvotes']:\n\t\t# Append the votes fields\n\t\tparams['include_fields'] += ',votes'\n\t\tparams['votes_fields'] = 'count,who'\n\n\t# Check whether we should show duplicates\n\tif self.config['options']['showduplicates']:\n\t\t# Append the duplicates field\n\t\tparams['include_fields'] += ',dupe_of,dupe_of.id'\n\n\t# Check whether we should show duplicates\n\tif self.config['options']['showattachments']:\n\t\t# Append the duplicates field\n\t\tparams['include_fields'] += ',attachments'\n\n\t# Check whether we should show comments\n\tif self.config['options']['showcomments']:\n\t\t# Append the comments fields\n\t\tparams['include_fields'] += ',comments'\n\n\t# Append the bug ids\n\tparams['ids'] = bug_ids\n\n\t# Return the amended parameters\n\treturn params\n\n", "def amend_bzparams(self, params, bug_ids):\n\tparams.update({'id': bug_ids})\n\t#print(\"[D] params after update: %s\" % params)\n\n\treturn params\n\n\n", "def amend_bzparams(self, params, bug_ids):\n\tif self.options.bug_comment != None:\n\t\tparams['comment'] = self.options.bug_comment\n\tif self.options.bug_status != None:\n\t\tparams['status'] = self.options.bug_status\n\tif self.options.bug_resolution != None:\n\t\tparams['resolution'] = self.options.bug_resolution\n\tif self.options.bug_target_milestone != None:\n\t\tparams['target_milestone'] = self.options.bug_target_milestone\n\tif self.options.bug_assignee != None:\n\t\tparams['assigned_to'] = self.options.bug_assignee\n\tif self.options.bug_cc != None:\n\t\tparams['cc'] = self.options.bug_cc\n\tif self.options.bug_fixed_in != None:\n\t\tparams['fixedIn'] = self.options.bug_fixed_in\n\tif self.options.bug_qa_contact != None:\n\t\tparams['qa_contact'] = self.options.bug_qa_contact\n\tif self.options.\n", "def amend_bzparams(self, params, bug_ids):\n\tparams[\"id\"] = bug_ids\n\n\n", "def amend_bzparams(self, params, bug_ids):\n\tparams['id'] = bug_ids\n\tparams['include_fields'] = self.bz_fields\n\tif self.include_comments:\n\t\tparams['include_fields'] += ['comments']\n\n", "def amend_bzparams(self, params, bug_ids):\n\tparams['include_fields'] = [\n\t\t'id',\n\t\t'keywords',\n\t\t'assigned_to',\n\t\t'bug_status',\n\t\t'creation_time',\n\t\t'component',\n\t\t'priority',\n\t\t'product',\n\t\t'severity',\n\t\t'bug_file_loc',\n\t\t'classification',\n\t\t'short_desc',\n\t\t'op_sys',\n\t\t'reporter',\n\t\t'bug_id',\n\t\t'bug_severity',\n\t\t'version',\n\t\t'cf_kernel',\n\t\t'cf_kernel_version',\n\t\t'cf_upstream_bug',\n\t\t'cf_upstream_status',\n\t\t'cf_git_commit',\n\t\t'cf_git_branch',\n\t\t'cf_git_describe',\n\t\t'cf_git_base_commit',\n\t\t'cf_git_base_branch',\n\t\t'cf_git_base_describe',\n\t\t'cf_git_patches_\n", "def amend_bzparams(self, params, bug_ids):\n\tparams['bug_id'] = \",\".join(str(bid) for bid in bug_ids)\n\treturn params\n\n", "def amend_bzparams(self, params, bug_ids):\n\t# TODO\n\treturn params\n\n", "def amend_bzparams(self, params, bug_ids):\n\tparams['bug_id'] = bug_ids\n\n\treturn params\n\n", "def amend_bzparams(self, params, bug_ids):\n\tif self.opts.bz_product:\n\t\tparams['product'] = self.opts.bz_product\n\tif self.opts.bz_component:\n\t\tparams['component'] = self.opts.bz_component\n\tif self.opts.bz_version:\n\t\tparams['version'] = self.opts.bz_version\n\tif self.opts.bz_status:\n\t\tparams['bug_status'] = self.opts.bz_status\n\n\tparams['include_fields'] = ['summary', 'id', 'status', 'resolution',\n\t\t'due_in', 'flags', 'assigned_to', 'priority', 'severity',\n\t\t'component', 'whiteboard', 'target_milestone', 'blocks',\n\t\t'depends_on', 'target_release', 'qa_contact']\n\n\tif bug_ids:\n\t\tparams['id'] = bug_ids\n\n\treturn params\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t# Check whether we should show votes\n\tif self.config['options']['showvotes']:\n\t\t# Append the votes fields\n\t\tparams['include_fields'] += ',votes'\n\t\tparams['votes_fields'] = 'count,who'\n\n\t# Check whether we should show duplicates\n\tif self.config['options']['showduplicates']:\n\t\t# Append the duplicates field\n\t\tparams['include_fields'] += ',dupe_of,dupe_of.id'\n\n\t# Check whether we should show duplicates\n\tif self.config['options']['showattachments']:\n\t\t# Append the duplicates field\n\t\tparams['include_fields'] += ',attachments'\n\n\t# Check whether we should show comments\n\tif self.config['options']['showcomments']:\n\t\t# Append the comments fields\n\t\tparams['include_fields'] += ',comments'\n\n\t# Append the bug ids\n\tparams['ids'] = bug_ids\n\n\t# Return the amended parameters\n\treturn params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tparams.update({'id': bug_ids})\n\t#print(\"[D] params after update: %s\" % params)\n\n\treturn params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tif self.options.bug_comment != None:\n\t\tparams['comment'] = self.options.bug_comment\n\tif self.options.bug_status != None:\n\t\tparams['status'] = self.options.bug_status\n\tif self.options.bug_resolution != None:\n\t\tparams['resolution'] = self.options.bug_resolution\n\tif self.options.bug_target_milestone != None:\n\t\tparams['target_milestone'] = self.options.bug_target_milestone\n\tif self.options.bug_assignee != None:\n\t\tparams['assigned_to'] = self.options.bug_assignee\n\tif self.options.bug_cc != None:\n\t\tparams['cc'] = self.options.bug_cc\n\tif self.options.bug_fixed_in != None:\n\t\tparams['fixedIn'] = self.options.bug_fixed_in\n\tif self.options.bug_qa_contact != None:\n\t\tparams['qa_contact'] = self.options.bug_qa_contact\n\tif self.options.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tparams[\"id\"] = bug_ids\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tparams['id'] = bug_ids\n\tparams['include_fields'] = self.bz_fields\n\tif self.include_comments:\n\t\tparams['include_fields'] += ['comments']\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tparams['include_fields'] = [\n\t\t'id',\n\t\t'keywords',\n\t\t'assigned_to',\n\t\t'bug_status',\n\t\t'creation_time',\n\t\t'component',\n\t\t'priority',\n\t\t'product',\n\t\t'severity',\n\t\t'bug_file_loc',\n\t\t'classification',\n\t\t'short_desc',\n\t\t'op_sys',\n\t\t'reporter',\n\t\t'bug_id',\n\t\t'bug_severity',\n\t\t'version',\n\t\t'cf_kernel',\n\t\t'cf_kernel_version',\n\t\t'cf_upstream_bug',\n\t\t'cf_upstream_status',\n\t\t'cf_git_commit',\n\t\t'cf_git_branch',\n\t\t'cf_git_describe',\n\t\t'cf_git_base_commit',\n\t\t'cf_git_base_branch',\n\t\t'cf_git_base_describe',\n\t\t'cf_git_patches_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tparams['bug_id'] = \",\".join(str(bid) for bid in bug_ids)\n\treturn params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t# TODO\n\treturn params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tparams['bug_id'] = bug_ids\n\n\treturn params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tif self.opts.bz_product:\n\t\tparams['product'] = self.opts.bz_product\n\tif self.opts.bz_component:\n\t\tparams['component'] = self.opts.bz_component\n\tif self.opts.bz_version:\n\t\tparams['version'] = self.opts.bz_version\n\tif self.opts.bz_status:\n\t\tparams['bug_status'] = self.opts.bz_status\n\n\tparams['include_fields'] = ['summary', 'id', 'status', 'resolution',\n\t\t'due_in', 'flags', 'assigned_to', 'priority', 'severity',\n\t\t'component', 'whiteboard', 'target_milestone', 'blocks',\n\t\t'depends_on', 'target_release', 'qa_contact']\n\n\tif bug_ids:\n\t\tparams['id'] = bug_ids\n\n\treturn params\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\tfor key, value in nodes:\n\t\t# If we've seen this key before, merge this value with the existing value.\n\t\tfor existing_key, existing_value in merged_nodes:\n\t\t\tif key.value == existing_key.value:\n\t\t\t\t# If the existing value is a mapping, merge any keys in the new value into it.\n\t\t\t\t# Otherwise, replace the existing value with the new one.\n\t\t\t\tif isinstance(existing_value, yaml.nodes.MappingNode):\n\t\t\t\t\tdeep_merge_nodes(value.value).extend(existing_value.value)\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes.remove((existing_key, existing_value))\n\t\t\t\t\tmerged_nodes.append((key, value))\n\n\t\t\t\tbreak\n\t\telse:\n\t\t\tmerged_nodes.append((key, value))\n\n\treturn merged_nodes\n\n\n", "def deep_merge_nodes(nodes):\n\tmerged = {}\n\tfor key, value in nodes:\n\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\tif key.value not in merged:\n\t\t\t\tmerged[key.value] = value.value\n\t\t\telse:\n\t\t\t\tmerged_value = merged[key.value]\n\t\t\t\tmerged[key.value] = deep_merge_nodes(\n\t\t\t\t\tmerged_value + value.value\n\t\t\t\t)\n\t\telse:\n\t\t\tmerged[key.value] = value\n\treturn list(merged.items())\n\n\n", "def deep_merge_nodes(nodes):\n\t# Start with an empty dictionary to de-duplicate node keys.\n\tnode_dict = {}\n\n\t# Iterate through the nodes.\n\tfor node_key, node_value in nodes:\n\t\t# If the key is already in the dictionary, deep merge the values.\n\t\tif node_key in node_dict:\n\t\t\tif isinstance(node_dict[node_key], ruamel.yaml.nodes.MappingNode):\n\t\t\t\tif isinstance(node_value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\tnode_dict[node_key].value = deep_merge_nodes(\n\t\t\t\t\t\tnode_dict[node_key].value + node_value.value\n\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\tnode_dict[node_key] = node_value\n\t\t\telse:\n\t\t\t\tnode_dict[node_key] = node_value\n\t\t# Otherwise, add the key and value to the dictionary.\n\t\telse:\n\t\t\tnode_\n", "def deep_merge_nodes(nodes):\n\tkeys_to_nodes = defaultdict(list)\n\tfor key, value in nodes:\n\t\tkeys_to_nodes[key.value].append(value)\n\n\tmerged_nodes = []\n\tfor key, values in keys_to_nodes.items():\n\t\tif any(not isinstance(value, yaml.MappingNode) for value in values):\n\t\t\t# If there are any non-MappingNode values, just use the last one.\n\t\t\tmerged_nodes.append((yaml.ScalarNode(tag='tag:yaml.org,2002:str', value=key), values[-1]))\n\t\telse:\n\t\t\t# Recurse into MappingNodes for deep merging.\n\t\t\tmerged_nodes.append(\n\t\t\t\t(yaml.ScalarNode(tag='tag:yaml.org,2002:str', value=key), deep_merge_nodes(list(values[0].value)))\n\t\t\t)\n\n\treturn merged_nodes\n", "def deep_merge_nodes(nodes):\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tkey, value = node\n\t\tif key.value not in merged_nodes:\n\t\t\tmerged_nodes[key.value] = value\n\t\telse:\n\t\t\tmerged_nodes[key.value] = _deep_merge_nodes(\n\t\t\t\tmerged_nodes[key.value], value\n\t\t\t)\n\treturn list(merged_nodes.items())\n\n\n", "def deep_merge_nodes(nodes):\n\tnodes_by_key = {}\n\tfor node_key, node_value in nodes:\n\t\tif isinstance(node_value, MappingNode):\n\t\t\t# If the node is a mapping, merge it with any previous node of the same key.\n\t\t\tif node_key.value in nodes_by_key:\n\t\t\t\texisting_node_value = nodes_by_key[node_key.value]\n\t\t\t\tif not isinstance(existing_node_value, MappingNode):\n\t\t\t\t\t# If the node value we're merging from is not a mapping, overwrite it.\n\t\t\t\t\tnodes_by_key[node_key.value] = node_value\n\t\t\t\telse:\n\t\t\t\t\t# If the node value we're merging from is a mapping, deep merge it.\n\t\t\t\t\texisting_node_value.value.extend(deep_merge_nodes(node_value.value))\n\t\t\telse:\n\t\t\t\tnodes_by_key[node_key.value] = node_value\n\t\n", "def deep_merge_nodes(nodes):\n\t# Collect the nodes by their keys.\n\tnodes_by_key = {}\n\tfor (key, value) in nodes:\n\t\tnodes_by_key[key] = value\n\n\t# Merge any duplicate nodes, deep merging their values if they're both MappingNodes.\n\tmerged_nodes = []\n\tfor (key, value) in nodes_by_key.items():\n\t\tif key in nodes_by_key:\n\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\tmerged_nodes.append((key, deep_merge_nodes(value.value)))\n\t\t\telse:\n\t\t\t\tmerged_nodes.append((key, value))\n\treturn merged_nodes\n\n\n", "def deep_merge_nodes(nodes):\n\tresult = []\n\n\tfor key, value in nodes:\n\t\tfor existing_key, existing_value in result:\n\t\t\tif key.value == existing_key.value:\n\t\t\t\tif isinstance(existing_value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\t\texisting_value.value = deep_merge_nodes(value.value + existing_value.value)\n\t\t\t\t\telse:\n\t\t\t\t\t\texisting_value.value.append((key, value))\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tresult.append((key, value))\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tresult.append((key, value))\n\n\treturn result\n\n\n", "def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\tfor key_node, value_node in nodes:\n\t\t# Look through previously merged nodes for a key matching the current node's key.\n\t\tmatching_merged_nodes = [\n\t\t\tmerged_node\n\t\t\tfor merged_node in merged_nodes\n\t\t\tif merged_node[0].value == key_node.value\n\t\t]\n\n\t\tif matching_merged_nodes:\n\t\t\t# If a match is found, deep merge the current value node into the value node of the\n\t\t\t# matching merged node.\n\t\t\tmerged_nodes = [\n\t\t\t\t(\n\t\t\t\t\tmerged_node[0] if merged_node[0].value != key_node.value else key_node,\n\t\t\t\t\tdeep_merge_nodes(\n\t\t\t\t\t\t[\n\t\t\t\t\t\t\t(merged_node[0], merged_node[1]),\n\t\t\t\t\t\t\t(key_node, value_node),\n\t\t\t\t\t\t\n", "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor key, value in nodes:\n\t\tif isinstance(value, MappingNode):\n\t\t\tif key in result:\n\t\t\t\tresult[key].value.extend(deep_merge_nodes(value.value))\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\t\telse:\n\t\t\tresult[key] = value\n\n\treturn list(result.items())\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\tfor key, value in nodes:\n\t\t# If we've seen this key before, merge this value with the existing value.\n\t\tfor existing_key, existing_value in merged_nodes:\n\t\t\tif key.value == existing_key.value:\n\t\t\t\t# If the existing value is a mapping, merge any keys in the new value into it.\n\t\t\t\t# Otherwise, replace the existing value with the new one.\n\t\t\t\tif isinstance(existing_value, yaml.nodes.MappingNode):\n\t\t\t\t\tdeep_merge_nodes(value.value).extend(existing_value.value)\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes.remove((existing_key, existing_value))\n\t\t\t\t\tmerged_nodes.append((key, value))\n\n\t\t\t\tbreak\n\t\telse:\n\t\t\tmerged_nodes.append((key, value))\n\n\treturn merged_nodes\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\tmerged = {}\n\tfor key, value in nodes:\n\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\tif key.value not in merged:\n\t\t\t\tmerged[key.value] = value.value\n\t\t\telse:\n\t\t\t\tmerged_value = merged[key.value]\n\t\t\t\tmerged[key.value] = deep_merge_nodes(\n\t\t\t\t\tmerged_value + value.value\n\t\t\t\t)\n\t\telse:\n\t\t\tmerged[key.value] = value\n\treturn list(merged.items())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t# Start with an empty dictionary to de-duplicate node keys.\n\tnode_dict = {}\n\n\t# Iterate through the nodes.\n\tfor node_key, node_value in nodes:\n\t\t# If the key is already in the dictionary, deep merge the values.\n\t\tif node_key in node_dict:\n\t\t\tif isinstance(node_dict[node_key], ruamel.yaml.nodes.MappingNode):\n\t\t\t\tif isinstance(node_value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\tnode_dict[node_key].value = deep_merge_nodes(\n\t\t\t\t\t\tnode_dict[node_key].value + node_value.value\n\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\tnode_dict[node_key] = node_value\n\t\t\telse:\n\t\t\t\tnode_dict[node_key] = node_value\n\t\t# Otherwise, add the key and value to the dictionary.\n\t\telse:\n\t\t\tnode_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tkeys_to_nodes = defaultdict(list)\n\tfor key, value in nodes:\n\t\tkeys_to_nodes[key.value].append(value)\n\n\tmerged_nodes = []\n\tfor key, values in keys_to_nodes.items():\n\t\tif any(not isinstance(value, yaml.MappingNode) for value in values):\n\t\t\t# If there are any non-MappingNode values, just use the last one.\n\t\t\tmerged_nodes.append((yaml.ScalarNode(tag='tag:yaml.org,2002:str', value=key), values[-1]))\n\t\telse:\n\t\t\t# Recurse into MappingNodes for deep merging.\n\t\t\tmerged_nodes.append(\n\t\t\t\t(yaml.ScalarNode(tag='tag:yaml.org,2002:str', value=key), deep_merge_nodes(list(values[0].value)))\n\t\t\t)\n\n\treturn merged_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tkey, value = node\n\t\tif key.value not in merged_nodes:\n\t\t\tmerged_nodes[key.value] = value\n\t\telse:\n\t\t\tmerged_nodes[key.value] = _deep_merge_nodes(\n\t\t\t\tmerged_nodes[key.value], value\n\t\t\t)\n\treturn list(merged_nodes.items())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tnodes_by_key = {}\n\tfor node_key, node_value in nodes:\n\t\tif isinstance(node_value, MappingNode):\n\t\t\t# If the node is a mapping, merge it with any previous node of the same key.\n\t\t\tif node_key.value in nodes_by_key:\n\t\t\t\texisting_node_value = nodes_by_key[node_key.value]\n\t\t\t\tif not isinstance(existing_node_value, MappingNode):\n\t\t\t\t\t# If the node value we're merging from is not a mapping, overwrite it.\n\t\t\t\t\tnodes_by_key[node_key.value] = node_value\n\t\t\t\telse:\n\t\t\t\t\t# If the node value we're merging from is a mapping, deep merge it.\n\t\t\t\t\texisting_node_value.value.extend(deep_merge_nodes(node_value.value))\n\t\t\telse:\n\t\t\t\tnodes_by_key[node_key.value] = node_value\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\t# Collect the nodes by their keys.\n\tnodes_by_key = {}\n\tfor (key, value) in nodes:\n\t\tnodes_by_key[key] = value\n\n\t# Merge any duplicate nodes, deep merging their values if they're both MappingNodes.\n\tmerged_nodes = []\n\tfor (key, value) in nodes_by_key.items():\n\t\tif key in nodes_by_key:\n\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\tmerged_nodes.append((key, deep_merge_nodes(value.value)))\n\t\t\telse:\n\t\t\t\tmerged_nodes.append((key, value))\n\treturn merged_nodes\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = []\n\n\tfor key, value in nodes:\n\t\tfor existing_key, existing_value in result:\n\t\t\tif key.value == existing_key.value:\n\t\t\t\tif isinstance(existing_value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\t\texisting_value.value = deep_merge_nodes(value.value + existing_value.value)\n\t\t\t\t\telse:\n\t\t\t\t\t\texisting_value.value.append((key, value))\n\t\t\t\t\tbreak\n\t\t\t\telse:\n\t\t\t\t\tresult.append((key, value))\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tresult.append((key, value))\n\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\tfor key_node, value_node in nodes:\n\t\t# Look through previously merged nodes for a key matching the current node's key.\n\t\tmatching_merged_nodes = [\n\t\t\tmerged_node\n\t\t\tfor merged_node in merged_nodes\n\t\t\tif merged_node[0].value == key_node.value\n\t\t]\n\n\t\tif matching_merged_nodes:\n\t\t\t# If a match is found, deep merge the current value node into the value node of the\n\t\t\t# matching merged node.\n\t\t\tmerged_nodes = [\n\t\t\t\t(\n\t\t\t\t\tmerged_node[0] if merged_node[0].value != key_node.value else key_node,\n\t\t\t\t\tdeep_merge_nodes(\n\t\t\t\t\t\t[\n\t\t\t\t\t\t\t(merged_node[0], merged_node[1]),\n\t\t\t\t\t\t\t(key_node, value_node),\n\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor key, value in nodes:\n\t\tif isinstance(value, MappingNode):\n\t\t\tif key in result:\n\t\t\t\tresult[key].value.extend(deep_merge_nodes(value.value))\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\t\telse:\n\t\t\tresult[key] = value\n\n\treturn list(result.items())\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Retrieve the current weather conditions and alert level for a given ZIP code.\")\n\tparser.add_argument(\"zipcode\", help=\"ZIP code of the location for which the weather will be retrieved\", type=int)\n\tparser.add_argument(\"--units\", help=\"units to retrieve the temperature in (Celsius or Fahrenheit, default is Fahrenheit)\", choices=[\"F\", \"C\"], default=\"F\")\n\tparser.add_argument(\"--alerts\", help=\"retrieve active weather alerts for the ZIP code\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--imperial\", help=\"retrieve the temperature in Fahrenheit (default)\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--metric\", help=\"retrieve the temperature in Celsius\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--verbose\", help=\"output verbose logging\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--version\", help=\"display version information\", action=\"store_true\", default=False)\n\treturn parser\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"This script takes a file containing a list of URLs and \"\n\t\t\"attempts to download them. The list must be formatted such that each URL is on a separate line.\")\n\tparser.add_argument(\"input_file\", type=str, help=\"Path to the file containing the list of URLs to download.\")\n\tparser.add_argument(\"-d\", \"--destination\", type=str, help=\"Destination directory to download files to. \"\n\t\t\"Defaults to the current working directory.\", default=os.getcwd())\n\tparser.add_argument(\"-i\", \"--ignore\", action=\"store_true\", help=\"Ignore any errors that occur while \"\n\t\t\"downloading files.\")\n\tparser.add_argument(\"-t\", \"--threads\", type=int, help=\"Number of threads to use for downloading. \"\n\t\t\"Defaults to the number of CPU cores on the system.\", default=multiprocessing.cpu_count())\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Print progress messages to\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Generate a set of random numbers and write them to a file.')\n\tparser.add_argument('-n', '--numbers', type=int, default=10,\n\t\t\t\t\t\thelp='number of random numbers to generate (default: 10)')\n\tparser.add_argument('-m', '--min', type=int, default=1,\n\t\t\t\t\t\thelp='minimum value for the numbers to generate (default: 1)')\n\tparser.add_argument('-M', '--max', type=int, default=100,\n\t\t\t\t\t\thelp='maximum value for the numbers to generate (default: 100)')\n\tparser.add_argument('-o', '--output-file', type=str, default='random_nums.txt',\n\t\t\t\t\t\thelp='name of the file to create (default: random_nums.txt)')\n\tparser.add_argument('-v', '--verbose', action='store_true',\n\t\t\t\t\t\thelp='show more output')\n\treturn parser\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description = \"Generate a text file containing paths to all the \"\n\t\t\"file in the given directory.\")\n\tparser.add_argument('-o', '--output', dest = 'output_path',\n\t\thelp = 'the path to the file to which the output will be written')\n\tparser.add_argument('-d', '--directory', dest = 'directory_path',\n\t\thelp = 'the path to the directory whose files should be listed')\n\tparser.add_argument('--crawl', dest = 'crawl',\n\t\taction = 'store_true', default = False,\n\t\thelp = 'recursively list the files in the given directory')\n\tparser.add_argument('--relative', dest = 'relative',\n\t\taction = 'store_true', default = False,\n\t\thelp = 'write relative paths to the files in the output file')\n\tparser.add_argument('-r', '--regular-expression', dest = 'regular_expression',\n\t\thelp = 'a regular expression that the file names must match')\n\tparser.add_argument\n", "def parse_arguments(*arguments):\n\tparser = ArgumentParser(description = \"Restarts the specified Service Manager services.\")\n\tparser.add_argument(\"-s\", \"--services\", help = \"A comma-separated list of Service Manager services to restart\", metavar = \"SERVICES\")\n\tparser.add_argument(\"-v\", \"--verbose\", help = \"Display all output from the Service Manager commands executed by this script\", action = \"store_true\")\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Given JSON-formatted data about an activity, \" +\n\t\t\"pretty-print the data.\")\n\tparser.add_argument(\"--input\", help=\"input file from which to read the data\", type=argparse.FileType(\"r\"),\n\t\tdefault=sys.stdin)\n\tparser.add_argument(\"--output\", help=\"file to which to write the pretty-printed data\", type=argparse.FileType(\"w\"),\n\t\tdefault=sys.stdout)\n\tparser.add_argument(\"--indent\", help=\"number of spaces to indent each level of the JSON data\", type=int,\n\t\tdefault=2)\n\tparser.add_argument(\"--sort-keys\", help=\"sort the keys of the JSON data\", action=\"store_true\")\n\tparser.add_argument(\"--separators\", help=\"tuple of characters to use as separators for the JSON data\",\n\t\tnargs=2)\n\treturn parser.parse_args()\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Generate a list of all the words used in a novel')\n\tparser.add_argument('-i', '--input', required=True, help='Novel file to parse')\n\tparser.add_argument('-o', '--output', required=True, help='Output file for parsed words')\n\tparser.add_argument('-n', '--number', default=None, help='Number of words to parse')\n\tparser.add_argument('-s', '--start', default=0, help='Index at which to start parsing words')\n\tparser.add_argument('-v', '--verbose', default=False, action='store_true', help='Enable verbose output')\n\tparser.add_argument('-d', '--debug', default=False, action='store_true', help='Enable debugging output')\n\tparser.add_argument('-V', '--version', action='version', version='%(prog)s version ' + __version__)\n\treturn parser.parse_args()\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description = \"Script to generate a subgraph of a given graph \"\n                                                  \"by extracting it from a specific part of the \"\n                                                  \"graph and then rewiring it. The extracted \"\n                                                  \"subgraph is rewired using the Barab\u00e1si-Albert \"\n                                                  \"model, which is a generative model for graphs \"\n                                                  \"that allows it to be grown from a small seed \"\n                                                  \"graph. The seed graph is specified by the \"\n                                                  \"user as a list of vertices in the given graph \"\n                                                  \"that is to be used for the rewiring. The \"\n                                                  \"rewiring is done by connecting the given \"\n                                                  \"vertices to the vertices in the graph that \"\n                                                  \"are not in the given list of vertices. The \"\n                                                  \"number of edges that are added from the \"\n                                                  \"given vertices to the rest of the vertices \"\n                                                  \"is specified as the second argument. The \"\n                                                  \"rewired graph is generated\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t description=\"This script takes as input a list of sequences of the same length and a number of clusters and performs a K-means clustering of the sequences.\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n\n\tparser.add_argument('--input', '-i', action='store', dest='input_path', required=False, default=None,\n\t\t\t\t\t\t\t\t\t\t\thelp=\"The path to the input file. If not specified, the input is read from STDIN.\")\n\n\tparser.add_argument('--output', '-o', action='store', dest='output_path', required=False, default=None,\n\t\t\t\t\t\t\t\t\t\t\thelp=\"The path to the output file. If not specified, the output is written to STDOUT.\")\n\n\tparser.add_argument('--clusters', '-c', action\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(usage='%(prog)s [options]', description=__doc__, epilog=__author__)\n\tparser.add_argument('-v', '--verbose', action='store_true', default=False, help='verbose output')\n\tparser.add_argument('-c', '--config', metavar='FILE', help='configuration file')\n\tparser.add_argument('-V', '--version', action='version', version='%(prog)s ' + '%s' % __version__)\n\treturn parser.parse_args(arguments)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Retrieve the current weather conditions and alert level for a given ZIP code.\")\n\tparser.add_argument(\"zipcode\", help=\"ZIP code of the location for which the weather will be retrieved\", type=int)\n\tparser.add_argument(\"--units\", help=\"units to retrieve the temperature in (Celsius or Fahrenheit, default is Fahrenheit)\", choices=[\"F\", \"C\"], default=\"F\")\n\tparser.add_argument(\"--alerts\", help=\"retrieve active weather alerts for the ZIP code\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--imperial\", help=\"retrieve the temperature in Fahrenheit (default)\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--metric\", help=\"retrieve the temperature in Celsius\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--verbose\", help=\"output verbose logging\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--version\", help=\"display version information\", action=\"store_true\", default=False)\n\treturn parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"This script takes a file containing a list of URLs and \"\n\t\t\"attempts to download them. The list must be formatted such that each URL is on a separate line.\")\n\tparser.add_argument(\"input_file\", type=str, help=\"Path to the file containing the list of URLs to download.\")\n\tparser.add_argument(\"-d\", \"--destination\", type=str, help=\"Destination directory to download files to. \"\n\t\t\"Defaults to the current working directory.\", default=os.getcwd())\n\tparser.add_argument(\"-i\", \"--ignore\", action=\"store_true\", help=\"Ignore any errors that occur while \"\n\t\t\"downloading files.\")\n\tparser.add_argument(\"-t\", \"--threads\", type=int, help=\"Number of threads to use for downloading. \"\n\t\t\"Defaults to the number of CPU cores on the system.\", default=multiprocessing.cpu_count())\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"Print progress messages to\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Generate a set of random numbers and write them to a file.')\n\tparser.add_argument('-n', '--numbers', type=int, default=10,\n\t\t\t\t\t\thelp='number of random numbers to generate (default: 10)')\n\tparser.add_argument('-m', '--min', type=int, default=1,\n\t\t\t\t\t\thelp='minimum value for the numbers to generate (default: 1)')\n\tparser.add_argument('-M', '--max', type=int, default=100,\n\t\t\t\t\t\thelp='maximum value for the numbers to generate (default: 100)')\n\tparser.add_argument('-o', '--output-file', type=str, default='random_nums.txt',\n\t\t\t\t\t\thelp='name of the file to create (default: random_nums.txt)')\n\tparser.add_argument('-v', '--verbose', action='store_true',\n\t\t\t\t\t\thelp='show more output')\n\treturn parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description = \"Generate a text file containing paths to all the \"\n\t\t\"file in the given directory.\")\n\tparser.add_argument('-o', '--output', dest = 'output_path',\n\t\thelp = 'the path to the file to which the output will be written')\n\tparser.add_argument('-d', '--directory', dest = 'directory_path',\n\t\thelp = 'the path to the directory whose files should be listed')\n\tparser.add_argument('--crawl', dest = 'crawl',\n\t\taction = 'store_true', default = False,\n\t\thelp = 'recursively list the files in the given directory')\n\tparser.add_argument('--relative', dest = 'relative',\n\t\taction = 'store_true', default = False,\n\t\thelp = 'write relative paths to the files in the output file')\n\tparser.add_argument('-r', '--regular-expression', dest = 'regular_expression',\n\t\thelp = 'a regular expression that the file names must match')\n\tparser.add_argument\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = ArgumentParser(description = \"Restarts the specified Service Manager services.\")\n\tparser.add_argument(\"-s\", \"--services\", help = \"A comma-separated list of Service Manager services to restart\", metavar = \"SERVICES\")\n\tparser.add_argument(\"-v\", \"--verbose\", help = \"Display all output from the Service Manager commands executed by this script\", action = \"store_true\")\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Given JSON-formatted data about an activity, \" +\n\t\t\"pretty-print the data.\")\n\tparser.add_argument(\"--input\", help=\"input file from which to read the data\", type=argparse.FileType(\"r\"),\n\t\tdefault=sys.stdin)\n\tparser.add_argument(\"--output\", help=\"file to which to write the pretty-printed data\", type=argparse.FileType(\"w\"),\n\t\tdefault=sys.stdout)\n\tparser.add_argument(\"--indent\", help=\"number of spaces to indent each level of the JSON data\", type=int,\n\t\tdefault=2)\n\tparser.add_argument(\"--sort-keys\", help=\"sort the keys of the JSON data\", action=\"store_true\")\n\tparser.add_argument(\"--separators\", help=\"tuple of characters to use as separators for the JSON data\",\n\t\tnargs=2)\n\treturn parser.parse_args()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Generate a list of all the words used in a novel')\n\tparser.add_argument('-i', '--input', required=True, help='Novel file to parse')\n\tparser.add_argument('-o', '--output', required=True, help='Output file for parsed words')\n\tparser.add_argument('-n', '--number', default=None, help='Number of words to parse')\n\tparser.add_argument('-s', '--start', default=0, help='Index at which to start parsing words')\n\tparser.add_argument('-v', '--verbose', default=False, action='store_true', help='Enable verbose output')\n\tparser.add_argument('-d', '--debug', default=False, action='store_true', help='Enable debugging output')\n\tparser.add_argument('-V', '--version', action='version', version='%(prog)s version ' + __version__)\n\treturn parser.parse_args()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description = \"Script to generate a subgraph of a given graph \"\n                                                  \"by extracting it from a specific part of the \"\n                                                  \"graph and then rewiring it. The extracted \"\n                                                  \"subgraph is rewired using the Barab\u00e1si-Albert \"\n                                                  \"model, which is a generative model for graphs \"\n                                                  \"that allows it to be grown from a small seed \"\n                                                  \"graph. The seed graph is specified by the \"\n                                                  \"user as a list of vertices in the given graph \"\n                                                  \"that is to be used for the rewiring. The \"\n                                                  \"rewiring is done by connecting the given \"\n                                                  \"vertices to the vertices in the graph that \"\n                                                  \"are not in the given list of vertices. The \"\n                                                  \"number of edges that are added from the \"\n                                                  \"given vertices to the rest of the vertices \"\n                                                  \"is specified as the second argument. The \"\n                                                  \"rewired graph is generated\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t description=\"This script takes as input a list of sequences of the same length and a number of clusters and performs a K-means clustering of the sequences.\"\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)\n\n\tparser.add_argument('--version', action='version', version='%(prog)s ' + VERSION)\n\n\tparser.add_argument('--input', '-i', action='store', dest='input_path', required=False, default=None,\n\t\t\t\t\t\t\t\t\t\t\thelp=\"The path to the input file. If not specified, the input is read from STDIN.\")\n\n\tparser.add_argument('--output', '-o', action='store', dest='output_path', required=False, default=None,\n\t\t\t\t\t\t\t\t\t\t\thelp=\"The path to the output file. If not specified, the output is written to STDOUT.\")\n\n\tparser.add_argument('--clusters', '-c', action\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(usage='%(prog)s [options]', description=__doc__, epilog=__author__)\n\tparser.add_argument('-v', '--verbose', action='store_true', default=False, help='verbose output')\n\tparser.add_argument('-c', '--config', metavar='FILE', help='configuration file')\n\tparser.add_argument('-V', '--version', action='version', version='%(prog)s ' + '%s' % __version__)\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tflags.extend(action.option_strings)\n\treturn ' '.join(flags)\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._SubParsersAction):\n\t\t\tfor choice, subparser in action.choices.items():\n\t\t\t\tflags.append(choice)\n\t\t\t\tflags += parser_flags(subparser)\n\t\telse:\n\t\t\tflags.append(action.option_strings[0])\n\treturn \" \".join(sorted(flags))\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(parser._option_string_actions.keys())\n\n", "def parser_flags(parser):\n\treturn ' '.join(\n\t\t['--{} {}'.format(flag, parser.get_default(flag)) for flag in parser._get_kwargs()]\n\t)\n\n", "def parser_flags(parser):\n\tflags = set()\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.update(action.option_strings)\n\treturn \" \".join(sorted(flags))\n\n\n", "def parser_flags(parser):\n\treturn \" \".join(sorted(\n\t\t# parser.format_argument_defaults is only available in Python 3.3+\n\t\t\"{} {}\".format(\n\t\t\taction.option_strings[0],\n\t\t\tparser.format_argument_defaults(action) if hasattr(parser, \"format_argument_defaults\") else action.default\n\t\t)\n\t\tfor action in parser._actions if action.option_strings\n\t))\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n\n", "def parser_flags(parser):\n\treturn \" \".join(flag for action in parser._actions for flag in action.option_strings)\n\n\n", "def parser_flags(parser):\n\ttry:\n\t\tflags_str = ' '.join(parser.format_help().splitlines()[1:])\n\texcept:\n\t\tflags_str = ''\n\n\treturn flags_str\n\n\n", "def parser_flags(parser):\n\treturn \"\" if not parser else \" \".join(parser.format_usage().replace(\"usage: \", \"\").split()[1:])\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif not action.option_strings:\n\t\t\tcontinue\n\t\tflags.extend(action.option_strings)\n\treturn ' '.join(flags)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._SubParsersAction):\n\t\t\tfor choice, subparser in action.choices.items():\n\t\t\t\tflags.append(choice)\n\t\t\t\tflags += parser_flags(subparser)\n\t\telse:\n\t\t\tflags.append(action.option_strings[0])\n\treturn \" \".join(sorted(flags))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(parser._option_string_actions.keys())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(\n\t\t['--{} {}'.format(flag, parser.get_default(flag)) for flag in parser._get_kwargs()]\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = set()\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.update(action.option_strings)\n\treturn \" \".join(sorted(flags))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(sorted(\n\t\t# parser.format_argument_defaults is only available in Python 3.3+\n\t\t\"{} {}\".format(\n\t\t\taction.option_strings[0],\n\t\t\tparser.format_argument_defaults(action) if hasattr(parser, \"format_argument_defaults\") else action.default\n\t\t)\n\t\tfor action in parser._actions if action.option_strings\n\t))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(flag for action in parser._actions for flag in action.option_strings)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\ttry:\n\t\tflags_str = ' '.join(parser.format_help().splitlines()[1:])\n\texcept:\n\t\tflags_str = ''\n\n\treturn flags_str\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \"\" if not parser else \" \".join(parser.format_usage().replace(\"usage: \", \"\").split()[1:])\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tprog=__file__,\n\t\tdescription=\"\"\"\n\t\t\tThis script is a tool for managing the configuration of the\n\t\t\tbasespace_to_irods_synchronizer script.\n\t\t\t\"\"\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter\n\t)\n\n\tparser.add_argument(\n\t\t\"-c\", \"--configuration-file\",\n\t\tmetavar=\"PATH\",\n\t\thelp=\"\"\"\n\t\t\tThe path to the configuration file to use.\n\t\t\t\"\"\",\n\t\tdefault=None\n\t)\n\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"\"\"\n\t\t\tThese are the subcommands available for the\n\t\t\tbasespace_to_irods_synchronizer configuration management tool.\n\t\t\t\"\"\",\n\t\thelp=\"\"\"\n\t\t\tUse this command to get help for each subcommand.\n\t\t\t\"\"\"\n\t)\n\n\t# The \"configure\" subcommand\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run the fenics-based version of the model from the paper 'A multiscale model of\"\n\t\t\t+ \" tumor-induced angiogenesis: The role of hypoxia, acidosis and metalloproteinases' by\"\n\t\t\t+ \" D.S. Chaves, C. Ribeiro & D.A. Knobel.\",\n\t\tepilog=\"If a subcommand is given, only the options relevant to that subcommand are parsed.\"\n\t\t\t+ \" If no subcommand is given, all options (i.e. all options from all subcommands) are\"\n\t\t\t+ \" parsed. For example, if you only want to run the model, you can use the command\"\n\t\t\t+ \" `python3 main.py run`, but if you want to extract data from a previously-run model and\"\n\t\t\t+ \" plot it, you can use the command `python3 main.py plot`. Finally, if you want to run\"\n\t\t\t+ \" the model and then plot the data it produces, you can use the command\"\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tdescription=\"\"\"\\\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(prog='samplx', description='Manage and run SAMPLing experiments.')\n\tsubparsers = parser.add_subparsers(dest='subparser_name', title='subcommands', metavar='subcommand')\n\n\t# Define a subparser to handle the \"samplx run\" command.\n\tparser_run = subparsers.add_parser(\n\t\t'run',\n\t\thelp='run a SAMPLing experiment',\n\t\tdescription='Run a SAMPLing experiment.',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=\"\"\"\\\n", "def parse_arguments(*unparsed_arguments):\n\tparser = create_argparser()\n\targs = parser.parse_args(*unparsed_arguments)\n\treturn args\n\n", "def parse_arguments(*unparsed_arguments):\n\tglobal_parser = argparse.ArgumentParser(add_help=False)\n\tglobal_parser.add_argument(\"--output-dir\", \"-o\", metavar=\"DIR\", type=str, required=True,\n\t\thelp=\"path to output directory\")\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Create a LaTeX report from a given set of annotations.\",\n\t\tparents=[global_parser])\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\",\n\t\thelp=\"print progress messages\")\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\",\n\t\thelp=\"print debug messages\")\n\tparser.add_argument(\"--source-dir\", \"-s\", metavar=\"DIR\", type=str, required=True,\n\t\thelp=\"path to directory containing source files\")\n\tparser.add_argument(\"--annotation-dir\", \"-a\", metavar=\"DIR\", type=str, required=True,\n\t\thelp=\"path to directory containing annotations\")\n\tparser.add_argument(\"--include\", \"-i\", metavar\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Manage and run SalsaTiger2 jobs\",\n\t\t\t\t\t\t\t\t\t formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=f\"%(prog)s {__version__}\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"count\", default=0,\n\t\t\t\t\t\thelp=\"Increase verbosity of output. Can be repeated to increase further.\")\n\tparser.add_argument(\"--quiet\", \"-q\", action=\"count\", default=0,\n\t\t\t\t\t\thelp=\"Decrease verbosity of output. Can be repeated to decrease further.\")\n\tparser.add_argument(\"--log-file\", \"-l\", type=argparse.FileType(\"a\"), default=sys.stderr,\n\t\t\t\t\t\thelp=\"File to write log messages to.\")\n\tparser.add_argument(\"--log-level\", default=\"INFO\",\n\t\t\t\t\t\tchoices=[\"DEBUG\", \"INFO\", \"WARNING\", \"\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\t\n\t# (Re)define the default version of the argument parser to include the common arguments.\n\tglobal default_config\n\tdefault_config = parser\n\tdefault_config.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n\tdefault_config.add_argument(\"-o\", \"--output-dir\", help=\"The directory to which output files will be written. If not given, output will be written to the current directory.\")\n\t\n\t# Define a subparser for the 'info' subcommand.\n\tinfo_config = subparsers.add_parser(\"info\", parents=[default_config])\n\tinfo_config.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\tinfo_config.add_argument(\"-s\", \"--sample-size\", type=int, help=\"The size of the sample to use to evaluate the results.\")\n\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t# Global arguments\n\tparser.add_argument('--root', type=str, required=True,\n\t\thelp='The path to the root of the project directory. This is the directory containing the \"data\" and \"logs\" directories, as well as the \"src\" directory. The path should be absolute.')\n\tparser.add_argument('--dataset', type=str, required=True,\n\t\thelp='The dataset name. This must be the name of a directory in the \"data\" directory, which contains the \"raw\" and \"processed\" subdirectories.')\n\tparser.add_argument('--logs', type=str, required=True,\n\t\thelp='The path to the logs directory. This is the directory that contains the \"global\" and \"dataset\" subdirectories. The path should be absolute.')\n\tparser.add_argument('--seed', type=int, required=True,\n\t\thelp='The random seed to use. This must be a\n", "def parse_arguments(*unparsed_arguments):\n\targ_parser = argparse.ArgumentParser(description=\"\"\"\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tprog=__file__,\n\t\tdescription=\"\"\"\n\t\t\tThis script is a tool for managing the configuration of the\n\t\t\tbasespace_to_irods_synchronizer script.\n\t\t\t\"\"\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter\n\t)\n\n\tparser.add_argument(\n\t\t\"-c\", \"--configuration-file\",\n\t\tmetavar=\"PATH\",\n\t\thelp=\"\"\"\n\t\t\tThe path to the configuration file to use.\n\t\t\t\"\"\",\n\t\tdefault=None\n\t)\n\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"\"\"\n\t\t\tThese are the subcommands available for the\n\t\t\tbasespace_to_irods_synchronizer configuration management tool.\n\t\t\t\"\"\",\n\t\thelp=\"\"\"\n\t\t\tUse this command to get help for each subcommand.\n\t\t\t\"\"\"\n\t)\n\n\t# The \"configure\" subcommand\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run the fenics-based version of the model from the paper 'A multiscale model of\"\n\t\t\t+ \" tumor-induced angiogenesis: The role of hypoxia, acidosis and metalloproteinases' by\"\n\t\t\t+ \" D.S. Chaves, C. Ribeiro & D.A. Knobel.\",\n\t\tepilog=\"If a subcommand is given, only the options relevant to that subcommand are parsed.\"\n\t\t\t+ \" If no subcommand is given, all options (i.e. all options from all subcommands) are\"\n\t\t\t+ \" parsed. For example, if you only want to run the model, you can use the command\"\n\t\t\t+ \" `python3 main.py run`, but if you want to extract data from a previously-run model and\"\n\t\t\t+ \" plot it, you can use the command `python3 main.py plot`. Finally, if you want to run\"\n\t\t\t+ \" the model and then plot the data it produces, you can use the command\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tdescription=\"\"\"\\\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(prog='samplx', description='Manage and run SAMPLing experiments.')\n\tsubparsers = parser.add_subparsers(dest='subparser_name', title='subcommands', metavar='subcommand')\n\n\t# Define a subparser to handle the \"samplx run\" command.\n\tparser_run = subparsers.add_parser(\n\t\t'run',\n\t\thelp='run a SAMPLing experiment',\n\t\tdescription='Run a SAMPLing experiment.',\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=\"\"\"\\\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = create_argparser()\n\targs = parser.parse_args(*unparsed_arguments)\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tglobal_parser = argparse.ArgumentParser(add_help=False)\n\tglobal_parser.add_argument(\"--output-dir\", \"-o\", metavar=\"DIR\", type=str, required=True,\n\t\thelp=\"path to output directory\")\n\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Create a LaTeX report from a given set of annotations.\",\n\t\tparents=[global_parser])\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"store_true\",\n\t\thelp=\"print progress messages\")\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\",\n\t\thelp=\"print debug messages\")\n\tparser.add_argument(\"--source-dir\", \"-s\", metavar=\"DIR\", type=str, required=True,\n\t\thelp=\"path to directory containing source files\")\n\tparser.add_argument(\"--annotation-dir\", \"-a\", metavar=\"DIR\", type=str, required=True,\n\t\thelp=\"path to directory containing annotations\")\n\tparser.add_argument(\"--include\", \"-i\", metavar\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Manage and run SalsaTiger2 jobs\",\n\t\t\t\t\t\t\t\t\t formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=f\"%(prog)s {__version__}\")\n\tparser.add_argument(\"--verbose\", \"-v\", action=\"count\", default=0,\n\t\t\t\t\t\thelp=\"Increase verbosity of output. Can be repeated to increase further.\")\n\tparser.add_argument(\"--quiet\", \"-q\", action=\"count\", default=0,\n\t\t\t\t\t\thelp=\"Decrease verbosity of output. Can be repeated to decrease further.\")\n\tparser.add_argument(\"--log-file\", \"-l\", type=argparse.FileType(\"a\"), default=sys.stderr,\n\t\t\t\t\t\thelp=\"File to write log messages to.\")\n\tparser.add_argument(\"--log-level\", default=\"INFO\",\n\t\t\t\t\t\tchoices=[\"DEBUG\", \"INFO\", \"WARNING\", \"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\t\n\t# (Re)define the default version of the argument parser to include the common arguments.\n\tglobal default_config\n\tdefault_config = parser\n\tdefault_config.add_argument(\"-v\", \"--verbose\", action=\"store_true\")\n\tdefault_config.add_argument(\"-o\", \"--output-dir\", help=\"The directory to which output files will be written. If not given, output will be written to the current directory.\")\n\t\n\t# Define a subparser for the 'info' subcommand.\n\tinfo_config = subparsers.add_parser(\"info\", parents=[default_config])\n\tinfo_config.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\tinfo_config.add_argument(\"-s\", \"--sample-size\", type=int, help=\"The size of the sample to use to evaluate the results.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t# Global arguments\n\tparser.add_argument('--root', type=str, required=True,\n\t\thelp='The path to the root of the project directory. This is the directory containing the \"data\" and \"logs\" directories, as well as the \"src\" directory. The path should be absolute.')\n\tparser.add_argument('--dataset', type=str, required=True,\n\t\thelp='The dataset name. This must be the name of a directory in the \"data\" directory, which contains the \"raw\" and \"processed\" subdirectories.')\n\tparser.add_argument('--logs', type=str, required=True,\n\t\thelp='The path to the logs directory. This is the directory that contains the \"global\" and \"dataset\" subdirectories. The path should be absolute.')\n\tparser.add_argument('--seed', type=int, required=True,\n\t\thelp='The random seed to use. This must be a\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\targ_parser = argparse.ArgumentParser(description=\"\"\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tresult = {}\n\tremaining_arguments = []\n\n\t# Build up a dictionary mapping from subparser name to a list of arguments to pass to it.\n\t#\n\t# We want to make sure that each subparser only gets its own arguments. For example, if a\n\t# user passes `--repository` and `--open`, we want the `open` subparser to get that, but\n\t# not the `add` subparser.\n\t#\n\t# We also want to make sure that each subparser gets all of its arguments, even if they\n\t# are interleaved with other arguments (for example, if a user passes `--open --repository\n\t# foo --add`, we want the `open` subparser to get `--repository foo` and the `add` subparser\n\t# to get `--add`).\n\t#\n\t# To accomplish that, we build up a dict mapping from subparser name to a list of arguments\n\t# to pass to that subparser.\n\t#\n\t# We do this by iterating through the arguments, keeping track of which subparser we're\n\t# passing arguments to\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparsers_for_parsing = {\n\t\tsubparser_name: subparser\n\t\tfor subparser_name, subparser in subparsers.items()\n\t\tif subparser_name in unparsed_arguments\n\t}\n\tif not subparsers_for_parsing:\n\t\traise argparse.ArgumentError(None, 'No action specified')\n\tif len(subparsers_for_parsing) > 1:\n\t\traise argparse.ArgumentError(None, 'More than one action specified')\n\tsubparser = subparsers_for_parsing.popitem()[1]\n\tparsed_arguments, unparsed_arguments = subparser.parse_known_args(\n\t\tunparsed_arguments,\n\t)\n\treturn {subparser.prog: parsed_arguments}, unparsed_arguments\n\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\targuments_by_subparser = {}\n\tremaining_arguments = []\n\tfor subparser_name, subparser in subparsers.items():\n\t\targuments_by_subparser[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tremaining_arguments = [\n\t\t\targument\n\t\t\tfor argument in remaining_arguments\n\t\t\tif argument not in arguments_by_subparser[subparser_name]\n\t\t]\n\treturn arguments_by_subparser, remaining_arguments\n\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparser_results = {}\n\tunclaimed_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tfor name, parser in subparsers.iteritems():\n\t\t\ttry:\n\t\t\t\tparser_results[name] = parser.parse_args(unparsed_arguments)\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\telse:\n\t\t\tunclaimed_arguments.append(argument)\n\treturn parser_results, unclaimed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Create a temporary argument parser that grabs all the subparser arguments.\n\ttemporary_parser = argparse.ArgumentParser(add_help=False)\n\tsubparser_argument = temporary_parser.add_argument(\"--subparser\")\n\t# Call parse_known_args() to get the requested subparser name.\n\t# We have to use parse_known_args() instead of parse_args() because we want to leave any\n\t# remaining unparsed arguments to the subparser.\n\targs, remaining_arguments = temporary_parser.parse_known_args(unparsed_arguments)\n\t# For each subparser, parse the remaining arguments.\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\t# If this subparser was requested, parse the remaining arguments with it.\n\t\tif subparser_name == args.subparser:\n\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(remaining_arguments)\n\t\t\tbreak\n\t# If no subparser was requested, return an empty\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# First, parse the arguments using all subparsers.\n\t# This will allow shared arguments like \"--repository\" to be available to all subparsers.\n\t# Let's pretend we're running a command like \"maint.py check --repository=foo\".\n\t# This will allow us to see \"--repository=foo\" in the result of, say, subparser1.parse_args(args).\n\t# We'll need to keep track of which subparser claimed which argument.\n\t# We'll do that by keeping track of the index of each argument in the original list.\n\t# We'll also need to keep track of which arguments were claimed by which subparser.\n\t# We'll do that by keeping track of the start and end index of each subparser's slice of the original list.\n\t# We'll also need to keep track of which subparsers were not used.\n\t# We'll do that by keeping track of which subparsers had their parse_args() method called.\n\t# We'll also need to keep track of which arguments were not claimed by any subparser.\n\t# We'll do that by keeping track of the\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\n\t# Get a list of all the subparsers that have been requested, and figure out which\n\t# arguments are for them.\n\tsubparser_names = set()\n\tsubparser_arguments = defaultdict(list)\n\tfor argument in unparsed_arguments:\n\t\tif argument in subparsers:\n\t\t\tsubparser_names.add(argument)\n\t\telse:\n\t\t\tsubparser_arguments[subparser_names.pop()].append(argument)\n\t\n\t# Parse all the arguments for each subparser, and collect the results.\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor subparser_name in subparser_names:\n\t\tparser = subparsers[subparser_name]\n\t\tparsed_arguments[subparser_name], remaining_arguments = parser.parse_known_args(subparser_arguments[subparser_name])\n\t\n\treturn parsed_arguments, remaining_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_subparsers = {}\n\tremaining_arguments = []\n\tif len(unparsed_arguments) == 0:\n\t\treturn parsed_subparsers, remaining_arguments\n\tif unparsed_arguments[0] in subparsers:\n\t\tparsed_subparsers[unparsed_arguments[0]] = subparsers[unparsed_arguments[0]].parse_args(unparsed_arguments[1:])\n\t\treturn parsed_subparsers, []\n\treturn parsed_subparsers, unparsed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Sequence of (subparser name, subparser argument parser, parsed arguments)\n\t# If a subparser's name doesn't appear in the command line arguments, its parsed arguments\n\t# will be None.\n\tsubparser_args = []\n\t# Sequence of arguments not claimed by any subparser\n\tremaining_arguments = []\n\n\t# Parse all arguments into a single namespace.\n\t# This is a bit of a hack to make it easier to collect the arguments for each subparser\n\t# after the fact.\n\tbase_namespace = argparse.Namespace()\n\tbase_parser = argparse.ArgumentParser()\n\tbase_parser.add_argument(\"--repository\", help=\"The URL of the repository to use.\")\n\tbase_parser.parse_args(unparsed_arguments, namespace=base_namespace)\n\n\t# Collect the arguments for each subparser\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser_args.append((subparser_name, subparser, None))\n\t\tsubparser_arguments = []\n\t\tfor arg in unparsed\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = list(subparsers.keys())\n\tsubparser_names.sort()\n\t\n\t# We'll use a dict to map from subparser name to the parsed arguments that subparser\n\t# consumed.\n\tparsed_arguments_by_subparser = {}\n\t\n\t# This is a list of the arguments not claimed by a subparser.\n\tunclaimed_arguments = []\n\t\n\t# We'll keep going until we've parsed all arguments.\n\twhile unparsed_arguments:\n\t\t# We'll take the first argument and try to find a subparser that can parse it.\n\t\targ = unparsed_arguments.pop(0)\n\t\t\n\t\t# If this argument is a subparser name, then we'll parse all the arguments that\n\t\t# follow using that subparser.\n\t\tif arg in subparser_names:\n\t\t\t# Make sure that we don't try to parse the arguments for this subparser\n\t\t\t# more than once.\n\t\t\tif arg in parsed_arguments_by_subparser:\n\t\t\t\traise Value\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tresult = {}\n\tremaining_arguments = []\n\n\t# Build up a dictionary mapping from subparser name to a list of arguments to pass to it.\n\t#\n\t# We want to make sure that each subparser only gets its own arguments. For example, if a\n\t# user passes `--repository` and `--open`, we want the `open` subparser to get that, but\n\t# not the `add` subparser.\n\t#\n\t# We also want to make sure that each subparser gets all of its arguments, even if they\n\t# are interleaved with other arguments (for example, if a user passes `--open --repository\n\t# foo --add`, we want the `open` subparser to get `--repository foo` and the `add` subparser\n\t# to get `--add`).\n\t#\n\t# To accomplish that, we build up a dict mapping from subparser name to a list of arguments\n\t# to pass to that subparser.\n\t#\n\t# We do this by iterating through the arguments, keeping track of which subparser we're\n\t# passing arguments to\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparsers_for_parsing = {\n\t\tsubparser_name: subparser\n\t\tfor subparser_name, subparser in subparsers.items()\n\t\tif subparser_name in unparsed_arguments\n\t}\n\tif not subparsers_for_parsing:\n\t\traise argparse.ArgumentError(None, 'No action specified')\n\tif len(subparsers_for_parsing) > 1:\n\t\traise argparse.ArgumentError(None, 'More than one action specified')\n\tsubparser = subparsers_for_parsing.popitem()[1]\n\tparsed_arguments, unparsed_arguments = subparser.parse_known_args(\n\t\tunparsed_arguments,\n\t)\n\treturn {subparser.prog: parsed_arguments}, unparsed_arguments\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\targuments_by_subparser = {}\n\tremaining_arguments = []\n\tfor subparser_name, subparser in subparsers.items():\n\t\targuments_by_subparser[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tremaining_arguments = [\n\t\t\targument\n\t\t\tfor argument in remaining_arguments\n\t\t\tif argument not in arguments_by_subparser[subparser_name]\n\t\t]\n\treturn arguments_by_subparser, remaining_arguments\n\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparser_results = {}\n\tunclaimed_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tfor name, parser in subparsers.iteritems():\n\t\t\ttry:\n\t\t\t\tparser_results[name] = parser.parse_args(unparsed_arguments)\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\telse:\n\t\t\tunclaimed_arguments.append(argument)\n\treturn parser_results, unclaimed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Create a temporary argument parser that grabs all the subparser arguments.\n\ttemporary_parser = argparse.ArgumentParser(add_help=False)\n\tsubparser_argument = temporary_parser.add_argument(\"--subparser\")\n\t# Call parse_known_args() to get the requested subparser name.\n\t# We have to use parse_known_args() instead of parse_args() because we want to leave any\n\t# remaining unparsed arguments to the subparser.\n\targs, remaining_arguments = temporary_parser.parse_known_args(unparsed_arguments)\n\t# For each subparser, parse the remaining arguments.\n\tparsed_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\t# If this subparser was requested, parse the remaining arguments with it.\n\t\tif subparser_name == args.subparser:\n\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(remaining_arguments)\n\t\t\tbreak\n\t# If no subparser was requested, return an empty\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# First, parse the arguments using all subparsers.\n\t# This will allow shared arguments like \"--repository\" to be available to all subparsers.\n\t# Let's pretend we're running a command like \"maint.py check --repository=foo\".\n\t# This will allow us to see \"--repository=foo\" in the result of, say, subparser1.parse_args(args).\n\t# We'll need to keep track of which subparser claimed which argument.\n\t# We'll do that by keeping track of the index of each argument in the original list.\n\t# We'll also need to keep track of which arguments were claimed by which subparser.\n\t# We'll do that by keeping track of the start and end index of each subparser's slice of the original list.\n\t# We'll also need to keep track of which subparsers were not used.\n\t# We'll do that by keeping track of which subparsers had their parse_args() method called.\n\t# We'll also need to keep track of which arguments were not claimed by any subparser.\n\t# We'll do that by keeping track of the\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t\n\t# Get a list of all the subparsers that have been requested, and figure out which\n\t# arguments are for them.\n\tsubparser_names = set()\n\tsubparser_arguments = defaultdict(list)\n\tfor argument in unparsed_arguments:\n\t\tif argument in subparsers:\n\t\t\tsubparser_names.add(argument)\n\t\telse:\n\t\t\tsubparser_arguments[subparser_names.pop()].append(argument)\n\t\n\t# Parse all the arguments for each subparser, and collect the results.\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor subparser_name in subparser_names:\n\t\tparser = subparsers[subparser_name]\n\t\tparsed_arguments[subparser_name], remaining_arguments = parser.parse_known_args(subparser_arguments[subparser_name])\n\t\n\treturn parsed_arguments, remaining_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_subparsers = {}\n\tremaining_arguments = []\n\tif len(unparsed_arguments) == 0:\n\t\treturn parsed_subparsers, remaining_arguments\n\tif unparsed_arguments[0] in subparsers:\n\t\tparsed_subparsers[unparsed_arguments[0]] = subparsers[unparsed_arguments[0]].parse_args(unparsed_arguments[1:])\n\t\treturn parsed_subparsers, []\n\treturn parsed_subparsers, unparsed_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Sequence of (subparser name, subparser argument parser, parsed arguments)\n\t# If a subparser's name doesn't appear in the command line arguments, its parsed arguments\n\t# will be None.\n\tsubparser_args = []\n\t# Sequence of arguments not claimed by any subparser\n\tremaining_arguments = []\n\n\t# Parse all arguments into a single namespace.\n\t# This is a bit of a hack to make it easier to collect the arguments for each subparser\n\t# after the fact.\n\tbase_namespace = argparse.Namespace()\n\tbase_parser = argparse.ArgumentParser()\n\tbase_parser.add_argument(\"--repository\", help=\"The URL of the repository to use.\")\n\tbase_parser.parse_args(unparsed_arguments, namespace=base_namespace)\n\n\t# Collect the arguments for each subparser\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser_args.append((subparser_name, subparser, None))\n\t\tsubparser_arguments = []\n\t\tfor arg in unparsed\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = list(subparsers.keys())\n\tsubparser_names.sort()\n\t\n\t# We'll use a dict to map from subparser name to the parsed arguments that subparser\n\t# consumed.\n\tparsed_arguments_by_subparser = {}\n\t\n\t# This is a list of the arguments not claimed by a subparser.\n\tunclaimed_arguments = []\n\t\n\t# We'll keep going until we've parsed all arguments.\n\twhile unparsed_arguments:\n\t\t# We'll take the first argument and try to find a subparser that can parse it.\n\t\targ = unparsed_arguments.pop(0)\n\t\t\n\t\t# If this argument is a subparser name, then we'll parse all the arguments that\n\t\t# follow using that subparser.\n\t\tif arg in subparser_names:\n\t\t\t# Make sure that we don't try to parse the arguments for this subparser\n\t\t\t# more than once.\n\t\t\tif arg in parsed_arguments_by_subparser:\n\t\t\t\traise Value\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description=\"A tool to create, modify, and delete Fermi LAT data products.\")\n\tsubparsers = top_parser.add_subparsers(title=\"subcommands\")\n\tsubparsers.required = True\n\n\t# Make the parser for the \"create\" subcommand.\n\tcreate_parser = subparsers.add_parser(\"create\", help=\"Create a new data product.\")\n\tcreate_parser.add_argument(\"type\", choices=[\"diffuse\", \"source\"], help=\"The type of the data product.\")\n\tcreate_parser.add_argument(\"name\", help=\"The name of the data product.\")\n\tcreate_parser.add_argument(\"--source-name\", help=\"The name of the point source for which to create the data product.\")\n\tcreate_parser.add_argument(\"--source-model\", help=\"The model of the point source for which to create the data product.\")\n\tcreate_parser.add_argument(\"--source-spectrum\", help=\"The spectral model of the point source for which to create the data product.\")\n\tcreate_parser.add_argument\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tprog='toss',\n\t\tdescription=\"\"\"\n\t\t\tToss is a command-line tool for managing a collection of\n\t\t\tinterconnected scripts, each of which does some small task. It\n\t\t\tprovides a common interface for interacting with all of these\n\t\t\tscripts.\n\t\t\"\"\")\n\ttop_parser.add_argument(\n\t\t'--verbose',\n\t\taction='count',\n\t\thelp='be verbose. Repeat to increase verbosity')\n\ttop_parser.add_argument(\n\t\t'--quiet',\n\t\taction='count',\n\t\thelp='be quiet. Repeat to decrease verbosity')\n\ttop_parser.add_argument(\n\t\t'--version',\n\t\taction='version',\n\t\tversion=__version__,\n\t\thelp='show the version number and exit')\n\n\tsubparsers = top_parser.add_subparsers(\n\t\thelp='sub-command help',\n\t\tdest='subparser_name')\n\tsubparsers.required =\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description='A script to obtain information about a Github repository.')\n\tsubparsers = top_parser.add_subparsers(title='commands', dest='command')\n\n\tparser_info = subparsers.add_parser('info', help='get repository information')\n\tparser_info.add_argument('repository', help='the repository to obtain information about', metavar='repository')\n\n\tparser_issues = subparsers.add_parser('issues', help='get repository issue information')\n\tparser_issues.add_argument('repository', help='the repository to obtain information about', metavar='repository')\n\n\tparser_contributors = subparsers.add_parser('contributors', help='get repository contributor information')\n\tparser_contributors.add_argument('repository', help='the repository to obtain information about', metavar='repository')\n\n\treturn top_parser, subparsers\n\n", "def make_parsers():\n\t# Top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tprog=PROGRAM_NAME,\n\t\tdescription=PROGRAM_DESCRIPTION,\n\t\tepilog=PROGRAM_EPILOG,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter\n\t)\n\tparser.add_argument(\n\t\t'-v', '--version',\n\t\taction='version',\n\t\tversion=PROGRAM_VERSION\n\t)\n\tparser.add_argument(\n\t\t'--debug',\n\t\taction='store_true',\n\t\thelp='Enable debug output'\n\t)\n\tparser.add_argument(\n\t\t'--log',\n\t\taction='store',\n\t\tmetavar='FILENAME',\n\t\thelp='Log output to the specified file'\n\t)\n\tparser.add_argument(\n\t\t'-c', '--config',\n\t\taction='store',\n\t\tmetavar='FILENAME',\n\t\thelp='Use the specified configuration file'\n\t)\n\tparser.add_argument(\n\t\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Process a list of URLs and test them for links to \\\n\t\t\t\t\t\t\t\t the specified target.\"\n\t)\n\tparser.add_argument(\n\t\t\"target\",\n\t\thelp=\"The target URL to search for. Ex: 'https://www.example.com/'\"\n\t)\n\tparser.add_argument(\n\t\t\"input\",\n\t\thelp=\"The location of the input file. Ex: 'input.txt'\"\n\t)\n\tparser.add_argument(\n\t\t\"output\",\n\t\thelp=\"The location of the output file. Ex: 'output.txt'\"\n\t)\n\tparser.add_argument(\n\t\t\"--threads\",\n\t\thelp=\"The number of threads to use for processing input. Default: 1\",\n\t\tdefault=1,\n\t\ttype=int\n\t)\n\tparser.add_argument(\n\t\t\"--timeout\",\n\t\thelp=\"The number of seconds to wait for a response before declaring \\\n\t\t\t\t\tURL unreachable. Default: 10\",\n\t\tdefault\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description=__doc__.strip())\n\tsubparsers = top_parser.add_subparsers(dest='subparser')\n\n\tparser_create = subparsers.add_parser('create', help='create a new project')\n\tparser_create.add_argument('project')\n\n\tparser_build = subparsers.add_parser('build', help='build a project')\n\tparser_build.add_argument('project')\n\n\tparser_run = subparsers.add_parser('run', help='run a project')\n\tparser_run.add_argument('project')\n\n\tparser_list = subparsers.add_parser('list', help='list projects')\n\n\tparser_clean = subparsers.add_parser('clean', help='clean a project')\n\tparser_clean.add_argument('project')\n\n\treturn top_parser, subparsers\n\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"refresh.py\",\n\t\tdescription=\"Refresh the current working directory from the remote.\")\n\tsubparsers = parser.add_subparsers()\n\n\tparser_refresh = subparsers.add_parser(\n\t\t\"refresh\",\n\t\thelp=\"Refresh the current working directory from the remote.\")\n\tparser_refresh.add_argument(\n\t\t\"-f\", \"--force\",\n\t\taction=\"store_true\",\n\t\thelp=\"Discard local modifications before refreshing.\")\n\tparser_refresh.add_argument(\n\t\t\"-t\", \"--test\",\n\t\taction=\"store_true\",\n\t\thelp=\"Test the current working directory, but don't refresh.\")\n\tparser_refresh.set_defaults(func=execute_refresh)\n\n\tparser_backup = subparsers.add_parser(\n\t\t\"backup\",\n\t\thelp=\"Backup the current working directory to the remote.\")\n\tparser_backup.add_argument(\n\t\t\"-f\", \"--force\",\n\t\t\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tdescription=\"Run the web server for the web application.\")\n\tsubparsers = top_parser.add_subparsers(help=\"sub-command help\")\n\n\t# A --debug flag on the top-level parser.\n\ttop_parser.add_argument(\"--debug\", action=\"store_true\",\n\t\t\t\t\t\t\thelp=\"Run the server in debug mode\")\n\n\t# A \"run\" subcommand with a --host and a --port flag.\n\trun_parser = subparsers.add_parser(\"run\", help=\"Run the web server\")\n\trun_parser.add_argument(\"--host\", default=\"localhost\",\n\t\t\t\t\t\t\thelp=\"The hostname to listen on\")\n\trun_parser.add_argument(\"--port\", type=int, default=5000,\n\t\t\t\t\t\t\thelp=\"The port of the webserver\")\n\trun_parser.set_defaults(func=run_command)\n\n\treturn top_parser, subparsers\n\n\n", "def make_parsers():\n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Script for generating and uploading a package to the '\n\t\t\t\t\t'Arch User Repository.')\n\tparser.add_argument('-c', '--config', metavar='FILE',\n\t\t\t\t\t\thelp='use FILE as the configuration file. '\n\t\t\t\t\t\t\t 'Default is ~/.config/aur-upload/aur-upload.conf.')\n\tparser.add_argument('-v', '--verbose', action='store_true',\n\t\t\t\t\t\thelp='enable verbose output.')\n\n\t# Create the subparsers\n\tsubparsers = parser.add_subparsers(dest='subcommand',\n\t\t\t\t\t\t\t\t\t   title='subcommands',\n\t\t\t\t\t\t\t\t\t   description='valid subcommands',\n\t\t\t\t\t\t\t\t\t   help='subcommand help')\n\n\t# Create the parser for the \"build\" subcommand\n\tparser_build = subparsers.add_parser('build',\n", "def make_parsers():\n\t# Setup the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"papip\",\n\t\tdescription=\"A tool to manage your PAPI account.\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=\"\"\"\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description=\"A tool to create, modify, and delete Fermi LAT data products.\")\n\tsubparsers = top_parser.add_subparsers(title=\"subcommands\")\n\tsubparsers.required = True\n\n\t# Make the parser for the \"create\" subcommand.\n\tcreate_parser = subparsers.add_parser(\"create\", help=\"Create a new data product.\")\n\tcreate_parser.add_argument(\"type\", choices=[\"diffuse\", \"source\"], help=\"The type of the data product.\")\n\tcreate_parser.add_argument(\"name\", help=\"The name of the data product.\")\n\tcreate_parser.add_argument(\"--source-name\", help=\"The name of the point source for which to create the data product.\")\n\tcreate_parser.add_argument(\"--source-model\", help=\"The model of the point source for which to create the data product.\")\n\tcreate_parser.add_argument(\"--source-spectrum\", help=\"The spectral model of the point source for which to create the data product.\")\n\tcreate_parser.add_argument\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tprog='toss',\n\t\tdescription=\"\"\"\n\t\t\tToss is a command-line tool for managing a collection of\n\t\t\tinterconnected scripts, each of which does some small task. It\n\t\t\tprovides a common interface for interacting with all of these\n\t\t\tscripts.\n\t\t\"\"\")\n\ttop_parser.add_argument(\n\t\t'--verbose',\n\t\taction='count',\n\t\thelp='be verbose. Repeat to increase verbosity')\n\ttop_parser.add_argument(\n\t\t'--quiet',\n\t\taction='count',\n\t\thelp='be quiet. Repeat to decrease verbosity')\n\ttop_parser.add_argument(\n\t\t'--version',\n\t\taction='version',\n\t\tversion=__version__,\n\t\thelp='show the version number and exit')\n\n\tsubparsers = top_parser.add_subparsers(\n\t\thelp='sub-command help',\n\t\tdest='subparser_name')\n\tsubparsers.required =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description='A script to obtain information about a Github repository.')\n\tsubparsers = top_parser.add_subparsers(title='commands', dest='command')\n\n\tparser_info = subparsers.add_parser('info', help='get repository information')\n\tparser_info.add_argument('repository', help='the repository to obtain information about', metavar='repository')\n\n\tparser_issues = subparsers.add_parser('issues', help='get repository issue information')\n\tparser_issues.add_argument('repository', help='the repository to obtain information about', metavar='repository')\n\n\tparser_contributors = subparsers.add_parser('contributors', help='get repository contributor information')\n\tparser_contributors.add_argument('repository', help='the repository to obtain information about', metavar='repository')\n\n\treturn top_parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t# Top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tprog=PROGRAM_NAME,\n\t\tdescription=PROGRAM_DESCRIPTION,\n\t\tepilog=PROGRAM_EPILOG,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter\n\t)\n\tparser.add_argument(\n\t\t'-v', '--version',\n\t\taction='version',\n\t\tversion=PROGRAM_VERSION\n\t)\n\tparser.add_argument(\n\t\t'--debug',\n\t\taction='store_true',\n\t\thelp='Enable debug output'\n\t)\n\tparser.add_argument(\n\t\t'--log',\n\t\taction='store',\n\t\tmetavar='FILENAME',\n\t\thelp='Log output to the specified file'\n\t)\n\tparser.add_argument(\n\t\t'-c', '--config',\n\t\taction='store',\n\t\tmetavar='FILENAME',\n\t\thelp='Use the specified configuration file'\n\t)\n\tparser.add_argument(\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Process a list of URLs and test them for links to \\\n\t\t\t\t\t\t\t\t the specified target.\"\n\t)\n\tparser.add_argument(\n\t\t\"target\",\n\t\thelp=\"The target URL to search for. Ex: 'https://www.example.com/'\"\n\t)\n\tparser.add_argument(\n\t\t\"input\",\n\t\thelp=\"The location of the input file. Ex: 'input.txt'\"\n\t)\n\tparser.add_argument(\n\t\t\"output\",\n\t\thelp=\"The location of the output file. Ex: 'output.txt'\"\n\t)\n\tparser.add_argument(\n\t\t\"--threads\",\n\t\thelp=\"The number of threads to use for processing input. Default: 1\",\n\t\tdefault=1,\n\t\ttype=int\n\t)\n\tparser.add_argument(\n\t\t\"--timeout\",\n\t\thelp=\"The number of seconds to wait for a response before declaring \\\n\t\t\t\t\tURL unreachable. Default: 10\",\n\t\tdefault\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description=__doc__.strip())\n\tsubparsers = top_parser.add_subparsers(dest='subparser')\n\n\tparser_create = subparsers.add_parser('create', help='create a new project')\n\tparser_create.add_argument('project')\n\n\tparser_build = subparsers.add_parser('build', help='build a project')\n\tparser_build.add_argument('project')\n\n\tparser_run = subparsers.add_parser('run', help='run a project')\n\tparser_run.add_argument('project')\n\n\tparser_list = subparsers.add_parser('list', help='list projects')\n\n\tparser_clean = subparsers.add_parser('clean', help='clean a project')\n\tparser_clean.add_argument('project')\n\n\treturn top_parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"refresh.py\",\n\t\tdescription=\"Refresh the current working directory from the remote.\")\n\tsubparsers = parser.add_subparsers()\n\n\tparser_refresh = subparsers.add_parser(\n\t\t\"refresh\",\n\t\thelp=\"Refresh the current working directory from the remote.\")\n\tparser_refresh.add_argument(\n\t\t\"-f\", \"--force\",\n\t\taction=\"store_true\",\n\t\thelp=\"Discard local modifications before refreshing.\")\n\tparser_refresh.add_argument(\n\t\t\"-t\", \"--test\",\n\t\taction=\"store_true\",\n\t\thelp=\"Test the current working directory, but don't refresh.\")\n\tparser_refresh.set_defaults(func=execute_refresh)\n\n\tparser_backup = subparsers.add_parser(\n\t\t\"backup\",\n\t\thelp=\"Backup the current working directory to the remote.\")\n\tparser_backup.add_argument(\n\t\t\"-f\", \"--force\",\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tdescription=\"Run the web server for the web application.\")\n\tsubparsers = top_parser.add_subparsers(help=\"sub-command help\")\n\n\t# A --debug flag on the top-level parser.\n\ttop_parser.add_argument(\"--debug\", action=\"store_true\",\n\t\t\t\t\t\t\thelp=\"Run the server in debug mode\")\n\n\t# A \"run\" subcommand with a --host and a --port flag.\n\trun_parser = subparsers.add_parser(\"run\", help=\"Run the web server\")\n\trun_parser.add_argument(\"--host\", default=\"localhost\",\n\t\t\t\t\t\t\thelp=\"The hostname to listen on\")\n\trun_parser.add_argument(\"--port\", type=int, default=5000,\n\t\t\t\t\t\t\thelp=\"The port of the webserver\")\n\trun_parser.set_defaults(func=run_command)\n\n\treturn top_parser, subparsers\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t# Create the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Script for generating and uploading a package to the '\n\t\t\t\t\t'Arch User Repository.')\n\tparser.add_argument('-c', '--config', metavar='FILE',\n\t\t\t\t\t\thelp='use FILE as the configuration file. '\n\t\t\t\t\t\t\t 'Default is ~/.config/aur-upload/aur-upload.conf.')\n\tparser.add_argument('-v', '--verbose', action='store_true',\n\t\t\t\t\t\thelp='enable verbose output.')\n\n\t# Create the subparsers\n\tsubparsers = parser.add_subparsers(dest='subcommand',\n\t\t\t\t\t\t\t\t\t   title='subcommands',\n\t\t\t\t\t\t\t\t\t   description='valid subcommands',\n\t\t\t\t\t\t\t\t\t   help='subcommand help')\n\n\t# Create the parser for the \"build\" subcommand\n\tparser_build = subparsers.add_parser('build',\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t# Setup the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"papip\",\n\t\tdescription=\"A tool to manage your PAPI account.\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tepilog=\"\"\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n\tresult = []\n\tfor key, value in nodes:\n\t\tfor index, existing_key, existing_value in enumerate(result):\n\t\t\tif key.value == existing_key.value:\n\t\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\tif isinstance(existing_value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\t\tresult[index][2].value += value.value\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ruamel.yaml.composer.ComposerError(\n\t\t\t\t\t\t\t\"cannot merge {!r} (duplicate)\".format(key.value)\n\t\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\t# Replace existing value if not a MappingNode.\n\t\t\t\t\tresult[index][2] = value\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tresult.append((key, value))\n\treturn result\n", "def deep_merge_nodes(nodes):\n\treturn_value = []\n\tfor key, value in nodes:\n\t\tif key in return_value:\n\t\t\treturn_value.remove(key)\n\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\tdeep_merge_nodes(value.value)\n\t\treturn_value.append((key, value))\n\treturn return_value\n\n\n", "def deep_merge_nodes(nodes):\n\t# Create an empty dictionary for storing deep-merged nodes.\n\tdeep_merged = {}\n\n\t# Iterate through the nodes, deep merging as necessary.\n\tfor node in nodes:\n\t\t# Extract the node key and value.\n\t\tkey, value = node\n\n\t\t# If the node key already exists in the dictionary, deep merge the node value with the\n\t\t# existing node value.\n\t\tif key.value in deep_merged:\n\t\t\tdeep_merged[key.value] = deep_merge(deep_merged[key.value], value)\n\t\telse:\n\t\t\tdeep_merged[key.value] = value\n\n\t# Return the deep-merged nodes as a list of tuples.\n\treturn list(deep_merged.items())\n\n\n", "def deep_merge_nodes(nodes):\n\tif not nodes:\n\t\treturn []\n\n\tresult = []\n\tkeys = set()\n\tfor key, value in nodes:\n\t\tif key.value in keys:\n\t\t\t# Update the last value we saw for this key, since it's already present in the result.\n\t\t\tresult[-1] = (key, _deep_merge_nodes(result[-1][1], value))\n\t\telse:\n\t\t\tkeys.add(key.value)\n\t\t\tresult.append((key, value))\n\n\treturn result\n\n\n", "def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\tfor node_key, node_value in nodes:\n\t\tif node_key in merged_nodes:\n\t\t\tmerged_nodes[node_key] = deep_merge_nodes(\n\t\t\t\tmerged_nodes[node_key], node_value)\n\t\telse:\n\t\t\tmerged_nodes[node_key] = node_value\n\n\treturn merged_nodes\n\n\n", "def deep_merge_nodes(nodes):\n\tnodes_by_key = {}\n\tfor key, value in nodes:\n\t\tif key.value in nodes_by_key:\n\t\t\tnodes_by_key[key.value] = _deep_merge_nodes(nodes_by_key[key.value], value)\n\t\telse:\n\t\t\tnodes_by_key[key.value] = value\n\treturn list(nodes_by_key.items())\n\n\n", "def deep_merge_nodes(nodes):\n\tnodes_by_key = {}\n\tfor key, node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\tnodes_by_key[key] = node\n\t\t\tcontinue\n\n\t\tif key not in nodes_by_key:\n\t\t\tnodes_by_key[key] = node\n\t\t\tcontinue\n\n\t\tassert isinstance(nodes_by_key[key], MappingNode)\n\t\tassert isinstance(node, MappingNode)\n\n\t\t# Recursively merge the nodes with this key.\n\t\tnodes_by_key[key].value = deep_merge_nodes(\n\t\t\tnodes_by_key[key].value + node.value\n\t\t)\n\n\treturn list(nodes_by_key.items())\n\n\n", "def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\t# First pass: Iterate through all the nodes and build an index of nodes by key.\n\tfor node in nodes:\n\t\tkey = node[0]\n\t\tvalue = node[1]\n\n\t\tif key not in merged_nodes:\n\t\t\tmerged_nodes.append(node)\n\t\telse:\n\t\t\tfor i, merged_node in enumerate(merged_nodes):\n\t\t\t\tif merged_node[0] == key:\n\t\t\t\t\tmerged_nodes[i] = (key, deep_merge_mapping_nodes(merged_node[1], value))\n\n\treturn merged_nodes\n\n\n", "def deep_merge_nodes(nodes):\n\tresult = []\n\tfor node in nodes:\n\t\tkey, value = node\n\t\t# Find the corresponding node in the result for this key, if any.\n\t\tcorresponding_node = None\n\t\tfor node in result:\n\t\t\tif key.value == node[0].value:\n\t\t\t\tcorresponding_node = node\n\t\t\t\tbreak\n\t\t# If there is no corresponding node, add this node to the result.\n\t\tif corresponding_node is None:\n\t\t\tresult.append(node)\n\t\t# If there is a corresponding node and its value is a mapping, merge the two mappings.\n\t\telif isinstance(corresponding_node[1], ruamel.yaml.nodes.MappingNode):\n\t\t\tif not isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\traise ValueError(\n\t\t\t\t\t'Expected mapping node for key \"{}\", but got {}'.format(key.value, value)\n\t\t\t\t)\n\t\t\tcorresponding_node[1].\n", "def deep_merge_nodes(nodes):\n\tresult = []\n\n\tfor key, value in nodes:\n\t\t# If this key is a duplicate...\n\t\tif any(key.value == result_key.value for result_key, _ in result):\n\t\t\t# Find the corresponding value in the result.\n\t\t\tfor index, (result_key, result_value) in enumerate(result):\n\t\t\t\tif key.value == result_key.value:\n\t\t\t\t\t# if this is a mapping, merge the values and replace the corresponding\n\t\t\t\t\t# result value.\n\t\t\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\t\tresult_value.value = deep_merge_nodes(\n\t\t\t\t\t\t\tvalue.value + result_value.value\n\t\t\t\t\t\t)\n\t\t\t\t\t# Otherwise, just replace the result value with the new value.\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[index] = (key, value)\n\t\t\t\t\tbreak\n\t\t# Otherwise,\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\tresult = []\n\tfor key, value in nodes:\n\t\tfor index, existing_key, existing_value in enumerate(result):\n\t\t\tif key.value == existing_key.value:\n\t\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\tif isinstance(existing_value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\t\tresult[index][2].value += value.value\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\traise ruamel.yaml.composer.ComposerError(\n\t\t\t\t\t\t\t\"cannot merge {!r} (duplicate)\".format(key.value)\n\t\t\t\t\t\t)\n\t\t\t\telse:\n\t\t\t\t\t# Replace existing value if not a MappingNode.\n\t\t\t\t\tresult[index][2] = value\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tresult.append((key, value))\n\treturn result\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\treturn_value = []\n\tfor key, value in nodes:\n\t\tif key in return_value:\n\t\t\treturn_value.remove(key)\n\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\tdeep_merge_nodes(value.value)\n\t\treturn_value.append((key, value))\n\treturn return_value\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\t# Create an empty dictionary for storing deep-merged nodes.\n\tdeep_merged = {}\n\n\t# Iterate through the nodes, deep merging as necessary.\n\tfor node in nodes:\n\t\t# Extract the node key and value.\n\t\tkey, value = node\n\n\t\t# If the node key already exists in the dictionary, deep merge the node value with the\n\t\t# existing node value.\n\t\tif key.value in deep_merged:\n\t\t\tdeep_merged[key.value] = deep_merge(deep_merged[key.value], value)\n\t\telse:\n\t\t\tdeep_merged[key.value] = value\n\n\t# Return the deep-merged nodes as a list of tuples.\n\treturn list(deep_merged.items())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tif not nodes:\n\t\treturn []\n\n\tresult = []\n\tkeys = set()\n\tfor key, value in nodes:\n\t\tif key.value in keys:\n\t\t\t# Update the last value we saw for this key, since it's already present in the result.\n\t\t\tresult[-1] = (key, _deep_merge_nodes(result[-1][1], value))\n\t\telse:\n\t\t\tkeys.add(key.value)\n\t\t\tresult.append((key, value))\n\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\tfor node_key, node_value in nodes:\n\t\tif node_key in merged_nodes:\n\t\t\tmerged_nodes[node_key] = deep_merge_nodes(\n\t\t\t\tmerged_nodes[node_key], node_value)\n\t\telse:\n\t\t\tmerged_nodes[node_key] = node_value\n\n\treturn merged_nodes\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tnodes_by_key = {}\n\tfor key, value in nodes:\n\t\tif key.value in nodes_by_key:\n\t\t\tnodes_by_key[key.value] = _deep_merge_nodes(nodes_by_key[key.value], value)\n\t\telse:\n\t\t\tnodes_by_key[key.value] = value\n\treturn list(nodes_by_key.items())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tnodes_by_key = {}\n\tfor key, node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\tnodes_by_key[key] = node\n\t\t\tcontinue\n\n\t\tif key not in nodes_by_key:\n\t\t\tnodes_by_key[key] = node\n\t\t\tcontinue\n\n\t\tassert isinstance(nodes_by_key[key], MappingNode)\n\t\tassert isinstance(node, MappingNode)\n\n\t\t# Recursively merge the nodes with this key.\n\t\tnodes_by_key[key].value = deep_merge_nodes(\n\t\t\tnodes_by_key[key].value + node.value\n\t\t)\n\n\treturn list(nodes_by_key.items())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tmerged_nodes = []\n\n\t# First pass: Iterate through all the nodes and build an index of nodes by key.\n\tfor node in nodes:\n\t\tkey = node[0]\n\t\tvalue = node[1]\n\n\t\tif key not in merged_nodes:\n\t\t\tmerged_nodes.append(node)\n\t\telse:\n\t\t\tfor i, merged_node in enumerate(merged_nodes):\n\t\t\t\tif merged_node[0] == key:\n\t\t\t\t\tmerged_nodes[i] = (key, deep_merge_mapping_nodes(merged_node[1], value))\n\n\treturn merged_nodes\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = []\n\tfor node in nodes:\n\t\tkey, value = node\n\t\t# Find the corresponding node in the result for this key, if any.\n\t\tcorresponding_node = None\n\t\tfor node in result:\n\t\t\tif key.value == node[0].value:\n\t\t\t\tcorresponding_node = node\n\t\t\t\tbreak\n\t\t# If there is no corresponding node, add this node to the result.\n\t\tif corresponding_node is None:\n\t\t\tresult.append(node)\n\t\t# If there is a corresponding node and its value is a mapping, merge the two mappings.\n\t\telif isinstance(corresponding_node[1], ruamel.yaml.nodes.MappingNode):\n\t\t\tif not isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\traise ValueError(\n\t\t\t\t\t'Expected mapping node for key \"{}\", but got {}'.format(key.value, value)\n\t\t\t\t)\n\t\t\tcorresponding_node[1].\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = []\n\n\tfor key, value in nodes:\n\t\t# If this key is a duplicate...\n\t\tif any(key.value == result_key.value for result_key, _ in result):\n\t\t\t# Find the corresponding value in the result.\n\t\t\tfor index, (result_key, result_value) in enumerate(result):\n\t\t\t\tif key.value == result_key.value:\n\t\t\t\t\t# if this is a mapping, merge the values and replace the corresponding\n\t\t\t\t\t# result value.\n\t\t\t\t\tif isinstance(value, ruamel.yaml.nodes.MappingNode):\n\t\t\t\t\t\tresult_value.value = deep_merge_nodes(\n\t\t\t\t\t\t\tvalue.value + result_value.value\n\t\t\t\t\t\t)\n\t\t\t\t\t# Otherwise, just replace the result value with the new value.\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[index] = (key, value)\n\t\t\t\t\tbreak\n\t\t# Otherwise,\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n\tdef _parse_args(args):\n\t\treturn parser.parse_args(args)\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument('-l', '--log-level',\n\t\t\t\t\t\thelp='set the logging level',\n\t\t\t\t\t\tchoices=['debug', 'info', 'warning', 'error', 'critical'],\n\t\t\t\t\t\tdefault='info')\n\tparser.add_argument('-d', '--source-dir',\n\t\t\t\t\t\thelp='path to source directory',\n\t\t\t\t\t\tdefault=os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))\n\tparser.add_argument('-p', '--properties',\n\t\t\t\t\t\thelp='path to properties file')\n\tparser.add_argument('-n', '--no-clean',\n\t\t\t\t\t\thelp='do not clean source directory',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\tdefault=False\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\n\t\t\"-m\", \"--method\",\n\t\tdest=\"method\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"the method of analysis to use\",\n\t\tchoices=method_choices\n\t)\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\",\n\t\tdest=\"verbose\",\n\t\tdefault=False,\n\t\taction=\"store_true\",\n\t\thelp=\"print more information while running\"\n\t)\n\tparser.add_argument(\n\t\t\"--output\",\n\t\tdest=\"output\",\n\t\ttype=str,\n\t\tdefault=\"\",\n\t\thelp=\"the directory in which to save the results\"\n\t)\n\tparser.add_argument(\n\t\t\"--reference\",\n\t\tdest=\"reference\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"the name of the reference set to use\"\n\t)\n\tparser.add_argument(\n\t\t\"--signific\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tThis script generates a simple CSV file that contains the same data as the original CSV file\n\tbut where the values of the columns specified by the user are replaced with values obtained from\n\tthe corresponding column in the original file.\n\t\t\"\"\", add_help=False)\n\tparser.add_argument('-i', '--input', help=\"the path to the input CSV file\")\n\tparser.add_argument('-o', '--output', help=\"the path to the output CSV file\")\n\tparser.add_argument('-c', '--column', help=\"\"\"\n\t\tthe name of a column in the input CSV file whose values should be replaced with values from\n\tthe corresponding column in the original file. This argument can be specified multiple times.\n\t\t\"\"\", action='append')\n\tparser.add_argument('-d', '--delimiter', help=\"\"\"\n\t\tthe character used to delimit values in the input CSV file. Defaults to the comma character.\n\t\t\"\"\", default=',')\n\tparser.add_argument('-e', '--encoding',\n", "def parse_arguments(*arguments):\n\tparser = ArgumentParser(\n\t\t\tdescription=\"\"\"\n\t\t\t\tThis script is designed to facilitate creating a Docker image that contains a\n\t\t\t\tKubernetes cluster.\n\n\t\t\t\tThe script will create a new Docker image that, when run, will start up a\n\t\t\t\tKubernetes cluster. The cluster will be made up of the master node, which\n\t\t\t\tcontains the Kubernetes API server, and one or more worker nodes.\n\n\t\t\t\tThe script will create a Kubernetes configuration file, which will be placed in\n\t\t\t\tthe `/root/.kube/config` directory in the Docker image, and will be used by the\n\t\t\t\tKubernetes cluster to configure itself. The configuration file will be encrypted\n\t\t\t\tusing a user-specified password (or a randomly-generated one, if no password is\n\t\t\t\tspecified), and the password will be written to the `/root/.kube/password` file\n\t\t\t\tin the Docker image.\n\n\t\t\t\tThe script will also create a Kubernetes manifests file, which will be\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tAutomate the process of downloading and installing the latest version of the Chrome\n\t\tbrowser, and then run the browser in headless mode.\n\t\t\"\"\")\n\n\t# add arguments for which we want to capture the value\n\tparser.add_argument('-d', '--download-dir', help='Specify the directory into which to download the Chrome installer. By default the current working directory is used.')\n\tparser.add_argument('-p', '--preferences-file', help='Specify a preferences file to use with the browser.')\n\tparser.add_argument('-s', '--start-url', help='Specify the URL to use to start the browser.')\n\n\t# add arguments for which we don't want to capture the value\n\tparser.add_argument('--overwrite-preferences-file', action='store_true', help='Overwrite the preferences file with default values, if it exists.')\n\tparser.add_argument('--overwrite-start-url', action='store_true', help='Overwrite the start URL with the default, if\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Reads a list of files and prints the first 10 lines of each one.\")\n\tparser.add_argument(\"files\", help=\"files to read\", nargs=\"+\")\n\n\treturn parser.parse_args(*arguments)\n\n", "def parse_arguments(*arguments):\n\targuments_parser = ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tLoad a CSV file of data and output an HTML table with the data in it.\n\t\t\t\"\"\"\n\t\t)\n\targuments_parser.add_argument(\n\t\tdest=\"input_filename\",\n\t\taction=\"store\",\n\t\ttype=str,\n\t\thelp=\"\"\"The path of a CSV file to read in.\"\"\"\n\t)\n\targuments_parser.add_argument(\n\t\tdest=\"output_filename\",\n\t\taction=\"store\",\n\t\ttype=str,\n\t\thelp=\"\"\"The path of an HTML file to write out.\"\"\"\n\t)\n\t#arguments_parser.add_argument(\n\t#\tdest=\"font\",\n\t#\taction=\"store\",\n\t#\ttype=str,\n\t#\thelp=\"\"\"The path of a font to use in the PDF.\"\"\"\n\t#)\n\treturn arguments_parser.parse_args(*arguments)\n\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\"-i\", \"--input\", dest=\"input\", default=None, type=str, help=\"path to the input file\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", default=None, type=str, help=\"path to the output file\")\n\tparser.add_argument(\"-a\", \"--all-output\", dest=\"all_output\", default=None, type=str, help=\"path to the output file\")\n\tparser.add_argument(\"-f\", \"--format\", dest=\"format\", default=\"xml\", type=str, help=\"output format: xml, tsv\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=version)\n\treturn parser.parse_args(args=arguments)\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\", action = \"store_true\",\n\t\thelp = \"Show verbose output.\"\n\t)\n\tparser.add_argument(\n\t\t\"--no-color\", action = \"store_true\",\n\t\thelp = \"Don't show colorized output.\"\n\t)\n\tparser.add_argument(\n\t\t\"--pull\", action = \"store_true\",\n\t\thelp = \"Pull from the remote repository before running the test suite.\"\n\t)\n\tparser.add_argument(\n\t\t\"--push\", action = \"store_true\",\n\t\thelp = \"Push to the remote repository after running the test suite.\"\n\t)\n\tparser.add_argument(\n\t\t\"--push-all\", action = \"store_true\",\n\t\thelp = \"Push all branches to the remote repository after running the test suite.\"\n\t)\n\tparser.add_argument(\n\t\t\"--commit-message\", type = str,\n\t\thelp = \"Use this commit message when pushing to the remote repository\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Visualize a sample of the data.')\n\tparser.add_argument('-s', '--samples', help='number of samples to visualize', type=int, default=5000)\n\tparser.add_argument('-n', '--no-show', help='do not show the plot; write it to the current directory', action='store_true')\n\tparser.add_argument('-o', '--output', help='output directory; default is the current directory', type=str, default='.')\n\tparser.add_argument('-p', '--prefix', help='prefix for the output file; default is \"samples\"', type=str, default='samples')\n\tparser.add_argument('-f', '--format', help='format of the output file; default is \"png\"', type=str, default='png')\n\tparser.add_argument('-l', '--load', help='file from which to load the data; default is \"data/data-train.npy\"', type=str, default='data/data-train.npy')\n\treturn\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\tdef _parse_args(args):\n\t\treturn parser.parse_args(args)\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument('-l', '--log-level',\n\t\t\t\t\t\thelp='set the logging level',\n\t\t\t\t\t\tchoices=['debug', 'info', 'warning', 'error', 'critical'],\n\t\t\t\t\t\tdefault='info')\n\tparser.add_argument('-d', '--source-dir',\n\t\t\t\t\t\thelp='path to source directory',\n\t\t\t\t\t\tdefault=os.path.join(os.path.dirname(__file__), os.pardir, os.pardir))\n\tparser.add_argument('-p', '--properties',\n\t\t\t\t\t\thelp='path to properties file')\n\tparser.add_argument('-n', '--no-clean',\n\t\t\t\t\t\thelp='do not clean source directory',\n\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\tdefault=False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\n\t\t\"-m\", \"--method\",\n\t\tdest=\"method\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"the method of analysis to use\",\n\t\tchoices=method_choices\n\t)\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\",\n\t\tdest=\"verbose\",\n\t\tdefault=False,\n\t\taction=\"store_true\",\n\t\thelp=\"print more information while running\"\n\t)\n\tparser.add_argument(\n\t\t\"--output\",\n\t\tdest=\"output\",\n\t\ttype=str,\n\t\tdefault=\"\",\n\t\thelp=\"the directory in which to save the results\"\n\t)\n\tparser.add_argument(\n\t\t\"--reference\",\n\t\tdest=\"reference\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"the name of the reference set to use\"\n\t)\n\tparser.add_argument(\n\t\t\"--signific\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tThis script generates a simple CSV file that contains the same data as the original CSV file\n\tbut where the values of the columns specified by the user are replaced with values obtained from\n\tthe corresponding column in the original file.\n\t\t\"\"\", add_help=False)\n\tparser.add_argument('-i', '--input', help=\"the path to the input CSV file\")\n\tparser.add_argument('-o', '--output', help=\"the path to the output CSV file\")\n\tparser.add_argument('-c', '--column', help=\"\"\"\n\t\tthe name of a column in the input CSV file whose values should be replaced with values from\n\tthe corresponding column in the original file. This argument can be specified multiple times.\n\t\t\"\"\", action='append')\n\tparser.add_argument('-d', '--delimiter', help=\"\"\"\n\t\tthe character used to delimit values in the input CSV file. Defaults to the comma character.\n\t\t\"\"\", default=',')\n\tparser.add_argument('-e', '--encoding',\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = ArgumentParser(\n\t\t\tdescription=\"\"\"\n\t\t\t\tThis script is designed to facilitate creating a Docker image that contains a\n\t\t\t\tKubernetes cluster.\n\n\t\t\t\tThe script will create a new Docker image that, when run, will start up a\n\t\t\t\tKubernetes cluster. The cluster will be made up of the master node, which\n\t\t\t\tcontains the Kubernetes API server, and one or more worker nodes.\n\n\t\t\t\tThe script will create a Kubernetes configuration file, which will be placed in\n\t\t\t\tthe `/root/.kube/config` directory in the Docker image, and will be used by the\n\t\t\t\tKubernetes cluster to configure itself. The configuration file will be encrypted\n\t\t\t\tusing a user-specified password (or a randomly-generated one, if no password is\n\t\t\t\tspecified), and the password will be written to the `/root/.kube/password` file\n\t\t\t\tin the Docker image.\n\n\t\t\t\tThe script will also create a Kubernetes manifests file, which will be\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tAutomate the process of downloading and installing the latest version of the Chrome\n\t\tbrowser, and then run the browser in headless mode.\n\t\t\"\"\")\n\n\t# add arguments for which we want to capture the value\n\tparser.add_argument('-d', '--download-dir', help='Specify the directory into which to download the Chrome installer. By default the current working directory is used.')\n\tparser.add_argument('-p', '--preferences-file', help='Specify a preferences file to use with the browser.')\n\tparser.add_argument('-s', '--start-url', help='Specify the URL to use to start the browser.')\n\n\t# add arguments for which we don't want to capture the value\n\tparser.add_argument('--overwrite-preferences-file', action='store_true', help='Overwrite the preferences file with default values, if it exists.')\n\tparser.add_argument('--overwrite-start-url', action='store_true', help='Overwrite the start URL with the default, if\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Reads a list of files and prints the first 10 lines of each one.\")\n\tparser.add_argument(\"files\", help=\"files to read\", nargs=\"+\")\n\n\treturn parser.parse_args(*arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\targuments_parser = ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tLoad a CSV file of data and output an HTML table with the data in it.\n\t\t\t\"\"\"\n\t\t)\n\targuments_parser.add_argument(\n\t\tdest=\"input_filename\",\n\t\taction=\"store\",\n\t\ttype=str,\n\t\thelp=\"\"\"The path of a CSV file to read in.\"\"\"\n\t)\n\targuments_parser.add_argument(\n\t\tdest=\"output_filename\",\n\t\taction=\"store\",\n\t\ttype=str,\n\t\thelp=\"\"\"The path of an HTML file to write out.\"\"\"\n\t)\n\t#arguments_parser.add_argument(\n\t#\tdest=\"font\",\n\t#\taction=\"store\",\n\t#\ttype=str,\n\t#\thelp=\"\"\"The path of a font to use in the PDF.\"\"\"\n\t#)\n\treturn arguments_parser.parse_args(*arguments)\n\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\"-i\", \"--input\", dest=\"input\", default=None, type=str, help=\"path to the input file\")\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", default=None, type=str, help=\"path to the output file\")\n\tparser.add_argument(\"-a\", \"--all-output\", dest=\"all_output\", default=None, type=str, help=\"path to the output file\")\n\tparser.add_argument(\"-f\", \"--format\", dest=\"format\", default=\"xml\", type=str, help=\"output format: xml, tsv\")\n\tparser.add_argument(\"-v\", \"--version\", action=\"version\", version=version)\n\treturn parser.parse_args(args=arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\", action = \"store_true\",\n\t\thelp = \"Show verbose output.\"\n\t)\n\tparser.add_argument(\n\t\t\"--no-color\", action = \"store_true\",\n\t\thelp = \"Don't show colorized output.\"\n\t)\n\tparser.add_argument(\n\t\t\"--pull\", action = \"store_true\",\n\t\thelp = \"Pull from the remote repository before running the test suite.\"\n\t)\n\tparser.add_argument(\n\t\t\"--push\", action = \"store_true\",\n\t\thelp = \"Push to the remote repository after running the test suite.\"\n\t)\n\tparser.add_argument(\n\t\t\"--push-all\", action = \"store_true\",\n\t\thelp = \"Push all branches to the remote repository after running the test suite.\"\n\t)\n\tparser.add_argument(\n\t\t\"--commit-message\", type = str,\n\t\thelp = \"Use this commit message when pushing to the remote repository\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Visualize a sample of the data.')\n\tparser.add_argument('-s', '--samples', help='number of samples to visualize', type=int, default=5000)\n\tparser.add_argument('-n', '--no-show', help='do not show the plot; write it to the current directory', action='store_true')\n\tparser.add_argument('-o', '--output', help='output directory; default is the current directory', type=str, default='.')\n\tparser.add_argument('-p', '--prefix', help='prefix for the output file; default is \"samples\"', type=str, default='samples')\n\tparser.add_argument('-f', '--format', help='format of the output file; default is \"png\"', type=str, default='png')\n\tparser.add_argument('-l', '--load', help='file from which to load the data; default is \"data/data-train.npy\"', type=str, default='data/data-train.npy')\n\treturn\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\treturn ' '.join(flags)\n\n\n", "def parser_flags(parser):\n\treturn ' '.join([flag for flag in parser._actions if isinstance(flag, argparse._StoreAction)])\n", "def parser_flags(parser):\n\treturn ' '.join(sorted([flag for flag in parser._option_string_actions.keys() if flag.startswith('-')]))\n\n", "def parser_flags(parser):\n\treturn ' '.join(map(lambda x: x.lstrip('-').upper(), parser.get_default_values().__dict__.values()))\n\n", "def parser_flags(parser):\n\treturn ' '.join(parser.format_option_strings(parser._actions))\n\n\n", "def parser_flags(parser):\n\tflag_list = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._StoreAction):\n\t\t\tif action.option_strings:\n\t\t\t\tflag_list.append(action.option_strings[-1])\n\t\t\telse:\n\t\t\t\tflag_list.append(action.dest)\n\treturn ' '.join(flag_list)\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(parser._get_optional_actions())\n\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(\" \".join(action.option_strings))\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn \" \".join(flags)\n\n", "def parser_flags(parser):\n\treturn \" \".join(parser._get_kwargs()[1]['option_strings'])\n\n", "def parser_flags(parser):\n\tflags = parser.format_usage()\n\tflags = re.sub(r\"^usage:\", \"\", flags, flags=re.IGNORECASE)\n\tflags = re.sub(r\"\\[.*\\]\", \"\", flags)\n\tflags = re.sub(r\"  +\", \" \", flags)\n\tflags = re.sub(r\"^\\s+|\\s+$\", \"\", flags)\n\treturn flags\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\treturn ' '.join(flags)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join([flag for flag in parser._actions if isinstance(flag, argparse._StoreAction)])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(sorted([flag for flag in parser._option_string_actions.keys() if flag.startswith('-')]))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(map(lambda x: x.lstrip('-').upper(), parser.get_default_values().__dict__.values()))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(parser.format_option_strings(parser._actions))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflag_list = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, argparse._StoreAction):\n\t\t\tif action.option_strings:\n\t\t\t\tflag_list.append(action.option_strings[-1])\n\t\t\telse:\n\t\t\t\tflag_list.append(action.dest)\n\treturn ' '.join(flag_list)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(parser._get_optional_actions())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(\" \".join(action.option_strings))\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn \" \".join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(parser._get_kwargs()[1]['option_strings'])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = parser.format_usage()\n\tflags = re.sub(r\"^usage:\", \"\", flags, flags=re.IGNORECASE)\n\tflags = re.sub(r\"\\[.*\\]\", \"\", flags)\n\tflags = re.sub(r\"  +\", \" \", flags)\n\tflags = re.sub(r\"^\\s+|\\s+$\", \"\", flags)\n\treturn flags\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "ques_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n\t# Get the main parser and all of its subparsers.\n\tparser = argparser()\n\tsubparsers = parser.subparsers.choices.values()\n\n\t# Build a map from subparser to its arguments.\n\tsubparsers_to_arguments = {}\n\tfor subparser in subparsers:\n\t\t# Get the subparser's arguments.\n\t\targuments = subparser.choices\n\t\t# Ignore the help argument.\n\t\targuments = [arg for arg in arguments if arg != 'help']\n\t\t# Ignore the --verbosity argument since we want to use a different\n\t\t# completion script for that.\n\t\targuments = [arg for arg in arguments if arg != 'verbosity']\n\t\t# Ignore the --config-path argument since we want to use a different\n\t\t# completion script for that.\n\t\targuments = [arg for arg in arguments if arg != 'config_path']\n\t\t# Ignore the --config-format argument since we want to use a different\n\t\t# completion script for that.\n\t\targuments = [arg for arg in arguments if\n", "def bash_completion():\n\timport argcomplete\n\tparser = _get_arg_parser()\n\treturn argcomplete.generate_bash_completion(parser)\n\n", "def bash_completion():\n\tparser = argparse.ArgumentParser(prog = 'borgmatic')\n\tsubparsers = parser.add_subparsers(dest = 'subcommand')\n\n\t# Add subparsers for all of borgmatic's commands.\n\tinitialize_parser = subparsers.add_parser('initialize', help = 'initialize a repository')\n\tcreate_parser = subparsers.add_parser('create', help = 'create a backup')\n\tlist_parser = subparsers.add_parser('list', help = 'list backups')\n\tcheck_parser = subparsers.add_parser('check', help = 'check backups')\n\tprune_parser = subparsers.add_parser('prune', help = 'prune backups')\n\textract_parser = subparsers.add_parser('extract', help = 'extract backups')\n\tmount_parser = subparsers.add_parser('mount', help = 'mount backups')\n\n\t# Add common arguments to each parser.\n\tfor parser in (initialize_parser, create_parser, list_parser, check_parser, prune_parser\n", "def bash_completion():\n\tparser = create_parser()\n\t# Create a new parser with no arguments that adds the borgmatic program name as the first\n\t# argument to the generated completion script. The completion script expects a program name.\n\t# The first argument is usually the program name, but when running a completion script, the\n\t# first argument is the name of the command to complete, so we need to add the program name.\n\tparser = argparse.ArgumentParser(\n\t\tprog='borgmatic', usage=parser.format_usage(), add_help=False\n\t)\n\tparser.add_argument('program_name')\n\n\t# The bash completion script expects a single-hyphen-prefixed version of the argument name,\n\t# so we need to do some preprocessing to generate that.\n\tdef add_argument(argument):\n\t\t# Create a new argument that has a single hyphen prefix and no dashes in the argument\n\t\t# name.\n\t\targument = copy.copy(argument)\n\t\targument.dest = argument.dest.replace('-', '_')\n\t\targument.option_strings = [\n\t\t\t'-' +\n", "def bash_completion():\n\tparser = get_parser()\n\tcompleter = Completer(parser)\n\treturn completer.bash_completion()\n\n\n", "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--version', action='version', version=__version__)\n\tparser.add_argument('-c', '--config', help='configuration file path')\n\tparser.add_argument('-l', '--logfile', help='log file path')\n\tparser.add_argument('--logging-config', help='logging configuration file path')\n\tparser.add_argument('-L', '--loglevel', help='log level')\n\tparser.add_argument(\n\t\t'-e', '--exclude', help='glob pattern to exclude from backup', action='append'\n\t)\n\tparser.add_argument(\n\t\t'-E', '--exclude-from', help='file containing glob patterns to exclude from backup',\n\t\taction='append'\n\t)\n\tparser.add_argument(\n\t\t'-i', '--include', help='glob pattern to include in backup', action='append'\n\t)\n\tparser.add_argument(\n\t\t'-I', '--include-from', help='file containing glob\n", "def bash_completion():\n\tdef _get_argparse_completion(parser):\n\t\t\"\"\"\n\t\tReturn a string containing the bash completion script for the given argparse parser.\n\t\t\"\"\"\n\t\tdef _handle_positional(argument):\n\t\t\t\"\"\"\n\t\t\tReturn a string containing the positional argument completion script for the given\n\t\t\targparse positional argument.\n\t\t\t\"\"\"\n\t\t\treturn \"__{}_positional_args() {{\\n    case \\\"${{COMP_WORDS[COMP_CWORD]}}\\\" in\\n{}\\n    esac\\n}}\\n\".format(\n\t\t\t\tparser.prog,\n\t\t\t\t\"\\n\".join(\n\t\t\t\t\t\"        {})\\n            COMPREPLY=($(compgen -W '{}' -- \\\"${{COMP_WORDS[COMP_CWORD]}}\\\"))\\n            return 0\\n            ;;\".format(\n\t\t\t\t\t\tn, completion_words)\n\t\t\t\t\tfor n, completion_words in enumerate(argument.completion_words, start=\n", "def bash_completion():\n\tparser = create_parser()\n\treturn parser.get_bash_completion()\n\n", "def bash_completion():\n\tdef generate_bash_completion_script(parser):\n\t\t# This function is based on the generate_bash_completion_script function in the\n\t\t# argcomplete library, available under the MIT license at\n\t\t# https://github.com/kislyuk/argcomplete/blob/master/argcomplete/__init__.py\n\t\twords = set()\n\t\tfor action in parser._actions:\n\t\t\tif action.choices is not None:\n\t\t\t\twords.update(action.choices.keys())\n\t\t\telse:\n\t\t\t\twords.update(action.option_strings)\n\t\t\t\tdefault = action.default\n\t\t\t\tif isinstance(default, (list, tuple)):\n\t\t\t\t\twords.update(default)\n\t\t\t\telse:\n\t\t\t\t\twords.add(default)\n\t\tif parser.epilog:\n\t\t\twords.update(parser.epilog.split())\n\t\twords.add('--help')\n\t\twords.add('--version')\n\t\treturn ' '.join\n", "def bash_completion():\n\tparser = argparse.ArgumentParser(add_help=False)\n\tsubparsers = parser.add_subparsers(dest='subparser_name')\n\n\t# Create the main parser.\n\tmain_parser = subparsers.add_parser('borgmatic')\n\tmain_parser.add_argument('--config', '-c')\n\tmain_parser.add_argument('--verbosity', '-v')\n\tmain_parser.add_argument('--logfile')\n\tmain_parser.add_argument('--lockfile')\n\tmain_parser.add_argument('--lockfile-timeout')\n\tmain_parser.add_argument('--lockfile-wait')\n\tmain_parser.add_argument('--hooks')\n\tmain_parser.add_argument('--log-json')\n\tmain_parser.add_argument('--log-json-config')\n\tmain_parser.add_argument('--hooks-json')\n\tmain_parser.add_argument('--hooks-json-config')\n\tmain_parser.add_argument('--dry-run')\n\tmain\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n\t# Get the main parser and all of its subparsers.\n\tparser = argparser()\n\tsubparsers = parser.subparsers.choices.values()\n\n\t# Build a map from subparser to its arguments.\n\tsubparsers_to_arguments = {}\n\tfor subparser in subparsers:\n\t\t# Get the subparser's arguments.\n\t\targuments = subparser.choices\n\t\t# Ignore the help argument.\n\t\targuments = [arg for arg in arguments if arg != 'help']\n\t\t# Ignore the --verbosity argument since we want to use a different\n\t\t# completion script for that.\n\t\targuments = [arg for arg in arguments if arg != 'verbosity']\n\t\t# Ignore the --config-path argument since we want to use a different\n\t\t# completion script for that.\n\t\targuments = [arg for arg in arguments if arg != 'config_path']\n\t\t# Ignore the --config-format argument since we want to use a different\n\t\t# completion script for that.\n\t\targuments = [arg for arg in arguments if\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\timport argcomplete\n\tparser = _get_arg_parser()\n\treturn argcomplete.generate_bash_completion(parser)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = argparse.ArgumentParser(prog = 'borgmatic')\n\tsubparsers = parser.add_subparsers(dest = 'subcommand')\n\n\t# Add subparsers for all of borgmatic's commands.\n\tinitialize_parser = subparsers.add_parser('initialize', help = 'initialize a repository')\n\tcreate_parser = subparsers.add_parser('create', help = 'create a backup')\n\tlist_parser = subparsers.add_parser('list', help = 'list backups')\n\tcheck_parser = subparsers.add_parser('check', help = 'check backups')\n\tprune_parser = subparsers.add_parser('prune', help = 'prune backups')\n\textract_parser = subparsers.add_parser('extract', help = 'extract backups')\n\tmount_parser = subparsers.add_parser('mount', help = 'mount backups')\n\n\t# Add common arguments to each parser.\n\tfor parser in (initialize_parser, create_parser, list_parser, check_parser, prune_parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = create_parser()\n\t# Create a new parser with no arguments that adds the borgmatic program name as the first\n\t# argument to the generated completion script. The completion script expects a program name.\n\t# The first argument is usually the program name, but when running a completion script, the\n\t# first argument is the name of the command to complete, so we need to add the program name.\n\tparser = argparse.ArgumentParser(\n\t\tprog='borgmatic', usage=parser.format_usage(), add_help=False\n\t)\n\tparser.add_argument('program_name')\n\n\t# The bash completion script expects a single-hyphen-prefixed version of the argument name,\n\t# so we need to do some preprocessing to generate that.\n\tdef add_argument(argument):\n\t\t# Create a new argument that has a single hyphen prefix and no dashes in the argument\n\t\t# name.\n\t\targument = copy.copy(argument)\n\t\targument.dest = argument.dest.replace('-', '_')\n\t\targument.option_strings = [\n\t\t\t'-' +\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = get_parser()\n\tcompleter = Completer(parser)\n\treturn completer.bash_completion()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--version', action='version', version=__version__)\n\tparser.add_argument('-c', '--config', help='configuration file path')\n\tparser.add_argument('-l', '--logfile', help='log file path')\n\tparser.add_argument('--logging-config', help='logging configuration file path')\n\tparser.add_argument('-L', '--loglevel', help='log level')\n\tparser.add_argument(\n\t\t'-e', '--exclude', help='glob pattern to exclude from backup', action='append'\n\t)\n\tparser.add_argument(\n\t\t'-E', '--exclude-from', help='file containing glob patterns to exclude from backup',\n\t\taction='append'\n\t)\n\tparser.add_argument(\n\t\t'-i', '--include', help='glob pattern to include in backup', action='append'\n\t)\n\tparser.add_argument(\n\t\t'-I', '--include-from', help='file containing glob\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tdef _get_argparse_completion(parser):\n\t\t\"\"\"\n\t\tReturn a string containing the bash completion script for the given argparse parser.\n\t\t\"\"\"\n\t\tdef _handle_positional(argument):\n\t\t\t\"\"\"\n\t\t\tReturn a string containing the positional argument completion script for the given\n\t\t\targparse positional argument.\n\t\t\t\"\"\"\n\t\t\treturn \"__{}_positional_args() {{\\n    case \\\"${{COMP_WORDS[COMP_CWORD]}}\\\" in\\n{}\\n    esac\\n}}\\n\".format(\n\t\t\t\tparser.prog,\n\t\t\t\t\"\\n\".join(\n\t\t\t\t\t\"        {})\\n            COMPREPLY=($(compgen -W '{}' -- \\\"${{COMP_WORDS[COMP_CWORD]}}\\\"))\\n            return 0\\n            ;;\".format(\n\t\t\t\t\t\tn, completion_words)\n\t\t\t\t\tfor n, completion_words in enumerate(argument.completion_words, start=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = create_parser()\n\treturn parser.get_bash_completion()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tdef generate_bash_completion_script(parser):\n\t\t# This function is based on the generate_bash_completion_script function in the\n\t\t# argcomplete library, available under the MIT license at\n\t\t# https://github.com/kislyuk/argcomplete/blob/master/argcomplete/__init__.py\n\t\twords = set()\n\t\tfor action in parser._actions:\n\t\t\tif action.choices is not None:\n\t\t\t\twords.update(action.choices.keys())\n\t\t\telse:\n\t\t\t\twords.update(action.option_strings)\n\t\t\t\tdefault = action.default\n\t\t\t\tif isinstance(default, (list, tuple)):\n\t\t\t\t\twords.update(default)\n\t\t\t\telse:\n\t\t\t\t\twords.add(default)\n\t\tif parser.epilog:\n\t\t\twords.update(parser.epilog.split())\n\t\twords.add('--help')\n\t\twords.add('--version')\n\t\treturn ' '.join\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = argparse.ArgumentParser(add_help=False)\n\tsubparsers = parser.add_subparsers(dest='subparser_name')\n\n\t# Create the main parser.\n\tmain_parser = subparsers.add_parser('borgmatic')\n\tmain_parser.add_argument('--config', '-c')\n\tmain_parser.add_argument('--verbosity', '-v')\n\tmain_parser.add_argument('--logfile')\n\tmain_parser.add_argument('--lockfile')\n\tmain_parser.add_argument('--lockfile-timeout')\n\tmain_parser.add_argument('--lockfile-wait')\n\tmain_parser.add_argument('--hooks')\n\tmain_parser.add_argument('--log-json')\n\tmain_parser.add_argument('--log-json-config')\n\tmain_parser.add_argument('--hooks-json')\n\tmain_parser.add_argument('--hooks-json-config')\n\tmain_parser.add_argument('--dry-run')\n\tmain\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"A tool to generate Python bindings for C libraries.\", formatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"print verbose debugging information\")\n\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=__version__)\n\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\")\n\n\t# Parse arguments for the \"generate\" subparser\n\tgenerate_parser = subparsers.add_parser(\"generate\", help=\"generate Python bindings for a C library\")\n\tgenerate_parser.add_argument(\"-c\", \"--config\", required=True, type=argparse.FileType(\"r\"), help=\"the configuration file to use\")\n\tgenerate_parser.add_argument(\"-o\", \"--output\", type=argparse.FileType(\"w\"), help=\"the output file to write to\")\n\tgenerate_parser.add_argument(\"-t\", \"--template\", type=argparse.\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run the specified test suites in a controlled way.\",\n\t\tepilog=\"\"\"\n\t\t\tAll positional arguments that are not recognized as subcommand names are passed through\n\t\t\tas arguments to the subcommand. This allows you to run this script like this:\n\t\t\t\tpython3 test.py all\n\t\t\twhich is equivalent to\n\t\t\t\tpython3 test.py run all\n\t\t\t\"\"\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\tparser.add_argument(\"--verbose\", action=\"store_true\")\n\tparser.add_argument(\"--test-suite-path\", action=\"store\", default=None,\n\t\thelp=\"Path to the directory containing the test suites (default: ../test_suites).\")\n\tparser.add_argument(\"--test-suite-list\", action=\"store\", default=None,\n\t\thelp=\"Path to the file listing the test\n", "def parse_arguments(*unparsed_arguments):\n\t# Initialize global subparser\n\tglobal_subparser = argparse.ArgumentParser(add_help=False)\n\tglobal_subparser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n\tglobal_subparser.add_argument(\"--log-file\", help=\"Log to file instead of stdout\")\n\tglobal_subparser.add_argument(\"--log-file-level\", default=\"DEBUG\", help=\"Logging level for log file\")\n\tglobal_subparser.add_argument(\"--log-stdout-level\", default=\"INFO\", help=\"Logging level for stdout\")\n\tglobal_subparser.add_argument(\"--log-file-max-bytes\", type=int, default=10485760,\n\t\thelp=\"Max size of log file before rotation (default: 10 MiB)\")\n\tglobal_subparser.add_argument(\"--log-file-backup-count\", type=int, default=5,\n\t\thelp=\"Number of log files to keep (default: 5)\")\n\n\t# Initialize top-level parser\n\tparser = argparse.Argument\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--debug\", action=\"store_true\", default=False, help=\"Enable debugging output\")\n\tparser.add_argument(\"--quiet\", action=\"store_true\", default=False, help=\"Suppress normal output\")\n\tparser.add_argument(\"--version\", action=\"store_true\", default=False, help=\"Display version information\")\n\tparser.add_argument(\"--config\", default=None, help=\"Path to the config file\")\n\tparser.add_argument(\"--logfile\", default=None, help=\"Path to the log file\")\n\tparser.add_argument(\"--loglevel\", default=\"INFO\", help=\"Level of detail for log messages\")\n\tparser.add_argument(\"--logformat\", default=LOG_FORMAT, help=\"Format for log messages\")\n\tparser.add_argument(\"--logdateformat\", default=LOG_DATE_FORMAT, help=\"Format for dates in log messages\")\n\tparser.add_argument(\"--logger\", default=None, help=\"Logger name\")\n\tparser.add_argument(\"--logtocon\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Manage your dotfiles.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\",\n\t\taction=\"store_const\", dest=\"log_level\",\n\t\tconst=logging.DEBUG, default=logging.INFO,\n\t\thelp=\"Be verbose.\",\n\t)\n\tparser.add_argument(\n\t\t\"-d\", \"--dotfiles-dir\",\n\t\tdefault=os.path.expanduser(\"~/dotfiles\"),\n\t)\n\tparser.add_argument(\n\t\t\"-b\", \"--backups-dir\",\n\t\tdefault=os.path.expanduser(\"~/.dotfiles_backup\"),\n\t)\n\tparser.add_argument(\n\t\t\"-c\", \"--config-file\",\n\t\tdefault=None,\n\t)\n\tparser.add_argument(\n\t\t\"-D\", \"--dry-run\",\n\t\taction=\"store_true\",\n\t\thelp=\"\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Query and manipulate a database of people and their connections.',\n\t\tepilog=\n\t\t\t'See the README for more information about the database and how to query it.',\n\t)\n\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\", action=\"count\", default=0, help=\n\t\t\t\"Increase the verbosity level. \"\n\t\t\t\"Can be specified multiple times to increase verbosity further.\"\n\t)\n\n\t# Add a subparser for each command\n\tsubparsers = parser.add_subparsers(title=\"Subcommands\")\n\tfor subcommand in subcommands:\n\t\tsubparsers.add_parser(\n\t\t\tname=subcommand.name,\n\t\t\thelp=subcommand.help_summary,\n\t\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t\tdescription=subcommand.help_description,\n\t\t\tepilog=subcommand.help_epilog,\n\t\t\tparents=[subcommand.parser]\n\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t This script allows managing the configuration of a DASK-enabled Python interpreter\n\t\t\t\t\t\t\t\t\t and the DASK worker processes running on a given host.\n\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t \"\"\")\n\tparser.add_argument(\"-c\", \"--config-file\", action=\"store\", default=\"~/.dask-cluster.yaml\", type=str,\n\t\t\t\t\t\thelp=\"\"\"\n\t\t\t\t\t\t\n\t\t\t\t\t\tPath to the DASK configuration file to be read.\n\t\t\t\t\t\t\n\t\t\t\t\t\t\"\"\")\n\tparser.add_argument(\"--show-config\", action=\"store_true\", help=\"\"\"\n\t\t\t\t\t\t\n\t\t\t\t\t\tDisplay the current DASK configuration file and exit.\n\t\t\t\t\t\t\n\t\t\t\t\t\t\"\"\")\n\tparser.add_argument(\"--\n", "def parse_arguments(*unparsed_arguments):\n\t\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tThis script provides a command-line interface to the TFS REST API.\n\t\t\"\"\", formatter_class=argparse.RawTextHelpFormatter)\n\t\n\t# Global arguments\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"\"\"\n\t\tIndicate that additional output should be written to standard output.\"\"\")\n\t\n\t# Subparsers\n\tsubparsers = parser.add_subparsers()\n\t\n\t# Subparser: get-changeset\n\tget_changeset_parser = subparsers.add_parser(\"get-changeset\", description=\"\"\"\n\t\tRetrieve information about a changeset.\n\t\t\"\"\", formatter_class=argparse.RawTextHelpFormatter)\n\tget_changeset_parser.add_argument(\"changeset_id\", type=int, help=\"\"\"\n\t\tThe changeset ID to retrieve.\"\"\")\n\tget_changeset_parser.set_defaults(func=get_changeset)\n\t\n\t# Subparser:\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t\t\"--config\",\n\t\tmetavar = \"CONFIG_PATH\",\n\t\trequired = True,\n\t\thelp = \"Path to the configuration file to use\",\n\t)\n\tparser.add_argument(\n\t\t\"--debug\",\n\t\taction = \"store_true\",\n\t\thelp = \"Run in debug mode\",\n\t)\n\tparser.add_argument(\n\t\t\"--log-path\",\n\t\tmetavar = \"LOG_PATH\",\n\t\thelp = \"Path to the log file to use\",\n\t)\n\tparser.add_argument(\n\t\t\"--time-format\",\n\t\tmetavar = \"TIME_FORMAT\",\n\t\thelp = \"strftime-compatible string to use for formatting log timestamps\",\n\t)\n\tparser.add_argument(\n\t\t\"--verbose\",\n\t\taction = \"store_true\",\n\t\thelp = \"Run in verbose mode\",\n\t)\n\tsubparsers = parser.add_subparsers(\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Normalize a corpus of text and save the result to a file.\")\n\tparser.add_argument('input_path', type=str, help=\"The path at which to read the input text file.\")\n\tparser.add_argument('output_path', type=str, help=\"The path at which to write the output text file.\")\n\tparser.add_argument('-v', '--verbose', action='store_true', help=\"Show verbose output.\")\n\n\tparser.add_argument(\n\t\t'--pre-normalization',\n\t\ttype=str,\n\t\tchoices=['none', 'porter', 'porter-stemmer', 'porter-stemmer-remover', 'porter-stemmer-remover-normalizer'],\n\t\tdefault='none',\n\t\thelp=\"Normalize the input text before running the main normalization step.\"\n\t)\n\tparser.add_argument(\n\t\t'--normalization',\n\t\ttype=str,\n\t\tchoices=['none', 'porter', 'porter-stemmer', 'porter-stemmer-remover', '\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"A tool to generate Python bindings for C libraries.\", formatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"print verbose debugging information\")\n\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=__version__)\n\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\")\n\n\t# Parse arguments for the \"generate\" subparser\n\tgenerate_parser = subparsers.add_parser(\"generate\", help=\"generate Python bindings for a C library\")\n\tgenerate_parser.add_argument(\"-c\", \"--config\", required=True, type=argparse.FileType(\"r\"), help=\"the configuration file to use\")\n\tgenerate_parser.add_argument(\"-o\", \"--output\", type=argparse.FileType(\"w\"), help=\"the output file to write to\")\n\tgenerate_parser.add_argument(\"-t\", \"--template\", type=argparse.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run the specified test suites in a controlled way.\",\n\t\tepilog=\"\"\"\n\t\t\tAll positional arguments that are not recognized as subcommand names are passed through\n\t\t\tas arguments to the subcommand. This allows you to run this script like this:\n\t\t\t\tpython3 test.py all\n\t\t\twhich is equivalent to\n\t\t\t\tpython3 test.py run all\n\t\t\t\"\"\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"%(prog)s \" + __version__)\n\tparser.add_argument(\"--verbose\", action=\"store_true\")\n\tparser.add_argument(\"--test-suite-path\", action=\"store\", default=None,\n\t\thelp=\"Path to the directory containing the test suites (default: ../test_suites).\")\n\tparser.add_argument(\"--test-suite-list\", action=\"store\", default=None,\n\t\thelp=\"Path to the file listing the test\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t# Initialize global subparser\n\tglobal_subparser = argparse.ArgumentParser(add_help=False)\n\tglobal_subparser.add_argument(\"--debug\", action=\"store_true\", help=\"Enable debug logging\")\n\tglobal_subparser.add_argument(\"--log-file\", help=\"Log to file instead of stdout\")\n\tglobal_subparser.add_argument(\"--log-file-level\", default=\"DEBUG\", help=\"Logging level for log file\")\n\tglobal_subparser.add_argument(\"--log-stdout-level\", default=\"INFO\", help=\"Logging level for stdout\")\n\tglobal_subparser.add_argument(\"--log-file-max-bytes\", type=int, default=10485760,\n\t\thelp=\"Max size of log file before rotation (default: 10 MiB)\")\n\tglobal_subparser.add_argument(\"--log-file-backup-count\", type=int, default=5,\n\t\thelp=\"Number of log files to keep (default: 5)\")\n\n\t# Initialize top-level parser\n\tparser = argparse.Argument\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--debug\", action=\"store_true\", default=False, help=\"Enable debugging output\")\n\tparser.add_argument(\"--quiet\", action=\"store_true\", default=False, help=\"Suppress normal output\")\n\tparser.add_argument(\"--version\", action=\"store_true\", default=False, help=\"Display version information\")\n\tparser.add_argument(\"--config\", default=None, help=\"Path to the config file\")\n\tparser.add_argument(\"--logfile\", default=None, help=\"Path to the log file\")\n\tparser.add_argument(\"--loglevel\", default=\"INFO\", help=\"Level of detail for log messages\")\n\tparser.add_argument(\"--logformat\", default=LOG_FORMAT, help=\"Format for log messages\")\n\tparser.add_argument(\"--logdateformat\", default=LOG_DATE_FORMAT, help=\"Format for dates in log messages\")\n\tparser.add_argument(\"--logger\", default=None, help=\"Logger name\")\n\tparser.add_argument(\"--logtocon\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Manage your dotfiles.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\",\n\t\taction=\"store_const\", dest=\"log_level\",\n\t\tconst=logging.DEBUG, default=logging.INFO,\n\t\thelp=\"Be verbose.\",\n\t)\n\tparser.add_argument(\n\t\t\"-d\", \"--dotfiles-dir\",\n\t\tdefault=os.path.expanduser(\"~/dotfiles\"),\n\t)\n\tparser.add_argument(\n\t\t\"-b\", \"--backups-dir\",\n\t\tdefault=os.path.expanduser(\"~/.dotfiles_backup\"),\n\t)\n\tparser.add_argument(\n\t\t\"-c\", \"--config-file\",\n\t\tdefault=None,\n\t)\n\tparser.add_argument(\n\t\t\"-D\", \"--dry-run\",\n\t\taction=\"store_true\",\n\t\thelp=\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Query and manipulate a database of people and their connections.',\n\t\tepilog=\n\t\t\t'See the README for more information about the database and how to query it.',\n\t)\n\n\tparser.add_argument(\n\t\t\"-v\", \"--verbose\", action=\"count\", default=0, help=\n\t\t\t\"Increase the verbosity level. \"\n\t\t\t\"Can be specified multiple times to increase verbosity further.\"\n\t)\n\n\t# Add a subparser for each command\n\tsubparsers = parser.add_subparsers(title=\"Subcommands\")\n\tfor subcommand in subcommands:\n\t\tsubparsers.add_parser(\n\t\t\tname=subcommand.name,\n\t\t\thelp=subcommand.help_summary,\n\t\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\t\tdescription=subcommand.help_description,\n\t\t\tepilog=subcommand.help_epilog,\n\t\t\tparents=[subcommand.parser]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t This script allows managing the configuration of a DASK-enabled Python interpreter\n\t\t\t\t\t\t\t\t\t and the DASK worker processes running on a given host.\n\t\t\t\t\t\t\t\t\t \n\t\t\t\t\t\t\t\t\t \"\"\")\n\tparser.add_argument(\"-c\", \"--config-file\", action=\"store\", default=\"~/.dask-cluster.yaml\", type=str,\n\t\t\t\t\t\thelp=\"\"\"\n\t\t\t\t\t\t\n\t\t\t\t\t\tPath to the DASK configuration file to be read.\n\t\t\t\t\t\t\n\t\t\t\t\t\t\"\"\")\n\tparser.add_argument(\"--show-config\", action=\"store_true\", help=\"\"\"\n\t\t\t\t\t\t\n\t\t\t\t\t\tDisplay the current DASK configuration file and exit.\n\t\t\t\t\t\t\n\t\t\t\t\t\t\"\"\")\n\tparser.add_argument(\"--\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tThis script provides a command-line interface to the TFS REST API.\n\t\t\"\"\", formatter_class=argparse.RawTextHelpFormatter)\n\t\n\t# Global arguments\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"\"\"\n\t\tIndicate that additional output should be written to standard output.\"\"\")\n\t\n\t# Subparsers\n\tsubparsers = parser.add_subparsers()\n\t\n\t# Subparser: get-changeset\n\tget_changeset_parser = subparsers.add_parser(\"get-changeset\", description=\"\"\"\n\t\tRetrieve information about a changeset.\n\t\t\"\"\", formatter_class=argparse.RawTextHelpFormatter)\n\tget_changeset_parser.add_argument(\"changeset_id\", type=int, help=\"\"\"\n\t\tThe changeset ID to retrieve.\"\"\")\n\tget_changeset_parser.set_defaults(func=get_changeset)\n\t\n\t# Subparser:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t\t\"--config\",\n\t\tmetavar = \"CONFIG_PATH\",\n\t\trequired = True,\n\t\thelp = \"Path to the configuration file to use\",\n\t)\n\tparser.add_argument(\n\t\t\"--debug\",\n\t\taction = \"store_true\",\n\t\thelp = \"Run in debug mode\",\n\t)\n\tparser.add_argument(\n\t\t\"--log-path\",\n\t\tmetavar = \"LOG_PATH\",\n\t\thelp = \"Path to the log file to use\",\n\t)\n\tparser.add_argument(\n\t\t\"--time-format\",\n\t\tmetavar = \"TIME_FORMAT\",\n\t\thelp = \"strftime-compatible string to use for formatting log timestamps\",\n\t)\n\tparser.add_argument(\n\t\t\"--verbose\",\n\t\taction = \"store_true\",\n\t\thelp = \"Run in verbose mode\",\n\t)\n\tsubparsers = parser.add_subparsers(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Normalize a corpus of text and save the result to a file.\")\n\tparser.add_argument('input_path', type=str, help=\"The path at which to read the input text file.\")\n\tparser.add_argument('output_path', type=str, help=\"The path at which to write the output text file.\")\n\tparser.add_argument('-v', '--verbose', action='store_true', help=\"Show verbose output.\")\n\n\tparser.add_argument(\n\t\t'--pre-normalization',\n\t\ttype=str,\n\t\tchoices=['none', 'porter', 'porter-stemmer', 'porter-stemmer-remover', 'porter-stemmer-remover-normalizer'],\n\t\tdefault='none',\n\t\thelp=\"Normalize the input text before running the main normalization step.\"\n\t)\n\tparser.add_argument(\n\t\t'--normalization',\n\t\ttype=str,\n\t\tchoices=['none', 'porter', 'porter-stemmer', 'porter-stemmer-remover', '\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_to_parsed = {}\n\tremainder = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed_args, remaining_args = subparser.parse_known_args(unparsed_arguments)\n\t\tsubparser_to_parsed[subparser_name] = parsed_args\n\t\tremainder += remaining_args\n\n\treturn subparser_to_parsed, remainder\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_arguments = {}\n\tsubparser_names = set(subparsers.keys())\n\tunclaimed_arguments = []\n\tfor i, arg in enumerate(unparsed_arguments):\n\t\tif arg in subparser_names:\n\t\t\tsubparser = subparsers[arg]\n\t\t\tsubparser_arguments[arg] = subparser.parse_args(unparsed_arguments[i + 1:])\n\t\t\tbreak\n\t\telse:\n\t\t\tunclaimed_arguments.append(arg)\n\treturn subparser_arguments, unclaimed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_subparser_arguments = {}\n\tremaining_arguments = []\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed, unparsed = subparser.parse_known_args(unparsed_arguments)\n\t\tif len(unparsed) != len(unparsed_arguments):\n\t\t\tparsed_subparser_arguments[subparser_name] = parsed\n\t\t\tunparsed_arguments = unparsed\n\treturn parsed_subparser_arguments, unparsed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\targuments_by_subparser = {}\n\t# The set of actions that have been attempted to be parsed. Keys are action names,\n\t# values are whether they were successfully parsed.\n\tparsed_actions = {}\n\t# The set of actions that have been requested to be parsed. Keys are action names,\n\t# values are the index of their first argument.\n\trequested_actions = {}\n\n\t# Parse the arguments into subparsers\n\tfor index, argument in enumerate(unparsed_arguments):\n\t\t# If the argument is a subparser name, then we have a new requested action\n\t\tif argument in subparsers:\n\t\t\trequested_actions[argument] = index\n\n\t\t# If the argument isn't a subparser name but is a flag, then the previous\n\t\t# subparser name is the name of the action.\n\t\tif argument.startswith('-'):\n\t\t\tif len(requested_actions) > 0:\n\t\t\t\t# If there's no previous subparser name, then it's either a bad\n\t\t\t\t# argument or a subparser name\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# TODO: This is pretty ugly. I'm sure there's a better way to do this.\n\tparsed_arguments = {}\n\tremaining_arguments = list(unparsed_arguments)\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tindex = remaining_arguments.index(subparser_name)\n\t\texcept ValueError:\n\t\t\tcontinue\n\t\tremaining_arguments = remaining_arguments[0:index] + remaining_arguments[index+1:]\n\t\tparsed_arguments[subparser_name] = subparser.parse_args(remaining_arguments)\n\treturn parsed_arguments, remaining_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = subparsers.keys()\n\tsubparser_name_to_parser = {name: subparser for name, subparser in subparsers.items()}\n\tsubparser_name_to_args = {name: [] for name in subparsers.keys()}\n\t# Parse arguments in a loop until we've exhausted all arguments, or until we've found a\n\t# subparser name to focus on.\n\twhile unparsed_arguments:\n\t\t# Find the first argument that matches a subparser name.\n\t\tfor i, arg in enumerate(unparsed_arguments):\n\t\t\tif arg in subparser_names:\n\t\t\t\t# We found a subparser name.\n\t\t\t\tsubparser_name = arg\n\t\t\t\t# Collect all arguments up to this point.\n\t\t\t\tsubparser_name_to_args[subparser_name] = unparsed_arguments[:i]\n\t\t\t\t# Remove those arguments from the unparsed list.\n\t\t\t\tunparsed_arguments = unparsed_arguments\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tunparsed_arguments = list(unparsed_arguments)\n\tsubparser_names = set(subparsers.keys())\n\tsubparser_names_from_arguments = set()\n\twhile len(unparsed_arguments) > 0:\n\t\tif unparsed_arguments[0] in subparser_names:\n\t\t\tsubparser_names_from_arguments.add(unparsed_arguments[0])\n\t\tunparsed_arguments = unparsed_arguments[1:]\n\tif subparser_names_from_arguments == set():\n\t\tsubparser_names_from_arguments.add('help')\n\tparsed_arguments = {}\n\tfor subparser_name in subparser_names_from_arguments:\n\t\tsubparser = subparsers[subparser_name]\n\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tunparsed_arguments = subparser.parse_known_args(un\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser.add_argument('--subparser-name', action='store_const', const=subparser_name, dest='subparser_name')\n\t\tsubparser_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tif subparser_arguments[subparser_name].subparser_name == subparser_name:\n\t\t\tbreak\n\telse:\n\t\traise ValueError('no valid subparser found')\n\tsubparser_arguments[subparser_name].subparser_name = None\n\treturn subparser_arguments, subparser_arguments[subparser_name].subparser_name, subparser_arguments[subparser_name].subparser_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tresult = {}\n\tunclaimed_arguments = []\n\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser.parse_args([argument])\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tif subparser_name not in result:\n\t\t\t\t\tresult[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\t\t\t\tbreak\n\n\t\telse:\n\t\t\tunclaimed_arguments.append(argument)\n\n\treturn result, unclaimed_arguments\n\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tremaining_arguments = []\n\tsubparser_arguments = {}\n\tfor subparser_name in subparsers:\n\t\tsubparser = subparsers[subparser_name]\n\t\tsubparser_arguments[subparser_name] = subparser.parse_known_args(unparsed_arguments)\n\t\tremaining_arguments += subparser_arguments[subparser_name][1]\n\treturn subparser_arguments, remaining_arguments\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_to_parsed = {}\n\tremainder = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed_args, remaining_args = subparser.parse_known_args(unparsed_arguments)\n\t\tsubparser_to_parsed[subparser_name] = parsed_args\n\t\tremainder += remaining_args\n\n\treturn subparser_to_parsed, remainder\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_arguments = {}\n\tsubparser_names = set(subparsers.keys())\n\tunclaimed_arguments = []\n\tfor i, arg in enumerate(unparsed_arguments):\n\t\tif arg in subparser_names:\n\t\t\tsubparser = subparsers[arg]\n\t\t\tsubparser_arguments[arg] = subparser.parse_args(unparsed_arguments[i + 1:])\n\t\t\tbreak\n\t\telse:\n\t\t\tunclaimed_arguments.append(arg)\n\treturn subparser_arguments, unclaimed_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_subparser_arguments = {}\n\tremaining_arguments = []\n\tfor subparser_name, subparser in subparsers.items():\n\t\tparsed, unparsed = subparser.parse_known_args(unparsed_arguments)\n\t\tif len(unparsed) != len(unparsed_arguments):\n\t\t\tparsed_subparser_arguments[subparser_name] = parsed\n\t\t\tunparsed_arguments = unparsed\n\treturn parsed_subparser_arguments, unparsed_arguments\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\targuments_by_subparser = {}\n\t# The set of actions that have been attempted to be parsed. Keys are action names,\n\t# values are whether they were successfully parsed.\n\tparsed_actions = {}\n\t# The set of actions that have been requested to be parsed. Keys are action names,\n\t# values are the index of their first argument.\n\trequested_actions = {}\n\n\t# Parse the arguments into subparsers\n\tfor index, argument in enumerate(unparsed_arguments):\n\t\t# If the argument is a subparser name, then we have a new requested action\n\t\tif argument in subparsers:\n\t\t\trequested_actions[argument] = index\n\n\t\t# If the argument isn't a subparser name but is a flag, then the previous\n\t\t# subparser name is the name of the action.\n\t\tif argument.startswith('-'):\n\t\t\tif len(requested_actions) > 0:\n\t\t\t\t# If there's no previous subparser name, then it's either a bad\n\t\t\t\t# argument or a subparser name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# TODO: This is pretty ugly. I'm sure there's a better way to do this.\n\tparsed_arguments = {}\n\tremaining_arguments = list(unparsed_arguments)\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tindex = remaining_arguments.index(subparser_name)\n\t\texcept ValueError:\n\t\t\tcontinue\n\t\tremaining_arguments = remaining_arguments[0:index] + remaining_arguments[index+1:]\n\t\tparsed_arguments[subparser_name] = subparser.parse_args(remaining_arguments)\n\treturn parsed_arguments, remaining_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = subparsers.keys()\n\tsubparser_name_to_parser = {name: subparser for name, subparser in subparsers.items()}\n\tsubparser_name_to_args = {name: [] for name in subparsers.keys()}\n\t# Parse arguments in a loop until we've exhausted all arguments, or until we've found a\n\t# subparser name to focus on.\n\twhile unparsed_arguments:\n\t\t# Find the first argument that matches a subparser name.\n\t\tfor i, arg in enumerate(unparsed_arguments):\n\t\t\tif arg in subparser_names:\n\t\t\t\t# We found a subparser name.\n\t\t\t\tsubparser_name = arg\n\t\t\t\t# Collect all arguments up to this point.\n\t\t\t\tsubparser_name_to_args[subparser_name] = unparsed_arguments[:i]\n\t\t\t\t# Remove those arguments from the unparsed list.\n\t\t\t\tunparsed_arguments = unparsed_arguments\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tunparsed_arguments = list(unparsed_arguments)\n\tsubparser_names = set(subparsers.keys())\n\tsubparser_names_from_arguments = set()\n\twhile len(unparsed_arguments) > 0:\n\t\tif unparsed_arguments[0] in subparser_names:\n\t\t\tsubparser_names_from_arguments.add(unparsed_arguments[0])\n\t\tunparsed_arguments = unparsed_arguments[1:]\n\tif subparser_names_from_arguments == set():\n\t\tsubparser_names_from_arguments.add('help')\n\tparsed_arguments = {}\n\tfor subparser_name in subparser_names_from_arguments:\n\t\tsubparser = subparsers[subparser_name]\n\t\tparsed_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tunparsed_arguments = subparser.parse_known_args(un\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_arguments = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\tsubparser.add_argument('--subparser-name', action='store_const', const=subparser_name, dest='subparser_name')\n\t\tsubparser_arguments[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\tif subparser_arguments[subparser_name].subparser_name == subparser_name:\n\t\t\tbreak\n\telse:\n\t\traise ValueError('no valid subparser found')\n\tsubparser_arguments[subparser_name].subparser_name = None\n\treturn subparser_arguments, subparser_arguments[subparser_name].subparser_name, subparser_arguments[subparser_name].subparser_arguments\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tresult = {}\n\tunclaimed_arguments = []\n\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser.parse_args([argument])\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tif subparser_name not in result:\n\t\t\t\t\tresult[subparser_name] = subparser.parse_args(unparsed_arguments)\n\t\t\t\t\tbreak\n\n\t\telse:\n\t\t\tunclaimed_arguments.append(argument)\n\n\treturn result, unclaimed_arguments\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tremaining_arguments = []\n\tsubparser_arguments = {}\n\tfor subparser_name in subparsers:\n\t\tsubparser = subparsers[subparser_name]\n\t\tsubparser_arguments[subparser_name] = subparser.parse_known_args(unparsed_arguments)\n\t\tremaining_arguments += subparser_arguments[subparser_name][1]\n\treturn subparser_arguments, remaining_arguments\n\n", "is_pass": false, "return_code": 2}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription = 'Perform a number of operations on an image.')\n\n\t# Add subparsers and return them.\n\tsubparsers = parser.add_subparsers(\n\t\ttitle = 'subcommands',\n\t\tdescription = 'valid subcommands')\n\n\tsubparsers.add_parser(\n\t\t'convert',\n\t\thelp = 'convert between image formats')\n\n\tsubparsers.add_parser(\n\t\t'compare',\n\t\thelp = 'compare images')\n\n\tsubparsers.add_parser(\n\t\t'histogram',\n\t\thelp = 'generate histograms')\n\n\tsubparsers.add_parser(\n\t\t'info',\n\t\thelp = 'print information about an image')\n\n\tsubparsers.add_parser(\n\t\t'crop',\n\t\thelp = 'crop an image')\n\n\tsubparsers.add_parser(\n\t\t'help',\n\t\thelp = 'print a help message')\n\n\treturn parser, subparsers\n\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tA tool to manage a PostgreSQL database with a schema defined in a\n\t\t\tYAML configuration file.\n\t\t\"\"\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tparser.add_argument(\n\t\t'-s', '--schema',\n\t\thelp=\"\"\"\n\t\t\tThe schema file to use. If not specified, the default schema file\n\t\t\tis used.\n\t\t\"\"\",\n\t\tdefault=os.path.join(os.path.dirname(r.__file__), 'schema.yaml'),\n\t)\n\tparser.add_argument(\n\t\t'--debug',\n\t\thelp=\"\"\"\n\t\t\tIf specified, debug information will be printed.\n\t\t\"\"\",\n\t\taction='store_true'\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\thelp='Actions to perform with the database.',\n\t\tdest='action',\n\t)\n\tsubparsers\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Gather and analyze statistics about the use of \"\n\t\t\"punctuation in a text.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"Each of these subcommands takes a text file as input and \"\n\t\t\"writes the results to a text file. The output file is named after \"\n\t\t\"the input file, with the extension changed to .txt. If the input \"\n\t\t\"file is not a .txt file, then .txt is appended to the input file \"\n\t\t\"name instead.\",\n\t\tdest=\"subcommand\"\n\t)\n\tsubparsers.required = True\n\n\tparser_punct_freq = subparsers.add_parser(\n\t\t\"punct-freq\",\n\t\thelp=\"List the punctuation character frequencies.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\t\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"pipenv\",\n\t\tdescription=\"Manage Python dependencies through Pipfile & Pipfile.lock.\",\n\t\tformatter_class=PipenvHelpFormatter\n\t)\n\tparser.add_argument(\n\t\t\"--where\", action=\"store_true\", dest=\"where\",\n\t\thelp=\"Output project home information.\"\n\t)\n\tparser.add_argument(\n\t\t\"--venv\", action=\"store_true\", dest=\"venv\",\n\t\thelp=\"Output virtualenv information.\"\n\t)\n\tparser.add_argument(\n\t\t\"--py\", action=\"store_true\", dest=\"py\",\n\t\thelp=\"Output Python interpreter information.\"\n\t)\n\tparser.add_argument(\n\t\t\"--envs\", action=\"store_true\", dest=\"envs\",\n\t\thelp=\"Output Environment Variable options.\"\n\t)\n\tparser.add_argument(\n\t\t\"--rm\", action=\"store_true\", dest=\"rm\",\n\t\thelp=\"Remove the virtualenv.\"\n\t)\n\tparser.add_argument(\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tVIFF command line interface.\n\t\t\"\"\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\n\ttop_parser.add_argument(\"-v\", \"--version\", action=\"version\", version=__version__)\n\n\tsubparsers = top_parser.add_subparsers(\n\t\tdest=\"command\",\n\t\thelp=\"\"\"\n\t\tSub-commands.\n\t\t\"\"\",\n\t)\n\n\t# Server command\n\tserver_parser = subparsers.add_parser(\n\t\t\"server\",\n\t\tdescription=\"\"\"\n\t\tStart a VIFF server.\n\t\t\"\"\",\n\t\thelp=\"\"\"\n\t\tStart a VIFF server.\n\t\t\"\"\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tserver_parser.add_argument(\n\t\t\"--port\",\n\t\ttype=int,\n\t\tdefault=12345,\n\t\thelp=\"\"\"\n\t\tPort\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=__doc__,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t)\n\tsubparsers = parser.add_subparsers(dest=\"subcommand\")\n\n\t# Subcommand: \"new\"\n\tnew_parser = subparsers.add_parser(\"new\", help=\"create a new project\")\n\tnew_parser.add_argument(\"name\", help=\"project name\")\n\tnew_parser.add_argument(\"--directory\", help=\"directory to create the new project in\")\n\tnew_parser.add_argument(\"--template\", help=\"template to use for the new project\")\n\n\t# Subcommand: \"install\"\n\tinstall_parser = subparsers.add_parser(\"install\", help=\"install a project\")\n\tinstall_parser.add_argument(\"name\", help=\"name of the project to install\")\n\n\t# Subcommand: \"update\"\n\tupdate_parser = subparsers.add_parser(\"update\", help=\"update installed projects\")\n\tupdate_parser.add_argument(\"-c\", \"--check\", action=\"\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\t# A subparser for the generate command.\n\tgenerate_parser = subparsers.add_parser(\n\t\t\"generate\",\n\t\thelp=\"Generate a new key pair and print a QR code for the public key.\")\n\n\tgenerate_parser.add_argument(\n\t\t\"--key-length\",\n\t\tmetavar=\"KEYLEN\",\n\t\ttype=int,\n\t\tdefault=2048,\n\t\thelp=\"set the key length (default: 2048)\")\n\n\tgenerate_parser.add_argument(\n\t\t\"--fingerprint\",\n\t\tmetavar=\"FINGERPRINT\",\n\t\ttype=valid_fingerprint,\n\t\thelp=\"set the fingerprint for the key\")\n\n\tgenerate_parser.add_argument(\n\t\t\"--name\",\n\t\tmetavar=\"NAME\",\n\t\ttype=str,\n\t\thelp=\"set the name for the key\")\n\n\tgenerate_parser\n", "def make_parsers():\n\t# top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tprog='pydyn',\n\t\tusage='%(prog)s [subcommand] [options]',\n\t)\n\n\t# subparsers\n\tsubparsers = parser.add_subparsers(\n\t\ttitle='subcommands',\n\t\tdest='subcommand',\n\t\tmetavar='',\n\t)\n\n\t# create parser\n\tcreate_parser = subparsers.add_parser(\n\t\t'create',\n\t\thelp='create a new project',\n\t)\n\tcreate_parser.add_argument(\n\t\t'project_name',\n\t\ttype=str,\n\t\thelp='name of the project directory to create',\n\t)\n\n\t# run parser\n\trun_parser = subparsers.add_parser(\n\t\t'run',\n\t\thelp='run a simulation',\n\t)\n\t# run parser arguments\n\trun_parser.add_argument(\n\t\t'project_name',\n\t\ttype=str,\n\t\thelp='name of the project directory to\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=PROG_NAME,\n\t\tdescription=\"Manage the users and groups on a system in a flat-file \"\n\t\t\t\"database.\",\n\t\tepilog=\"See '{0} <subcommand> --help' to read about a specific \"\n\t\t\t\"subcommand.\".format(PROG_NAME),\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tadd_help=False,\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"Valid subcommands\",\n\t\thelp=\"Additional help\",\n\t)\n\n\t# Add subparsers for each subcommand.\n\tsubparsers.add_parser(\n\t\t\"addgroup\",\n\t\tdescription=\"Add a group to the system\",\n\t\tepilog=\"If GID is not specified, the next available GID will be used.\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\thelp=\"add a group to the system\n", "def make_parsers():\n\t# top level parser\n\tparser = argparse.ArgumentParser(prog='vcf_to_fasta',\n\t\t\t\t\t\t\t\t\t description='Converts a VCF file to a FASTA file.')\n\t# subparsers\n\tsubparsers = parser.add_subparsers(title='subcommands',\n\t\t\t\t\t\t\t\t\t   dest='subcommand',\n\t\t\t\t\t\t\t\t\t   metavar='subcommand',\n\t\t\t\t\t\t\t\t\t   help='subcommand help')\n\t# subcommand 'tofasta'\n\tparser_tofasta = subparsers.add_parser('tofasta',\n\t\t\t\t\t\t\t\t\t\t   help='Converts a VCF file to a FASTA file.')\n\tparser_tofasta.add_argument('vcf',\n\t\t\t\t\t\t\t\tmetavar='VCF',\n\t\t\t\t\t\t\t\thelp='VCF file to be converted to FASTA file.')\n\tparser_tofasta.add_argument('\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription = 'Perform a number of operations on an image.')\n\n\t# Add subparsers and return them.\n\tsubparsers = parser.add_subparsers(\n\t\ttitle = 'subcommands',\n\t\tdescription = 'valid subcommands')\n\n\tsubparsers.add_parser(\n\t\t'convert',\n\t\thelp = 'convert between image formats')\n\n\tsubparsers.add_parser(\n\t\t'compare',\n\t\thelp = 'compare images')\n\n\tsubparsers.add_parser(\n\t\t'histogram',\n\t\thelp = 'generate histograms')\n\n\tsubparsers.add_parser(\n\t\t'info',\n\t\thelp = 'print information about an image')\n\n\tsubparsers.add_parser(\n\t\t'crop',\n\t\thelp = 'crop an image')\n\n\tsubparsers.add_parser(\n\t\t'help',\n\t\thelp = 'print a help message')\n\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tA tool to manage a PostgreSQL database with a schema defined in a\n\t\t\tYAML configuration file.\n\t\t\"\"\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tparser.add_argument(\n\t\t'-s', '--schema',\n\t\thelp=\"\"\"\n\t\t\tThe schema file to use. If not specified, the default schema file\n\t\t\tis used.\n\t\t\"\"\",\n\t\tdefault=os.path.join(os.path.dirname(r.__file__), 'schema.yaml'),\n\t)\n\tparser.add_argument(\n\t\t'--debug',\n\t\thelp=\"\"\"\n\t\t\tIf specified, debug information will be printed.\n\t\t\"\"\",\n\t\taction='store_true'\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\thelp='Actions to perform with the database.',\n\t\tdest='action',\n\t)\n\tsubparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Gather and analyze statistics about the use of \"\n\t\t\"punctuation in a text.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"Each of these subcommands takes a text file as input and \"\n\t\t\"writes the results to a text file. The output file is named after \"\n\t\t\"the input file, with the extension changed to .txt. If the input \"\n\t\t\"file is not a .txt file, then .txt is appended to the input file \"\n\t\t\"name instead.\",\n\t\tdest=\"subcommand\"\n\t)\n\tsubparsers.required = True\n\n\tparser_punct_freq = subparsers.add_parser(\n\t\t\"punct-freq\",\n\t\thelp=\"List the punctuation character frequencies.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"pipenv\",\n\t\tdescription=\"Manage Python dependencies through Pipfile & Pipfile.lock.\",\n\t\tformatter_class=PipenvHelpFormatter\n\t)\n\tparser.add_argument(\n\t\t\"--where\", action=\"store_true\", dest=\"where\",\n\t\thelp=\"Output project home information.\"\n\t)\n\tparser.add_argument(\n\t\t\"--venv\", action=\"store_true\", dest=\"venv\",\n\t\thelp=\"Output virtualenv information.\"\n\t)\n\tparser.add_argument(\n\t\t\"--py\", action=\"store_true\", dest=\"py\",\n\t\thelp=\"Output Python interpreter information.\"\n\t)\n\tparser.add_argument(\n\t\t\"--envs\", action=\"store_true\", dest=\"envs\",\n\t\thelp=\"Output Environment Variable options.\"\n\t)\n\tparser.add_argument(\n\t\t\"--rm\", action=\"store_true\", dest=\"rm\",\n\t\thelp=\"Remove the virtualenv.\"\n\t)\n\tparser.add_argument(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tVIFF command line interface.\n\t\t\"\"\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\n\ttop_parser.add_argument(\"-v\", \"--version\", action=\"version\", version=__version__)\n\n\tsubparsers = top_parser.add_subparsers(\n\t\tdest=\"command\",\n\t\thelp=\"\"\"\n\t\tSub-commands.\n\t\t\"\"\",\n\t)\n\n\t# Server command\n\tserver_parser = subparsers.add_parser(\n\t\t\"server\",\n\t\tdescription=\"\"\"\n\t\tStart a VIFF server.\n\t\t\"\"\",\n\t\thelp=\"\"\"\n\t\tStart a VIFF server.\n\t\t\"\"\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter,\n\t)\n\tserver_parser.add_argument(\n\t\t\"--port\",\n\t\ttype=int,\n\t\tdefault=12345,\n\t\thelp=\"\"\"\n\t\tPort\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=__doc__,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t)\n\tsubparsers = parser.add_subparsers(dest=\"subcommand\")\n\n\t# Subcommand: \"new\"\n\tnew_parser = subparsers.add_parser(\"new\", help=\"create a new project\")\n\tnew_parser.add_argument(\"name\", help=\"project name\")\n\tnew_parser.add_argument(\"--directory\", help=\"directory to create the new project in\")\n\tnew_parser.add_argument(\"--template\", help=\"template to use for the new project\")\n\n\t# Subcommand: \"install\"\n\tinstall_parser = subparsers.add_parser(\"install\", help=\"install a project\")\n\tinstall_parser.add_argument(\"name\", help=\"name of the project to install\")\n\n\t# Subcommand: \"update\"\n\tupdate_parser = subparsers.add_parser(\"update\", help=\"update installed projects\")\n\tupdate_parser.add_argument(\"-c\", \"--check\", action=\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tsubparsers = parser.add_subparsers(dest=\"command\")\n\n\t# A subparser for the generate command.\n\tgenerate_parser = subparsers.add_parser(\n\t\t\"generate\",\n\t\thelp=\"Generate a new key pair and print a QR code for the public key.\")\n\n\tgenerate_parser.add_argument(\n\t\t\"--key-length\",\n\t\tmetavar=\"KEYLEN\",\n\t\ttype=int,\n\t\tdefault=2048,\n\t\thelp=\"set the key length (default: 2048)\")\n\n\tgenerate_parser.add_argument(\n\t\t\"--fingerprint\",\n\t\tmetavar=\"FINGERPRINT\",\n\t\ttype=valid_fingerprint,\n\t\thelp=\"set the fingerprint for the key\")\n\n\tgenerate_parser.add_argument(\n\t\t\"--name\",\n\t\tmetavar=\"NAME\",\n\t\ttype=str,\n\t\thelp=\"set the name for the key\")\n\n\tgenerate_parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t# top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tprog='pydyn',\n\t\tusage='%(prog)s [subcommand] [options]',\n\t)\n\n\t# subparsers\n\tsubparsers = parser.add_subparsers(\n\t\ttitle='subcommands',\n\t\tdest='subcommand',\n\t\tmetavar='',\n\t)\n\n\t# create parser\n\tcreate_parser = subparsers.add_parser(\n\t\t'create',\n\t\thelp='create a new project',\n\t)\n\tcreate_parser.add_argument(\n\t\t'project_name',\n\t\ttype=str,\n\t\thelp='name of the project directory to create',\n\t)\n\n\t# run parser\n\trun_parser = subparsers.add_parser(\n\t\t'run',\n\t\thelp='run a simulation',\n\t)\n\t# run parser arguments\n\trun_parser.add_argument(\n\t\t'project_name',\n\t\ttype=str,\n\t\thelp='name of the project directory to\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=PROG_NAME,\n\t\tdescription=\"Manage the users and groups on a system in a flat-file \"\n\t\t\t\"database.\",\n\t\tepilog=\"See '{0} <subcommand> --help' to read about a specific \"\n\t\t\t\"subcommand.\".format(PROG_NAME),\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tadd_help=False,\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"subcommands\",\n\t\tdescription=\"Valid subcommands\",\n\t\thelp=\"Additional help\",\n\t)\n\n\t# Add subparsers for each subcommand.\n\tsubparsers.add_parser(\n\t\t\"addgroup\",\n\t\tdescription=\"Add a group to the system\",\n\t\tepilog=\"If GID is not specified, the next available GID will be used.\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\thelp=\"add a group to the system\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t# top level parser\n\tparser = argparse.ArgumentParser(prog='vcf_to_fasta',\n\t\t\t\t\t\t\t\t\t description='Converts a VCF file to a FASTA file.')\n\t# subparsers\n\tsubparsers = parser.add_subparsers(title='subcommands',\n\t\t\t\t\t\t\t\t\t   dest='subcommand',\n\t\t\t\t\t\t\t\t\t   metavar='subcommand',\n\t\t\t\t\t\t\t\t\t   help='subcommand help')\n\t# subcommand 'tofasta'\n\tparser_tofasta = subparsers.add_parser('tofasta',\n\t\t\t\t\t\t\t\t\t\t   help='Converts a VCF file to a FASTA file.')\n\tparser_tofasta.add_argument('vcf',\n\t\t\t\t\t\t\t\tmetavar='VCF',\n\t\t\t\t\t\t\t\thelp='VCF file to be converted to FASTA file.')\n\tparser_tofasta.add_argument('\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_10.py", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "ques_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n\twhile True:\n\t\tfor r in response:\n\t\t\tyield r\n\t\tif '_next_id' not in response._meta:\n\t\t\tbreak\n\t\tnext_id = response._meta['_next_id']\n\t\tresponse = infoblox.get_object(object_type, return_fields=return_fields, max_results=max_results, _max_results=max_results, _return_fields=return_fields, _paging_identifier=next_id)\n\n", "def paging(response, max_results):\n\tif max_results > 0:\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n\telse:\n\t\tyield response\n\n\n", "def paging(response, max_results):\n\tif max_results is None or max_results > 500:\n\t\tmax_results = 500\n\tif 'meta' not in response:\n\t\tyield response\n\telse:\n\t\ttotal_count = response['meta']['total_count']\n\t\tif total_count <= max_results:\n\t\t\tyield response\n\t\telse:\n\t\t\tfor offset in range(0, total_count, max_results):\n\t\t\t\tfor res in response['objects']:\n\t\t\t\t\tyield res\n\t\t\t\tresponse = wapi_get('/wapi/v2.7.1/' + response['_ref'].split('/')[-1],\n\t\t\t\t                    params={'_paging': '1', '_return_as_object': '1',\n\t\t\t\t                            '_max_results': str(max_results), '_return_fields+': '',\n\t\t\t\t                            '_offset': str(offset + max_results)})\n\t\t\t\tif '_ref' not in response or response['_ref']\n", "def paging(response, max_results):\n\toffset = 0\n\twhile True:\n\t\tresponse_page = response[offset:offset + max_results]\n\t\tif not response_page:\n\t\t\tbreak\n\t\tyield response_page\n\t\toffset += max_results\n\n", "def paging(response, max_results):\n\tif max_results is not None:\n\t\tresponse = response[:max_results]\n\tfor i in xrange(0, len(response), 200):\n\t\tyield response[i:i + 200]\n\n\n", "def paging(response, max_results):\n\tif len(response) > max_results:\n\t\tfor page in range(0, len(response), max_results):\n\t\t\tyield response[page:page + max_results]\n\telse:\n\t\tyield response\n\n\n", "def paging(response, max_results):\n\t# Get total number of objects in response\n\ttotal_objects = response.total_objects\n\t# Get total number of pages in response\n\ttotal_pages = response.total_pages\n\t# Get current page\n\tcurrent_page = response.current_page\n\t# Get number of objects in current page\n\tcurrent_objects = response.current_objects\n\t# Get number of objects in current page\n\tcurrent_objects = response.current_objects\n\t# Compute the number of pages to be returned\n\tif current_objects <= max_results:\n\t\tpages_to_return = total_pages\n\telse:\n\t\tpages_to_return = math.ceil(current_objects / max_results)\n\t# Return pages\n\tfor page in range(1, pages_to_return + 1):\n\t\t# Compute the number of objects to be returned in this page\n\t\tif page == pages_to_return:\n\t\t\tobjects_to_return = current_objects - (max_results * (page - 1))\n\t\telse:\n\t\t\tobjects_to_return = max_results\n\t\t#\n", "def paging(response, max_results):\n\tmax_pages = math.ceil(response[0][\"total_count\"]/max_results)\n\tprint(\"Total pages: {}\".format(max_pages))\n\tfor page_num in range(0, max_pages):\n\t\tprint(\"Page: {}\".format(page_num))\n\t\tresponse = fw.get(\n\t\t\t\"record:a\",\n\t\t\t{\"_paging\": \"1\", \"_return_as_object\": \"1\", \"max_results\": max_results, \"offset\": page_num*max_results}\n\t\t)\n\t\tyield response\n\n", "def paging(response, max_results):\n\twhile len(response) > 0:\n\t\tresponse = infoblox.paging(response, max_results)\n\t\tyield response\n\n\n", "def paging(response, max_results):\n\tpage = []\n\tfor item in response:\n\t\tpage.append(item)\n\t\tif len(page) == max_results:\n\t\t\tyield page\n\t\t\tpage = []\n\tyield page\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n\twhile True:\n\t\tfor r in response:\n\t\t\tyield r\n\t\tif '_next_id' not in response._meta:\n\t\t\tbreak\n\t\tnext_id = response._meta['_next_id']\n\t\tresponse = infoblox.get_object(object_type, return_fields=return_fields, max_results=max_results, _max_results=max_results, _return_fields=return_fields, _paging_identifier=next_id)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\tif max_results > 0:\n\t\tfor i in range(0, len(response), max_results):\n\t\t\tyield response[i:i + max_results]\n\telse:\n\t\tyield response\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\tif max_results is None or max_results > 500:\n\t\tmax_results = 500\n\tif 'meta' not in response:\n\t\tyield response\n\telse:\n\t\ttotal_count = response['meta']['total_count']\n\t\tif total_count <= max_results:\n\t\t\tyield response\n\t\telse:\n\t\t\tfor offset in range(0, total_count, max_results):\n\t\t\t\tfor res in response['objects']:\n\t\t\t\t\tyield res\n\t\t\t\tresponse = wapi_get('/wapi/v2.7.1/' + response['_ref'].split('/')[-1],\n\t\t\t\t                    params={'_paging': '1', '_return_as_object': '1',\n\t\t\t\t                            '_max_results': str(max_results), '_return_fields+': '',\n\t\t\t\t                            '_offset': str(offset + max_results)})\n\t\t\t\tif '_ref' not in response or response['_ref']\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\toffset = 0\n\twhile True:\n\t\tresponse_page = response[offset:offset + max_results]\n\t\tif not response_page:\n\t\t\tbreak\n\t\tyield response_page\n\t\toffset += max_results\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\tif max_results is not None:\n\t\tresponse = response[:max_results]\n\tfor i in xrange(0, len(response), 200):\n\t\tyield response[i:i + 200]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\tif len(response) > max_results:\n\t\tfor page in range(0, len(response), max_results):\n\t\t\tyield response[page:page + max_results]\n\telse:\n\t\tyield response\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t# Get total number of objects in response\n\ttotal_objects = response.total_objects\n\t# Get total number of pages in response\n\ttotal_pages = response.total_pages\n\t# Get current page\n\tcurrent_page = response.current_page\n\t# Get number of objects in current page\n\tcurrent_objects = response.current_objects\n\t# Get number of objects in current page\n\tcurrent_objects = response.current_objects\n\t# Compute the number of pages to be returned\n\tif current_objects <= max_results:\n\t\tpages_to_return = total_pages\n\telse:\n\t\tpages_to_return = math.ceil(current_objects / max_results)\n\t# Return pages\n\tfor page in range(1, pages_to_return + 1):\n\t\t# Compute the number of objects to be returned in this page\n\t\tif page == pages_to_return:\n\t\t\tobjects_to_return = current_objects - (max_results * (page - 1))\n\t\telse:\n\t\t\tobjects_to_return = max_results\n\t\t#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\tmax_pages = math.ceil(response[0][\"total_count\"]/max_results)\n\tprint(\"Total pages: {}\".format(max_pages))\n\tfor page_num in range(0, max_pages):\n\t\tprint(\"Page: {}\".format(page_num))\n\t\tresponse = fw.get(\n\t\t\t\"record:a\",\n\t\t\t{\"_paging\": \"1\", \"_return_as_object\": \"1\", \"max_results\": max_results, \"offset\": page_num*max_results}\n\t\t)\n\t\tyield response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\twhile len(response) > 0:\n\t\tresponse = infoblox.paging(response, max_results)\n\t\tyield response\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\tpage = []\n\tfor item in response:\n\t\tpage.append(item)\n\t\tif len(page) == max_results:\n\t\t\tyield page\n\t\t\tpage = []\n\tyield page\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_20.py", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "ques_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif re.match(r'^-?\\d+$', size):\n\t\treturn int(size)\n\telif re.match(r'^-?\\d+[KMGTPEZY]?$', size):\n\t\tmatches = re.match(r'^-?(\\d+)([KMGTPEZY])?$', size)\n\t\tnum, prefix = int(matches.group(1)), matches.group(2)\n\t\tif num < 0:\n\t\t\treturn -size_to_bytes(str(-num) + prefix)\n\t\telif prefix == 'K':\n\t\t\treturn num * 1000\n\t\telif prefix == 'M':\n\t\t\treturn num * 1000 * 1000\n\t\telif prefix == 'G':\n\t\t\treturn num * 1000 * 1000 * 1000\n\t\telif prefix == 'T':\n\t\t\treturn num * 1000 * 1000 * 1000 * 1000\n\t\telif prefix == 'P':\n\t\t\treturn num * 1000 * 1000 * 1000 * 1000 * 1000\n\t\t\n", "def size_to_bytes(size: str) -> int:\n\ttry:\n\t\treturn int(size)\n\texcept ValueError:\n\t\tpass\n\n\tunit_multiplier = {\n\t\t\"K\": 1000,\n\t\t\"M\": 1000000,\n\t\t\"G\": 1000000000,\n\t\t\"T\": 1000000000000,\n\t}\n\n\tunit = size[-1].upper()\n\tif unit not in unit_multiplier:\n\t\traise ValueError(\n\t\t\tf\"Invalid size unit: '{unit}'. Valid units are: {unit_multiplier.keys()}\"\n\t\t)\n\tmultiplier = unit_multiplier[unit]\n\tnumber = int(size[:-1])\n\treturn round(number * multiplier)\n\n\n", "def size_to_bytes(size: str) -> int:\n\tif size.isdigit():\n\t\treturn int(size)\n\telse:\n\t\tsize = size.upper()\n\t\tif size.endswith(\"B\"):\n\t\t\tsize = size[:-1]\n\t\tif size.endswith(\"K\"):\n\t\t\treturn int(size[:-1]) * 10**3\n\t\tif size.endswith(\"M\"):\n\t\t\treturn int(size[:-1]) * 10**6\n\t\tif size.endswith(\"G\"):\n\t\t\treturn int(size[:-1]) * 10**9\n", "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tsize_mapping = {\n\t\t\"B\": 1,\n\t\t\"K\": 1024,\n\t\t\"M\": 1024*1024,\n\t\t\"G\": 1024*1024*1024,\n\t\t\"T\": 1024*1024*1024*1024,\n\t}\n\tif size[-1] in size_mapping:\n\t\tsize = int(size[:-1]) * size_mapping[size[-1]]\n\telse:\n\t\tsize = int(size)\n\treturn size\n\n", "def size_to_bytes(size: str) -> int:\n\tif not isinstance(size, str):\n\t\traise TypeError(\"size_to_bytes: size must be a string\")\n\tif size.isdigit():\n\t\treturn int(size)\n\telse:\n\t\tsize_regexp = re.compile(r\"([0-9]+)\\s*(?:([kmgtpezy]?i?b))?$\", re.IGNORECASE)\n\t\tsize_match = size_regexp.match(size)\n\t\tif size_match:\n\t\t\tsize = float(size_match.group(1))\n\t\t\tif size_match.group(2):\n\t\t\t\tunit = size_match.group(2).lower()\n\t\t\t\tif unit == \"b\":\n\t\t\t\t\treturn int(size)\n\t\t\t\telse:\n\t\t\t\t\tunits = [\"k\", \"m\", \"g\", \"t\", \"p\", \"e\", \"z\", \"y\"]\n\t\t\t\t\texponent = units.index(unit[0]) + 1\n\t\t\t\t\tif unit\n", "def size_to_bytes(size: str) -> int:\n\tif not isinstance(size, str):\n\t\traise TypeError(\"Unsupported type: '%s'\" % type(size))\n\n\tsize = size.upper()\n\tif not size.isdigit():\n\t\tif size[-1] not in [\"K\", \"M\", \"G\", \"T\"]:\n\t\t\traise ValueError(\"Unknown size: '%s'\" % size)\n\t\tif not size[:-1].isdigit():\n\t\t\traise ValueError(\"Unknown size: '%s'\" % size)\n\n\t\tfactor = 1024\n\t\tif size[-1] == \"K\":\n\t\t\texponent = 1\n\t\telif size[-1] == \"M\":\n\t\t\texponent = 2\n\t\telif size[-1] == \"G\":\n\t\t\texponent = 3\n\t\telif size[-1] == \"T\":\n\t\t\texponent = 4\n\t\telse:\n\t\t\traise ValueError(\"Unknown size: '%s'\" % size)\n\n\t\treturn int(float(size[:-1]) * (factor ** exponent))\n", "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif not re.search(r'^\\d+[KMGT]?$', size):\n\t\traise ValueError(\"Invalid file size: %s\" % size)\n\n\t# Remove the non-digit characters from the string.\n\tsize = re.sub(\"\\D\", \"\", size)\n\n\tmultiplier = 1\n\tif size[-1] == 'K':\n\t\tmultiplier = 1000\n\t\tsize = size[:-1]\n\telif size[-1] == 'M':\n\t\tmultiplier = 1000 * 1000\n\t\tsize = size[:-1]\n\telif size[-1] == 'G':\n\t\tmultiplier = 1000 * 1000 * 1000\n\t\tsize = size[:-1]\n\telif size[-1] == 'T':\n\t\tmultiplier = 1000 * 1000 * 1000 * 1000\n\t\tsize = size[:-1]\n\n\tsize = int(size)\n\tsize *= multiplier\n\n\treturn size\n\n\n", "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tnumber = int(float(size[:-1]))\n\tunit = size[-1]\n\tif unit == 'K':\n\t\treturn number * 1000\n\telif unit == 'M':\n\t\treturn number * 1000 * 1000\n\telif unit == 'G':\n\t\treturn number * 1000 * 1000 * 1000\n\telif unit == 'T':\n\t\treturn number * 1000 * 1000 * 1000 * 1000\n\telif unit == 'P':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000\n\telif unit == 'E':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000 * 1000\n\telif unit == 'Z':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000 * 1000 * 1000\n\telif unit == 'Y':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000 * 1000 * 1000 * 1000\n\telse:\n\t\treturn number\n\n\n", "def size_to_bytes(size: str) -> int:\n\tif size == \"\":\n\t\treturn 0\n\n\tsuffixes = {\n\t\t\"K\": 1000,\n\t\t\"M\": 1000 * 1000,\n\t\t\"G\": 1000 * 1000 * 1000,\n\t\t\"T\": 1000 * 1000 * 1000 * 1000,\n\t\t\"P\": 1000 * 1000 * 1000 * 1000 * 1000,\n\t}\n\n\tmultiplier = 1\n\tsuffix = size[-1]\n\tif suffix in suffixes:\n\t\tmultiplier = suffixes[suffix]\n\t\tsize = size[:-1]\n\treturn int(size) * multiplier\n\n\n", "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif size.endswith(\"K\"):\n\t\treturn int(size[:-1]) * 1000\n\telif size.endswith(\"M\"):\n\t\treturn int(size[:-1]) * 1000 * 1000\n\telif size.endswith(\"G\"):\n\t\treturn int(size[:-1]) * 1000 * 1000 * 1000\n\telif size.endswith(\"T\"):\n\t\treturn int(size[:-1]) * 1000 * 1000 * 1000 * 1000\n\telse:\n\t\treturn int(size)\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif re.match(r'^-?\\d+$', size):\n\t\treturn int(size)\n\telif re.match(r'^-?\\d+[KMGTPEZY]?$', size):\n\t\tmatches = re.match(r'^-?(\\d+)([KMGTPEZY])?$', size)\n\t\tnum, prefix = int(matches.group(1)), matches.group(2)\n\t\tif num < 0:\n\t\t\treturn -size_to_bytes(str(-num) + prefix)\n\t\telif prefix == 'K':\n\t\t\treturn num * 1000\n\t\telif prefix == 'M':\n\t\t\treturn num * 1000 * 1000\n\t\telif prefix == 'G':\n\t\t\treturn num * 1000 * 1000 * 1000\n\t\telif prefix == 'T':\n\t\t\treturn num * 1000 * 1000 * 1000 * 1000\n\t\telif prefix == 'P':\n\t\t\treturn num * 1000 * 1000 * 1000 * 1000 * 1000\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\ttry:\n\t\treturn int(size)\n\texcept ValueError:\n\t\tpass\n\n\tunit_multiplier = {\n\t\t\"K\": 1000,\n\t\t\"M\": 1000000,\n\t\t\"G\": 1000000000,\n\t\t\"T\": 1000000000000,\n\t}\n\n\tunit = size[-1].upper()\n\tif unit not in unit_multiplier:\n\t\traise ValueError(\n\t\t\tf\"Invalid size unit: '{unit}'. Valid units are: {unit_multiplier.keys()}\"\n\t\t)\n\tmultiplier = unit_multiplier[unit]\n\tnumber = int(size[:-1])\n\treturn round(number * multiplier)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tif size.isdigit():\n\t\treturn int(size)\n\telse:\n\t\tsize = size.upper()\n\t\tif size.endswith(\"B\"):\n\t\t\tsize = size[:-1]\n\t\tif size.endswith(\"K\"):\n\t\t\treturn int(size[:-1]) * 10**3\n\t\tif size.endswith(\"M\"):\n\t\t\treturn int(size[:-1]) * 10**6\n\t\tif size.endswith(\"G\"):\n\t\t\treturn int(size[:-1]) * 10**9\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tsize_mapping = {\n\t\t\"B\": 1,\n\t\t\"K\": 1024,\n\t\t\"M\": 1024*1024,\n\t\t\"G\": 1024*1024*1024,\n\t\t\"T\": 1024*1024*1024*1024,\n\t}\n\tif size[-1] in size_mapping:\n\t\tsize = int(size[:-1]) * size_mapping[size[-1]]\n\telse:\n\t\tsize = int(size)\n\treturn size\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tif not isinstance(size, str):\n\t\traise TypeError(\"size_to_bytes: size must be a string\")\n\tif size.isdigit():\n\t\treturn int(size)\n\telse:\n\t\tsize_regexp = re.compile(r\"([0-9]+)\\s*(?:([kmgtpezy]?i?b))?$\", re.IGNORECASE)\n\t\tsize_match = size_regexp.match(size)\n\t\tif size_match:\n\t\t\tsize = float(size_match.group(1))\n\t\t\tif size_match.group(2):\n\t\t\t\tunit = size_match.group(2).lower()\n\t\t\t\tif unit == \"b\":\n\t\t\t\t\treturn int(size)\n\t\t\t\telse:\n\t\t\t\t\tunits = [\"k\", \"m\", \"g\", \"t\", \"p\", \"e\", \"z\", \"y\"]\n\t\t\t\t\texponent = units.index(unit[0]) + 1\n\t\t\t\t\tif unit\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tif not isinstance(size, str):\n\t\traise TypeError(\"Unsupported type: '%s'\" % type(size))\n\n\tsize = size.upper()\n\tif not size.isdigit():\n\t\tif size[-1] not in [\"K\", \"M\", \"G\", \"T\"]:\n\t\t\traise ValueError(\"Unknown size: '%s'\" % size)\n\t\tif not size[:-1].isdigit():\n\t\t\traise ValueError(\"Unknown size: '%s'\" % size)\n\n\t\tfactor = 1024\n\t\tif size[-1] == \"K\":\n\t\t\texponent = 1\n\t\telif size[-1] == \"M\":\n\t\t\texponent = 2\n\t\telif size[-1] == \"G\":\n\t\t\texponent = 3\n\t\telif size[-1] == \"T\":\n\t\t\texponent = 4\n\t\telse:\n\t\t\traise ValueError(\"Unknown size: '%s'\" % size)\n\n\t\treturn int(float(size[:-1]) * (factor ** exponent))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif not re.search(r'^\\d+[KMGT]?$', size):\n\t\traise ValueError(\"Invalid file size: %s\" % size)\n\n\t# Remove the non-digit characters from the string.\n\tsize = re.sub(\"\\D\", \"\", size)\n\n\tmultiplier = 1\n\tif size[-1] == 'K':\n\t\tmultiplier = 1000\n\t\tsize = size[:-1]\n\telif size[-1] == 'M':\n\t\tmultiplier = 1000 * 1000\n\t\tsize = size[:-1]\n\telif size[-1] == 'G':\n\t\tmultiplier = 1000 * 1000 * 1000\n\t\tsize = size[:-1]\n\telif size[-1] == 'T':\n\t\tmultiplier = 1000 * 1000 * 1000 * 1000\n\t\tsize = size[:-1]\n\n\tsize = int(size)\n\tsize *= multiplier\n\n\treturn size\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tnumber = int(float(size[:-1]))\n\tunit = size[-1]\n\tif unit == 'K':\n\t\treturn number * 1000\n\telif unit == 'M':\n\t\treturn number * 1000 * 1000\n\telif unit == 'G':\n\t\treturn number * 1000 * 1000 * 1000\n\telif unit == 'T':\n\t\treturn number * 1000 * 1000 * 1000 * 1000\n\telif unit == 'P':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000\n\telif unit == 'E':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000 * 1000\n\telif unit == 'Z':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000 * 1000 * 1000\n\telif unit == 'Y':\n\t\treturn number * 1000 * 1000 * 1000 * 1000 * 1000 * 1000 * 1000 * 1000\n\telse:\n\t\treturn number\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tif size == \"\":\n\t\treturn 0\n\n\tsuffixes = {\n\t\t\"K\": 1000,\n\t\t\"M\": 1000 * 1000,\n\t\t\"G\": 1000 * 1000 * 1000,\n\t\t\"T\": 1000 * 1000 * 1000 * 1000,\n\t\t\"P\": 1000 * 1000 * 1000 * 1000 * 1000,\n\t}\n\n\tmultiplier = 1\n\tsuffix = size[-1]\n\tif suffix in suffixes:\n\t\tmultiplier = suffixes[suffix]\n\t\tsize = size[:-1]\n\treturn int(size) * multiplier\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif size.endswith(\"K\"):\n\t\treturn int(size[:-1]) * 1000\n\telif size.endswith(\"M\"):\n\t\treturn int(size[:-1]) * 1000 * 1000\n\telif size.endswith(\"G\"):\n\t\treturn int(size[:-1]) * 1000 * 1000 * 1000\n\telif size.endswith(\"T\"):\n\t\treturn int(size[:-1]) * 1000 * 1000 * 1000 * 1000\n\telse:\n\t\treturn int(size)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_14.py", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "ques_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n\n", "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n\n\n", "def _dictsum(dicts):\n\tresult = defaultdict(int)\n\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] += v\n\n\treturn result\n\n", "def _dictsum(dicts):\n\tif not dicts:\n\t\treturn {}\n\tif len(dicts) == 1:\n\t\treturn dicts[0]\n\n\tret = {}\n\tfor d in dicts:\n\t\tfor k, v in d.iteritems():\n\t\t\tret[k] = ret.get(k, 0) + v\n\treturn ret\n\n", "def _dictsum(dicts):\n\tr = {}\n\tfor d in dicts:\n\t\tfor key, value in d.iteritems():\n\t\t\tr[key] = r.get(key, 0) + value\n\treturn r\n\n", "def _dictsum(dicts):\n\tif not dicts:\n\t\treturn {}\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n\n", "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = v + result.get(k, 0)\n\treturn result\n\n", "def _dictsum(dicts):\n\td = {}\n\tfor k in dicts[0]:\n\t\td[k] = sum(d.get(k, 0) + dic.get(k, 0) for dic in dicts)\n\treturn d\n\n", "def _dictsum(dicts):\n\treturn dict(map(lambda x: (x[0], sum(x[1])),\n\t\t  reduce(lambda x, y: x + y,\n\t\t\t map(lambda d: d.items(), dicts)).__iter__()))\n\n", "def _dictsum(dicts):\n\tret = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k in ret:\n\t\t\t\tret[k] += d[k]\n\t\t\telse:\n\t\t\t\tret[k] = d[k]\n\treturn ret\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tresult = defaultdict(int)\n\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] += v\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\tif not dicts:\n\t\treturn {}\n\tif len(dicts) == 1:\n\t\treturn dicts[0]\n\n\tret = {}\n\tfor d in dicts:\n\t\tfor k, v in d.iteritems():\n\t\t\tret[k] = ret.get(k, 0) + v\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\tr = {}\n\tfor d in dicts:\n\t\tfor key, value in d.iteritems():\n\t\t\tr[key] = r.get(key, 0) + value\n\treturn r\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\tif not dicts:\n\t\treturn {}\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = result.get(k, 0) + v\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tresult[k] = v + result.get(k, 0)\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\td = {}\n\tfor k in dicts[0]:\n\t\td[k] = sum(d.get(k, 0) + dic.get(k, 0) for dic in dicts)\n\treturn d\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\treturn dict(map(lambda x: (x[0], sum(x[1])),\n\t\t  reduce(lambda x, y: x + y,\n\t\t\t map(lambda d: d.items(), dicts)).__iter__()))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\tret = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k in ret:\n\t\t\t\tret[k] += d[k]\n\t\t\telse:\n\t\t\t\tret[k] = d[k]\n\treturn ret\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_15.py", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "ques_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor arg, value in url_args.items():\n\t\t\turl = url.replace(arg, value)\n\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif url_args is None:\n\t\treturn url\n\tfor arg in url_args:\n\t\turl = url.replace(\"$%s\" % arg, url_args[arg])\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\t_url = url\n\tfor key, value in url_args.iteritems():\n\t\t_url = _url.replace(\"<%s>\" % key, value)\n\treturn _url\n\n\n", "def _replace_url_args(url, url_args):\n\tfor key, value in url_args.items():\n\t\turl = url.replace('<{}>'.format(key), value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\tfor arg in url_args:\n\t\turl = url.replace(\"<{0}>\".format(arg), str(url_args[arg]))\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tfor key, val in url_args.items():\n\t\turl = url.replace(key, str(val))\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\tfor item in url_args:\n\t\turl = url.replace(item, url_args[item])\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tfor key, value in url_args.items():\n\t\turl = url.replace(key, value)\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\tif url.find('{') != -1:\n\t\tfor key in url_args:\n\t\t\turl = url.replace('{' + key + '}', str(url_args[key]))\n\treturn url\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor arg, value in url_args.items():\n\t\t\turl = url.replace(arg, value)\n\n\treturn url\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args is None:\n\t\treturn url\n\tfor arg in url_args:\n\t\turl = url.replace(\"$%s\" % arg, url_args[arg])\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\t_url = url\n\tfor key, value in url_args.iteritems():\n\t\t_url = _url.replace(\"<%s>\" % key, value)\n\treturn _url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tfor key, value in url_args.items():\n\t\turl = url.replace('<{}>'.format(key), value)\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tfor arg in url_args:\n\t\turl = url.replace(\"<{0}>\".format(arg), str(url_args[arg]))\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tfor key, val in url_args.items():\n\t\turl = url.replace(key, str(val))\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tfor item in url_args:\n\t\turl = url.replace(item, url_args[item])\n\treturn url\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tfor key, value in url_args.items():\n\t\turl = url.replace(key, value)\n\treturn url\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url.find('{') != -1:\n\t\tfor key in url_args:\n\t\t\turl = url.replace('{' + key + '}', str(url_args[key]))\n\treturn url\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_5.py", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "ques_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: any) -> bool:\n\treturn val is None or val == 'None' or val == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn val is None or (isinstance(val, str) and val.strip().upper() == \"NONE\")\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn val not in _none_strings\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == \"none\"\n\n\n", "def is_none_string(val: any) -> bool:\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val.lower() in ['none', 'null', 'n/a', '']\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn val in (None, 'None', 'none', 'NONE', '', 'Null', 'NULL', 'null', 'NULL', 'nan', 'NaN', 'NAN', 'NA', 'na')\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn val is None or val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() in [\"none\", \"null\", \"nil\"]\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn val.lower() in [\"none\", \"null\", \"\"]\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn val == \"None\"\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: any) -> bool:\n\treturn val is None or val == 'None' or val == 'none'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn val is None or (isinstance(val, str) and val.strip().upper() == \"NONE\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn val not in _none_strings\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == \"none\"\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\tif not isinstance(val, str):\n\t\treturn False\n\treturn val.lower() in ['none', 'null', 'n/a', '']\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn val in (None, 'None', 'none', 'NONE', '', 'Null', 'NULL', 'null', 'NULL', 'nan', 'NaN', 'NAN', 'NA', 'na')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn val is None or val.lower() == 'none'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() in [\"none\", \"null\", \"nil\"]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn val.lower() in [\"none\", \"null\", \"\"]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn val == \"None\"\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_1.py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "ques_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise Exception(\"Path %s does not start with root %s\" % (path, root))\n\treturn path[len(root):]\n\n", "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise ValueError('%s is not a subpath of %s' % (path, root))\n\treturn path[len(root):]\n\n", "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise Exception('%s is not a subpath of %s' % (path, root))\n\treturn path[len(root):]\n\n\n", "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise ValueError(\"%s does not start with %s\" % (path, root))\n\treturn path[len(root):]\n\n\n", "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise ValueError(\"Path does not start with root: %s.\" % path)\n\treturn path[len(root):]\n\n", "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):].lstrip('/')\n\telse:\n\t\traise ValueError(\"'%s' is not a subpath of '%s'\" % (path, root))\n\n\n", "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise ValueError(path)\n\n\n", "def strip_root(path, root):\n\treturn path[len(root) + 1:]\n\n", "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\n\traise ValueError(path)\n\n\n", "def strip_root(path, root):\n\tif root == os.path.dirname(path):\n\t\treturn os.path.basename(path)\n\telif root == path:\n\t\treturn ''\n\n\tif not path.startswith(root):\n\t\traise Exception('path does not start with root')\n\n\treturn path[len(root):].lstrip('/')\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise Exception(\"Path %s does not start with root %s\" % (path, root))\n\treturn path[len(root):]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise ValueError('%s is not a subpath of %s' % (path, root))\n\treturn path[len(root):]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise Exception('%s is not a subpath of %s' % (path, root))\n\treturn path[len(root):]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise ValueError(\"%s does not start with %s\" % (path, root))\n\treturn path[len(root):]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise ValueError(\"Path does not start with root: %s.\" % path)\n\treturn path[len(root):]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):].lstrip('/')\n\telse:\n\t\traise ValueError(\"'%s' is not a subpath of '%s'\" % (path, root))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise ValueError(path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\treturn path[len(root) + 1:]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\n\traise ValueError(path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif root == os.path.dirname(path):\n\t\treturn os.path.basename(path)\n\telif root == path:\n\t\treturn ''\n\n\tif not path.startswith(root):\n\t\traise Exception('path does not start with root')\n\n\treturn path[len(root):].lstrip('/')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_8.py", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n\treturn ' '.join([\n\t\tf for f in parser._option_string_actions\n\t\tif f not in parser.ignored_flags\n\t])\n\n", "def parser_flags(parser):\n\treturn ' '.join([\n\t\tf'--{opt}'\n\t\tfor opt in parser._option_string_actions\n\t])\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor name, arg in parser._actions[1:]:\n\t\tif not isinstance(arg, argparse._StoreAction):\n\t\t\tcontinue\n\t\tflags.append(\"--%s\" % name.replace(\"_\", \"-\"))\n\t\tif arg.default:\n\t\t\tflags.append(\"(default: %s)\" % arg.default)\n\treturn \" \".join(flags)\n\n", "def parser_flags(parser):\n\treturn ' '.join(\n\t\t['{} {}'.format(action.option_strings[0], action.dest)\n\t\t\tfor action in parser._actions\n\t\t\tif action.option_strings\n\t\t])\n\n\n", "def parser_flags(parser):\n\treturn \" \".join(parser._get_flags_for_actions(parser._actions))\n\n", "def parser_flags(parser):\n\treturn ' '.join(list(map(lambda x: x.lstrip('-'), parser._option_string_actions.keys())))\n\n", "def parser_flags(parser):\n\treturn \" \".join(parser._option_string_actions.keys())\n\n\n", "def parser_flags(parser):\n\treturn \" \".join(parser._option_string_actions.keys())\n\n\n", "def parser_flags(parser):\n\targs = vars(parser.parse_args())\n\tflags = []\n\tfor name, value in args.items():\n\t\tif value == True:\n\t\t\tflags.append('--' + name)\n\t\telif value != None:\n\t\t\tflags.append('--' + name + ' ' + value)\n\treturn ' '.join(flags)\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != \"help\" and action.dest != \"parsed\":\n\t\t\tfor flag in action.option_strings:\n\t\t\t\tflags.append(flag)\n\treturn \" \".join(flags)\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\treturn ' '.join([\n\t\tf for f in parser._option_string_actions\n\t\tif f not in parser.ignored_flags\n\t])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join([\n\t\tf'--{opt}'\n\t\tfor opt in parser._option_string_actions\n\t])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor name, arg in parser._actions[1:]:\n\t\tif not isinstance(arg, argparse._StoreAction):\n\t\t\tcontinue\n\t\tflags.append(\"--%s\" % name.replace(\"_\", \"-\"))\n\t\tif arg.default:\n\t\t\tflags.append(\"(default: %s)\" % arg.default)\n\treturn \" \".join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(\n\t\t['{} {}'.format(action.option_strings[0], action.dest)\n\t\t\tfor action in parser._actions\n\t\t\tif action.option_strings\n\t\t])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(parser._get_flags_for_actions(parser._actions))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(list(map(lambda x: x.lstrip('-'), parser._option_string_actions.keys())))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(parser._option_string_actions.keys())\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(parser._option_string_actions.keys())\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\targs = vars(parser.parse_args())\n\tflags = []\n\tfor name, value in args.items():\n\t\tif value == True:\n\t\t\tflags.append('--' + name)\n\t\telif value != None:\n\t\t\tflags.append('--' + name + ' ' + value)\n\treturn ' '.join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest != \"help\" and action.dest != \"parsed\":\n\t\t\tfor flag in action.option_strings:\n\t\t\t\tflags.append(flag)\n\treturn \" \".join(flags)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_19.py", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "ques_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"{} already processed\".format(path_name), file=sys.stderr)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed:\", path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed \", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed', path_name, file=sys.stderr)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"[-] '{}' already processed.\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint(\"Warning: {} has already been processed\".format(abs_path))\n\t\treturn True\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"Processing: {}\".format(abs_path))\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Path has already been processed: \" + path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Directory already processed:\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed\", path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"{} already processed\".format(path_name), file=sys.stderr)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed:\", path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed \", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('Already processed', path_name, file=sys.stderr)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"[-] '{}' already processed.\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tprint(\"Warning: {} has already been processed\".format(abs_path))\n\t\treturn True\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"Processing: {}\".format(abs_path))\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Path has already been processed: \" + path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Directory already processed:\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed\", path_name)\n\t\treturn True\n\tprocessed.add(path_name)\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_9.py", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "ques_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\tpts = []\n\tfor i, j, k in zip(vertices, vertices[1:], vertices[2:]):\n\t\tpts.append((i, j, k))\n\tpts.append((vertices[-2], vertices[-1], vertices[0]))\n\tpts.append((vertices[-1], vertices[0], vertices[1]))\n\treturn pts\n\n", "def vertex3tuple(vertices):\n\tv=[]\n\tfor i in range(len(vertices)):\n\t\tv.append([vertices[i-2], vertices[i-1], vertices[i]])\n\treturn v\n\n", "def vertex3tuple(vertices):\n\ttriples = []\n\tfor idx in range(0, len(vertices)):\n\t\ttriple = []\n\t\ttriple.append(vertices[idx-1])\n\t\ttriple.append(vertices[idx])\n\t\ttriple.append(vertices[(idx+1) % len(vertices)])\n\t\ttriples.append(tuple(triple))\n\treturn triples\n\n", "def vertex3tuple(vertices):\n\tvertices_3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertices_3tuple.append(vertices[i-1] + vertices[i] + vertices[(i+1) % len(vertices)])\n\treturn vertices_3tuple\n\n", "def vertex3tuple(vertices):\n\treturn [(vertices[-1], vertices[0], vertices[1])] + [(vertices[i-1], vertices[i], vertices[i+1]) for i in range(1, len(vertices) - 1)] + [(vertices[-2], vertices[-1], vertices[0])]\n\n", "def vertex3tuple(vertices):\n\treturn [ (v[2], v[0], v[1]) for v in vertices ]\n\n", "def vertex3tuple(vertices):\n\tassert len(vertices)>=3\n\tv1 = vertices[:-2]\n\tv2 = vertices[1:-1]\n\tv3 = vertices[2:]\n\treturn zip(v1, v2, v3)\n\n", "def vertex3tuple(vertices):\n\treturn [ (vertices[-1], vertices[0], vertices[1]) ] + [ (vertices[x], vertices[x+1], vertices[x+2]) for x in range(len(vertices)-2) ]\n\n\n", "def vertex3tuple(vertices):\n\t#create a list of 3tuple of vertices, including the vertex and the 2 points on both sides of the vertex\n\treturn [(vertices[-1], vertices[0], vertices[1]),\n\t\t\t(vertices[0], vertices[1], vertices[2]),\n\t\t\t(vertices[1], vertices[2], vertices[3]),\n\t\t\t(vertices[2], vertices[3], vertices[0])]\n\n\n", "def vertex3tuple(vertices):\n\tvertices = list(vertices)\n\tif len(vertices) == 1:\n\t\treturn (vertices[0], vertices[0], vertices[0])\n\telif len(vertices) == 2:\n\t\treturn (vertices[0], vertices[0], vertices[1]), (vertices[1], vertices[1], vertices[0])\n\telse:\n\t\tlast = vertices[-1]\n\t\tfirst = vertices[0]\n\t\treturn tuple((last, vertices[i], vertices[i+1]) for i in range(len(vertices) - 1))\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\tpts = []\n\tfor i, j, k in zip(vertices, vertices[1:], vertices[2:]):\n\t\tpts.append((i, j, k))\n\tpts.append((vertices[-2], vertices[-1], vertices[0]))\n\tpts.append((vertices[-1], vertices[0], vertices[1]))\n\treturn pts\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\tv=[]\n\tfor i in range(len(vertices)):\n\t\tv.append([vertices[i-2], vertices[i-1], vertices[i]])\n\treturn v\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\ttriples = []\n\tfor idx in range(0, len(vertices)):\n\t\ttriple = []\n\t\ttriple.append(vertices[idx-1])\n\t\ttriple.append(vertices[idx])\n\t\ttriple.append(vertices[(idx+1) % len(vertices)])\n\t\ttriples.append(tuple(triple))\n\treturn triples\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\tvertices_3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertices_3tuple.append(vertices[i-1] + vertices[i] + vertices[(i+1) % len(vertices)])\n\treturn vertices_3tuple\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\treturn [(vertices[-1], vertices[0], vertices[1])] + [(vertices[i-1], vertices[i], vertices[i+1]) for i in range(1, len(vertices) - 1)] + [(vertices[-2], vertices[-1], vertices[0])]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\treturn [ (v[2], v[0], v[1]) for v in vertices ]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\tassert len(vertices)>=3\n\tv1 = vertices[:-2]\n\tv2 = vertices[1:-1]\n\tv3 = vertices[2:]\n\treturn zip(v1, v2, v3)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\treturn [ (vertices[-1], vertices[0], vertices[1]) ] + [ (vertices[x], vertices[x+1], vertices[x+2]) for x in range(len(vertices)-2) ]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\t#create a list of 3tuple of vertices, including the vertex and the 2 points on both sides of the vertex\n\treturn [(vertices[-1], vertices[0], vertices[1]),\n\t\t\t(vertices[0], vertices[1], vertices[2]),\n\t\t\t(vertices[1], vertices[2], vertices[3]),\n\t\t\t(vertices[2], vertices[3], vertices[0])]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\tvertices = list(vertices)\n\tif len(vertices) == 1:\n\t\treturn (vertices[0], vertices[0], vertices[0])\n\telif len(vertices) == 2:\n\t\treturn (vertices[0], vertices[0], vertices[1]), (vertices[1], vertices[1], vertices[0])\n\telse:\n\t\tlast = vertices[-1]\n\t\tfirst = vertices[0]\n\t\treturn tuple((last, vertices[i], vertices[i+1]) for i in range(len(vertices) - 1))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_6.py", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\telse:\n\t\tdigits = []\n\t\twhile number > 0:\n\t\t\tdigits.append(alphabet[number % len(alphabet)])\n\t\t\tnumber //= len(alphabet)\n\t\tif padding is not None:\n\t\t\twhile len(digits) < padding:\n\t\t\t\tdigits.append(alphabet[0])\n\t\tdigits.reverse()\n\t\treturn ''.join(digits)\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number < 0:\n\t\traise ValueError(\"number must be positive\")\n\n\tif padding is not None:\n\t\tif padding < 0:\n\t\t\traise ValueError(\"padding must be non-negative\")\n\n\t\tif number >= len(alphabet) ** padding:\n\t\t\traise ValueError(\"number is too large\")\n\n\t\talphabet = [alphabet[0]] * padding + list(alphabet)\n\n\tstring = []\n\twhile number != 0:\n\t\tnumber, digit = divmod(number, len(alphabet))\n\t\tstring.append(alphabet[digit])\n\tif padding is None:\n\t\tstring.reverse()\n\n\treturn \"\".join(string or alphabet[0:1])\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\tif padding is not None:\n\t\t\treturn alphabet[0] * padding\n\t\treturn alphabet[0]\n\n\tstring = []\n\twhile number > 0:\n\t\tnumber, index = divmod(number, len(alphabet))\n\t\tstring.append(alphabet[index])\n\n\tif padding is not None:\n\t\tstring.extend([alphabet[0]] * (padding - len(string)))\n\n\treturn \"\".join(reversed(string))\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t# Example:\n\t# int_to_string(123, ['a', 'b', 'c', 'd'])\n\t# => 'dcd'\n\n\tif padding is not None:\n\t\tnumber += len(alphabet) ** padding\n\t\tpadding = None\n\n\tbase = len(alphabet)\n\toutput = ''\n\twhile number > 0:\n\t\toutput += alphabet[number % base]\n\t\tnumber //= base\n\n\tif padding is not None:\n\t\toutput += alphabet[0] * padding\n\n\treturn output[::-1]\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t# The number will be written as s[i] + s[i-1] ... + s[0] * base^i\n\t# For example, base=10, number=12345: s = [5, 4, 3, 2, 1]\n\t# Because 5 + 4*10 + 3*100 + 2*1000 + 1*10000 = 12345\n\ts = []\n\twhile number > 0:\n\t\tnumber, digit = divmod(number, len(alphabet))\n\t\ts.append(alphabet[digit])\n\ts.reverse()\n\t# Add padding, if needed\n\tif padding is not None:\n\t\ts.extend(alphabet[0] * (padding - len(s)))\n\treturn ''.join(s)\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\tdigits = []\n\twhile number > 0:\n\t\tnumber, digit = divmod(number, len(alphabet))\n\t\tdigits.append(alphabet[digit])\n\tdigits.reverse()\n\treturn \"\".join(digits) if padding is None else \"\".join(digits).rjust(padding, alphabet[0])\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\telif number < 0:\n\t\traise ValueError(\"number must be positive\")\n\n\tresult = \"\"\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\n\tif padding is not None:\n\t\tif len(result) < padding:\n\t\t\tresult += alphabet[0] * (padding - len(result))\n\n\treturn result[::-1]\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\treturn_value = \"\"\n\twhile number != 0:\n\t\tdigit = number % len(alphabet)\n\t\tnumber = number // len(alphabet)\n\t\treturn_value = alphabet[digit] + return_value\n\tif padding is not None:\n\t\treturn_value = return_value.rjust(padding, alphabet[0])\n\treturn return_value\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\telif number < 0:\n\t\traise ValueError(\"Only positive numbers can be converted to strings.\")\n\tresult = []\n\twhile number > 0:\n\t\tresult.append(alphabet[number % len(alphabet)])\n\t\tnumber //= len(alphabet)\n\tresult.reverse()\n\treturn \"\".join(result).rjust(padding, alphabet[0])\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tdigits = []\n\tif number == 0:\n\t\tdigits.append(alphabet[0])\n\twhile number > 0:\n\t\tdigits.append(alphabet[number % base])\n\t\tnumber //= base\n\tif padding is not None:\n\t\tdigits.extend(alphabet[0] * (padding - len(digits)))\n\treturn \"\".join(reversed(digits))\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\telse:\n\t\tdigits = []\n\t\twhile number > 0:\n\t\t\tdigits.append(alphabet[number % len(alphabet)])\n\t\t\tnumber //= len(alphabet)\n\t\tif padding is not None:\n\t\t\twhile len(digits) < padding:\n\t\t\t\tdigits.append(alphabet[0])\n\t\tdigits.reverse()\n\t\treturn ''.join(digits)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number < 0:\n\t\traise ValueError(\"number must be positive\")\n\n\tif padding is not None:\n\t\tif padding < 0:\n\t\t\traise ValueError(\"padding must be non-negative\")\n\n\t\tif number >= len(alphabet) ** padding:\n\t\t\traise ValueError(\"number is too large\")\n\n\t\talphabet = [alphabet[0]] * padding + list(alphabet)\n\n\tstring = []\n\twhile number != 0:\n\t\tnumber, digit = divmod(number, len(alphabet))\n\t\tstring.append(alphabet[digit])\n\tif padding is None:\n\t\tstring.reverse()\n\n\treturn \"\".join(string or alphabet[0:1])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\tif padding is not None:\n\t\t\treturn alphabet[0] * padding\n\t\treturn alphabet[0]\n\n\tstring = []\n\twhile number > 0:\n\t\tnumber, index = divmod(number, len(alphabet))\n\t\tstring.append(alphabet[index])\n\n\tif padding is not None:\n\t\tstring.extend([alphabet[0]] * (padding - len(string)))\n\n\treturn \"\".join(reversed(string))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t# Example:\n\t# int_to_string(123, ['a', 'b', 'c', 'd'])\n\t# => 'dcd'\n\n\tif padding is not None:\n\t\tnumber += len(alphabet) ** padding\n\t\tpadding = None\n\n\tbase = len(alphabet)\n\toutput = ''\n\twhile number > 0:\n\t\toutput += alphabet[number % base]\n\t\tnumber //= base\n\n\tif padding is not None:\n\t\toutput += alphabet[0] * padding\n\n\treturn output[::-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\t# The number will be written as s[i] + s[i-1] ... + s[0] * base^i\n\t# For example, base=10, number=12345: s = [5, 4, 3, 2, 1]\n\t# Because 5 + 4*10 + 3*100 + 2*1000 + 1*10000 = 12345\n\ts = []\n\twhile number > 0:\n\t\tnumber, digit = divmod(number, len(alphabet))\n\t\ts.append(alphabet[digit])\n\ts.reverse()\n\t# Add padding, if needed\n\tif padding is not None:\n\t\ts.extend(alphabet[0] * (padding - len(s)))\n\treturn ''.join(s)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\tdigits = []\n\twhile number > 0:\n\t\tnumber, digit = divmod(number, len(alphabet))\n\t\tdigits.append(alphabet[digit])\n\tdigits.reverse()\n\treturn \"\".join(digits) if padding is None else \"\".join(digits).rjust(padding, alphabet[0])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\telif number < 0:\n\t\traise ValueError(\"number must be positive\")\n\n\tresult = \"\"\n\twhile number > 0:\n\t\tresult += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\n\tif padding is not None:\n\t\tif len(result) < padding:\n\t\t\tresult += alphabet[0] * (padding - len(result))\n\n\treturn result[::-1]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\treturn_value = \"\"\n\twhile number != 0:\n\t\tdigit = number % len(alphabet)\n\t\tnumber = number // len(alphabet)\n\t\treturn_value = alphabet[digit] + return_value\n\tif padding is not None:\n\t\treturn_value = return_value.rjust(padding, alphabet[0])\n\treturn return_value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\telif number < 0:\n\t\traise ValueError(\"Only positive numbers can be converted to strings.\")\n\tresult = []\n\twhile number > 0:\n\t\tresult.append(alphabet[number % len(alphabet)])\n\t\tnumber //= len(alphabet)\n\tresult.reverse()\n\treturn \"\".join(result).rjust(padding, alphabet[0])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tdigits = []\n\tif number == 0:\n\t\tdigits.append(alphabet[0])\n\twhile number > 0:\n\t\tdigits.append(alphabet[number % base])\n\t\tnumber //= base\n\tif padding is not None:\n\t\tdigits.extend(alphabet[0] * (padding - len(digits)))\n\treturn \"\".join(reversed(digits))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_12.py", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "ques_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif register_value in flow:\n\t\t\tflow[register_number] = flow.pop(register_value)\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow_params_key, flow_params_value in flow_params.items():\n\t\tif flow_params_key == \"register_value\":\n\t\t\tflow_params[register_number] = flow_params_value\n\t\t\tdel flow_params[flow_params_key]\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tif not isinstance(register_value, str):\n\t\traise ValueError(\"Flow parameter '{}' value must be a string\".format(register_value))\n\tif register_value not in flow_params:\n\t\traise ValueError(\"Flow parameter '{}' is not defined\".format(register_value))\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif isinstance(flow_params[flow], dict):\n\t\t\t_replace_register(flow_params[flow], register_number, register_value)\n\t\telif flow_params[flow] == register_value:\n\t\t\tflow_params[flow] = register_number\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif register_value in flow:\n\t\t\tflow[register_number] = flow.pop(register_value)\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tfor key in flow_params[flow]:\n\t\t\tif flow_params[flow][key] == register_value:\n\t\t\t\tflow_params[flow][key] = register_number\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow_id in flow_params:\n\t\tif register_value in flow_params[flow_id]:\n\t\t\tflow_params[flow_id][register_number] = flow_params[flow_id][register_value]\n\t\t\tdel flow_params[flow_id][register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tfor key, value in flow['parameters'].items():\n\t\t\tif key == register_value:\n\t\t\t\tflow['parameters'][key] = register_number\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif \"register_value\" in flow:\n\t\t\tflow[\"register_value\"] = register_number\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tfor key in flow_params[flow]:\n\t\t\tif key == register_value:\n\t\t\t\tflow_params[flow][key] = register_number\n\t\t\t\tbreak\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif register_value in flow:\n\t\t\tflow[register_number] = flow.pop(register_value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow_params_key, flow_params_value in flow_params.items():\n\t\tif flow_params_key == \"register_value\":\n\t\t\tflow_params[register_number] = flow_params_value\n\t\t\tdel flow_params[flow_params_key]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tif not isinstance(register_value, str):\n\t\traise ValueError(\"Flow parameter '{}' value must be a string\".format(register_value))\n\tif register_value not in flow_params:\n\t\traise ValueError(\"Flow parameter '{}' is not defined\".format(register_value))\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif isinstance(flow_params[flow], dict):\n\t\t\t_replace_register(flow_params[flow], register_number, register_value)\n\t\telif flow_params[flow] == register_value:\n\t\t\tflow_params[flow] = register_number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif register_value in flow:\n\t\t\tflow[register_number] = flow.pop(register_value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tfor key in flow_params[flow]:\n\t\t\tif flow_params[flow][key] == register_value:\n\t\t\t\tflow_params[flow][key] = register_number\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow_id in flow_params:\n\t\tif register_value in flow_params[flow_id]:\n\t\t\tflow_params[flow_id][register_number] = flow_params[flow_id][register_value]\n\t\t\tdel flow_params[flow_id][register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tfor key, value in flow['parameters'].items():\n\t\t\tif key == register_value:\n\t\t\t\tflow['parameters'][key] = register_number\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tif \"register_value\" in flow:\n\t\t\tflow[\"register_value\"] = register_number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tfor flow in flow_params:\n\t\tfor key in flow_params[flow]:\n\t\t\tif key == register_value:\n\t\t\t\tflow_params[flow][key] = register_number\n\t\t\t\tbreak\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_2.py", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "ques_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\t\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_3.py", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "ques_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tyield subclass\n\t\tfor subclass in subclasses(subclass):\n\t\t\tyield subclass\n\n", "def subclasses(cls):\n\t# http://stackoverflow.com/questions/3862310/how-can-i-find-all-subclasses-of-a-class-given-its-name\n\tyield cls\n\tfor subclass in cls.__subclasses__():\n\t\tfor subsubclass in subclasses(subclass):\n\t\t\tyield subsubclass\n\n", "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tfor sub in subclasses(subclass):\n\t\t\tyield sub\n\tyield cls\n\n", "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\treturn subs + [g for s in subs for g in subclasses(s)]\n\n\n", "def subclasses(cls):\n\tseen = set()\n\tqueue = [cls]\n\twhile queue:\n\t\tcls = queue.pop()\n\t\tif cls in seen:\n\t\t\tcontinue\n\t\tyield cls\n\t\tseen.add(cls)\n\t\tfor c in cls.__subclasses__():\n\t\t\tqueue.append(c)\n\t\tfor s in cls.__bases__:\n\t\t\tqueue.append(s)\n\n", "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs += subclasses(sub)\n\treturn subs\n\n\n", "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tfor subsubclass in subclasses(subclass):\n\t\t\tyield subsubclass\n\t\tyield subclass\n\n", "def subclasses(cls):\n\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n\n", "def subclasses(cls):\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tyield subclass\n\t\tfor subclass in subclasses(subclass):\n\t\t\tyield subclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\t# http://stackoverflow.com/questions/3862310/how-can-i-find-all-subclasses-of-a-class-given-its-name\n\tyield cls\n\tfor subclass in cls.__subclasses__():\n\t\tfor subsubclass in subclasses(subclass):\n\t\t\tyield subsubclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tfor sub in subclasses(subclass):\n\t\t\tyield sub\n\tyield cls\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\treturn subs + [g for s in subs for g in subclasses(s)]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tseen = set()\n\tqueue = [cls]\n\twhile queue:\n\t\tcls = queue.pop()\n\t\tif cls in seen:\n\t\t\tcontinue\n\t\tyield cls\n\t\tseen.add(cls)\n\t\tfor c in cls.__subclasses__():\n\t\t\tqueue.append(c)\n\t\tfor s in cls.__bases__:\n\t\t\tqueue.append(s)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs += subclasses(sub)\n\treturn subs\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tfor subsubclass in subclasses(subclass):\n\t\t\tyield subsubclass\n\t\tyield subclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tsubclasses = cls.__subclasses__()\n\tfor subclass in subclasses:\n\t\tsubclasses.extend(subclasses(subclass))\n\treturn subclasses\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_7.py", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n\treturn sum(\n\t\talphabet.index(char) * len(alphabet) ** exponent\n\t\tfor exponent, char in enumerate(string[::-1])\n\t)\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tresult = 0\n\tfor letter in string:\n\t\tresult *= len(alphabet)\n\t\tresult += alphabet.index(letter)\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tval = 0\n\tpower = 1\n\tfor digit in reversed(string):\n\t\tval += power * alphabet.index(digit)\n\t\tpower *= len(alphabet)\n\treturn val\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tval = 0\n\tfor c in string:\n\t\tval *= len(alphabet)\n\t\tval += alphabet.index(c)\n\treturn val\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tresult = 0\n\tfor digit in string:\n\t\tresult = result * len(alphabet) + alphabet.index(digit)\n\treturn result\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tvalue = 0\n\tfor c in string:\n\t\tvalue = alphabet.index(c) + value * len(alphabet)\n\treturn value\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif len(string) == 0:\n\t\treturn 0\n\telse:\n\t\treturn alphabet.index(string[0]) + len(alphabet) * string_to_int(string[1:], alphabet)\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif not string:\n\t\treturn 0\n\n\treturn alphabet.index(string[0]) * (len(alphabet) ** (len(string) - 1)) + \\\n\t\tstring_to_int(string[1:], alphabet)\n\t\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tresult = 0\n\tfor i, c in enumerate(string):\n\t\tresult += alphabet.index(c) * (len(alphabet) ** (len(string) - i - 1))\n\treturn result\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tnumber: int = 0\n\tfor i, char in enumerate(string):\n\t\tnumber += alphabet.index(char) * len(alphabet) ** (len(string) - 1 - i)\n\n\treturn number\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\treturn sum(\n\t\talphabet.index(char) * len(alphabet) ** exponent\n\t\tfor exponent, char in enumerate(string[::-1])\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tresult = 0\n\tfor letter in string:\n\t\tresult *= len(alphabet)\n\t\tresult += alphabet.index(letter)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tval = 0\n\tpower = 1\n\tfor digit in reversed(string):\n\t\tval += power * alphabet.index(digit)\n\t\tpower *= len(alphabet)\n\treturn val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tval = 0\n\tfor c in string:\n\t\tval *= len(alphabet)\n\t\tval += alphabet.index(c)\n\treturn val\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tresult = 0\n\tfor digit in string:\n\t\tresult = result * len(alphabet) + alphabet.index(digit)\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tvalue = 0\n\tfor c in string:\n\t\tvalue = alphabet.index(c) + value * len(alphabet)\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif len(string) == 0:\n\t\treturn 0\n\telse:\n\t\treturn alphabet.index(string[0]) + len(alphabet) * string_to_int(string[1:], alphabet)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif not string:\n\t\treturn 0\n\n\treturn alphabet.index(string[0]) * (len(alphabet) ** (len(string) - 1)) + \\\n\t\tstring_to_int(string[1:], alphabet)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tresult = 0\n\tfor i, c in enumerate(string):\n\t\tresult += alphabet.index(c) * (len(alphabet) ** (len(string) - i - 1))\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tnumber: int = 0\n\tfor i, char in enumerate(string):\n\t\tnumber += alphabet.index(char) * len(alphabet) ** (len(string) - 1 - i)\n\n\treturn number\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_24.py", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "ques_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tif not url.endswith('.tar.gz'):\n\t\traise ValueError('url should end with \".tar.gz\"')\n\n\tarchive_path = destination_path / url.split('/')[-1]\n\tresponse = requests.get(url, stream=True)\n\twith archive_path.open('wb') as fd:\n\t\tfor chunk in response.iter_content(chunk_size=128):\n\t\t\tfd.write(chunk)\n\ttar = tarfile.open(archive_path, 'r:gz')\n\ttar.extractall(destination_path)\n\ttar.close()\n\treturn destination_path / 'desc'\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = download_file(url, destination_path)\n\treturn extract_archive(archive_path, destination_path)\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlog.info(f\"Downloading '{url}' to '{destination_path}'\")\n\twith requests.get(url, stream=True) as r:\n\t\tr.raise_for_status()\n\t\twith tarfile.open(fileobj=io.BytesIO(r.content), mode=\"r:gz\") as tar:\n\t\t\ttar.extractall(destination_path)\n\t\t\treturn destination_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.info(f'Downloading repo archive from {url}')\n\ttry:\n\t\ttmp_path = download_and_extract(url, destination_path)\n\texcept Exception as e:\n\t\tlogger.error(e)\n\t\treturn None\n\treturn tmp_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tdestination_path.mkdir(exist_ok=True)\n\tif not destination_path.is_dir():\n\t\traise ValueError(f'{destination_path} is not a directory')\n\n\t# Download archive\n\ttar_gz_archive_name = url.split('/')[-1]\n\ttar_gz_archive_path = destination_path / tar_gz_archive_name\n\tif not tar_gz_archive_path.exists():\n\t\tdownload(url, tar_gz_archive_path)\n\tif not tar_gz_archive_path.is_file():\n\t\traise ValueError(f'{tar_gz_archive_path} is not a file')\n\n\t# Extract archive\n\ttar_file_name = tar_gz_archive_name.replace('.gz', '')\n\ttar_file_path = destination_path / tar_file_name\n\tif not tar_file_path.exists():\n\t\twith gzip.open(tar_gz_archive_path, 'rb') as f_in, open(tar_file_path, 'wb') as\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.info(f'Downloading repo archive: {url}')\n\t# Download\n\tarchive_path = destination_path / url.split('/')[-1]\n\tif archive_path.exists():\n\t\tlogger.info(f'Archive already exists: {archive_path}')\n\telse:\n\t\ttry:\n\t\t\twith archive_path.open(mode='wb') as f:\n\t\t\t\tf.write(requests.get(url).content)\n\t\texcept Exception as e:\n\t\t\traise e\n\tlogger.info(f'Extracting archive: \"{archive_path}\" in \"{destination_path}\"')\n\t# Extract\n\textracted_path = destination_path / archive_path.stem\n\tif extracted_path.exists():\n\t\tlogger.info(f'Archive already extracted: {extracted_path}')\n\telse:\n\t\ttry:\n\t\t\twith tarfile.open(archive_path) as f:\n\t\t\t\tf.extractall(str(destination_path))\n\t\t\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tprint(\"Downloading repo archive...\")\n\tarchive_path = download_file(url, destination_path)\n\tprint(\"Extracting repo archive...\")\n\textracted_path = extract_file(archive_path)\n\treturn extracted_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tpath = destination_path / url.split('/')[-1]\n\tif path.exists():\n\t\treturn path\n\n\tlogger.info('Downloading .tar.gz archive from \"%s\"', url)\n\tresponse = requests.get(url, stream=True)\n\twith tempfile.TemporaryDirectory() as temp_dir:\n\t\ttemp_path = Path(temp_dir) / url.split('/')[-1]\n\t\twith temp_path.open('wb') as temp_file:\n\t\t\tfor chunk in response.iter_content(chunk_size=chunk_size):\n\t\t\t\tif chunk:\n\t\t\t\t\ttemp_file.write(chunk)\n\n\t\twith tarfile.open(temp_path) as tar:\n\t\t\ttar.extractall(destination_path)\n\n\treturn path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.debug('get_repo_archive(%s)', url)\n\tdestination_path.mkdir(parents=True, exist_ok=True)\n\tr = requests.get(url)\n\tif r.status_code != 200:\n\t\traise ValueError(f'status code is {r.status_code}')\n\twith tarfile.open(fileobj=io.BytesIO(r.content)) as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = destination_path / url.split('/')[-1]\n\tif not archive_path.exists():\n\t\tprint(f\"Downloading {url} ...\")\n\t\tdownload(url, archive_path)\n\telse:\n\t\tprint(f\"{archive_path} already exists, skipping download.\")\n\n\tprint(f\"Extracting {archive_path} ...\")\n\twith tarfile.open(archive_path) as tf:\n\t\ttf.extractall(destination_path)\n\n\treturn destination_path / 'desc'\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tif not url.endswith('.tar.gz'):\n\t\traise ValueError('url should end with \".tar.gz\"')\n\n\tarchive_path = destination_path / url.split('/')[-1]\n\tresponse = requests.get(url, stream=True)\n\twith archive_path.open('wb') as fd:\n\t\tfor chunk in response.iter_content(chunk_size=128):\n\t\t\tfd.write(chunk)\n\ttar = tarfile.open(archive_path, 'r:gz')\n\ttar.extractall(destination_path)\n\ttar.close()\n\treturn destination_path / 'desc'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = download_file(url, destination_path)\n\treturn extract_archive(archive_path, destination_path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlog.info(f\"Downloading '{url}' to '{destination_path}'\")\n\twith requests.get(url, stream=True) as r:\n\t\tr.raise_for_status()\n\t\twith tarfile.open(fileobj=io.BytesIO(r.content), mode=\"r:gz\") as tar:\n\t\t\ttar.extractall(destination_path)\n\t\t\treturn destination_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.info(f'Downloading repo archive from {url}')\n\ttry:\n\t\ttmp_path = download_and_extract(url, destination_path)\n\texcept Exception as e:\n\t\tlogger.error(e)\n\t\treturn None\n\treturn tmp_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tdestination_path.mkdir(exist_ok=True)\n\tif not destination_path.is_dir():\n\t\traise ValueError(f'{destination_path} is not a directory')\n\n\t# Download archive\n\ttar_gz_archive_name = url.split('/')[-1]\n\ttar_gz_archive_path = destination_path / tar_gz_archive_name\n\tif not tar_gz_archive_path.exists():\n\t\tdownload(url, tar_gz_archive_path)\n\tif not tar_gz_archive_path.is_file():\n\t\traise ValueError(f'{tar_gz_archive_path} is not a file')\n\n\t# Extract archive\n\ttar_file_name = tar_gz_archive_name.replace('.gz', '')\n\ttar_file_path = destination_path / tar_file_name\n\tif not tar_file_path.exists():\n\t\twith gzip.open(tar_gz_archive_path, 'rb') as f_in, open(tar_file_path, 'wb') as\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.info(f'Downloading repo archive: {url}')\n\t# Download\n\tarchive_path = destination_path / url.split('/')[-1]\n\tif archive_path.exists():\n\t\tlogger.info(f'Archive already exists: {archive_path}')\n\telse:\n\t\ttry:\n\t\t\twith archive_path.open(mode='wb') as f:\n\t\t\t\tf.write(requests.get(url).content)\n\t\texcept Exception as e:\n\t\t\traise e\n\tlogger.info(f'Extracting archive: \"{archive_path}\" in \"{destination_path}\"')\n\t# Extract\n\textracted_path = destination_path / archive_path.stem\n\tif extracted_path.exists():\n\t\tlogger.info(f'Archive already extracted: {extracted_path}')\n\telse:\n\t\ttry:\n\t\t\twith tarfile.open(archive_path) as f:\n\t\t\t\tf.extractall(str(destination_path))\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tprint(\"Downloading repo archive...\")\n\tarchive_path = download_file(url, destination_path)\n\tprint(\"Extracting repo archive...\")\n\textracted_path = extract_file(archive_path)\n\treturn extracted_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tpath = destination_path / url.split('/')[-1]\n\tif path.exists():\n\t\treturn path\n\n\tlogger.info('Downloading .tar.gz archive from \"%s\"', url)\n\tresponse = requests.get(url, stream=True)\n\twith tempfile.TemporaryDirectory() as temp_dir:\n\t\ttemp_path = Path(temp_dir) / url.split('/')[-1]\n\t\twith temp_path.open('wb') as temp_file:\n\t\t\tfor chunk in response.iter_content(chunk_size=chunk_size):\n\t\t\t\tif chunk:\n\t\t\t\t\ttemp_file.write(chunk)\n\n\t\twith tarfile.open(temp_path) as tar:\n\t\t\ttar.extractall(destination_path)\n\n\treturn path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.debug('get_repo_archive(%s)', url)\n\tdestination_path.mkdir(parents=True, exist_ok=True)\n\tr = requests.get(url)\n\tif r.status_code != 200:\n\t\traise ValueError(f'status code is {r.status_code}')\n\twith tarfile.open(fileobj=io.BytesIO(r.content)) as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = destination_path / url.split('/')[-1]\n\tif not archive_path.exists():\n\t\tprint(f\"Downloading {url} ...\")\n\t\tdownload(url, archive_path)\n\telse:\n\t\tprint(f\"{archive_path} already exists, skipping download.\")\n\n\tprint(f\"Extracting {archive_path} ...\")\n\twith tarfile.open(archive_path) as tf:\n\t\ttf.extractall(destination_path)\n\n\treturn destination_path / 'desc'\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_20.py", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "ques_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n\tif platform.system() == \"Darwin\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def os_is_mac():\n\treturn sys.platform == 'darwin'\n\n", "def os_is_mac():\n\treturn os.uname()[0] == \"Darwin\"\n\n\n", "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n\n", "def os_is_mac():\n\treturn OS_MAC in sys.platform\n\n\n", "def os_is_mac():\n\treturn os.name == 'posix' and sys.platform == 'darwin'\n\n\n", "def os_is_mac():\n\treturn 'darwin' in sys.platform\n\n", "def os_is_mac():\n\treturn platform.system() == 'Darwin'\n\n\n", "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n", "def os_is_mac():\n\treturn platform.system() == 'Darwin'\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n\tif platform.system() == \"Darwin\":\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn sys.platform == 'darwin'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn os.uname()[0] == \"Darwin\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn OS_MAC in sys.platform\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn os.name == 'posix' and sys.platform == 'darwin'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn 'darwin' in sys.platform\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn platform.system() == 'Darwin'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn platform.system() == 'Darwin'\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_14.py", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "ques_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n\t# this is a bit of a hack, but it works\n\tnew_dict = {}\n\tfor key in item:\n\t\tnew_key = fnmatch.translate(key)\n\t\tnew_dict[new_key] = item[key]\n\treturn new_dict\n\n\n", "def regex_dict(item):\n\tresult = {}\n\tfor key in item:\n\t\tresult[fnmatch.translate(key)] = item[key]\n\treturn result\n\n\n", "def regex_dict(item):\n\treturn {re.compile(fnmatch.translate(key)): value for key, value in item.items()}\n\n\n", "def regex_dict(item):\n\tif type(item) is dict:\n\t\treturn {fnmatch.translate(k): v for k, v in item.items()}\n\telse:\n\t\treturn item\n\n", "def regex_dict(item):\n\tregex_keys = {}\n\tfor key, value in item.items():\n\t\tregex_keys[fnmatch.translate(key)] = value\n\treturn regex_keys\n\n", "def regex_dict(item):\n\tif not isinstance(item, dict):\n\t\treturn item\n\tnew_dict = {}\n\tfor k, v in item.items():\n\t\tnew_dict[fnmatch.translate(k)] = v\n\treturn new_dict\n\n\n", "def regex_dict(item):\n\tif not isinstance(item, dict):\n\t\treturn item\n\tregexes = {}\n\tfor key, value in item.items():\n\t\tregexes[re.compile(fnmatch.translate(key))] = value\n\treturn regexes\n\n", "def regex_dict(item):\n\tif not isinstance(item, dict):\n\t\treturn item\n\treturn dict(((re.compile(fnmatch.translate(k)), regex_dict(v)) for k, v in item.iteritems()))\n\n", "def regex_dict(item):\n\tnew_dict = {}\n\tfor k, v in item.items():\n\t\tif isinstance(v, dict):\n\t\t\tnew_dict[re.compile(fnmatch.translate(k))] = regex_dict(v)\n\t\telse:\n\t\t\tnew_dict[re.compile(fnmatch.translate(k))] = v\n\treturn new_dict\n\n", "def regex_dict(item):\n\tregex_keys = {}\n\tfor key, value in item.items():\n\t\tregex_keys[fnmatch_to_regex(key)] = value\n\treturn regex_keys\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n\t# this is a bit of a hack, but it works\n\tnew_dict = {}\n\tfor key in item:\n\t\tnew_key = fnmatch.translate(key)\n\t\tnew_dict[new_key] = item[key]\n\treturn new_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tresult = {}\n\tfor key in item:\n\t\tresult[fnmatch.translate(key)] = item[key]\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\treturn {re.compile(fnmatch.translate(key)): value for key, value in item.items()}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tif type(item) is dict:\n\t\treturn {fnmatch.translate(k): v for k, v in item.items()}\n\telse:\n\t\treturn item\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tregex_keys = {}\n\tfor key, value in item.items():\n\t\tregex_keys[fnmatch.translate(key)] = value\n\treturn regex_keys\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tif not isinstance(item, dict):\n\t\treturn item\n\tnew_dict = {}\n\tfor k, v in item.items():\n\t\tnew_dict[fnmatch.translate(k)] = v\n\treturn new_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tif not isinstance(item, dict):\n\t\treturn item\n\tregexes = {}\n\tfor key, value in item.items():\n\t\tregexes[re.compile(fnmatch.translate(key))] = value\n\treturn regexes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tif not isinstance(item, dict):\n\t\treturn item\n\treturn dict(((re.compile(fnmatch.translate(k)), regex_dict(v)) for k, v in item.iteritems()))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tnew_dict = {}\n\tfor k, v in item.items():\n\t\tif isinstance(v, dict):\n\t\t\tnew_dict[re.compile(fnmatch.translate(k))] = regex_dict(v)\n\t\telse:\n\t\t\tnew_dict[re.compile(fnmatch.translate(k))] = v\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tregex_keys = {}\n\tfor key, value in item.items():\n\t\tregex_keys[fnmatch_to_regex(key)] = value\n\treturn regex_keys\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_4.py", "name": "unquote", "docstring": "Remove quote from the given name.", "ques_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "def unquote(name):\n\tif name.startswith('\"') and name.endswith('\"'):\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "def unquote(name):\n\tif name[0] == '\"':\n\t\treturn name[1:-1].replace('\"\"', '\"')\n\telse:\n\t\treturn name\n\n\n", "def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\tname = name[1:-1]\n\treturn name\n\n", "def unquote(name):\n\tif name[0] == name[-1] and name.startswith((\"'\", '\"')):\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "def unquote(name):\n\treturn name[1:-1]\n\n", "def unquote(name):\n\tif name and name[0] in '\\'\"':\n\t\tname = name[1:-1]\n\treturn name\n\n", "def unquote(name):\n\treturn name.replace('\"', '')\n\n", "def unquote(name):\n\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name.startswith('\"') and name.endswith('\"'):\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name[0] == '\"':\n\t\treturn name[1:-1].replace('\"\"', '\"')\n\telse:\n\t\treturn name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\tname = name[1:-1]\n\treturn name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name[0] == name[-1] and name.startswith((\"'\", '\"')):\n\t\tname = name[1:-1]\n\treturn name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn name[1:-1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name and name[0] in '\\'\"':\n\t\tname = name[1:-1]\n\treturn name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn name.replace('\"', '')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_21.py", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "ques_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = (os.name == 'nt')\n\tassert platform in (0, 1), \"Platform must be either 0 (Windows) or 1 (POSIX)\"\n\tif platform:\n\t\treturn shlex_split_posix(s)\n\telse:\n\t\treturn shlex_split_ntcmd(s)\n\n", "def split(s, platform='this'):\n\tassert platform in '01this'\n\n\tif platform == 'this':\n\t\tplatform = 0 if os.name == 'nt' else 1\n\n\tif platform == 1:\n\t\t# POSIX\n\t\t# TODO: quotes and escape chars\n\t\treturn [m.start() for m in re.finditer('\\s+', s)], re.split('\\s+', s)\n\telse:\n\t\t# Windows CMD\n\t\t# TODO: quotes and escape chars\n\t\treturn [m.start() for m in re.finditer('\\s+', s)], re.split('\\s+', s)\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = is_platform_posix\n\tif platform:\n\t\tif isinstance(s, unicode):\n\t\t\treturn shlex.split(s.encode('utf-8'))\n\t\telse:\n\t\t\treturn shlex.split(s)\n\telse:\n\t\tif not isinstance(s, unicode):\n\t\t\ts = s.decode('mbcs')\n\t\treturn shlex_win.split(s)\n\n", "def split(s, platform='this'):\n\tif platform=='this':\n\t\tplatform=is_posix\n\tif platform:\n\t\treturn _split_posix(s)\n\telse:\n\t\treturn _split_windows(s)\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this_platform\n\n\tif platform == 1:\n\t\treturn [ x for x in re.split(r'(?<!\\\\)(?:\"(?:\"\"|[^\"])*\"|[^\\s]+|\\s+)', s) if x ]\n\telif platform == 0:\n\t\treturn [ x for x in re.split(r'(?:\"(?:\"\"|[^\"])*\"|[^\\s]+|\\s+)', s) if x ]\n\telse:\n\t\traise ValueError('unknown platform {}'.format(platform))\n\n\n", "def split(s, platform='this'):\n\t# This is a wrapper for shlex.split() that works for\n\t# Windows and POSIX, with the same semantics.\n\t# It is a faster version of split_argv.py\n\t# (using regular expressions instead of shlex)\n\t#\n\t# From https://github.com/pytoolz/toolz/blob/master/toolz/curried/os.py\n\t# Copyright (c) 2012, Matthew Rocklin and contributors\n\t# All rights reserved.\n\t#\n\t# Redistribution and use in source and binary forms, with or without\n\t# modification, are permitted provided that the following conditions are\n\t# met:\n\t#\n\t#     * Redistributions of source code must retain the above copyright\n\t#       notice, this list of conditions and the following disclaimer.\n\t#     * Redistributions in binary form must reproduce the above\n\t#       copyright notice, this list of conditions and the following\n\t#       disclaimer in the documentation and/or other materials provided\n\t#       with the distribution.\n\t#     * Neither the name of Matthew Rocklin nor the names of other\n\t#       contributors\n", "def split(s, platform='this'):\n\treturn re.findall(shlex.shlex(s, posix=platform!=0))\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = (os.name == 'posix')\n\t\n\tif platform:\n\t\t# POSIX-style\n\t\treturn list(re.findall(r'''((?:'[^']*'|\"[^\"]*\"|[^'\"\\s]+)+)''', s))\n\telse:\n\t\t# Windows-style\n\t\treturn list(re.findall(r'''([^'\"\\s]*(?:\"(?:[^\\\\\"]|\\\\.)*\"|'(?:[^\\\\']|\\\\.)*')?[^'\"\\s]*)''', s))\n\n", "def split(s, platform='this'):\n\tif platform is None or platform == 'this':\n\t\tplatform = (os.name == 'nt')\n\telif isinstance(platform, bool):\n\t\tplatform = not platform\n\telif isinstance(platform, int):\n\t\tplatform = bool(platform)\n\telse:\n\t\traise ValueError(\"unsupported platform\")\n\n\tif platform:\n\t\t# POSIX-like - shlex.split()\n\t\treturn shlex.split(s)\n\telse:\n\t\t# Windows-like - CMD style\n\t\treturn cmd_split(s)\n", "def split(s, platform='this'):\n\t# print(s)\n\tif platform == 'this':\n\t\tplatform = int(os.name == 'posix')\n\tre_quoted = r'''(?<!\\\\)(?:\\\\\\\\)*\"(?:[^\"\\\\]|\\\\.)*\"'''\n\tre_unquoted = r\"\"\"[^\\s\"']+\"\"\"\n\tre_pattern = re.compile(r'\\s*(%s|%s)' % (re_quoted, re_unquoted), re.S)\n\t# print(s)\n\ttokens = re.findall(re_pattern, s)\n\t# print(tokens)\n\tif platform:\n\t\ttokens = [re.sub(r'\\\\(.)', r'\\1', token) for token in tokens]\n\telse:\n\t\ttokens = [re.sub(r'\"((?:[^\"\\\\]|\\\\.)*)\"', r'\\1', token) for token in tokens]\n\treturn tokens\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = (os.name == 'nt')\n\tassert platform in (0, 1), \"Platform must be either 0 (Windows) or 1 (POSIX)\"\n\tif platform:\n\t\treturn shlex_split_posix(s)\n\telse:\n\t\treturn shlex_split_ntcmd(s)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tassert platform in '01this'\n\n\tif platform == 'this':\n\t\tplatform = 0 if os.name == 'nt' else 1\n\n\tif platform == 1:\n\t\t# POSIX\n\t\t# TODO: quotes and escape chars\n\t\treturn [m.start() for m in re.finditer('\\s+', s)], re.split('\\s+', s)\n\telse:\n\t\t# Windows CMD\n\t\t# TODO: quotes and escape chars\n\t\treturn [m.start() for m in re.finditer('\\s+', s)], re.split('\\s+', s)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = is_platform_posix\n\tif platform:\n\t\tif isinstance(s, unicode):\n\t\t\treturn shlex.split(s.encode('utf-8'))\n\t\telse:\n\t\t\treturn shlex.split(s)\n\telse:\n\t\tif not isinstance(s, unicode):\n\t\t\ts = s.decode('mbcs')\n\t\treturn shlex_win.split(s)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform=='this':\n\t\tplatform=is_posix\n\tif platform:\n\t\treturn _split_posix(s)\n\telse:\n\t\treturn _split_windows(s)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this_platform\n\n\tif platform == 1:\n\t\treturn [ x for x in re.split(r'(?<!\\\\)(?:\"(?:\"\"|[^\"])*\"|[^\\s]+|\\s+)', s) if x ]\n\telif platform == 0:\n\t\treturn [ x for x in re.split(r'(?:\"(?:\"\"|[^\"])*\"|[^\\s]+|\\s+)', s) if x ]\n\telse:\n\t\traise ValueError('unknown platform {}'.format(platform))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t# This is a wrapper for shlex.split() that works for\n\t# Windows and POSIX, with the same semantics.\n\t# It is a faster version of split_argv.py\n\t# (using regular expressions instead of shlex)\n\t#\n\t# From https://github.com/pytoolz/toolz/blob/master/toolz/curried/os.py\n\t# Copyright (c) 2012, Matthew Rocklin and contributors\n\t# All rights reserved.\n\t#\n\t# Redistribution and use in source and binary forms, with or without\n\t# modification, are permitted provided that the following conditions are\n\t# met:\n\t#\n\t#     * Redistributions of source code must retain the above copyright\n\t#       notice, this list of conditions and the following disclaimer.\n\t#     * Redistributions in binary form must reproduce the above\n\t#       copyright notice, this list of conditions and the following\n\t#       disclaimer in the documentation and/or other materials provided\n\t#       with the distribution.\n\t#     * Neither the name of Matthew Rocklin nor the names of other\n\t#       contributors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\treturn re.findall(shlex.shlex(s, posix=platform!=0))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = (os.name == 'posix')\n\t\n\tif platform:\n\t\t# POSIX-style\n\t\treturn list(re.findall(r'''((?:'[^']*'|\"[^\"]*\"|[^'\"\\s]+)+)''', s))\n\telse:\n\t\t# Windows-style\n\t\treturn list(re.findall(r'''([^'\"\\s]*(?:\"(?:[^\\\\\"]|\\\\.)*\"|'(?:[^\\\\']|\\\\.)*')?[^'\"\\s]*)''', s))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform is None or platform == 'this':\n\t\tplatform = (os.name == 'nt')\n\telif isinstance(platform, bool):\n\t\tplatform = not platform\n\telif isinstance(platform, int):\n\t\tplatform = bool(platform)\n\telse:\n\t\traise ValueError(\"unsupported platform\")\n\n\tif platform:\n\t\t# POSIX-like - shlex.split()\n\t\treturn shlex.split(s)\n\telse:\n\t\t# Windows-like - CMD style\n\t\treturn cmd_split(s)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\t# print(s)\n\tif platform == 'this':\n\t\tplatform = int(os.name == 'posix')\n\tre_quoted = r'''(?<!\\\\)(?:\\\\\\\\)*\"(?:[^\"\\\\]|\\\\.)*\"'''\n\tre_unquoted = r\"\"\"[^\\s\"']+\"\"\"\n\tre_pattern = re.compile(r'\\s*(%s|%s)' % (re_quoted, re_unquoted), re.S)\n\t# print(s)\n\ttokens = re.findall(re_pattern, s)\n\t# print(tokens)\n\tif platform:\n\t\ttokens = [re.sub(r'\\\\(.)', r'\\1', token) for token in tokens]\n\telse:\n\t\ttokens = [re.sub(r'\"((?:[^\"\\\\]|\\\\.)*)\"', r'\\1', token) for token in tokens]\n\treturn tokens\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_15.py", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "ques_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\ttmp_dir = tmp_path + \"/\" + filename.split(\".\")[0] + \"-\" + str(uuid.uuid1())[:8]\n\tos.makedirs(tmp_dir, exist_ok=True)\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(tmp_dir)\n\treturn \"file://\" + tmp_dir\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.rsplit(os.path.sep, 1)[-1]\n\t# TODO: handle the case where the archive is not an archive\n\tfile_repo_url = os.path.join(\n\t\ttmp_path, os.path.splitext(filename)[0]\n\t)\n\tif os.path.isdir(file_repo_url):\n\t\treturn file_repo_url\n\n\tos.makedirs(file_repo_url)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=file_repo_url)\n\treturn file_repo_url\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\tdir_name = filename.split(\".\")[0]\n\ttarget_dir = tmp_path / dir_name\n\tif target_dir.exists():\n\t\tshutil.rmtree(target_dir)\n\tos.makedirs(target_dir, exist_ok=True)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(target_dir)\n\t\ttar.close()\n\treturn str(target_dir)\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t# TODO: check if file exists\n\tarchive_path = Path(archive_path)\n\ttmp_path = Path(tmp_path)\n\ttemp_dir = tmp_path / \"temp\"\n\tif filename is None:\n\t\tfilename = archive_path.name.split(\".\")[0]\n\textract_dir = tmp_path / filename\n\tif extract_dir.exists():\n\t\tlogger.info(f\"extract_dir {extract_dir} already exists, removing it\")\n\t\tshutil.rmtree(extract_dir)\n\tlogger.info(\n\t\tf\"extracting {archive_path} to {extract_dir} in temp dir {temp_dir}\"\n\t)\n\textract_path = extract_archive(archive_path, temp_dir)\n\tlogger.info(f\"extracted {archive_path} to {extract_path}\")\n\tif extract_path.exists():\n\t\tlogger.info(f\"extract_path {extract_path} already exists, removing it\")\n\t\tshutil.r\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(exist_ok=True)\n\ttmp_path = tmp_path / filename\n\t# open the archive, read its contents\n\twith open(archive_path, \"rb\") as archive:\n\t\t# write the contents to the temp path\n\t\twith open(tmp_path, \"wb\") as tmp_file:\n\t\t\tshutil.copyfileobj(archive, tmp_file)\n\t# extract the archive to the specified path\n\twith tarfile.open(tmp_path, \"r:*\") as tf:\n\t\ttf.extractall(path=tmp_path.parent)\n\t# return the extracted path\n\treturn str(tmp_path.parent)\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\t# strip out the extension.\n\tfilename, ext = os.path.splitext(filename)\n\t# todo: check that it is a tar.gz file.\n\tdest_path = tmp_path / filename\n\tif dest_path.exists():\n\t\tshutil.rmtree(dest_path)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(dest_path)\n\treturn str(dest_path)\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\n\ttmp_file = tmp_path / filename\n\ttmp_file.write_bytes(requests.get(archive_path).content)\n\n\trepo_path = tmp_path / filename.split(\".\")[0]\n\trepo_path.mkdir(exist_ok=True)\n\n\ttarfile.open(tmp_file).extractall(path=repo_path)\n\treturn repo_path\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tfilename = filename or path.basename(archive_path)\n\n\t# If the file is gzipped, uncompress it.\n\tif filename.endswith(\".gz\"):\n\t\twith gzip.open(archive_path, \"rb\") as gzipped:\n\t\t\twith tempfile.NamedTemporaryFile(delete=False, dir=tmp_path, suffix=\".tar\") as tmp:\n\t\t\t\t# Copy the contents of the gzipped file into the temporary file.\n\t\t\t\tcopyfileobj(gzipped, tmp)\n\t\t\t\ttmp_archive_path = tmp.name\n\telse:\n\t\ttmp_archive_path = archive_path\n\n\trepo_path = path.join(tmp_path, \"repo\")\n\twith tarfile.open(tmp_archive_path) as tar:\n\t\ttar.extractall(path=repo_path)\n\t\ttar.close()\n\tos.remove(tmp_archive_path)\n\treturn repo_path\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.rsplit('/', 1)[-1]\n\tfile_repo_path = str(tmp_path / filename)\n\n\tif filename.endswith(\".tar.gz\"):\n\t\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(path=file_repo_path)\n\telif filename.endswith(\".tar.bz2\"):\n\t\twith tarfile.open(archive_path, \"r:bz2\") as tar:\n\t\t\ttar.extractall(path=file_repo_path)\n\telif filename.endswith(\".zip\"):\n\t\twith zipfile.ZipFile(archive_path, \"r\") as z:\n\t\t\tz.extractall(path=file_repo_path)\n\treturn file_repo_path\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\tif isinstance(tmp_path, str):\n\t\ttmp_path = Path(tmp_path)\n\n\ttmp_path = tmp_path / filename.split(\".\")[0]\n\ttmp_path.mkdir(exist_ok=True)\n\n\tif not tmp_path.exists():\n\t\traise ValueError(f\"{tmp_path} does not exist\")\n\n\twith TemporaryDirectory(dir=tmp_path) as dir_path:\n\t\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(dir_path)\n\n\t\t\treturn f\"file://{dir_path}\"\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\ttmp_dir = tmp_path + \"/\" + filename.split(\".\")[0] + \"-\" + str(uuid.uuid1())[:8]\n\tos.makedirs(tmp_dir, exist_ok=True)\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(tmp_dir)\n\treturn \"file://\" + tmp_dir\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.rsplit(os.path.sep, 1)[-1]\n\t# TODO: handle the case where the archive is not an archive\n\tfile_repo_url = os.path.join(\n\t\ttmp_path, os.path.splitext(filename)[0]\n\t)\n\tif os.path.isdir(file_repo_url):\n\t\treturn file_repo_url\n\n\tos.makedirs(file_repo_url)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(path=file_repo_url)\n\treturn file_repo_url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\tdir_name = filename.split(\".\")[0]\n\ttarget_dir = tmp_path / dir_name\n\tif target_dir.exists():\n\t\tshutil.rmtree(target_dir)\n\tos.makedirs(target_dir, exist_ok=True)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(target_dir)\n\t\ttar.close()\n\treturn str(target_dir)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\t# TODO: check if file exists\n\tarchive_path = Path(archive_path)\n\ttmp_path = Path(tmp_path)\n\ttemp_dir = tmp_path / \"temp\"\n\tif filename is None:\n\t\tfilename = archive_path.name.split(\".\")[0]\n\textract_dir = tmp_path / filename\n\tif extract_dir.exists():\n\t\tlogger.info(f\"extract_dir {extract_dir} already exists, removing it\")\n\t\tshutil.rmtree(extract_dir)\n\tlogger.info(\n\t\tf\"extracting {archive_path} to {extract_dir} in temp dir {temp_dir}\"\n\t)\n\textract_path = extract_archive(archive_path, temp_dir)\n\tlogger.info(f\"extracted {archive_path} to {extract_path}\")\n\tif extract_path.exists():\n\t\tlogger.info(f\"extract_path {extract_path} already exists, removing it\")\n\t\tshutil.r\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(exist_ok=True)\n\ttmp_path = tmp_path / filename\n\t# open the archive, read its contents\n\twith open(archive_path, \"rb\") as archive:\n\t\t# write the contents to the temp path\n\t\twith open(tmp_path, \"wb\") as tmp_file:\n\t\t\tshutil.copyfileobj(archive, tmp_file)\n\t# extract the archive to the specified path\n\twith tarfile.open(tmp_path, \"r:*\") as tf:\n\t\ttf.extractall(path=tmp_path.parent)\n\t# return the extracted path\n\treturn str(tmp_path.parent)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = os.path.basename(archive_path)\n\t# strip out the extension.\n\tfilename, ext = os.path.splitext(filename)\n\t# todo: check that it is a tar.gz file.\n\tdest_path = tmp_path / filename\n\tif dest_path.exists():\n\t\tshutil.rmtree(dest_path)\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(dest_path)\n\treturn str(dest_path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\n\ttmp_file = tmp_path / filename\n\ttmp_file.write_bytes(requests.get(archive_path).content)\n\n\trepo_path = tmp_path / filename.split(\".\")[0]\n\trepo_path.mkdir(exist_ok=True)\n\n\ttarfile.open(tmp_file).extractall(path=repo_path)\n\treturn repo_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tfilename = filename or path.basename(archive_path)\n\n\t# If the file is gzipped, uncompress it.\n\tif filename.endswith(\".gz\"):\n\t\twith gzip.open(archive_path, \"rb\") as gzipped:\n\t\t\twith tempfile.NamedTemporaryFile(delete=False, dir=tmp_path, suffix=\".tar\") as tmp:\n\t\t\t\t# Copy the contents of the gzipped file into the temporary file.\n\t\t\t\tcopyfileobj(gzipped, tmp)\n\t\t\t\ttmp_archive_path = tmp.name\n\telse:\n\t\ttmp_archive_path = archive_path\n\n\trepo_path = path.join(tmp_path, \"repo\")\n\twith tarfile.open(tmp_archive_path) as tar:\n\t\ttar.extractall(path=repo_path)\n\t\ttar.close()\n\tos.remove(tmp_archive_path)\n\treturn repo_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.rsplit('/', 1)[-1]\n\tfile_repo_path = str(tmp_path / filename)\n\n\tif filename.endswith(\".tar.gz\"):\n\t\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(path=file_repo_path)\n\telif filename.endswith(\".tar.bz2\"):\n\t\twith tarfile.open(archive_path, \"r:bz2\") as tar:\n\t\t\ttar.extractall(path=file_repo_path)\n\telif filename.endswith(\".zip\"):\n\t\twith zipfile.ZipFile(archive_path, \"r\") as z:\n\t\t\tz.extractall(path=file_repo_path)\n\treturn file_repo_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\tif isinstance(tmp_path, str):\n\t\ttmp_path = Path(tmp_path)\n\n\ttmp_path = tmp_path / filename.split(\".\")[0]\n\ttmp_path.mkdir(exist_ok=True)\n\n\tif not tmp_path.exists():\n\t\traise ValueError(f\"{tmp_path} does not exist\")\n\n\twith TemporaryDirectory(dir=tmp_path) as dir_path:\n\t\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(dir_path)\n\n\t\t\treturn f\"file://{dir_path}\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_5.py", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "ques_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n\tignore = subprocess.Popen(['git', 'ls-files', '-o', '--exclude-standard'], stdout=subprocess.PIPE).communicate()[0]\n\tignore = ignore.splitlines()\n\tignore = sorted(ignore)\n\tfor i in ignore:\n\t\tignored.append('\"' + i + '\"')\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "def addignored(ignored):\n\tignored = subprocess.check_output(['git','ls-files','-o','--exclude-standard'])\n\tignored = ignored.splitlines()\n\tignored = [x for x in ignored if x.startswith('.')]\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "def addignored(ignored):\n\t\n\tignored = os.popen('git ls-files --ignored --exclude-standard').read()\n\tignored = ignored.split('\\n')\n\tignored = filter(lambda x: not x.startswith('#'), ignored)\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "def addignored(ignored):\n\tignored = subprocess.check_output(['git','ls-files','-o','-i','-z']).split('\\0')\n\tignored = [filename for filename in ignored if filename.startswith('.')]\n\treturn ','.join(ignored)\n\n", "def addignored(ignored):\n\tfilelist = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--exclude-standard\"]).decode('utf-8')\n\tfilelist = filelist.split('\\n')\n\tfilelist = [x for x in filelist if x.endswith(ignored)]\n\treturn ','.join(filelist)\n\n", "def addignored(ignored):\n\t# Get the list of files from the git status command\n\toutput = subprocess.check_output([\"git\", \"status\", \"--porcelain\"]).decode()\n\t# Turn the output into a list\n\toutput = output.split(\"\\n\")\n\t# Remove any empty elements in the list\n\toutput = list(filter(None, output))\n\t# Remove any elements that are not ignored\n\toutput = [i for i in output if i.startswith(\"!!\")]\n\t# Remove the leading \"!!\"\n\toutput = [i[2:] for i in output]\n\t# Remove any elements that are directories\n\toutput = [i for i in output if os.path.isfile(i)]\n\t# If the list is not empty\n\tif output:\n\t\t# Join the list with commas\n\t\toutput = \",\".join(output)\n\t\t# Append the ignored list to the existing ignored list\n\t\tignored = ignored + \",\" + output\n\t\t# Return the ignored list\n\t\treturn ignored\n\t# Otherwise\n\telse:\n\t\t# Return the ignored list\n\t\treturn ignored\n", "def addignored(ignored):\n\tignored = \",\".join(sorted(subprocess.check_output([\"git\", \"ls-files\", \"--ignored\", \"--exclude-standard\", \"--others\", \"--directory\"]).split()))\n\treturn ignored\n\n", "def addignored(ignored):\n\tfilelist = gitcommand('git ls-files -i --exclude-standard')\n\tfilelist = filelist.split()\n\tfilelist = [file for file in filelist if file not in ignored]\n\tfilelist = ','.join(filelist)\n\treturn filelist\n\n", "def addignored(ignored):\n\traw = subprocess.Popen(['git', 'ls-files', '-i', '--exclude-standard'], stdout=subprocess.PIPE)\n\tignored = subprocess.check_output(('grep', '-v', '^#'), stdin=raw.stdout)\n\tignored = ignored.split('\\n')\n\tignored = [x for x in ignored if x]\n\treturn ','.join(ignored)\n\n", "def addignored(ignored):\n\tignored = subprocess.check_output(['git', 'ls-files', '-i', '-o', '--exclude-standard']).splitlines()\n\tignored.sort()\n\treturn \", \".join(ignored)\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n\tignore = subprocess.Popen(['git', 'ls-files', '-o', '--exclude-standard'], stdout=subprocess.PIPE).communicate()[0]\n\tignore = ignore.splitlines()\n\tignore = sorted(ignore)\n\tfor i in ignore:\n\t\tignored.append('\"' + i + '\"')\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tignored = subprocess.check_output(['git','ls-files','-o','--exclude-standard'])\n\tignored = ignored.splitlines()\n\tignored = [x for x in ignored if x.startswith('.')]\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t\n\tignored = os.popen('git ls-files --ignored --exclude-standard').read()\n\tignored = ignored.split('\\n')\n\tignored = filter(lambda x: not x.startswith('#'), ignored)\n\tignored = ','.join(ignored)\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tignored = subprocess.check_output(['git','ls-files','-o','-i','-z']).split('\\0')\n\tignored = [filename for filename in ignored if filename.startswith('.')]\n\treturn ','.join(ignored)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tfilelist = subprocess.check_output([\"git\", \"ls-files\", \"--others\", \"--exclude-standard\"]).decode('utf-8')\n\tfilelist = filelist.split('\\n')\n\tfilelist = [x for x in filelist if x.endswith(ignored)]\n\treturn ','.join(filelist)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t# Get the list of files from the git status command\n\toutput = subprocess.check_output([\"git\", \"status\", \"--porcelain\"]).decode()\n\t# Turn the output into a list\n\toutput = output.split(\"\\n\")\n\t# Remove any empty elements in the list\n\toutput = list(filter(None, output))\n\t# Remove any elements that are not ignored\n\toutput = [i for i in output if i.startswith(\"!!\")]\n\t# Remove the leading \"!!\"\n\toutput = [i[2:] for i in output]\n\t# Remove any elements that are directories\n\toutput = [i for i in output if os.path.isfile(i)]\n\t# If the list is not empty\n\tif output:\n\t\t# Join the list with commas\n\t\toutput = \",\".join(output)\n\t\t# Append the ignored list to the existing ignored list\n\t\tignored = ignored + \",\" + output\n\t\t# Return the ignored list\n\t\treturn ignored\n\t# Otherwise\n\telse:\n\t\t# Return the ignored list\n\t\treturn ignored\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tignored = \",\".join(sorted(subprocess.check_output([\"git\", \"ls-files\", \"--ignored\", \"--exclude-standard\", \"--others\", \"--directory\"]).split()))\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tfilelist = gitcommand('git ls-files -i --exclude-standard')\n\tfilelist = filelist.split()\n\tfilelist = [file for file in filelist if file not in ignored]\n\tfilelist = ','.join(filelist)\n\treturn filelist\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\traw = subprocess.Popen(['git', 'ls-files', '-i', '--exclude-standard'], stdout=subprocess.PIPE)\n\tignored = subprocess.check_output(('grep', '-v', '^#'), stdin=raw.stdout)\n\tignored = ignored.split('\\n')\n\tignored = [x for x in ignored if x]\n\treturn ','.join(ignored)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tignored = subprocess.check_output(['git', 'ls-files', '-i', '-o', '--exclude-standard']).splitlines()\n\tignored.sort()\n\treturn \", \".join(ignored)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_11.py", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "ques_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n\treturn False\n\n\n", "def match(filename):\n\tif filename.endswith('.tar.gz'):\n\t\treturn True\n\treturn False\n\n", "def match(filename):\n\tif re.match('.*\\.csv$', filename, re.IGNORECASE):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "def match(filename):\n\treturn filename.endswith(\".qvm\")\n\n\n", "def match(filename):\n\treturn filename.endswith('.plist')\n\n", "def match(filename):\n\treturn filename.endswith(\".db\")\n\n\n", "def match(filename):\n\treturn re.match(r'.*[.](wav|mp3|ogg|flac|wma)$', filename, re.IGNORECASE) is not None\n\n", "def match(filename):\n\treturn filename.endswith(\".pcap\")\n\n", "def match(filename):\n\treturn filename.lower().endswith(\".mdf\")\n\n\n", "def match(filename):\n\treturn filename.endswith('_descriptor.bin')\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\tif filename.endswith('.tar.gz'):\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\tif re.match('.*\\.csv$', filename, re.IGNORECASE):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.endswith(\".qvm\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.endswith('.plist')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.endswith(\".db\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn re.match(r'.*[.](wav|mp3|ogg|flac|wma)$', filename, re.IGNORECASE) is not None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.endswith(\".pcap\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.lower().endswith(\".mdf\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.endswith('_descriptor.bin')\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_1.py", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "ques_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tmatch = re.match(r\"(\\d+)\\s*(\\w+)\", frequency)\n\tif match is None:\n\t\traise ValueError(\"could not parse frequency '{}'\".format(frequency))\n\tamount = int(match.group(1))\n\tunit = match.group(2)\n\n\tif unit == \"second\" or unit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=amount)\n\telif unit == \"minute\" or unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=amount)\n\telif unit == \"hour\" or unit == \"hours\":\n\t\treturn datetime.timedelta(hours=amount)\n\telif unit == \"day\" or unit == \"days\":\n\t\treturn datetime.timedelta(days=amount)\n\telif unit == \"week\" or unit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=amount)\n\telif unit == \"month\" or unit == \"months\":\n\t\n", "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\telif frequency == \"always\":\n\t\treturn \"always\"\n\telse:\n\t\ttry:\n\t\t\tnumber, unit = frequency.split()\n\t\t\tnumber = int(number)\n\t\t\tunit = unit[0]\n\t\t\tif unit == 'd':\n\t\t\t\treturn datetime.timedelta(days=number)\n\t\t\telif unit == 'w':\n\t\t\t\treturn datetime.timedelta(weeks=number)\n\t\t\telif unit == 'm':\n\t\t\t\treturn datetime.timedelta(weeks=number*4)\n\t\t\telif unit == 'y':\n\t\t\t\treturn datetime.timedelta(weeks=number*52)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Unable to parse frequency from '{}'\".format(frequency))\n\t\texcept Exception as e:\n\t\t\traise ValueError(\"Unable to parse frequency from '{}'\".format(frequency))\n\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\t\n\tfrequency_regex = re.match(r\"(\\d+)\\s+(\\w+)\", frequency)\n\tif frequency_regex is None:\n\t\traise ValueError(\"Unrecognized frequency format: %s\" % frequency)\n\t\n\tfrequency_number, frequency_unit = frequency_regex.groups()\n\tif frequency_unit not in (\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"):\n\t\traise ValueError(\"Unrecognized frequency unit: %s\" % frequency_unit)\n\t\n\treturn datetime.timedelta(**{frequency_unit: int(frequency_number)})\n\n", "def parse_frequency(frequency):\n\tif frequency == \"always\" or frequency is None:\n\t\treturn None\n\telse:\n\t\tmatch = FREQUENCY_RE.match(frequency)\n\t\tif match is None:\n\t\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\t(number, unit) = match.groups()\n\t\tnumber = int(number)\n\t\tif unit == \"seconds\" or unit == \"second\":\n\t\t\treturn datetime.timedelta(seconds=number)\n\t\telif unit == \"minutes\" or unit == \"minute\":\n\t\t\treturn datetime.timedelta(minutes=number)\n\t\telif unit == \"hours\" or unit == \"hour\":\n\t\t\treturn datetime.timedelta(hours=number)\n\t\telif unit == \"days\" or unit == \"day\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif unit == \"weeks\" or unit == \"week\":\n\t\t\treturn datetime.timedelta(days=number * 7)\n\t\telif unit == \"months\"\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\tif frequency == \"never\":\n\t\treturn datetime.timedelta(weeks=52*10)\n\tfrequency = frequency.lower()\n\tmatch = re.match(r'^\\s*(\\d+)\\s*(\\w+)\\s*$', frequency)\n\tif match:\n\t\tnumber = int(match.group(1))\n\t\tunit = match.group(2)\n\t\tif unit == \"day\" or unit == \"days\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif unit == \"week\" or unit == \"weeks\":\n\t\t\treturn datetime.timedelta(weeks=number)\n\t\telif unit == \"month\" or unit == \"months\":\n\t\t\treturn datetime.timedelta(weeks=number*4)\n\t\telif unit == \"year\" or unit == \"years\":\n\t\t\treturn datetime.timedelta(weeks=number*52)\n\traise ValueError(\"Invalid frequency: {}\".format(\n", "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tnumber, unit = frequency.split(\" \")\n\tnumber = int(number)\n\tif unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=number)\n\telse:\n\t\traise ValueError(\"invalid unit: %s\" % unit)\n\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tfrequency_re = re.compile(r\"^(\\d+) *(\\w+)$\")\n\tmatch = frequency_re.match(frequency)\n\tif match is None:\n\t\traise ValueError(f\"Could not parse frequency: {frequency}\")\n\n\tnum, unit = match.groups()\n\tnum = int(num)\n\n\tif unit.startswith(\"min\"):\n\t\treturn datetime.timedelta(minutes=num)\n\telif unit.startswith(\"hour\"):\n\t\treturn datetime.timedelta(hours=num)\n\telif unit.startswith(\"day\"):\n\t\treturn datetime.timedelta(days=num)\n\telif unit.startswith(\"week\"):\n\t\treturn datetime.timedelta(weeks=num)\n\telif unit.startswith(\"month\"):\n\t\treturn datetime.timedelta(weeks=4 * num)\n\telif unit.startswith(\"year\"):\n\t\treturn datetime\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\tmatch = re.match(r\"(\\d+)\\s+(\\w+)\", frequency)\n\tif not match:\n\t\traise ValueError(\"Unable to parse frequency '%s'\" % frequency)\n\tnumber = int(match.group(1))\n\tunit = match.group(2)\n\tif unit.lower() == \"minute\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit.lower() == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit.lower() == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit.lower() == \"week\":\n\t\treturn datetime.timedelta(weeks=number)\n\telif unit.lower() == \"month\":\n\t\treturn datetime.timedelta(days=30*number)\n\telif unit.lower() == \"year\":\n\t\treturn datetime.timedelta(days=365*number)\n\t\n", "def parse_frequency(frequency):\n\tif frequency == \"always\":\n\t\treturn None\n\n\tif frequency is None:\n\t\treturn None\n\n\tif frequency == \"weekly\":\n\t\treturn datetime.timedelta(weeks=1)\n\n\tif frequency == \"daily\":\n\t\treturn datetime.timedelta(days=1)\n\n\tif frequency == \"monthly\":\n\t\treturn datetime.timedelta(weeks=4)\n\n\tif frequency == \"yearly\":\n\t\treturn datetime.timedelta(weeks=52)\n\n\traise ValueError(f\"Unknown frequency: {frequency}\")\n\n\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tfrequency = frequency.strip()\n\tmatch = FREQUENCY_PATTERN.match(frequency)\n\tif match is None:\n\t\traise ValueError(\"Cannot parse frequency %r\" % frequency)\n\n\tnumber, unit = match.groups()\n\tnumber = int(number)\n\n\tif unit in (\"minutes\", \"min\"):\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit in (\"hours\", \"hour\"):\n\t\treturn datetime.timedelta(hours=number)\n\telif unit in (\"days\", \"day\"):\n\t\treturn datetime.timedelta(days=number)\n\telif unit in (\"weeks\", \"week\"):\n\t\treturn datetime.timedelta(weeks=number)\n\telif unit in (\"months\", \"month\"):\n\t\treturn datetime.timedelta(days=number*30)\n\telif unit in (\"years\", \"year\"):\n\t\treturn datetime.timedelta(days=number*365)\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tmatch = re.match(r\"(\\d+)\\s*(\\w+)\", frequency)\n\tif match is None:\n\t\traise ValueError(\"could not parse frequency '{}'\".format(frequency))\n\tamount = int(match.group(1))\n\tunit = match.group(2)\n\n\tif unit == \"second\" or unit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=amount)\n\telif unit == \"minute\" or unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=amount)\n\telif unit == \"hour\" or unit == \"hours\":\n\t\treturn datetime.timedelta(hours=amount)\n\telif unit == \"day\" or unit == \"days\":\n\t\treturn datetime.timedelta(days=amount)\n\telif unit == \"week\" or unit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=amount)\n\telif unit == \"month\" or unit == \"months\":\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\telif frequency == \"always\":\n\t\treturn \"always\"\n\telse:\n\t\ttry:\n\t\t\tnumber, unit = frequency.split()\n\t\t\tnumber = int(number)\n\t\t\tunit = unit[0]\n\t\t\tif unit == 'd':\n\t\t\t\treturn datetime.timedelta(days=number)\n\t\t\telif unit == 'w':\n\t\t\t\treturn datetime.timedelta(weeks=number)\n\t\t\telif unit == 'm':\n\t\t\t\treturn datetime.timedelta(weeks=number*4)\n\t\t\telif unit == 'y':\n\t\t\t\treturn datetime.timedelta(weeks=number*52)\n\t\t\telse:\n\t\t\t\traise ValueError(\"Unable to parse frequency from '{}'\".format(frequency))\n\t\texcept Exception as e:\n\t\t\traise ValueError(\"Unable to parse frequency from '{}'\".format(frequency))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\t\n\tfrequency_regex = re.match(r\"(\\d+)\\s+(\\w+)\", frequency)\n\tif frequency_regex is None:\n\t\traise ValueError(\"Unrecognized frequency format: %s\" % frequency)\n\t\n\tfrequency_number, frequency_unit = frequency_regex.groups()\n\tif frequency_unit not in (\"seconds\", \"minutes\", \"hours\", \"days\", \"weeks\", \"months\", \"years\"):\n\t\traise ValueError(\"Unrecognized frequency unit: %s\" % frequency_unit)\n\t\n\treturn datetime.timedelta(**{frequency_unit: int(frequency_number)})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency == \"always\" or frequency is None:\n\t\treturn None\n\telse:\n\t\tmatch = FREQUENCY_RE.match(frequency)\n\t\tif match is None:\n\t\t\traise ValueError(\"Invalid frequency: %s\" % frequency)\n\t\t(number, unit) = match.groups()\n\t\tnumber = int(number)\n\t\tif unit == \"seconds\" or unit == \"second\":\n\t\t\treturn datetime.timedelta(seconds=number)\n\t\telif unit == \"minutes\" or unit == \"minute\":\n\t\t\treturn datetime.timedelta(minutes=number)\n\t\telif unit == \"hours\" or unit == \"hour\":\n\t\t\treturn datetime.timedelta(hours=number)\n\t\telif unit == \"days\" or unit == \"day\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif unit == \"weeks\" or unit == \"week\":\n\t\t\treturn datetime.timedelta(days=number * 7)\n\t\telif unit == \"months\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\tif frequency == \"never\":\n\t\treturn datetime.timedelta(weeks=52*10)\n\tfrequency = frequency.lower()\n\tmatch = re.match(r'^\\s*(\\d+)\\s*(\\w+)\\s*$', frequency)\n\tif match:\n\t\tnumber = int(match.group(1))\n\t\tunit = match.group(2)\n\t\tif unit == \"day\" or unit == \"days\":\n\t\t\treturn datetime.timedelta(days=number)\n\t\telif unit == \"week\" or unit == \"weeks\":\n\t\t\treturn datetime.timedelta(weeks=number)\n\t\telif unit == \"month\" or unit == \"months\":\n\t\t\treturn datetime.timedelta(weeks=number*4)\n\t\telif unit == \"year\" or unit == \"years\":\n\t\t\treturn datetime.timedelta(weeks=number*52)\n\traise ValueError(\"Invalid frequency: {}\".format(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tnumber, unit = frequency.split(\" \")\n\tnumber = int(number)\n\tif unit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit == \"days\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit == \"weeks\":\n\t\treturn datetime.timedelta(weeks=number)\n\telse:\n\t\traise ValueError(\"invalid unit: %s\" % unit)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tfrequency_re = re.compile(r\"^(\\d+) *(\\w+)$\")\n\tmatch = frequency_re.match(frequency)\n\tif match is None:\n\t\traise ValueError(f\"Could not parse frequency: {frequency}\")\n\n\tnum, unit = match.groups()\n\tnum = int(num)\n\n\tif unit.startswith(\"min\"):\n\t\treturn datetime.timedelta(minutes=num)\n\telif unit.startswith(\"hour\"):\n\t\treturn datetime.timedelta(hours=num)\n\telif unit.startswith(\"day\"):\n\t\treturn datetime.timedelta(days=num)\n\telif unit.startswith(\"week\"):\n\t\treturn datetime.timedelta(weeks=num)\n\telif unit.startswith(\"month\"):\n\t\treturn datetime.timedelta(weeks=4 * num)\n\telif unit.startswith(\"year\"):\n\t\treturn datetime\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\tmatch = re.match(r\"(\\d+)\\s+(\\w+)\", frequency)\n\tif not match:\n\t\traise ValueError(\"Unable to parse frequency '%s'\" % frequency)\n\tnumber = int(match.group(1))\n\tunit = match.group(2)\n\tif unit.lower() == \"minute\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit.lower() == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif unit.lower() == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif unit.lower() == \"week\":\n\t\treturn datetime.timedelta(weeks=number)\n\telif unit.lower() == \"month\":\n\t\treturn datetime.timedelta(days=30*number)\n\telif unit.lower() == \"year\":\n\t\treturn datetime.timedelta(days=365*number)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency == \"always\":\n\t\treturn None\n\n\tif frequency is None:\n\t\treturn None\n\n\tif frequency == \"weekly\":\n\t\treturn datetime.timedelta(weeks=1)\n\n\tif frequency == \"daily\":\n\t\treturn datetime.timedelta(days=1)\n\n\tif frequency == \"monthly\":\n\t\treturn datetime.timedelta(weeks=4)\n\n\tif frequency == \"yearly\":\n\t\treturn datetime.timedelta(weeks=52)\n\n\traise ValueError(f\"Unknown frequency: {frequency}\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\tfrequency = frequency.strip()\n\tmatch = FREQUENCY_PATTERN.match(frequency)\n\tif match is None:\n\t\traise ValueError(\"Cannot parse frequency %r\" % frequency)\n\n\tnumber, unit = match.groups()\n\tnumber = int(number)\n\n\tif unit in (\"minutes\", \"min\"):\n\t\treturn datetime.timedelta(minutes=number)\n\telif unit in (\"hours\", \"hour\"):\n\t\treturn datetime.timedelta(hours=number)\n\telif unit in (\"days\", \"day\"):\n\t\treturn datetime.timedelta(days=number)\n\telif unit in (\"weeks\", \"week\"):\n\t\treturn datetime.timedelta(weeks=number)\n\telif unit in (\"months\", \"month\"):\n\t\treturn datetime.timedelta(days=number*30)\n\telif unit in (\"years\", \"year\"):\n\t\treturn datetime.timedelta(days=number*365)\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_18.py", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "ques_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n\treturn host in ['localhost', '127.0.0.1', '::1', 'localhost6', socket.gethostname()]\n\n\n", "def is_local(host):\n\treturn host in ['localhost', '127.0.0.1', '::1', get_localhost_ip()]\n\n\n", "def is_local(host):\n\ttry:\n\t\tif is_ipv4(host):\n\t\t\treturn host == '127.0.0.1'\n\t\telse:\n\t\t\treturn host == 'localhost'\n\texcept:\n\t\treturn False\n\n\n", "def is_local(host):\n\treturn host in ('localhost', '127.0.0.1', '::1')\n\n\n", "def is_local(host):\n\tif host == 'localhost' or host == '127.0.0.1' or host == '::1':\n\t\treturn True\n\treturn False\n\n\n", "def is_local(host):\n\tif host == 'localhost':\n\t\treturn True\n\tif host == '127.0.0.1':\n\t\treturn True\n\treturn False\n\n\n", "def is_local(host):\n\ttry:\n\t\treturn socket.gethostbyname(host) == '127.0.0.1'\n\texcept Exception:\n\t\treturn False\n\n\n", "def is_local(host):\n\treturn host in [\"localhost\", \"127.0.0.1\"]\n\n\n", "def is_local(host):\n\tif host == '127.0.0.1':\n\t\treturn True\n\tif host == 'localhost':\n\t\treturn True\n\tif host == '::1':\n\t\treturn True\n\tif host == socket.gethostname():\n\t\treturn True\n\treturn False\n\n\n", "def is_local(host):\n\treturn host in ('localhost', '127.0.0.1', '::1')\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n\treturn host in ['localhost', '127.0.0.1', '::1', 'localhost6', socket.gethostname()]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\treturn host in ['localhost', '127.0.0.1', '::1', get_localhost_ip()]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\ttry:\n\t\tif is_ipv4(host):\n\t\t\treturn host == '127.0.0.1'\n\t\telse:\n\t\t\treturn host == 'localhost'\n\texcept:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\treturn host in ('localhost', '127.0.0.1', '::1')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\tif host == 'localhost' or host == '127.0.0.1' or host == '::1':\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\tif host == 'localhost':\n\t\treturn True\n\tif host == '127.0.0.1':\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\ttry:\n\t\treturn socket.gethostbyname(host) == '127.0.0.1'\n\texcept Exception:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\treturn host in [\"localhost\", \"127.0.0.1\"]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\tif host == '127.0.0.1':\n\t\treturn True\n\tif host == 'localhost':\n\t\treturn True\n\tif host == '::1':\n\t\treturn True\n\tif host == socket.gethostname():\n\t\treturn True\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\treturn host in ('localhost', '127.0.0.1', '::1')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_8.py", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "ques_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n\treturn [make_find_path_glob(path) for path in find_paths]\n\n\n", "def make_find_paths(find_paths):\n\t# pylint: disable=unused-argument\n\tdef _make_find_paths(context):\n\t\tif context.args.find_paths:\n\t\t\tresult = []\n\t\t\tfor value in context.args.find_paths:\n\t\t\t\tif value.startswith('pp:'):\n\t\t\t\t\tresult.append(value)\n\t\t\t\telse:\n\t\t\t\t\tresult.append('sh:**/*%s*/**' % value)\n\t\t\tcontext.args.find_paths = result\n\treturn _make_find_paths\n\n", "def make_find_paths(find_paths):\n\tdef transform_path(path):\n\t\tif path.startswith('sh:') or path.startswith('pp:'):\n\t\t\treturn path\n\t\treturn 'sh:**/*%s*/**' % path\n\treturn [transform_path(path) for path in find_paths]\n\n", "def make_find_paths(find_paths):\n\tif find_paths is None:\n\t\treturn None\n\n\tdef transform_path(path):\n\t\treturn 'sh:**/*{}*/**'.format(path)\n\n\treturn [transform_path(path) if not path.startswith('pp:') else path for path in find_paths]\n", "def make_find_paths(find_paths):\n\timport re\n\tdef is_glob(p):\n\t\treturn re.match(r'[*?\\[\\]{}]', p)\n\tdef is_pp(p):\n\t\treturn p.startswith('pp:')\n\tdef is_sh(p):\n\t\treturn p.startswith('sh:')\n\tdef is_path_fragment(p):\n\t\treturn not is_glob(p) and not is_pp(p) and not is_sh(p)\n\tdef path_fragment_to_glob_pattern(p):\n\t\treturn 'sh:**/*{}*/**'.format(p)\n\treturn [\n\t\tpath_fragment_to_glob_pattern(p)\n\t\tif is_path_fragment(p)\n\t\telse p\n\t\tfor p in find_paths\n\t]\n\n", "def make_find_paths(find_paths):\n\treturn [\n\t\tmake_find_path_glob(find_path)\n\t\tfor find_path in find_paths\n\t]\n\n", "def make_find_paths(find_paths):\n\treturn [find_path_to_glob(x) for x in find_paths]\n\n\n", "def make_find_paths(find_paths):\n\tpatterns = []\n\tfor path in find_paths:\n\t\tif ':' in path:\n\t\t\tif path.startswith('pp:'):\n\t\t\t\t# Pretend pattern\n\t\t\t\tpatterns.append(path)\n\t\t\telif path.startswith('sh:'):\n\t\t\t\t# Shell glob\n\t\t\t\tpatterns.append(path)\n\t\t\telif path.startswith('re:'):\n\t\t\t\t# Regex\n\t\t\t\tpatterns.append(path)\n\t\t\telse:\n\t\t\t\t# Pretend pattern\n\t\t\t\tpatterns.append('pp:' + path)\n\t\telse:\n\t\t\tpatterns.append('sh:**/*' + path + '*/**')\n\treturn patterns\n\n", "def make_find_paths(find_paths):\n\tfind_paths = list(find_paths)\n\tfor i, path in enumerate(find_paths):\n\t\tif not path.startswith(('pp:', 'sh:')):\n\t\t\tfind_paths[i] = 'sh:**/*{}*/**'.format(path)\n\treturn find_paths\n\n", "def make_find_paths(find_paths):\n\ttransformed_find_paths = []\n\tfor path in find_paths:\n\t\tif path.startswith(\"pp:\") or path.startswith(\"sh:\"):\n\t\t\t# Pass through any patterns untouched.\n\t\t\ttransformed_find_paths.append(path)\n\t\telse:\n\t\t\t# Transform any path fragments into glob patterns.\n\t\t\ttransformed_find_paths.append(\"sh:**/*{}*/**\".format(path))\n\treturn transformed_find_paths\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n\treturn [make_find_path_glob(path) for path in find_paths]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\t# pylint: disable=unused-argument\n\tdef _make_find_paths(context):\n\t\tif context.args.find_paths:\n\t\t\tresult = []\n\t\t\tfor value in context.args.find_paths:\n\t\t\t\tif value.startswith('pp:'):\n\t\t\t\t\tresult.append(value)\n\t\t\t\telse:\n\t\t\t\t\tresult.append('sh:**/*%s*/**' % value)\n\t\t\tcontext.args.find_paths = result\n\treturn _make_find_paths\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tdef transform_path(path):\n\t\tif path.startswith('sh:') or path.startswith('pp:'):\n\t\t\treturn path\n\t\treturn 'sh:**/*%s*/**' % path\n\treturn [transform_path(path) for path in find_paths]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tif find_paths is None:\n\t\treturn None\n\n\tdef transform_path(path):\n\t\treturn 'sh:**/*{}*/**'.format(path)\n\n\treturn [transform_path(path) if not path.startswith('pp:') else path for path in find_paths]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\timport re\n\tdef is_glob(p):\n\t\treturn re.match(r'[*?\\[\\]{}]', p)\n\tdef is_pp(p):\n\t\treturn p.startswith('pp:')\n\tdef is_sh(p):\n\t\treturn p.startswith('sh:')\n\tdef is_path_fragment(p):\n\t\treturn not is_glob(p) and not is_pp(p) and not is_sh(p)\n\tdef path_fragment_to_glob_pattern(p):\n\t\treturn 'sh:**/*{}*/**'.format(p)\n\treturn [\n\t\tpath_fragment_to_glob_pattern(p)\n\t\tif is_path_fragment(p)\n\t\telse p\n\t\tfor p in find_paths\n\t]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\treturn [\n\t\tmake_find_path_glob(find_path)\n\t\tfor find_path in find_paths\n\t]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\treturn [find_path_to_glob(x) for x in find_paths]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tpatterns = []\n\tfor path in find_paths:\n\t\tif ':' in path:\n\t\t\tif path.startswith('pp:'):\n\t\t\t\t# Pretend pattern\n\t\t\t\tpatterns.append(path)\n\t\t\telif path.startswith('sh:'):\n\t\t\t\t# Shell glob\n\t\t\t\tpatterns.append(path)\n\t\t\telif path.startswith('re:'):\n\t\t\t\t# Regex\n\t\t\t\tpatterns.append(path)\n\t\t\telse:\n\t\t\t\t# Pretend pattern\n\t\t\t\tpatterns.append('pp:' + path)\n\t\telse:\n\t\t\tpatterns.append('sh:**/*' + path + '*/**')\n\treturn patterns\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tfind_paths = list(find_paths)\n\tfor i, path in enumerate(find_paths):\n\t\tif not path.startswith(('pp:', 'sh:')):\n\t\t\tfind_paths[i] = 'sh:**/*{}*/**'.format(path)\n\treturn find_paths\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\ttransformed_find_paths = []\n\tfor path in find_paths:\n\t\tif path.startswith(\"pp:\") or path.startswith(\"sh:\"):\n\t\t\t# Pass through any patterns untouched.\n\t\t\ttransformed_find_paths.append(path)\n\t\telse:\n\t\t\t# Transform any path fragments into glob patterns.\n\t\t\ttransformed_find_paths.append(\"sh:**/*{}*/**\".format(path))\n\treturn transformed_find_paths\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_19.py", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "ques_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n\tif os.name == \"nt\":\n\t\tif os.getenv(\"MSYSTEM\") == \"MINGW32\":\n\t\t\treturn True\n\treturn False\n\n\n", "def is_gitbash():\n\treturn os.environ.get('TERM', None) == 'msys'\n\n\n", "def is_gitbash():\n\treturn os.path.exists('/etc/bash.bashrc')\n\n\n", "def is_gitbash():\n\tif sys.platform == 'win32':\n\t\treturn os.path.basename(os.environ.get('SHELL', '')) == 'bash.exe'\n\treturn False\n\n\n\n", "def is_gitbash():\n\treturn os.environ.get('MSYSTEM')\n\n\n", "def is_gitbash():\n\ttry:\n\t\treturn os.environ['MSYSTEM'] == 'MINGW32'\n\texcept KeyError:\n\t\treturn False\n\n\n", "def is_gitbash():\n\t# get current platform\n\tplatform = sys.platform\n\tif platform in [\"win32\", \"cygwin\"]:\n\t\t# if platform is windows, check if gitbash is used or not\n\t\tif os.getenv(\"MSYSTEM\") is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "def is_gitbash():\n\t# gitbash is a special windows shell\n\tif os.name == \"nt\" and \"git-bash\" in os.getenv(\"TERM\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_gitbash():\n\treturn os.path.isdir(\"/c\")\n\n\n", "def is_gitbash():\n\tif re.match(r\".*MINGW.*\", os.environ['TERM'], re.IGNORECASE):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n\tif os.name == \"nt\":\n\t\tif os.getenv(\"MSYSTEM\") == \"MINGW32\":\n\t\t\treturn True\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\treturn os.environ.get('TERM', None) == 'msys'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\treturn os.path.exists('/etc/bash.bashrc')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\tif sys.platform == 'win32':\n\t\treturn os.path.basename(os.environ.get('SHELL', '')) == 'bash.exe'\n\treturn False\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\treturn os.environ.get('MSYSTEM')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\ttry:\n\t\treturn os.environ['MSYSTEM'] == 'MINGW32'\n\texcept KeyError:\n\t\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\t# get current platform\n\tplatform = sys.platform\n\tif platform in [\"win32\", \"cygwin\"]:\n\t\t# if platform is windows, check if gitbash is used or not\n\t\tif os.getenv(\"MSYSTEM\") is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\t# gitbash is a special windows shell\n\tif os.name == \"nt\" and \"git-bash\" in os.getenv(\"TERM\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\treturn os.path.isdir(\"/c\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\tif re.match(r\".*MINGW.*\", os.environ['TERM'], re.IGNORECASE):\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_9.py", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "ques_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename):\n\t\tif overwrite:\n\t\t\tos.remove(config_filename)\n\t\telse:\n\t\t\traise Exception('Config file %s already exists, aborting' % config_filename)\n\tdirname = os.path.dirname(config_filename)\n\tif not os.path.exists(dirname):\n\t\tos.makedirs(dirname)\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\tlogger.error(\"%s already exists. Not overwriting without --overwrite\", config_filename)\n\t\traise FileExistsError(config_filename)\n\n\tlogger.debug(\"Writing configuration to %s\", config_filename)\n\ttry:\n\t\tos.makedirs(os.path.dirname(config_filename), exist_ok=True)\n\texcept OSError as e:\n\t\tlogger.error(\"Error creating directory %s: %s\", os.path.dirname(config_filename), e)\n\t\tsys.exit(1)\n\n\ttry:\n\t\twith open(config_filename, 'w') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\texcept OSError as e:\n\t\tlogger.error(\"Error writing to %s: %s\", config_filename, e)\n\t\tsys.exit(1)\n\n\t# Change permissions to be safe\n\ttry:\n\t\tos.chmod(config_filename, mode)\n\t\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise FileExistsError(\"File already exists: %s\" % config_filename)\n\tdirectory = os.path.dirname(config_filename)\n\tif not os.path.exists(directory):\n\t\tos.makedirs(directory, mode=0o755)\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\treturn False\n\n\tos.makedirs(os.path.dirname(config_filename), exist_ok=True)\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n\treturn True\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise RuntimeError(\"File exists, will not overwrite: {}\".format(config_filename))\n\tconfig_dir = os.path.dirname(config_filename)\n\tif not os.path.exists(config_dir):\n\t\tos.makedirs(config_dir, mode=0o700)\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# TODO: This should be a method on the Config class, as it's not really a function\n\t# but an action on a specific object.\n\tif config_filename == '-':\n\t\tdest_fd = sys.stdout\n\t\tdest_fd.write(rendered_config)\n\t\tdest_fd.write(\"\\n\")\n\t\tdest_fd.flush()\n\telse:\n\t\tdest_filename = os.path.expanduser(config_filename)\n\t\tif os.path.exists(dest_filename) and not overwrite:\n\t\t\traise ConfigurationError(\"%s already exists; not overwriting\" % config_filename)\n\t\tpath = os.path.dirname(dest_filename)\n\t\tif path:\n\t\t\ttry:\n\t\t\t\tos.makedirs(path)\n\t\t\texcept OSError:\n\t\t\t\tpass\n\t\tdest_fd = os.open(dest_filename, os.O_WRONLY | os.O_CREAT, mode)\n\t\ttry:\n\t\t\tos.write(dest_\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise ConfigurationError('Config file already exists: %s' % config_filename)\n\n\tparent_dir = os.path.dirname(config_filename)\n\tif parent_dir:\n\t\tos.makedirs(parent_dir, exist_ok=True)\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename):\n\t\tif not overwrite:\n\t\t\traise IOError(\"{} already exists; refusing to overwrite\".format(config_filename))\n\telse:\n\t\t# Path doesn't exist; create containing directories.\n\t\tos.makedirs(os.path.dirname(config_filename), exist_ok=True)\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# First, create the directory this file will live in, if needed\n\tconfig_path = os.path.dirname(config_filename)\n\tif not os.path.exists(config_path):\n\t\tos.makedirs(config_path)\n\n\t# Next, if the file already exists, and we're not overwriting, abort\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tlog.critical(\"Configuration file already exists, and overwrite not specified: \" + config_filename)\n\t\tsys.exit(1)\n\n\t# Then, write the file out\n\twith open(config_filename, \"w\") as config_file:\n\t\tconfig_file.write(rendered_config)\n\n\t# And finally, set the permissions on the file to the requested mode\n\tos.chmod(config_filename, mode)\n\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif config_filename.endswith('/'):\n\t\traise ValueError('config_filename should be a file, not a directory')\n\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise ValueError('Config file already exists: {}'.format(config_filename))\n\n\t# Ensure the parent directory exists\n\tparent_dir = os.path.dirname(config_filename)\n\tif not os.path.exists(parent_dir):\n\t\tos.makedirs(parent_dir)\n\n\t# Write the file\n\twith open(config_filename, 'w') as fh:\n\t\tfh.write(rendered_config)\n\n\tos.chmod(config_filename, mode)\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename):\n\t\tif overwrite:\n\t\t\tos.remove(config_filename)\n\t\telse:\n\t\t\traise Exception('Config file %s already exists, aborting' % config_filename)\n\tdirname = os.path.dirname(config_filename)\n\tif not os.path.exists(dirname):\n\t\tos.makedirs(dirname)\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\tlogger.error(\"%s already exists. Not overwriting without --overwrite\", config_filename)\n\t\traise FileExistsError(config_filename)\n\n\tlogger.debug(\"Writing configuration to %s\", config_filename)\n\ttry:\n\t\tos.makedirs(os.path.dirname(config_filename), exist_ok=True)\n\texcept OSError as e:\n\t\tlogger.error(\"Error creating directory %s: %s\", os.path.dirname(config_filename), e)\n\t\tsys.exit(1)\n\n\ttry:\n\t\twith open(config_filename, 'w') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\texcept OSError as e:\n\t\tlogger.error(\"Error writing to %s: %s\", config_filename, e)\n\t\tsys.exit(1)\n\n\t# Change permissions to be safe\n\ttry:\n\t\tos.chmod(config_filename, mode)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise FileExistsError(\"File already exists: %s\" % config_filename)\n\tdirectory = os.path.dirname(config_filename)\n\tif not os.path.exists(directory):\n\t\tos.makedirs(directory, mode=0o755)\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\treturn False\n\n\tos.makedirs(os.path.dirname(config_filename), exist_ok=True)\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise RuntimeError(\"File exists, will not overwrite: {}\".format(config_filename))\n\tconfig_dir = os.path.dirname(config_filename)\n\tif not os.path.exists(config_dir):\n\t\tos.makedirs(config_dir, mode=0o700)\n\twith open(config_filename, \"w\") as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# TODO: This should be a method on the Config class, as it's not really a function\n\t# but an action on a specific object.\n\tif config_filename == '-':\n\t\tdest_fd = sys.stdout\n\t\tdest_fd.write(rendered_config)\n\t\tdest_fd.write(\"\\n\")\n\t\tdest_fd.flush()\n\telse:\n\t\tdest_filename = os.path.expanduser(config_filename)\n\t\tif os.path.exists(dest_filename) and not overwrite:\n\t\t\traise ConfigurationError(\"%s already exists; not overwriting\" % config_filename)\n\t\tpath = os.path.dirname(dest_filename)\n\t\tif path:\n\t\t\ttry:\n\t\t\t\tos.makedirs(path)\n\t\t\texcept OSError:\n\t\t\t\tpass\n\t\tdest_fd = os.open(dest_filename, os.O_WRONLY | os.O_CREAT, mode)\n\t\ttry:\n\t\t\tos.write(dest_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise ConfigurationError('Config file already exists: %s' % config_filename)\n\n\tparent_dir = os.path.dirname(config_filename)\n\tif parent_dir:\n\t\tos.makedirs(parent_dir, exist_ok=True)\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename):\n\t\tif not overwrite:\n\t\t\traise IOError(\"{} already exists; refusing to overwrite\".format(config_filename))\n\telse:\n\t\t# Path doesn't exist; create containing directories.\n\t\tos.makedirs(os.path.dirname(config_filename), exist_ok=True)\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# First, create the directory this file will live in, if needed\n\tconfig_path = os.path.dirname(config_filename)\n\tif not os.path.exists(config_path):\n\t\tos.makedirs(config_path)\n\n\t# Next, if the file already exists, and we're not overwriting, abort\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tlog.critical(\"Configuration file already exists, and overwrite not specified: \" + config_filename)\n\t\tsys.exit(1)\n\n\t# Then, write the file out\n\twith open(config_filename, \"w\") as config_file:\n\t\tconfig_file.write(rendered_config)\n\n\t# And finally, set the permissions on the file to the requested mode\n\tos.chmod(config_filename, mode)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif config_filename.endswith('/'):\n\t\traise ValueError('config_filename should be a file, not a directory')\n\n\tif os.path.exists(config_filename) and not overwrite:\n\t\traise ValueError('Config file already exists: {}'.format(config_filename))\n\n\t# Ensure the parent directory exists\n\tparent_dir = os.path.dirname(config_filename)\n\tif not os.path.exists(parent_dir):\n\t\tos.makedirs(parent_dir)\n\n\t# Write the file\n\twith open(config_filename, 'w') as fh:\n\t\tfh.write(rendered_config)\n\n\tos.chmod(config_filename, mode)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_22.py", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "ques_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n\tif script.strip().startswith(\"#!\"):\n\t\tscript = script[script.find('\\n')+1:]\n\tresult = []\n\tfor line in script.split('\\n'):\n\t\tline = line.strip()\n\t\tif line.startswith('#') or not line:\n\t\t\tcontinue\n\t\tif line.startswith('cd'):\n\t\t\tline = 'cd \"%s\"' % line.split(' ', 1)[1]\n\t\tresult.append(line)\n\treturn seperator.join(result)\n\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n\n", "def oneline(script, seperator=\" && \"):\n\tscript = '\\n'.join(script)\n\tscript = script.replace('\\n', seperator)\n\treturn script\n\n", "def oneline(script, seperator=\" && \"):\n\tif script is None:\n\t\treturn \"\"\n\tscript = [x.strip() for x in script.split(\"\\n\") if x.strip() != \"\"]\n\treturn seperator.join(script)\n\n\n", "def oneline(script, seperator=\" && \"):\n\tif isinstance(script, str):\n\t\treturn seperator.join(script.split(\"\\n\"))\n\telse:\n\t\treturn seperator.join(script)\n\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script)\n\n", "def oneline(script, seperator=\" && \"):\n\tresult = \"\"\n\tfor line in script.splitlines():\n\t\tline = line.strip()\n\t\tif line.startswith(\"#\"):\n\t\t\tcontinue\n\t\tif not line:\n\t\t\tcontinue\n\t\tresult += line + seperator\n\treturn result[:-len(seperator)]\n\n", "def oneline(script, seperator=\" && \"):\n\tcmds = []\n\tfor line in script.split(\"\\n\"):\n\t\tif not line:\n\t\t\tcontinue\n\t\tif line.startswith(\"#\"):\n\t\t\tcontinue\n\t\tif \"cd \" in line:\n\t\t\tline = \" && \".join([cmd.strip() for cmd in line.split(\" && \") if cmd.strip()])\n\t\tcmds.append(line)\n\treturn seperator.join(cmds)\n\n\n", "def oneline(script, seperator=\" && \"):\n\tresult = []\n\tfor line in script:\n\t\tline2 = line.strip()\n\t\tif line2:\n\t\t\tresult.append(line2)\n\treturn seperator.join(result)\n", "def oneline(script, seperator=\" && \"):\n\tif isinstance(script, (list, tuple)):\n\t\tscript = seperator.join(script)\n\treturn script\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n\tif script.strip().startswith(\"#!\"):\n\t\tscript = script[script.find('\\n')+1:]\n\tresult = []\n\tfor line in script.split('\\n'):\n\t\tline = line.strip()\n\t\tif line.startswith('#') or not line:\n\t\t\tcontinue\n\t\tif line.startswith('cd'):\n\t\t\tline = 'cd \"%s\"' % line.split(' ', 1)[1]\n\t\tresult.append(line)\n\treturn seperator.join(result)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tscript = '\\n'.join(script)\n\tscript = script.replace('\\n', seperator)\n\treturn script\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tif script is None:\n\t\treturn \"\"\n\tscript = [x.strip() for x in script.split(\"\\n\") if x.strip() != \"\"]\n\treturn seperator.join(script)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tif isinstance(script, str):\n\t\treturn seperator.join(script.split(\"\\n\"))\n\telse:\n\t\treturn seperator.join(script)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tresult = \"\"\n\tfor line in script.splitlines():\n\t\tline = line.strip()\n\t\tif line.startswith(\"#\"):\n\t\t\tcontinue\n\t\tif not line:\n\t\t\tcontinue\n\t\tresult += line + seperator\n\treturn result[:-len(seperator)]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tcmds = []\n\tfor line in script.split(\"\\n\"):\n\t\tif not line:\n\t\t\tcontinue\n\t\tif line.startswith(\"#\"):\n\t\t\tcontinue\n\t\tif \"cd \" in line:\n\t\t\tline = \" && \".join([cmd.strip() for cmd in line.split(\" && \") if cmd.strip()])\n\t\tcmds.append(line)\n\treturn seperator.join(cmds)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tresult = []\n\tfor line in script:\n\t\tline2 = line.strip()\n\t\tif line2:\n\t\t\tresult.append(line2)\n\treturn seperator.join(result)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tif isinstance(script, (list, tuple)):\n\t\tscript = seperator.join(script)\n\treturn script\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_16.py", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "ques_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# available in python 3.5\n\tif extra_env is None:\n\t\textra_env = {}\n\tdef run_in_subprocess():\n\t\tenv = dict(os.environ)\n\t\tenv.update(extra_env)\n\t\tsubprocess.run([sys.executable, '-c', 'from {} import {}; {}(*sys.argv[1:])'.format(\n\t\t\tfunc.__module__, func.__name__, func.__name__)], \n\t\t\ttimeout=timeout,\n\t\t\tenv=env,\n\t\t\t*args)\n\twith multiprocessing.Pool(1) as pool:\n\t\tpool.map(run_in_subprocess, [None])\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif not extra_env:\n\t\textra_env = {}\n\tif isinstance(func, str):\n\t\tcmd = [sys.executable, '-m', func]\n\telse:\n\t\tcmd = [sys.executable, '-c', 'import ' + func.__module__ + ';' + func.__name__ + '()']\n\tcmd.extend(args)\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\treturn subprocess.run(cmd, env=env, timeout=timeout)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tcmd = [\n\t\tsys.executable,\n\t\t'-m',\n\t\tfunc.__module__,\n\t\tfunc.__name__,\n\t\t*args,\n\t]\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\tproc = subprocess.run(\n\t\tcmd, \n\t\tstdout=subprocess.PIPE, \n\t\tstderr=subprocess.PIPE, \n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\t# print(proc.stdout.decode('utf-8'))\n\t# print(proc.stderr.decode('utf-8'))\n\treturn proc\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tenv = copy.copy(os.environ)\n\tenv.update(extra_env)\n\tp = subprocess.Popen(\n\t\t[sys.executable, \"-m\", func.__module__, *args],\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tenv=env,\n\t)\n\ttry:\n\t\tstdout, stderr = p.communicate(timeout=timeout)\n\texcept subprocess.TimeoutExpired:\n\t\tp.kill()\n\t\tstdout, stderr = p.communicate()\n\tif p.returncode != 0:\n\t\traise subprocess.CalledProcessError(\n\t\t\tp.returncode, [sys.executable, \"-m\", func.__module__]\n\t\t)\n\treturn stdout, stderr\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\t# We use subprocess.run instead of subprocess.Popen to avoid zombie\n\t# processes.  See:\n\t# https://stackoverflow.com/questions/4789837/how-to-terminate-a-python-subprocess-launched-with-shell-true\n\tprocess = subprocess.run(['python', '-m', func.__module__, func.__name__] + list(args), env=env)\n\treturn process.returncode\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tfrom subprocess import run as subprocess_run\n\n\t# set environment variables\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\t# import the function's module\n\tmodule = func.__module__\n\tmod = importlib.import_module(module)\n\n\t# get the module's path\n\tmod_path = os.path.dirname(mod.__file__)\n\n\t# run the function in a subprocess\n\tcmd = [sys.executable, os.path.join(mod_path, '__main__.py'), func.__name__] + list(args)\n\tresult = subprocess_run(cmd, env=env, timeout=timeout)\n\tif result.returncode == 0:\n\t\treturn\n\telse:\n\t\traise subprocess.CalledProcessError(result.returncode, cmd)\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif 'win' in sys.platform:\n\t\tkwargs = {\n\t\t\t'creationflags': subprocess.CREATE_NEW_PROCESS_GROUP,\n\t\t\t'close_fds': True,\n\t\t\t'env': os.environ.copy()\n\t\t}\n\telse:\n\t\tkwargs = {\n\t\t\t'preexec_fn': os.setsid,\n\t\t\t'close_fds': True,\n\t\t\t'env': os.environ.copy()\n\t\t}\n\n\tif extra_env is not None:\n\t\tkwargs['env'].update(extra_env)\n\n\tif timeout > 0:\n\t\t# Create a thread that will raise an exception in the main thread\n\t\t# after timeout seconds.\n\t\tthread = threading.Thread(target=thread_helper, args=(timeout,))\n\t\tthread.start()\n\n\t\ttry:\n\t\t\tsubprocess.run([sys.executable, '-c',\n\t\t\t\t\t\t\tcompile('import sys; from {module} import {function};\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# Set up the command\n\tif not isinstance(func, str):\n\t\tfunc_name = func.__name__\n\t\tfunc_mod = inspect.getmodule(func).__name__\n\telse:\n\t\tfunc_name = func\n\t\tfunc_mod = None\n\n\tcmd = [\n\t\tsys.executable,\n\t\t'-m', func_mod,\n\t\tfunc_name,\n\t\t*args,\n\t]\n\tcmd = [str(x) for x in cmd]\n\n\t# Run the command\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tp = subprocess.run(cmd, env=env, timeout=timeout)\n\t# p.check_returncode()\n\n\treturn p.returncode\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\timport os\n\timport sys\n\timport subprocess\n\timport traceback\n\timport pickle\n\timport argparse\n\timport multiprocessing\n\timport tempfile\n\timport shutil\n\timport logging\n\timport functools\n\n\tlogging.basicConfig(\n\t\t\tlevel=logging.DEBUG,\n\t\t\tformat='%(asctime)s.%(msecs)06d %(levelname)s %(name)s: %(message)s',\n\t\t\tdatefmt='%H:%M:%S'\n\t\t\t)\n\n\tdef setup_signal_handler(signal_num, handler):\n\t\timport signal\n\t\tsignal.signal(signal_num, handler)\n\n\tdef get_logger(name):\n\t\treturn logging.getLogger(name)\n\n\tdef get_tmpdir():\n\t\treturn tempfile.mkdtemp()\n\n\tdef remove_tmpdir(tmpdir):\n\t\tshutil.rmtree(tmpdir, ignore_errors=True)\n\n\tdef get_argparser():\n\t\tparser =\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# The following code has been adapted from:\n\t# https://stackoverflow.com/questions/375427/non-blocking-read-on-a-subprocess-pipe-in-python\n\t#\n\t# This allows us to capture stdout and stderr, but also terminate the\n\t# child process if it doesn't finish within the timeout period.\n\t#\n\t# Note that we don't currently have any tests for this functionality,\n\t# so we can't be sure that it is working as intended.\n\t#\n\tassert timeout is not None\n\tif os.name == 'nt':\n\t\traise RuntimeError('Timeout is not supported on Windows.')\n\telse:\n\t\tp = subprocess.Popen(\n\t\t\t[sys.executable, '-c', 'import %s; %s.%s()' % (func.__module__, func.__module__, func.__name__)] + list(args),\n\t\t\tstdout=subprocess.PIPE, stderr=subprocess.PIPE, env=extra_env)\n\t\tt = threading.\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# available in python 3.5\n\tif extra_env is None:\n\t\textra_env = {}\n\tdef run_in_subprocess():\n\t\tenv = dict(os.environ)\n\t\tenv.update(extra_env)\n\t\tsubprocess.run([sys.executable, '-c', 'from {} import {}; {}(*sys.argv[1:])'.format(\n\t\t\tfunc.__module__, func.__name__, func.__name__)], \n\t\t\ttimeout=timeout,\n\t\t\tenv=env,\n\t\t\t*args)\n\twith multiprocessing.Pool(1) as pool:\n\t\tpool.map(run_in_subprocess, [None])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif not extra_env:\n\t\textra_env = {}\n\tif isinstance(func, str):\n\t\tcmd = [sys.executable, '-m', func]\n\telse:\n\t\tcmd = [sys.executable, '-c', 'import ' + func.__module__ + ';' + func.__name__ + '()']\n\tcmd.extend(args)\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\treturn subprocess.run(cmd, env=env, timeout=timeout)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tcmd = [\n\t\tsys.executable,\n\t\t'-m',\n\t\tfunc.__module__,\n\t\tfunc.__name__,\n\t\t*args,\n\t]\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\tproc = subprocess.run(\n\t\tcmd, \n\t\tstdout=subprocess.PIPE, \n\t\tstderr=subprocess.PIPE, \n\t\tenv=env,\n\t\ttimeout=timeout,\n\t)\n\t# print(proc.stdout.decode('utf-8'))\n\t# print(proc.stderr.decode('utf-8'))\n\treturn proc\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tenv = copy.copy(os.environ)\n\tenv.update(extra_env)\n\tp = subprocess.Popen(\n\t\t[sys.executable, \"-m\", func.__module__, *args],\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=subprocess.PIPE,\n\t\tenv=env,\n\t)\n\ttry:\n\t\tstdout, stderr = p.communicate(timeout=timeout)\n\texcept subprocess.TimeoutExpired:\n\t\tp.kill()\n\t\tstdout, stderr = p.communicate()\n\tif p.returncode != 0:\n\t\traise subprocess.CalledProcessError(\n\t\t\tp.returncode, [sys.executable, \"-m\", func.__module__]\n\t\t)\n\treturn stdout, stderr\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\t# We use subprocess.run instead of subprocess.Popen to avoid zombie\n\t# processes.  See:\n\t# https://stackoverflow.com/questions/4789837/how-to-terminate-a-python-subprocess-launched-with-shell-true\n\tprocess = subprocess.run(['python', '-m', func.__module__, func.__name__] + list(args), env=env)\n\treturn process.returncode\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tfrom subprocess import run as subprocess_run\n\n\t# set environment variables\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\n\t# import the function's module\n\tmodule = func.__module__\n\tmod = importlib.import_module(module)\n\n\t# get the module's path\n\tmod_path = os.path.dirname(mod.__file__)\n\n\t# run the function in a subprocess\n\tcmd = [sys.executable, os.path.join(mod_path, '__main__.py'), func.__name__] + list(args)\n\tresult = subprocess_run(cmd, env=env, timeout=timeout)\n\tif result.returncode == 0:\n\t\treturn\n\telse:\n\t\traise subprocess.CalledProcessError(result.returncode, cmd)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif 'win' in sys.platform:\n\t\tkwargs = {\n\t\t\t'creationflags': subprocess.CREATE_NEW_PROCESS_GROUP,\n\t\t\t'close_fds': True,\n\t\t\t'env': os.environ.copy()\n\t\t}\n\telse:\n\t\tkwargs = {\n\t\t\t'preexec_fn': os.setsid,\n\t\t\t'close_fds': True,\n\t\t\t'env': os.environ.copy()\n\t\t}\n\n\tif extra_env is not None:\n\t\tkwargs['env'].update(extra_env)\n\n\tif timeout > 0:\n\t\t# Create a thread that will raise an exception in the main thread\n\t\t# after timeout seconds.\n\t\tthread = threading.Thread(target=thread_helper, args=(timeout,))\n\t\tthread.start()\n\n\t\ttry:\n\t\t\tsubprocess.run([sys.executable, '-c',\n\t\t\t\t\t\t\tcompile('import sys; from {module} import {function};\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# Set up the command\n\tif not isinstance(func, str):\n\t\tfunc_name = func.__name__\n\t\tfunc_mod = inspect.getmodule(func).__name__\n\telse:\n\t\tfunc_name = func\n\t\tfunc_mod = None\n\n\tcmd = [\n\t\tsys.executable,\n\t\t'-m', func_mod,\n\t\tfunc_name,\n\t\t*args,\n\t]\n\tcmd = [str(x) for x in cmd]\n\n\t# Run the command\n\tenv = os.environ.copy()\n\tif extra_env:\n\t\tenv.update(extra_env)\n\tp = subprocess.run(cmd, env=env, timeout=timeout)\n\t# p.check_returncode()\n\n\treturn p.returncode\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\timport os\n\timport sys\n\timport subprocess\n\timport traceback\n\timport pickle\n\timport argparse\n\timport multiprocessing\n\timport tempfile\n\timport shutil\n\timport logging\n\timport functools\n\n\tlogging.basicConfig(\n\t\t\tlevel=logging.DEBUG,\n\t\t\tformat='%(asctime)s.%(msecs)06d %(levelname)s %(name)s: %(message)s',\n\t\t\tdatefmt='%H:%M:%S'\n\t\t\t)\n\n\tdef setup_signal_handler(signal_num, handler):\n\t\timport signal\n\t\tsignal.signal(signal_num, handler)\n\n\tdef get_logger(name):\n\t\treturn logging.getLogger(name)\n\n\tdef get_tmpdir():\n\t\treturn tempfile.mkdtemp()\n\n\tdef remove_tmpdir(tmpdir):\n\t\tshutil.rmtree(tmpdir, ignore_errors=True)\n\n\tdef get_argparser():\n\t\tparser =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# The following code has been adapted from:\n\t# https://stackoverflow.com/questions/375427/non-blocking-read-on-a-subprocess-pipe-in-python\n\t#\n\t# This allows us to capture stdout and stderr, but also terminate the\n\t# child process if it doesn't finish within the timeout period.\n\t#\n\t# Note that we don't currently have any tests for this functionality,\n\t# so we can't be sure that it is working as intended.\n\t#\n\tassert timeout is not None\n\tif os.name == 'nt':\n\t\traise RuntimeError('Timeout is not supported on Windows.')\n\telse:\n\t\tp = subprocess.Popen(\n\t\t\t[sys.executable, '-c', 'import %s; %s.%s()' % (func.__module__, func.__module__, func.__name__)] + list(args),\n\t\t\tstdout=subprocess.PIPE, stderr=subprocess.PIPE, env=extra_env)\n\t\tt = threading.\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_6.py", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "ques_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n\tvar_name, default = matcher.group(1), matcher.group(2)\n\tvar_value = os.environ.get(var_name)\n\tif var_value is None:\n\t\tif default is None:\n\t\t\traise Error(\"Environment variable {} is not defined\".format(var_name))\n\t\telse:\n\t\t\tvar_value = default\n\treturn var_value\n\n", "def _resolve_string(matcher):\n\ttry:\n\t\tvalue = _resolve_variable(matcher.group(1))\n\t\tif value is None:\n\t\t\tif matcher.group(2) is None:\n\t\t\t\traise Error('No value defined for environment variable %s' % matcher.group(1))\n\t\t\telse:\n\t\t\t\tvalue = matcher.group(2)\n\texcept Error as e:\n\t\traise Error('Failed to resolve %s: %s' % (matcher.group(0), e))\n\treturn value\n\n", "def _resolve_string(matcher):\n\tvariable_name = matcher.group(1)\n\tif variable_name not in os.environ:\n\t\tif matcher.group(2):\n\t\t\treturn matcher.group(2)\n\t\telse:\n\t\t\traise Error(f'Variable {variable_name} not defined in environment')\n\treturn os.environ[variable_name]\n\n\n", "def _resolve_string(matcher):\n\ttry:\n\t\tname, default = matcher.groups()\n\t\tif default is None:\n\t\t\treturn os.environ[name]\n\t\treturn os.getenv(name, default)\n\texcept KeyError:\n\t\traise Error(\"Unknown environment variable: %s\" % name)\n\n", "def _resolve_string(matcher):\n\tif matcher.group(4) is not None:\n\t\treturn matcher.group(4)\n\telse:\n\t\treturn os.environ[matcher.group(2)]\n\n\n", "def _resolve_string(matcher):\n\tname = matcher.group(1)\n\tdefault = matcher.group(3)\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Error('Variable {} not defined'.format(name))\n\n", "def _resolve_string(matcher):\n\tname, default = _split_matcher(matcher)\n\ttry:\n\t\treturn os.environ[name]\n\texcept KeyError:\n\t\tif default is not None:\n\t\t\treturn str(default)\n\t\telse:\n\t\t\traise Error(\"environment variable '%s' is not defined\" % name)\n\n", "def _resolve_string(matcher):\n\tname, default = matcher.group('name'), matcher.group('default')\n\tvalue = os.environ.get(name)\n\tif value is None and default is None:\n\t\traise Error(\"Environment variable {} is not defined\".format(name))\n\treturn value or default\n\n\n", "def _resolve_string(matcher):\n\tname, default = matcher.group(1), matcher.group(2)\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tif default is None:\n\t\t\traise Error('variable %s not defined in environment' % (name))\n\t\telse:\n\t\t\tvalue = default\n\treturn value\n\n", "def _resolve_string(matcher):\n\tname = matcher.group(1)\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tdefault = matcher.group(2)\n\t\tif default is not None:\n\t\t\treturn default\n\t\traise Error('environment variable %s not defined' % name)\n\treturn value\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n\tvar_name, default = matcher.group(1), matcher.group(2)\n\tvar_value = os.environ.get(var_name)\n\tif var_value is None:\n\t\tif default is None:\n\t\t\traise Error(\"Environment variable {} is not defined\".format(var_name))\n\t\telse:\n\t\t\tvar_value = default\n\treturn var_value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\ttry:\n\t\tvalue = _resolve_variable(matcher.group(1))\n\t\tif value is None:\n\t\t\tif matcher.group(2) is None:\n\t\t\t\traise Error('No value defined for environment variable %s' % matcher.group(1))\n\t\t\telse:\n\t\t\t\tvalue = matcher.group(2)\n\texcept Error as e:\n\t\traise Error('Failed to resolve %s: %s' % (matcher.group(0), e))\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tvariable_name = matcher.group(1)\n\tif variable_name not in os.environ:\n\t\tif matcher.group(2):\n\t\t\treturn matcher.group(2)\n\t\telse:\n\t\t\traise Error(f'Variable {variable_name} not defined in environment')\n\treturn os.environ[variable_name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\ttry:\n\t\tname, default = matcher.groups()\n\t\tif default is None:\n\t\t\treturn os.environ[name]\n\t\treturn os.getenv(name, default)\n\texcept KeyError:\n\t\traise Error(\"Unknown environment variable: %s\" % name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tif matcher.group(4) is not None:\n\t\treturn matcher.group(4)\n\telse:\n\t\treturn os.environ[matcher.group(2)]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group(1)\n\tdefault = matcher.group(3)\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Error('Variable {} not defined'.format(name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname, default = _split_matcher(matcher)\n\ttry:\n\t\treturn os.environ[name]\n\texcept KeyError:\n\t\tif default is not None:\n\t\t\treturn str(default)\n\t\telse:\n\t\t\traise Error(\"environment variable '%s' is not defined\" % name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname, default = matcher.group('name'), matcher.group('default')\n\tvalue = os.environ.get(name)\n\tif value is None and default is None:\n\t\traise Error(\"Environment variable {} is not defined\".format(name))\n\treturn value or default\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname, default = matcher.group(1), matcher.group(2)\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tif default is None:\n\t\t\traise Error('variable %s not defined in environment' % (name))\n\t\telse:\n\t\t\tvalue = default\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group(1)\n\tvalue = os.environ.get(name)\n\tif value is None:\n\t\tdefault = matcher.group(2)\n\t\tif default is not None:\n\t\t\treturn default\n\t\traise Error('environment variable %s not defined' % name)\n\treturn value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_2.py", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "ques_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlsplit(image_href)\n\tloc = components.netloc\n\tscheme = components.scheme\n\n\tuse_ssl = (scheme == 'https')\n\tif use_ssl:\n\t\tnetloc = loc\n\telse:\n\t\tnetloc = '%s:%s' % (loc, DEFAULT_PORT)\n\n\tpath = components.path.split('/')\n\timage_id = path[-1]\n\n\treturn (image_id, netloc, use_ssl)\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlparse(image_href)\n\tloc = '{}://{}'.format(components.scheme, components.netloc)\n\timage_id = components.path.lstrip('/')\n\tif not image_id:\n\t\traise ValueError(\"Invalid image href '{}'\".format(image_href))\n\treturn image_id, loc, components.scheme == 'https'\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed = urlparse(image_href)\n\tloc = parsed.netloc or CONF.glance_host\n\timage_id = parsed.path.split('/')[-1]\n\n\t# if we have an empty netloc, we want to use the default\n\t# glance_host (which comes from the config), but it defaults\n\t# to '' instead of None, so we have to special case it.\n\tif not loc:\n\t\tloc = CONF.glance_host\n\n\t# default to using ssl for parsing\n\tuse_ssl = True\n\tif parsed.scheme == 'http':\n\t\tuse_ssl = False\n\n\treturn image_id, loc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlparse(image_href)\n\tscheme, netloc, path, query, fragment = components\n\t# must start with 'glance://'\n\tif scheme != 'glance':\n\t\traise ValueError('not a glance href')\n\t# the path must contain a single segment (the image id)\n\tpath_elems = [p for p in path.split('/') if p]\n\tif len(path_elems) != 1:\n\t\traise ValueError('malformed glance href')\n\timage_id = path_elems[0]\n\t# if the netloc is empty, we use the default, else we parse it\n\tnetloc = netloc.strip('/')\n\tif not netloc:\n\t\tnetloc = '127.0.0.1:9292'\n\tnetloc_elems = netloc.split(':')\n\tif len(netloc_elems) == 1:\n\t\t# no port number, so we assume port 9292\n\t\tnetloc_elems.append('9292')\n\tif len(netloc_elems) !=\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparts = urlsplit(image_href)\n\tscheme = parts.scheme\n\n\t# If we get something like 'vsphere://...' here, it's because\n\t# the user specified a vCenter URL instead of an ESXi hostname.\n\tif scheme == 'vsphere':\n\t\tscheme = 'https'\n\n\t# Can't use the 'netloc' field of the ParseResult because it\n\t# doesn't handle IPv6 addresses properly\n\tnetloc = parts.hostname\n\tif parts.port:\n\t\tnetloc += ':' + str(parts.port)\n\tif parts.username and parts.password:\n\t\tnetloc = '{}:{}@{}'.format(parts.username, parts.password, netloc)\n\telif parts.username:\n\t\tnetloc = '{}@{}'.format(parts.username, netloc)\n\n\tif not parts.path:\n\t\traise ValueError('Image href must include datastore path')\n\n\tif parts.path[0] == '/':\n\t\traise ValueError('Image path must be relative')\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tif image_href is None:\n\t\treturn None\n\n\ttry:\n\t\t(image_id, netloc) = image_href.split('/')[-2:]\n\texcept ValueError:\n\t\traise ValueError('Invalid image href')\n\n\tuse_ssl = False\n\tif netloc.startswith('https://'):\n\t\tuse_ssl = True\n\t\tnetloc = netloc[8:]\n\n\treturn (image_id, netloc, use_ssl)\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or 'cdn.scryfall.com'\n\tuse_ssl = url.scheme == 'https'\n\tpath = url.path\n\tpath_components = path.split('/')\n\timage_id = path_components[-1]\n\treturn image_id, netloc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\tloc = parsed_url.netloc\n\t#FIXME(bcwaldon): account for URL schemes other than http/https\n\tuse_ssl = parsed_url.scheme == 'https'\n\timage_id = parsed_url.path.split('/')[-1]\n\n\tif not image_id:\n\t\traise ValueError('Invalid image href')\n\n\treturn image_id, loc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\tloc = parsed_url.netloc or 'cdn.discordapp.com'\n\tpath = split(parsed_url.path)\n\timage_id = path[-1]\n\tuse_ssl = parsed_url.scheme == 'https'\n\treturn image_id, loc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\timage_id = parsed_url.path.split('/')[-1]\n\n\tif image_id == \"\":\n\t\traise ValueError(\"Image href is invalid: %s\" % image_href)\n\n\treturn (image_id, parsed_url.netloc, parsed_url.scheme == 'https')\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlsplit(image_href)\n\tloc = components.netloc\n\tscheme = components.scheme\n\n\tuse_ssl = (scheme == 'https')\n\tif use_ssl:\n\t\tnetloc = loc\n\telse:\n\t\tnetloc = '%s:%s' % (loc, DEFAULT_PORT)\n\n\tpath = components.path.split('/')\n\timage_id = path[-1]\n\n\treturn (image_id, netloc, use_ssl)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlparse(image_href)\n\tloc = '{}://{}'.format(components.scheme, components.netloc)\n\timage_id = components.path.lstrip('/')\n\tif not image_id:\n\t\traise ValueError(\"Invalid image href '{}'\".format(image_href))\n\treturn image_id, loc, components.scheme == 'https'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed = urlparse(image_href)\n\tloc = parsed.netloc or CONF.glance_host\n\timage_id = parsed.path.split('/')[-1]\n\n\t# if we have an empty netloc, we want to use the default\n\t# glance_host (which comes from the config), but it defaults\n\t# to '' instead of None, so we have to special case it.\n\tif not loc:\n\t\tloc = CONF.glance_host\n\n\t# default to using ssl for parsing\n\tuse_ssl = True\n\tif parsed.scheme == 'http':\n\t\tuse_ssl = False\n\n\treturn image_id, loc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlparse(image_href)\n\tscheme, netloc, path, query, fragment = components\n\t# must start with 'glance://'\n\tif scheme != 'glance':\n\t\traise ValueError('not a glance href')\n\t# the path must contain a single segment (the image id)\n\tpath_elems = [p for p in path.split('/') if p]\n\tif len(path_elems) != 1:\n\t\traise ValueError('malformed glance href')\n\timage_id = path_elems[0]\n\t# if the netloc is empty, we use the default, else we parse it\n\tnetloc = netloc.strip('/')\n\tif not netloc:\n\t\tnetloc = '127.0.0.1:9292'\n\tnetloc_elems = netloc.split(':')\n\tif len(netloc_elems) == 1:\n\t\t# no port number, so we assume port 9292\n\t\tnetloc_elems.append('9292')\n\tif len(netloc_elems) !=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparts = urlsplit(image_href)\n\tscheme = parts.scheme\n\n\t# If we get something like 'vsphere://...' here, it's because\n\t# the user specified a vCenter URL instead of an ESXi hostname.\n\tif scheme == 'vsphere':\n\t\tscheme = 'https'\n\n\t# Can't use the 'netloc' field of the ParseResult because it\n\t# doesn't handle IPv6 addresses properly\n\tnetloc = parts.hostname\n\tif parts.port:\n\t\tnetloc += ':' + str(parts.port)\n\tif parts.username and parts.password:\n\t\tnetloc = '{}:{}@{}'.format(parts.username, parts.password, netloc)\n\telif parts.username:\n\t\tnetloc = '{}@{}'.format(parts.username, netloc)\n\n\tif not parts.path:\n\t\traise ValueError('Image href must include datastore path')\n\n\tif parts.path[0] == '/':\n\t\traise ValueError('Image path must be relative')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tif image_href is None:\n\t\treturn None\n\n\ttry:\n\t\t(image_id, netloc) = image_href.split('/')[-2:]\n\texcept ValueError:\n\t\traise ValueError('Invalid image href')\n\n\tuse_ssl = False\n\tif netloc.startswith('https://'):\n\t\tuse_ssl = True\n\t\tnetloc = netloc[8:]\n\n\treturn (image_id, netloc, use_ssl)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or 'cdn.scryfall.com'\n\tuse_ssl = url.scheme == 'https'\n\tpath = url.path\n\tpath_components = path.split('/')\n\timage_id = path_components[-1]\n\treturn image_id, netloc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\tloc = parsed_url.netloc\n\t#FIXME(bcwaldon): account for URL schemes other than http/https\n\tuse_ssl = parsed_url.scheme == 'https'\n\timage_id = parsed_url.path.split('/')[-1]\n\n\tif not image_id:\n\t\traise ValueError('Invalid image href')\n\n\treturn image_id, loc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\tloc = parsed_url.netloc or 'cdn.discordapp.com'\n\tpath = split(parsed_url.path)\n\timage_id = path[-1]\n\tuse_ssl = parsed_url.scheme == 'https'\n\treturn image_id, loc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\timage_id = parsed_url.path.split('/')[-1]\n\n\tif image_id == \"\":\n\t\traise ValueError(\"Image href is invalid: %s\" % image_href)\n\n\treturn (image_id, parsed_url.netloc, parsed_url.scheme == 'https')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_13.py", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "ques_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"remove_ending_os_sep(): input is not a list\")\n\tfor i, s in enumerate(input_list):\n\t\tif len(s) > 1 and s[-1] == os.path.sep:\n\t\t\tinput_list[i] = s[:-1]\n\treturn input_list\n\n", "def remove_ending_os_sep(input_list):\n\tbasestring = (str, unicode)\n\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Expected list\")\n\n\tresult = []\n\tfor i in input_list:\n\t\tif isinstance(i, basestring):\n\t\t\tif len(i) > 1 and i[-1] == os.sep:\n\t\t\t\ti = i[:-1]\n\n\t\tresult.append(i)\n\n\treturn result\n\n", "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Expected a list\")\n\n\tfor i in range(len(input_list) - 1, -1, -1):\n\t\tif len(input_list[i]) > 1 and input_list[i].endswith(os.sep):\n\t\t\tinput_list[i] = input_list[i][:-1]\n\n\treturn input_list\n\n\n", "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\tmsg = 'Input is not a list'\n\t\tlog.error(msg)\n\t\traise TypeError(msg)\n\n\tprocessed_list = []\n\tfor item in input_list:\n\t\tif item[-1] == os.sep:\n\t\t\tprocessed_list.append(item[:-1])\n\t\telse:\n\t\t\tprocessed_list.append(item)\n\treturn processed_list\n\n\n", "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError\n\t\n\toutput_list = []\n\tfor path in input_list:\n\t\tif len(path) > 1 and path[-1] == os.sep:\n\t\t\tpath = path[:-1]\n\t\toutput_list.append(path)\n\treturn output_list\n\n\n", "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Input list must be of type list\")\n\tfor index, item in enumerate(input_list):\n\t\tif len(item) > 1 and item.endswith(os.sep):\n\t\t\tinput_list[index] = item[:-1]\n\treturn input_list\n\n", "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"expected 'list', got '%s'\" % type(input_list))\n\n\tfor i in range(0, len(input_list)):\n\t\tif (len(input_list[i]) > 1) and (input_list[i][-1] == os.sep):\n\t\t\tinput_list[i] = input_list[i][:-1]\n\n\treturn input_list\n\n\n", "def remove_ending_os_sep(input_list):\n\tfor i in range(len(input_list)):\n\t\tif len(input_list[i]) > 1 and input_list[i][-1] == os.sep:\n\t\t\tinput_list[i] = input_list[i][:-1]\n\treturn input_list\n\n\n\n", "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Input must be a list of strings\")\n\tfor i in input_list:\n\t\tif not isinstance(i, str):\n\t\t\traise TypeError(\"Input must be a list of strings\")\n\n\treturn [x[:-1] if len(x) > 1 and x.endswith(os.sep) else x for x in input_list]\n\n\n", "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"input_list type is not list\")\n\n\tprocessed_list = []\n\tfor item in input_list:\n\t\tif len(item) > 1 and item.endswith(os.sep):\n\t\t\tprocessed_list.append(item[:-1])\n\t\telse:\n\t\t\tprocessed_list.append(item)\n\n\treturn processed_list\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"remove_ending_os_sep(): input is not a list\")\n\tfor i, s in enumerate(input_list):\n\t\tif len(s) > 1 and s[-1] == os.path.sep:\n\t\t\tinput_list[i] = s[:-1]\n\treturn input_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tbasestring = (str, unicode)\n\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Expected list\")\n\n\tresult = []\n\tfor i in input_list:\n\t\tif isinstance(i, basestring):\n\t\t\tif len(i) > 1 and i[-1] == os.sep:\n\t\t\t\ti = i[:-1]\n\n\t\tresult.append(i)\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Expected a list\")\n\n\tfor i in range(len(input_list) - 1, -1, -1):\n\t\tif len(input_list[i]) > 1 and input_list[i].endswith(os.sep):\n\t\t\tinput_list[i] = input_list[i][:-1]\n\n\treturn input_list\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\tmsg = 'Input is not a list'\n\t\tlog.error(msg)\n\t\traise TypeError(msg)\n\n\tprocessed_list = []\n\tfor item in input_list:\n\t\tif item[-1] == os.sep:\n\t\t\tprocessed_list.append(item[:-1])\n\t\telse:\n\t\t\tprocessed_list.append(item)\n\treturn processed_list\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError\n\t\n\toutput_list = []\n\tfor path in input_list:\n\t\tif len(path) > 1 and path[-1] == os.sep:\n\t\t\tpath = path[:-1]\n\t\toutput_list.append(path)\n\treturn output_list\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Input list must be of type list\")\n\tfor index, item in enumerate(input_list):\n\t\tif len(item) > 1 and item.endswith(os.sep):\n\t\t\tinput_list[index] = item[:-1]\n\treturn input_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"expected 'list', got '%s'\" % type(input_list))\n\n\tfor i in range(0, len(input_list)):\n\t\tif (len(input_list[i]) > 1) and (input_list[i][-1] == os.sep):\n\t\t\tinput_list[i] = input_list[i][:-1]\n\n\treturn input_list\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tfor i in range(len(input_list)):\n\t\tif len(input_list[i]) > 1 and input_list[i][-1] == os.sep:\n\t\t\tinput_list[i] = input_list[i][:-1]\n\treturn input_list\n\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"Input must be a list of strings\")\n\tfor i in input_list:\n\t\tif not isinstance(i, str):\n\t\t\traise TypeError(\"Input must be a list of strings\")\n\n\treturn [x[:-1] if len(x) > 1 and x.endswith(os.sep) else x for x in input_list]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif not isinstance(input_list, list):\n\t\traise TypeError(\"input_list type is not list\")\n\n\tprocessed_list = []\n\tfor item in input_list:\n\t\tif len(item) > 1 and item.endswith(os.sep):\n\t\t\tprocessed_list.append(item[:-1])\n\t\telse:\n\t\t\tprocessed_list.append(item)\n\n\treturn processed_list\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_17.py", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "ques_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tif not pattern:\n\t\treturn None\n\treturn re.compile(pattern, re.I)\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.escape(pattern)\n\t\tpattern = pattern.replace('\\\\*', '.*')\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tif pattern.startswith('/') and pattern.endswith('/'):\n\t\tpattern = pattern[1:-1]\n\treturn re.compile(pattern)\n\t\n", "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\treturn re.compile(pattern, re.I)\n\n", "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tpattern = pattern.replace(\".\", \"\\.\")\n\tpattern = pattern.replace(\"*\", \".*\")\n\tpattern = pattern.replace(\"?\", \"\\?\")\n\tpattern = pattern.replace(\"+\", \"\\+\")\n\tpattern = pattern.replace(\"{\", \"\\{\")\n\tpattern = pattern.replace(\"}\", \"\\}\")\n\tpattern = pattern.replace(\"(\", \"\\(\")\n\tpattern = pattern.replace(\")\", \"\\)\")\n\tpattern = pattern.replace(\"|\", \"\\|\")\n\tpattern = pattern.replace(\"^\", \"\\^\")\n\tpattern = pattern.replace(\"$\", \"\\$\")\n\tpattern = pattern.replace(\"[\", \"\\[\")\n\tpattern = pattern.replace(\"]\", \"\\]\")\n\tpattern = pattern.replace(\"\\\\\", \"\\\\\\\\\")\n\tpattern = pattern.replace(\"/\", \"\\/\")\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\tif not pattern:\n\t\treturn pattern\n\tif isinstance(pattern, str) and strip:\n\t\tpattern = pattern.strip()\n\tif isinstance(pattern, str):\n\t\treturn re.compile(fnmatch.translate(pattern), re.IGNORECASE)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\tif pattern is None:\n\t\treturn None\n\tpattern = pattern.strip()\n\tif strip:\n\t\treturn re.compile(pattern)\n\treturn re.compile(pattern, re.DOTALL)\n\n", "def get_pattern(pattern, strip=True):\n\tif pattern is None:\n\t\tpattern = \"\"\n\telse:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tif not len(pattern):\n\t\t\tpattern = \"\"\n\t\telse:\n\t\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\tif pattern.startswith('?'):\n\t\tpattern = pattern[1:]\n\n\tif pattern.startswith('/'):\n\t\tpattern = pattern[1:]\n\n\tif pattern.endswith('/'):\n\t\tpattern = pattern[:-1]\n\n\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif not pattern:\n\t\treturn None\n\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern)\n\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\treturn pattern\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tif not pattern:\n\t\treturn None\n\treturn re.compile(pattern, re.I)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.escape(pattern)\n\t\tpattern = pattern.replace('\\\\*', '.*')\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tif pattern.startswith('/') and pattern.endswith('/'):\n\t\tpattern = pattern[1:-1]\n\treturn re.compile(pattern)\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\treturn re.compile(pattern, re.I)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tpattern = pattern.replace(\".\", \"\\.\")\n\tpattern = pattern.replace(\"*\", \".*\")\n\tpattern = pattern.replace(\"?\", \"\\?\")\n\tpattern = pattern.replace(\"+\", \"\\+\")\n\tpattern = pattern.replace(\"{\", \"\\{\")\n\tpattern = pattern.replace(\"}\", \"\\}\")\n\tpattern = pattern.replace(\"(\", \"\\(\")\n\tpattern = pattern.replace(\")\", \"\\)\")\n\tpattern = pattern.replace(\"|\", \"\\|\")\n\tpattern = pattern.replace(\"^\", \"\\^\")\n\tpattern = pattern.replace(\"$\", \"\\$\")\n\tpattern = pattern.replace(\"[\", \"\\[\")\n\tpattern = pattern.replace(\"]\", \"\\]\")\n\tpattern = pattern.replace(\"\\\\\", \"\\\\\\\\\")\n\tpattern = pattern.replace(\"/\", \"\\/\")\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif not pattern:\n\t\treturn pattern\n\tif isinstance(pattern, str) and strip:\n\t\tpattern = pattern.strip()\n\tif isinstance(pattern, str):\n\t\treturn re.compile(fnmatch.translate(pattern), re.IGNORECASE)\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif pattern is None:\n\t\treturn None\n\tpattern = pattern.strip()\n\tif strip:\n\t\treturn re.compile(pattern)\n\treturn re.compile(pattern, re.DOTALL)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif pattern is None:\n\t\tpattern = \"\"\n\telse:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tif not len(pattern):\n\t\t\tpattern = \"\"\n\t\telse:\n\t\t\tpattern = re.compile(pattern, re.IGNORECASE)\n\treturn pattern\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\tif pattern.startswith('?'):\n\t\tpattern = pattern[1:]\n\n\tif pattern.startswith('/'):\n\t\tpattern = pattern[1:]\n\n\tif pattern.endswith('/'):\n\t\tpattern = pattern[:-1]\n\n\treturn pattern\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif not pattern:\n\t\treturn None\n\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern)\n\n\tif strip:\n\t\tpattern = pattern.strip()\n\n\treturn pattern\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_7.py", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tp = None\n\tfor c in commands:\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = subprocess.Popen(c + args, cwd=cwd, stdout=subprocess.PIPE,\n\t\t\t\t\t\t\t\t stderr=(subprocess.PIPE if hide_stderr\n\t\t\t\t\t\t\t\t\t\t else None),\n\t\t\t\t\t\t\t\t env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = get_exception()\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tcontinue\n\t\t\tif verbose:\n\t\t\t\tprint(\"unable to run %s\" % args[0])\n\t\t\t\tprint(e)\n\t\t\treturn None\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"unable to find command, tried %s\" % (commands,))\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tcommand_args = command.split()\n\t\tif not command_args:\n\t\t\tcontinue\n\t\tif args:\n\t\t\tcommand_args.extend(args)\n\t\tif verbose:\n\t\t\tprint('Running: ' + ' '.join(command_args))\n\t\tp = subprocess.Popen(\n\t\t\tcommand_args,\n\t\t\tcwd=cwd,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\tenv=env,\n\t\t)\n\t\tstdout, stderr = p.communicate()\n\t\tif stdout:\n\t\t\tprint(stdout.decode())\n\t\tif stderr:\n\t\t\tprint(stderr.decode())\n\t\tif p.returncode != 0:\n\t\t\traise SystemExit(p.returncode)\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint 'Running command: %s %s' % (command, args)\n\t\tif hide_stderr:\n\t\t\twith open(os.devnull, 'w') as devnull:\n\t\t\t\tsubprocess.check_call([command] + args, cwd=cwd, stdout=devnull, stderr=devnull, env=env)\n\t\telse:\n\t\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env)\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\tif not cwd:\n\t\t\tcwd = os.getcwd()\n\t\tif not isinstance(commands, list):\n\t\t\tcommands = [commands]\n\t\tif not isinstance(args, list):\n\t\t\targs = [args]\n\t\t# TODO: subprocess.check_call() actually does not work\n\t\t# with some commands, so we use subprocess.Popen() instead.\n\t\tprocess = subprocess.Popen(commands + args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\toutput, error = process.communicate()\n\t\t# TODO: this is a hack to get rid of the '\\r' character on Windows\n\t\tif not verbose:\n\t\t\toutput = output.replace('\\r', '')\n\t\t\terror = error.replace('\\r', '')\n\t\tif error and not hide_stderr:\n\t\t\tprint 'ERROR: %s' % error.strip()\n\t\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert(isinstance(commands, (list, tuple)) or isinstance(commands, str))\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t\n\tif args is None:\n\t\targs = []\n\tif isinstance(args, str):\n\t\targs = [args]\n\t\n\tif env is None:\n\t\tenv = os.environ\n\t\n\tif 'PYTHONPATH' in env:\n\t\tenv['PYTHONPATH'] = env['PYTHONPATH'] + os.pathsep + os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\telse:\n\t\tenv['PYTHONPATH'] = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\t\n\tfor command in commands:\n\t\tif isinstance(command, str):\n\t\t\tif command.startswith('python ') and sys.platform\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# We create a temporary file to hold the output of the commands, if we are\n\t# going to capture it.\n\tif hide_stderr:\n\t\tstderr = open(os.devnull, 'wb')\n\telse:\n\t\tstderr = sys.stderr\n\n\tif isinstance(commands, basestring):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint 'Running: %s' % command\n\t\ttry:\n\t\t\tsubprocess.check_call(command, cwd=cwd, stdout=sys.stdout, stderr=stderr, shell=True, env=env)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tif verbose:\n\t\t\t\tprint 'Command failed with exit code %d.' % e.returncode\n\t\t\treturn e.returncode\n\treturn 0\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\t# We don't use shell=True here because it is a security hazard.\n\t# See https://docs.python.org/3/library/subprocess.html#frequently-used-arguments\n\t# for more information.\n\tsubprocess_kwargs = {\n\t\t\"stdout\": subprocess.PIPE,\n\t\t\"stderr\": subprocess.PIPE,\n\t\t\"cwd\": cwd,\n\t\t\"env\": env,\n\t}\n\n\t# If we're on Windows, we need to set shell=True so that the .bat files\n\t# can be executed.\n\tif sys.platform == \"win32\":\n\t\tsubprocess_kwargs[\"shell\"] = True\n\n\tfor command in commands:\n\t\ttry:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Running command: {0}\".format(command))\n\t\t\t\tprint(\"In directory: {0}\".format\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\n\tpopen_kwargs = dict(\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\tif os.name == 'nt':\n\t\tpopen_kwargs['shell'] = True\n\n\tfor command in commands:\n\t\tcommand_line = command % args\n\t\tif verbose:\n\t\t\tprint command_line\n\n\t\ttry:\n\t\t\tchild = subprocess.Popen(command_line, **popen_kwargs)\n\t\texcept OSError, e:\n\t\t\tif e.errno == 2:\n\t\t\t\tsys.stderr.write('Error: Command \"%s\" is not installed or not in the path.\\n' % command[0])\n\t\t\t\tsys.exit(1)\n\t\t\telse:\n\t\t\t\traise\n\n\t\toutput = child.communicate()[0]\n\t\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tpopen_kwargs = {\n\t\t\"cwd\": cwd,\n\t\t\"env\": env,\n\t\t\"stdout\": subprocess.PIPE,\n\t\t\"universal_newlines\": True,\n\t}\n\tif hide_stderr:\n\t\tpopen_kwargs[\"stderr\"] = subprocess.PIPE\n\telif verbose:\n\t\tpopen_kwargs[\"stderr\"] = subprocess.STDOUT\n\telse:\n\t\tpopen_kwargs[\"stderr\"] = None\n\tfor c in commands:\n\t\ttry:\n\t\t\tif verbose:\n\t\t\t\tprint(\" \".join(c + args))\n\t\t\tp = subprocess.Popen(c + args, **popen_kwargs)\n\t\t\toutput = p.communicate()[0]\n\t\t\tif p.returncode != 0:\n\t\t\t\tif verbose or not hide_stderr:\n\t\t\t\t\t# output already printed\n\t\t\t\t\tsys\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif env is None:\n\t\tenv = environ.copy()\n\telse:\n\t\tenv = env.copy()\n\n\tif 'PATH' in env:\n\t\tenv['PATH'] = env['PATH'].split(':')\n\telse:\n\t\tenv['PATH'] = os.environ['PATH'].split(':')\n\n\ttry:\n\t\tos.unsetenv('LD_LIBRARY_PATH')\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\tos.unsetenv('DYLD_LIBRARY_PATH')\n\texcept AttributeError:\n\t\tpass\n\n\tif not verbose:\n\t\tstdout = stderr = open(devnull, 'wb')\n\telif hide_stderr:\n\t\tstdout = sys.stdout\n\t\tstderr = open(devnull, 'wb')\n\telse:\n\t\tstdout = sys.stdout\n\t\tstderr = sys.stderr\n\n\tfor command in\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tp = None\n\tfor c in commands:\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = subprocess.Popen(c + args, cwd=cwd, stdout=subprocess.PIPE,\n\t\t\t\t\t\t\t\t stderr=(subprocess.PIPE if hide_stderr\n\t\t\t\t\t\t\t\t\t\t else None),\n\t\t\t\t\t\t\t\t env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = get_exception()\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tcontinue\n\t\t\tif verbose:\n\t\t\t\tprint(\"unable to run %s\" % args[0])\n\t\t\t\tprint(e)\n\t\t\treturn None\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"unable to find command, tried %s\" % (commands,))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tcommand_args = command.split()\n\t\tif not command_args:\n\t\t\tcontinue\n\t\tif args:\n\t\t\tcommand_args.extend(args)\n\t\tif verbose:\n\t\t\tprint('Running: ' + ' '.join(command_args))\n\t\tp = subprocess.Popen(\n\t\t\tcommand_args,\n\t\t\tcwd=cwd,\n\t\t\tstdout=subprocess.PIPE,\n\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\tenv=env,\n\t\t)\n\t\tstdout, stderr = p.communicate()\n\t\tif stdout:\n\t\t\tprint(stdout.decode())\n\t\tif stderr:\n\t\t\tprint(stderr.decode())\n\t\tif p.returncode != 0:\n\t\t\traise SystemExit(p.returncode)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint 'Running command: %s %s' % (command, args)\n\t\tif hide_stderr:\n\t\t\twith open(os.devnull, 'w') as devnull:\n\t\t\t\tsubprocess.check_call([command] + args, cwd=cwd, stdout=devnull, stderr=devnull, env=env)\n\t\telse:\n\t\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\tif not cwd:\n\t\t\tcwd = os.getcwd()\n\t\tif not isinstance(commands, list):\n\t\t\tcommands = [commands]\n\t\tif not isinstance(args, list):\n\t\t\targs = [args]\n\t\t# TODO: subprocess.check_call() actually does not work\n\t\t# with some commands, so we use subprocess.Popen() instead.\n\t\tprocess = subprocess.Popen(commands + args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\toutput, error = process.communicate()\n\t\t# TODO: this is a hack to get rid of the '\\r' character on Windows\n\t\tif not verbose:\n\t\t\toutput = output.replace('\\r', '')\n\t\t\terror = error.replace('\\r', '')\n\t\tif error and not hide_stderr:\n\t\t\tprint 'ERROR: %s' % error.strip()\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert(isinstance(commands, (list, tuple)) or isinstance(commands, str))\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t\n\tif args is None:\n\t\targs = []\n\tif isinstance(args, str):\n\t\targs = [args]\n\t\n\tif env is None:\n\t\tenv = os.environ\n\t\n\tif 'PYTHONPATH' in env:\n\t\tenv['PYTHONPATH'] = env['PYTHONPATH'] + os.pathsep + os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\telse:\n\t\tenv['PYTHONPATH'] = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\t\n\tfor command in commands:\n\t\tif isinstance(command, str):\n\t\t\tif command.startswith('python ') and sys.platform\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# We create a temporary file to hold the output of the commands, if we are\n\t# going to capture it.\n\tif hide_stderr:\n\t\tstderr = open(os.devnull, 'wb')\n\telse:\n\t\tstderr = sys.stderr\n\n\tif isinstance(commands, basestring):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint 'Running: %s' % command\n\t\ttry:\n\t\t\tsubprocess.check_call(command, cwd=cwd, stdout=sys.stdout, stderr=stderr, shell=True, env=env)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tif verbose:\n\t\t\t\tprint 'Command failed with exit code %d.' % e.returncode\n\t\t\treturn e.returncode\n\treturn 0\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\t# We don't use shell=True here because it is a security hazard.\n\t# See https://docs.python.org/3/library/subprocess.html#frequently-used-arguments\n\t# for more information.\n\tsubprocess_kwargs = {\n\t\t\"stdout\": subprocess.PIPE,\n\t\t\"stderr\": subprocess.PIPE,\n\t\t\"cwd\": cwd,\n\t\t\"env\": env,\n\t}\n\n\t# If we're on Windows, we need to set shell=True so that the .bat files\n\t# can be executed.\n\tif sys.platform == \"win32\":\n\t\tsubprocess_kwargs[\"shell\"] = True\n\n\tfor command in commands:\n\t\ttry:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Running command: {0}\".format(command))\n\t\t\t\tprint(\"In directory: {0}\".format\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\n\tpopen_kwargs = dict(\n\t\tstdout=subprocess.PIPE,\n\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\tcwd=cwd,\n\t\tenv=env,\n\t)\n\tif os.name == 'nt':\n\t\tpopen_kwargs['shell'] = True\n\n\tfor command in commands:\n\t\tcommand_line = command % args\n\t\tif verbose:\n\t\t\tprint command_line\n\n\t\ttry:\n\t\t\tchild = subprocess.Popen(command_line, **popen_kwargs)\n\t\texcept OSError, e:\n\t\t\tif e.errno == 2:\n\t\t\t\tsys.stderr.write('Error: Command \"%s\" is not installed or not in the path.\\n' % command[0])\n\t\t\t\tsys.exit(1)\n\t\t\telse:\n\t\t\t\traise\n\n\t\toutput = child.communicate()[0]\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, (list, tuple))\n\tpopen_kwargs = {\n\t\t\"cwd\": cwd,\n\t\t\"env\": env,\n\t\t\"stdout\": subprocess.PIPE,\n\t\t\"universal_newlines\": True,\n\t}\n\tif hide_stderr:\n\t\tpopen_kwargs[\"stderr\"] = subprocess.PIPE\n\telif verbose:\n\t\tpopen_kwargs[\"stderr\"] = subprocess.STDOUT\n\telse:\n\t\tpopen_kwargs[\"stderr\"] = None\n\tfor c in commands:\n\t\ttry:\n\t\t\tif verbose:\n\t\t\t\tprint(\" \".join(c + args))\n\t\t\tp = subprocess.Popen(c + args, **popen_kwargs)\n\t\t\toutput = p.communicate()[0]\n\t\t\tif p.returncode != 0:\n\t\t\t\tif verbose or not hide_stderr:\n\t\t\t\t\t# output already printed\n\t\t\t\t\tsys\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif env is None:\n\t\tenv = environ.copy()\n\telse:\n\t\tenv = env.copy()\n\n\tif 'PATH' in env:\n\t\tenv['PATH'] = env['PATH'].split(':')\n\telse:\n\t\tenv['PATH'] = os.environ['PATH'].split(':')\n\n\ttry:\n\t\tos.unsetenv('LD_LIBRARY_PATH')\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\tos.unsetenv('DYLD_LIBRARY_PATH')\n\texcept AttributeError:\n\t\tpass\n\n\tif not verbose:\n\t\tstdout = stderr = open(devnull, 'wb')\n\telif hide_stderr:\n\t\tstdout = sys.stdout\n\t\tstderr = open(devnull, 'wb')\n\telse:\n\t\tstdout = sys.stdout\n\t\tstderr = sys.stderr\n\n\tfor command in\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_4.py", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "ques_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(str(target))\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\tpattern = re.compile(r\"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\")\n\tif re.match(pattern, target):\n\t\treturn True\n\telse:\n\t\treturn False\n\t\t\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept AttributeError:\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn target.count('.') == 3\n\texcept socket.error:\n\t\treturn False\n\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\treturn re.match(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$', target) is not None\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\texcept AttributeError:\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn target.count('.') == 3\n\texcept socket.error:\n\t\treturn False\n\treturn True\n\n", "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(str(target))\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\tpattern = re.compile(r\"((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\")\n\tif re.match(pattern, target):\n\t\treturn True\n\telse:\n\t\treturn False\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept AttributeError:\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn target.count('.') == 3\n\texcept socket.error:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\treturn re.match(r'^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$', target) is not None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\texcept AttributeError:\n\t\ttry:\n\t\t\tsocket.inet_aton(target)\n\t\texcept socket.error:\n\t\t\treturn False\n\t\treturn target.count('.') == 3\n\texcept socket.error:\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\texcept:\n\t\treturn False\n\telse:\n\t\treturn True\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_11.py", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "ques_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\troots = set() if roots is None else roots\n\tgraph.addNodes(roots)\n\tfor child in graph.transitive_closure_of(prop):\n\t\tif not graph.has_predicate((child, prop)):\n\t\t\troots.add(child)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor child, parent in graph.subject_objects(prop):\n\t\tif parent not in roots:\n\t\t\troots.add(child)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\telse:\n\t\troots = set(roots)\n\tfor (child, parent) in graph.subject_objects(prop):\n\t\tif child not in roots:\n\t\t\troots.add(parent)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor child, parent in graph.subject_objects(prop):\n\t\troots.add(parent)\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor child, parent in graph.subject_objects(prop):\n\t\tif child in roots:\n\t\t\tcontinue\n\t\troots.add(child)\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(predicate=prop):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\troots = roots or set()\n\tfor s, p, o in graph:\n\t\tif p == prop:\n\t\t\troots.add(s)\n\t\t\troots.add(o)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\tif o not in roots:\n\t\t\troots.add(o)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\troots = set() if roots is None else roots\n\tgraph.addNodes(roots)\n\tfor child in graph.transitive_closure_of(prop):\n\t\tif not graph.has_predicate((child, prop)):\n\t\t\troots.add(child)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor child, parent in graph.subject_objects(prop):\n\t\tif parent not in roots:\n\t\t\troots.add(child)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\telse:\n\t\troots = set(roots)\n\tfor (child, parent) in graph.subject_objects(prop):\n\t\tif child not in roots:\n\t\t\troots.add(parent)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor child, parent in graph.subject_objects(prop):\n\t\troots.add(parent)\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor child, parent in graph.subject_objects(prop):\n\t\tif child in roots:\n\t\t\tcontinue\n\t\troots.add(child)\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(predicate=prop):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\troots = roots or set()\n\tfor s, p, o in graph:\n\t\tif p == prop:\n\t\t\troots.add(s)\n\t\t\troots.add(o)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor node in graph.subjects(prop, None):\n\t\tif node not in roots:\n\t\t\troots.add(node)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\tif o not in roots:\n\t\t\troots.add(o)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_1.py", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "ques_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper).strip()\n\n\n", "def _dump_string(obj, dumper=None):\n\tif isinstance(obj, str):\n\t\treturn obj\n\telse:\n\t\treturn obj.encode('utf-8')\n\n", "def _dump_string(obj, dumper=None):\n\treturn obj\n\n", "def _dump_string(obj, dumper=None):\n\tif isinstance(obj, unicode):\n\t\treturn obj.encode('utf-8')\n\treturn str(obj)\n\n", "def _dump_string(obj, dumper=None):\n\tdumper = dumper or json.dumps\n\treturn dumper(obj, ensure_ascii=False, separators=(',', ':'))\n\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = YAMLDumper()\n\treturn dumper.dump(obj)\n\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = default_dumper\n\treturn dumper(obj)\n\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.dump\n\treturn dumper(obj, encoding='utf-8', allow_unicode=True, default_flow_style=False)\n\n\n", "def _dump_string(obj, dumper=None):\n\t# TODO: Remove this method in the future.\n\t# I might have to keep it if I want to be able to dump to a string\n\t# with py2\n\treturn _dump(obj, dumper=dumper, fp=io.StringIO()).getvalue()\n\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = _dumper\n\n\treturn dumper(obj).encode('utf-8')\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper).strip()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif isinstance(obj, str):\n\t\treturn obj\n\telse:\n\t\treturn obj.encode('utf-8')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif isinstance(obj, unicode):\n\t\treturn obj.encode('utf-8')\n\treturn str(obj)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tdumper = dumper or json.dumps\n\treturn dumper(obj, ensure_ascii=False, separators=(',', ':'))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = YAMLDumper()\n\treturn dumper.dump(obj)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = default_dumper\n\treturn dumper(obj)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.dump\n\treturn dumper(obj, encoding='utf-8', allow_unicode=True, default_flow_style=False)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\t# TODO: Remove this method in the future.\n\t# I might have to keep it if I want to be able to dump to a string\n\t# with py2\n\treturn _dump(obj, dumper=dumper, fp=io.StringIO()).getvalue()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = _dumper\n\n\treturn dumper(obj).encode('utf-8')\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_8.py", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "ques_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tfile_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfile_handler.setFormatter(file_formatter)\n\n\tconsole_formatter = logging.Formatter('%(message)s')\n\tconsole_handler.setFormatter(console_formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t# Log to file\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\t# Log to console\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tconsole_handler.setFormatter(formatter)\n\tlogger.addHandler(console_handler)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\treturn logger\n\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(levelname)s - %(asctime)s - %(name)s - %(message)s')\n\n\tif debug:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\t\tch.setFormatter(formatter)\n\t\tlogger.addHandler(ch)\n\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tif debug:\n\t\tlog_level = logging.DEBUG\n\telse:\n\t\tlog_level = logging.INFO\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(log_level)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(log_level)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(log_level)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.propagate = False\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tif debug:\n\t\tch.setLevel(logging.DEBUG)\n\telse:\n\t\tch.setLevel(logging.INFO)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t# TODO: Check we are logging to file and stderr\n\tl = logging.getLogger(name)\n\tl.setLevel(logging.DEBUG)\n\n\t# Create console handler and set level to debug\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# Create file handler and set level to debug\n\tfh = logging.FileHandler(logfile, mode='a')\n\tfh.setLevel(logging.DEBUG)\n\n\t# Create formatter\n\tformatter = logging.Formatter('%(asctime)s %(name)s: %(levelname)s %(message)s',\n\t\t\t\t\t\t\t\t  datefmt='%Y-%m-%d %H:%M:%S')\n\n\t# Add formatter to ch\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# Add ch and fh to logger\n\tl.addHandler(ch)\n\tl.addHandler(fh)\n\treturn l\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tformatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n\tlogger.setLevel(logging.DEBUG)\n\t\n\t# set up console logging\n\tif debug:\n\t\tconsole = logging.StreamHandler()\n\t\tconsole.setLevel(logging.DEBUG)\n\t\tconsole.setFormatter(formatter)\n\t\tlogger.addHandler(console)\n\n\t# set up file logging\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\treturn logger\n\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tconsole = logging.StreamHandler()\n\tconsole.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tconsole.setFormatter(formatter)\n\tlogger.addHandler(console)\n\tif debug:\n\t\tlogger.info('Logger created in debug mode')\n\telse:\n\t\tlogger.info('Logger created in release mode')\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tif logfile:\n\t\tlog_handler = logging.FileHandler(logfile, mode='a')\n\t\tlog_handler.setLevel(logging.DEBUG)\n\t\tlog_handler.setFormatter(formatter)\n\t\tlogger.addHandler(log_handler)\n\tif debug:\n\t\tprint_handler = logging.StreamHandler(sys.stdout)\n\t\tprint_handler.setLevel(logging.DEBUG)\n\t\tprint_handler.setFormatter(formatter)\n\t\tlogger.addHandler(print_handler)\n\treturn logger\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\n\tfile_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfile_handler.setFormatter(file_formatter)\n\n\tconsole_formatter = logging.Formatter('%(message)s')\n\tconsole_handler.setFormatter(console_formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG if debug else logging.INFO)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t# Log to file\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\t# Log to console\n\tconsole_handler = logging.StreamHandler()\n\tconsole_handler.setLevel(logging.DEBUG if debug else logging.INFO)\n\tconsole_handler.setFormatter(formatter)\n\tlogger.addHandler(console_handler)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler = logging.FileHandler(logfile)\n\thandler.setFormatter(formatter)\n\tlogger.addHandler(handler)\n\n\treturn logger\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(levelname)s - %(asctime)s - %(name)s - %(message)s')\n\n\tif debug:\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\t\tch.setFormatter(formatter)\n\t\tlogger.addHandler(ch)\n\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tif debug:\n\t\tlog_level = logging.DEBUG\n\telse:\n\t\tlog_level = logging.INFO\n\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(log_level)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(log_level)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(log_level)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.propagate = False\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tif debug:\n\t\tch.setLevel(logging.DEBUG)\n\telse:\n\t\tch.setLevel(logging.INFO)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t# TODO: Check we are logging to file and stderr\n\tl = logging.getLogger(name)\n\tl.setLevel(logging.DEBUG)\n\n\t# Create console handler and set level to debug\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# Create file handler and set level to debug\n\tfh = logging.FileHandler(logfile, mode='a')\n\tfh.setLevel(logging.DEBUG)\n\n\t# Create formatter\n\tformatter = logging.Formatter('%(asctime)s %(name)s: %(levelname)s %(message)s',\n\t\t\t\t\t\t\t\t  datefmt='%Y-%m-%d %H:%M:%S')\n\n\t# Add formatter to ch\n\tch.setFormatter(formatter)\n\tfh.setFormatter(formatter)\n\n\t# Add ch and fh to logger\n\tl.addHandler(ch)\n\tl.addHandler(fh)\n\treturn l\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tformatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')\n\tlogger.setLevel(logging.DEBUG)\n\t\n\t# set up console logging\n\tif debug:\n\t\tconsole = logging.StreamHandler()\n\t\tconsole.setLevel(logging.DEBUG)\n\t\tconsole.setFormatter(formatter)\n\t\tlogger.addHandler(console)\n\n\t# set up file logging\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(formatter)\n\tlogger.addHandler(file_handler)\n\n\treturn logger\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tconsole = logging.StreamHandler()\n\tconsole.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tconsole.setFormatter(formatter)\n\tlogger.addHandler(console)\n\tif debug:\n\t\tlogger.info('Logger created in debug mode')\n\telse:\n\t\tlogger.info('Logger created in release mode')\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tif logfile:\n\t\tlog_handler = logging.FileHandler(logfile, mode='a')\n\t\tlog_handler.setLevel(logging.DEBUG)\n\t\tlog_handler.setFormatter(formatter)\n\t\tlogger.addHandler(log_handler)\n\tif debug:\n\t\tprint_handler = logging.StreamHandler(sys.stdout)\n\t\tprint_handler.setLevel(logging.DEBUG)\n\t\tprint_handler.setFormatter(formatter)\n\t\tlogger.addHandler(print_handler)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_6.py", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "ques_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tif not isinstance(shape, tuple):\n\t\tshape = (shape,)\n\tif dtype is None:\n\t\tdtype = np.dtype(\"float32\")\n\tif dtype.kind in \"uif\":\n\t\ta = np.random.randint(0, 100, shape)\n\t\ta = a.astype(dtype)\n\telif dtype.kind == \"c\":\n\t\ta = np.random.randint(0, 100, shape) + 1j * np.random.randint(0, 100, shape)\n\t\ta = a.astype(dtype)\n\telif dtype.kind == \"S\":\n\t\ta = np.random.randint(0, 100, shape)\n\t\ta = a.astype(\"S10\")\n\telse:\n\t\traise TypeError(\"dtype is not supported: %s\" % dtype)\n\treturn a\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tif dtype == np.dtype(\"float32\"):\n\t\treturn np.zeros(shape, dtype=dtype)\n\telif dtype == np.dtype(\"float64\"):\n\t\treturn np.zeros(shape, dtype=dtype)\n\telif dtype == np.dtype(\"int32\"):\n\t\treturn np.zeros(shape, dtype=dtype)\n\telse:\n\t\traise ValueError(\"Invalid dtype. Must be one of np.float32, np.float64, np.int32\")\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tif isinstance(shape, int):\n\t\tshape = (shape,)\n\treturn np.zeros(shape, dtype=dtype)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tif not isinstance(shape, tuple):\n\t\tshape = (shape,)\n\tif dtype is None:\n\t\tdtype = np.dtype(\"float32\")\n\tif dtype.kind in \"uif\":\n\t\ta = np.random.randint(0, 100, shape)\n\t\ta = a.astype(dtype)\n\telif dtype.kind == \"c\":\n\t\ta = np.random.randint(0, 100, shape) + 1j * np.random.randint(0, 100, shape)\n\t\ta = a.astype(dtype)\n\telif dtype.kind == \"S\":\n\t\ta = np.random.randint(0, 100, shape)\n\t\ta = a.astype(\"S10\")\n\telse:\n\t\traise TypeError(\"dtype is not supported: %s\" % dtype)\n\treturn a\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tif dtype == np.dtype(\"float32\"):\n\t\treturn np.zeros(shape, dtype=dtype)\n\telif dtype == np.dtype(\"float64\"):\n\t\treturn np.zeros(shape, dtype=dtype)\n\telif dtype == np.dtype(\"int32\"):\n\t\treturn np.zeros(shape, dtype=dtype)\n\telse:\n\t\traise ValueError(\"Invalid dtype. Must be one of np.float32, np.float64, np.int32\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tarray = np.zeros(shape, dtype=dtype)\n\treturn array\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\tif isinstance(shape, int):\n\t\tshape = (shape,)\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_2.py", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "ques_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n\treturn np.exp(-0.5*((x-0.2)/0.1)**2)\n\n\n", "def gaussian(x):\n\treturn np.exp(-(x-0.2)**2/(2*0.1**2))/(np.sqrt(2*np.pi)*0.1)\n\n", "def gaussian(x):\n\treturn 1./(np.sqrt(2.*np.pi)*0.1)*np.exp(-(x-0.2)**2/(2.*0.1**2))\n\n", "def gaussian(x):\n\treturn np.exp(-np.power(x - 0.2, 2.) / (2 * np.power(0.1, 2.)))\n\n\n", "def gaussian(x):\n\treturn np.exp(-0.5 * ((x - 0.2) / 0.1)**2)\n\n", "def gaussian(x):\n\ta0 = 1.0 / (0.1 * np.sqrt(2 * np.pi))\n\ta1 = 0.2\n\ta2 = 0.1\n\n\treturn a0 * np.exp(-((x - a1) / a2)**2 / 2)\n\n\n", "def gaussian(x):\n\t#return np.exp(-0.5 * (x - 0.2)**2 / 0.1**2)\n\treturn np.exp(-1.0 * (x - 0.2)**2)\n\n", "def gaussian(x):\n\treturn 1.0/(2.0*np.pi*0.1**2)**0.5*np.exp(-0.5*(x-0.2)**2/(0.1**2))\n\n\n", "def gaussian(x):\n\treturn np.exp(-0.5*((x-0.2)/0.1)**2)\n\n", "def gaussian(x):\n\treturn np.exp(-(x-0.2)**2/0.1**2)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n\treturn np.exp(-0.5*((x-0.2)/0.1)**2)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\treturn np.exp(-(x-0.2)**2/(2*0.1**2))/(np.sqrt(2*np.pi)*0.1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\treturn 1./(np.sqrt(2.*np.pi)*0.1)*np.exp(-(x-0.2)**2/(2.*0.1**2))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\treturn np.exp(-np.power(x - 0.2, 2.) / (2 * np.power(0.1, 2.)))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\treturn np.exp(-0.5 * ((x - 0.2) / 0.1)**2)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\ta0 = 1.0 / (0.1 * np.sqrt(2 * np.pi))\n\ta1 = 0.2\n\ta2 = 0.1\n\n\treturn a0 * np.exp(-((x - a1) / a2)**2 / 2)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\t#return np.exp(-0.5 * (x - 0.2)**2 / 0.1**2)\n\treturn np.exp(-1.0 * (x - 0.2)**2)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\treturn 1.0/(2.0*np.pi*0.1**2)**0.5*np.exp(-0.5*(x-0.2)**2/(0.1**2))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\treturn np.exp(-0.5*((x-0.2)/0.1)**2)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\treturn np.exp(-(x-0.2)**2/0.1**2)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_13.py", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "ques_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t# Load and validate each configuration file.\n\tconfigurations = {}\n\tlog_records = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename) as config_file:\n\t\t\t\tconfig = json.load(config_file)\n\t\texcept (OSError, IOError, ValueError) as e:\n\t\t\tlog_records.append(logging.LogRecord(logger.name, logging.ERROR, filename, None,\n\t\t\t\t\t\t\t\t\t\t\t\t  'Error loading configuration file: {0}'.format(e), None, None))\n\t\telse:\n\t\t\t# Apply any overrides to the configuration.\n\t\t\tif overrides:\n\t\t\t\tconfig = apply_overrides(config, overrides)\n\t\t\t# Resolve any environment variables.\n\t\t\tif resolve_env:\n\t\t\t\tconfig = resolve_environment_variables(config)\n\t\t\t# Validate the configuration.\n\t\t\tlog_records.extend(validate_configuration(config,\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tconfig_filename = os.path.abspath(config_filename)\n\t\ttry:\n\t\t\twith open(config_filename, encoding='utf-8') as config_file:\n\t\t\t\tconfig_data = config_file.read()\n\t\texcept OSError as e:\n\t\t\trecord = logging.LogRecord(__name__, logging.ERROR, config_filename, 0, 'unable to read configuration file \"{filename}\": {error}', (), None, 'load_configurations')\n\t\t\trecord.filename = config_filename\n\t\t\trecord.error = e\n\t\t\terrors.append(record)\n\t\t\tcontinue\n\t\ttry:\n\t\t\tconfiguration = json.loads(config_data)\n\t\texcept UnicodeError as e:\n\t\t\trecord = logging.LogRecord(__name__, logging.ERROR, config_filename, 0, 'unable to decode configuration file \"{filename}\": {error}', (), None, 'load_configurations')\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tvalidator = ConfigValidator(overrides)\n\terrs = []\n\tconfigs = {}\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename) as f:\n\t\t\t\tconfigs[filename] = validator.validate(json.load(f), resolve_env=resolve_env)\n\t\texcept IOError as e:\n\t\t\terrs.append(logging.LogRecord(\n\t\t\t\tname=\"config\",\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=filename,\n\t\t\t\tmsg=\"Unable to load configuration file: %s\" % e,\n\t\t\t))\n\t\texcept ValueError as e:\n\t\t\terrs.append(logging.LogRecord(\n\t\t\t\tname=\"config\",\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=filename,\n\t\t\t\tmsg=\"Unable to parse configuration file: %s\" % e,\n\t\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\tlog_records = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigurations[config_filename] = parse_configuration(config_filename, overrides, resolve_env)\n\t\texcept ConfigurationParseError as exc:\n\t\t\tlog_records.append(exc.log_record)\n\t\texcept Exception:\n\t\t\tlog_records.append(LogRecord(logging.ERROR, \"Unhandled exception while parsing configuration file {0}\", config_filename))\n\treturn configurations, log_records\n\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as config_file:\n\t\t\t\tconfig = json.load(config_file)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = resolve_environment(config)\n\t\t\t\tif overrides:\n\t\t\t\t\tconfig = merge_configurations(config, overrides)\n\t\t\t\tvalidate_configuration(config)\n\t\t\t\tconfigs[config_filename] = config\n\t\texcept (json.JSONDecodeError, KeyError) as err:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=config_filename,\n\t\t\t\t\tlineno=err.lineno,\n\t\t\t\t\tmsg=err,\n\t\t\t\t\targs=()\n\t\t\t\t)\n\t\t\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tif resolve_env:\n\t\toverrides = resolve_environment_overrides(overrides)\n\treturn load_configurations_from_dict(dict(zip(config_filenames, map(json.load, config_filenames))), overrides)\n\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tconfig_dict = {}\n\tlogs = []\n\tfor cfg_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(cfg_filename, 'r') as cfg_file:\n\t\t\t\tcfg = json.load(cfg_file)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tcfg = resolve_environment_variables(cfg)\n\t\t\t\tcfg = merge_configurations(cfg, overrides)\n\t\t\t\tvalidate_configuration(cfg)\n\t\t\t\tconfig_dict[cfg_filename] = cfg\n\t\texcept Exception as e:\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\tname='config',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=cfg_filename,\n\t\t\t\tlineno=0,\n\t\t\t\tmsg=str(e),\n\t\t\t\targs=(),\n\t\t\t\texc_info=None\n\t\t\t))\n\t\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\tconfig_errors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\tconfigs[config_filename] = config\n\t\texcept ValidationError as e:\n\t\t\tconfig_errors.append(logging.LogRecord(\n\t\t\t\tname=__name__,\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=e.errors[0].line_number,\n\t\t\t\tmsg=str(e),\n\t\t\t\targs=(),\n\t\t\t\texc_info=None\n\t\t\t))\n\treturn configs, config_errors\n\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t# Load the configuration data.\n\tconfigs = {}\n\tlogs = []\n\tfor filename in config_filenames:\n\t\tconfig, log = load_configuration(filename, overrides, resolve_env)\n\t\tif config:\n\t\t\tconfigs[filename] = config\n\t\tif log:\n\t\t\tlogs.append(log)\n\treturn configs, logs\n\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tlog_records = []\n\tconfigs = OrderedDict()\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\texcept ConfigError as e:\n\t\t\tlog_records.append(logging.LogRecord(\n\t\t\t\tname=__name__,\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=e.lineno,\n\t\t\t\tmsg=e.message,\n\t\t\t\targs=(),\n\t\t\t\texc_info=None,\n\t\t\t\tfunc=None,\n\t\t\t))\n\treturn configs, log_records\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t# Load and validate each configuration file.\n\tconfigurations = {}\n\tlog_records = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename) as config_file:\n\t\t\t\tconfig = json.load(config_file)\n\t\texcept (OSError, IOError, ValueError) as e:\n\t\t\tlog_records.append(logging.LogRecord(logger.name, logging.ERROR, filename, None,\n\t\t\t\t\t\t\t\t\t\t\t\t  'Error loading configuration file: {0}'.format(e), None, None))\n\t\telse:\n\t\t\t# Apply any overrides to the configuration.\n\t\t\tif overrides:\n\t\t\t\tconfig = apply_overrides(config, overrides)\n\t\t\t# Resolve any environment variables.\n\t\t\tif resolve_env:\n\t\t\t\tconfig = resolve_environment_variables(config)\n\t\t\t# Validate the configuration.\n\t\t\tlog_records.extend(validate_configuration(config,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tconfig_filename = os.path.abspath(config_filename)\n\t\ttry:\n\t\t\twith open(config_filename, encoding='utf-8') as config_file:\n\t\t\t\tconfig_data = config_file.read()\n\t\texcept OSError as e:\n\t\t\trecord = logging.LogRecord(__name__, logging.ERROR, config_filename, 0, 'unable to read configuration file \"{filename}\": {error}', (), None, 'load_configurations')\n\t\t\trecord.filename = config_filename\n\t\t\trecord.error = e\n\t\t\terrors.append(record)\n\t\t\tcontinue\n\t\ttry:\n\t\t\tconfiguration = json.loads(config_data)\n\t\texcept UnicodeError as e:\n\t\t\trecord = logging.LogRecord(__name__, logging.ERROR, config_filename, 0, 'unable to decode configuration file \"{filename}\": {error}', (), None, 'load_configurations')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tvalidator = ConfigValidator(overrides)\n\terrs = []\n\tconfigs = {}\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename) as f:\n\t\t\t\tconfigs[filename] = validator.validate(json.load(f), resolve_env=resolve_env)\n\t\texcept IOError as e:\n\t\t\terrs.append(logging.LogRecord(\n\t\t\t\tname=\"config\",\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=filename,\n\t\t\t\tmsg=\"Unable to load configuration file: %s\" % e,\n\t\t\t))\n\t\texcept ValueError as e:\n\t\t\terrs.append(logging.LogRecord(\n\t\t\t\tname=\"config\",\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=filename,\n\t\t\t\tmsg=\"Unable to parse configuration file: %s\" % e,\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\tlog_records = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigurations[config_filename] = parse_configuration(config_filename, overrides, resolve_env)\n\t\texcept ConfigurationParseError as exc:\n\t\t\tlog_records.append(exc.log_record)\n\t\texcept Exception:\n\t\t\tlog_records.append(LogRecord(logging.ERROR, \"Unhandled exception while parsing configuration file {0}\", config_filename))\n\treturn configurations, log_records\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as config_file:\n\t\t\t\tconfig = json.load(config_file)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = resolve_environment(config)\n\t\t\t\tif overrides:\n\t\t\t\t\tconfig = merge_configurations(config, overrides)\n\t\t\t\tvalidate_configuration(config)\n\t\t\t\tconfigs[config_filename] = config\n\t\texcept (json.JSONDecodeError, KeyError) as err:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=config_filename,\n\t\t\t\t\tlineno=err.lineno,\n\t\t\t\t\tmsg=err,\n\t\t\t\t\targs=()\n\t\t\t\t)\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tif resolve_env:\n\t\toverrides = resolve_environment_overrides(overrides)\n\treturn load_configurations_from_dict(dict(zip(config_filenames, map(json.load, config_filenames))), overrides)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tconfig_dict = {}\n\tlogs = []\n\tfor cfg_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(cfg_filename, 'r') as cfg_file:\n\t\t\t\tcfg = json.load(cfg_file)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tcfg = resolve_environment_variables(cfg)\n\t\t\t\tcfg = merge_configurations(cfg, overrides)\n\t\t\t\tvalidate_configuration(cfg)\n\t\t\t\tconfig_dict[cfg_filename] = cfg\n\t\texcept Exception as e:\n\t\t\tlogs.append(logging.LogRecord(\n\t\t\t\tname='config',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=cfg_filename,\n\t\t\t\tlineno=0,\n\t\t\t\tmsg=str(e),\n\t\t\t\targs=(),\n\t\t\t\texc_info=None\n\t\t\t))\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\tconfig_errors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\t\tconfigs[config_filename] = config\n\t\texcept ValidationError as e:\n\t\t\tconfig_errors.append(logging.LogRecord(\n\t\t\t\tname=__name__,\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=e.errors[0].line_number,\n\t\t\t\tmsg=str(e),\n\t\t\t\targs=(),\n\t\t\t\texc_info=None\n\t\t\t))\n\treturn configs, config_errors\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\t# Load the configuration data.\n\tconfigs = {}\n\tlogs = []\n\tfor filename in config_filenames:\n\t\tconfig, log = load_configuration(filename, overrides, resolve_env)\n\t\tif config:\n\t\t\tconfigs[filename] = config\n\t\tif log:\n\t\t\tlogs.append(log)\n\treturn configs, logs\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tlog_records = []\n\tconfigs = OrderedDict()\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfigs[config_filename] = load_configuration(config_filename, overrides=overrides, resolve_env=resolve_env)\n\t\texcept ConfigError as e:\n\t\t\tlog_records.append(logging.LogRecord(\n\t\t\t\tname=__name__,\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=e.lineno,\n\t\t\t\tmsg=e.message,\n\t\t\t\targs=(),\n\t\t\t\texc_info=None,\n\t\t\t\tfunc=None,\n\t\t\t))\n\treturn configs, log_records\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_3.py", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "ques_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\treturn obj\n\n", "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj\n\treturn str(obj, \"UTF-8\")\n\n", "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn obj\n\n", "def force_string(obj):\n\tif is_string(obj):\n\t\treturn obj.encode()\n\treturn obj\n\n", "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\treturn obj\n\n", "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\telif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn str(obj).encode('utf-8')\n\n", "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\treturn str(obj).encode('utf-8')\n\n", "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn obj\n\n", "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\telif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\traise TypeError(\"Expected a string or bytes\")\n\n", "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('UTF-8')\n\treturn obj\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj\n\treturn str(obj, \"UTF-8\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif is_string(obj):\n\t\treturn obj.encode()\n\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\telif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn str(obj).encode('utf-8')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\treturn str(obj).encode('utf-8')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj\n\telif isinstance(obj, str):\n\t\treturn obj.encode('utf-8')\n\telse:\n\t\traise TypeError(\"Expected a string or bytes\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\treturn obj.decode('UTF-8')\n\treturn obj\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "ques_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n\traise NotImplementedError\n\n\t# if not (0 <= ticks < 86400000000000):\n\t# \traise ValueError('ticks out of range')\n\t# days = ticks // 86400000000000\n\t# ticks %= 86400000000000\n\t# return cls(\n\t# \tdays, ticks // 1000000000,\n\t# \tticks % 1000000000, tzinfo=tz)\n\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif not cls.is_valid_ticks(ticks):\n\t\traise ValueError(\"ticks is out of bounds (0 <= ticks < 86400000000000)\")\n\treturn cls(ticks, tz)\n\n\t# }}}\n\n\t# {{{ from_time(cls, time, tz=None)\n\n\t@classmethod\n\tdef from_time(cls, time, tz=None):\n\t\t\"\"\"\n\t\tCreate a time from a time object.\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif tz is None:\n\t\ttz = Time._timezone.get_local()\n\tif not isinstance(tz, Timezone):\n\t\ttz = Timezone(tz)\n\tif ticks > 86400000000000:\n\t\traise ValueError(\"ticks must be <= 86400000000000\")\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be >= 0\")\n\treturn cls(ticks, tz)\n\n\t@classmethod\n\tdef from_time(cls, time, tz=None):\n\t\"\"\"\n\tCreate a time from a time object.\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif tz is None:\n\t\treturn cls(ticks)\n\telse:\n\t\treturn cls(ticks, tz)\n\t\n", "def from_ticks(cls, ticks, tz=None):\n\treturn cls.from_microseconds((ticks + 500) // 1000, tz)\n\n\t# }}}\n\n\t# {{{ to_ticks\n\n\t@classmethod\n\tdef to_ticks(cls, time):\n\t\t\"\"\"\n\t\tConvert a time to ticks (nanoseconds since midnight).\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\treturn cls.from_ticks_fraction(ticks, 0, tz)\n\n\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks out of bounds\")\n\treturn cls(ticks, tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\tpass\n\n", "def from_ticks(cls, ticks, tz=None):\n\tpass\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif not 0 <= ticks < 86400000000000:\n\t\traise ValueError('ticks out of bounds (0 <= ticks < 86400000000000)')\n\treturn cls(ticks=ticks, tz=tz)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n\traise NotImplementedError\n\n\t# if not (0 <= ticks < 86400000000000):\n\t# \traise ValueError('ticks out of range')\n\t# days = ticks // 86400000000000\n\t# ticks %= 86400000000000\n\t# return cls(\n\t# \tdays, ticks // 1000000000,\n\t# \tticks % 1000000000, tzinfo=tz)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif not cls.is_valid_ticks(ticks):\n\t\traise ValueError(\"ticks is out of bounds (0 <= ticks < 86400000000000)\")\n\treturn cls(ticks, tz)\n\n\t# }}}\n\n\t# {{{ from_time(cls, time, tz=None)\n\n\t@classmethod\n\tdef from_time(cls, time, tz=None):\n\t\t\"\"\"\n\t\tCreate a time from a time object.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif tz is None:\n\t\ttz = Time._timezone.get_local()\n\tif not isinstance(tz, Timezone):\n\t\ttz = Timezone(tz)\n\tif ticks > 86400000000000:\n\t\traise ValueError(\"ticks must be <= 86400000000000\")\n\tif ticks < 0:\n\t\traise ValueError(\"ticks must be >= 0\")\n\treturn cls(ticks, tz)\n\n\t@classmethod\n\tdef from_time(cls, time, tz=None):\n\t\"\"\"\n\tCreate a time from a time object.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif tz is None:\n\t\treturn cls(ticks)\n\telse:\n\t\treturn cls(ticks, tz)\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\treturn cls.from_microseconds((ticks + 500) // 1000, tz)\n\n\t# }}}\n\n\t# {{{ to_ticks\n\n\t@classmethod\n\tdef to_ticks(cls, time):\n\t\t\"\"\"\n\t\tConvert a time to ticks (nanoseconds since midnight).\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\treturn cls.from_ticks_fraction(ticks, 0, tz)\n\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks out of bounds\")\n\treturn cls(ticks, tz)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tpass\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tpass\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif not 0 <= ticks < 86400000000000:\n\t\traise ValueError('ticks out of bounds (0 <= ticks < 86400000000000)')\n\treturn cls(ticks=ticks, tz=tz)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n\t# This will raise TypeError if protocol_version is not a tuple\n\tversions = cls._protocol_handlers if protocol_version is None else (protocol_version,)\n\treturn {v: cls._protocol_handlers[v] for v in versions if v in cls._protocol_handlers}\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\thandlers = {}\n\n\tif protocol_version is None:\n\t\tfor handler in cls._protocol_handlers:\n\t\t\thandlers[handler.version] = handler\n\telse:\n\t\tfor handler in cls._protocol_handlers:\n\t\t\tif handler.version == protocol_version:\n\t\t\t\thandlers[handler.version] = handler\n\t\t\t\tbreak\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be a tuple\")\n\t\telif protocol_version not in cls._protocol_handlers:\n\t\t\treturn {}\n\t\telse:\n\t\t\treturn {\n\t\t\t\tprotocol_version: cls._protocol_handlers[protocol_version]\n\t\t\t}\n\telse:\n\t\treturn cls._protocol_handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple\")\n\thandlers = {\n\t\t(3, 5): BoltProtocol35,\n\t\t(3, 6): BoltProtocol36,\n\t\t(3, 7): BoltProtocol37,\n\t\t(3, 8): BoltProtocol38,\n\t\t(4, 0): BoltProtocol40,\n\t}\n\tif protocol_version is None:\n\t\treturn handlers\n\telse:\n\t\tif protocol_version in handlers:\n\t\t\treturn {protocol_version: handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be a tuple\")\n\t\tif protocol_version in cls._handlers_by_version:\n\t\t\treturn {protocol_version: cls._handlers_by_version[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn cls._handlers_by_version\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be passed as tuple\")\n\thandlers = {}\n\tfor subclass in cls.__subclasses__():\n\t\tversion = subclass.protocol_version\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = subclass\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Unsupported protocol version: {0}\".format(protocol_version))\n\thandlers = {}\n\tfor version in cls._PROTOCOL_HANDLERS:\n\t\tif protocol_version is None or version == protocol_version:\n\t\t\thandlers[version] = cls._PROTOCOL_HANDLERS[version]\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple) and protocol_version is not None:\n\t\traise TypeError(\"Protocol version must be passed as a tuple\")\n\n\thandlers = {}\n\tfor version, handler in cls._handlers.items():\n\t\tif protocol_version is None or version == protocol_version:\n\t\t\thandlers[version] = handler\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\ttry:\n\t\t\treturn {\n\t\t\t\tprotocol_version: cls._handler_classes[protocol_version]\n\t\t\t}\n\t\texcept KeyError:\n\t\t\treturn {}\n\treturn cls._handler_classes.copy()\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed as a tuple\")\n\t\tif protocol_version in cls._handlers_by_version:\n\t\t\treturn {protocol_version: cls._handlers_by_version[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn cls._handlers_by_version\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\t# This will raise TypeError if protocol_version is not a tuple\n\tversions = cls._protocol_handlers if protocol_version is None else (protocol_version,)\n\treturn {v: cls._protocol_handlers[v] for v in versions if v in cls._protocol_handlers}\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\thandlers = {}\n\n\tif protocol_version is None:\n\t\tfor handler in cls._protocol_handlers:\n\t\t\thandlers[handler.version] = handler\n\telse:\n\t\tfor handler in cls._protocol_handlers:\n\t\t\tif handler.version == protocol_version:\n\t\t\t\thandlers[handler.version] = handler\n\t\t\t\tbreak\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be a tuple\")\n\t\telif protocol_version not in cls._protocol_handlers:\n\t\t\treturn {}\n\t\telse:\n\t\t\treturn {\n\t\t\t\tprotocol_version: cls._protocol_handlers[protocol_version]\n\t\t\t}\n\telse:\n\t\treturn cls._protocol_handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple\")\n\thandlers = {\n\t\t(3, 5): BoltProtocol35,\n\t\t(3, 6): BoltProtocol36,\n\t\t(3, 7): BoltProtocol37,\n\t\t(3, 8): BoltProtocol38,\n\t\t(4, 0): BoltProtocol40,\n\t}\n\tif protocol_version is None:\n\t\treturn handlers\n\telse:\n\t\tif protocol_version in handlers:\n\t\t\treturn {protocol_version: handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be a tuple\")\n\t\tif protocol_version in cls._handlers_by_version:\n\t\t\treturn {protocol_version: cls._handlers_by_version[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn cls._handlers_by_version\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Protocol version must be passed as tuple\")\n\thandlers = {}\n\tfor subclass in cls.__subclasses__():\n\t\tversion = subclass.protocol_version\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = subclass\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None and not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"Unsupported protocol version: {0}\".format(protocol_version))\n\thandlers = {}\n\tfor version in cls._PROTOCOL_HANDLERS:\n\t\tif protocol_version is None or version == protocol_version:\n\t\t\thandlers[version] = cls._PROTOCOL_HANDLERS[version]\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple) and protocol_version is not None:\n\t\traise TypeError(\"Protocol version must be passed as a tuple\")\n\n\thandlers = {}\n\tfor version, handler in cls._handlers.items():\n\t\tif protocol_version is None or version == protocol_version:\n\t\t\thandlers[version] = handler\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\ttry:\n\t\t\treturn {\n\t\t\t\tprotocol_version: cls._handler_classes[protocol_version]\n\t\t\t}\n\t\texcept KeyError:\n\t\t\treturn {}\n\treturn cls._handler_classes.copy()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"Protocol version must be passed as a tuple\")\n\t\tif protocol_version in cls._handlers_by_version:\n\t\t\treturn {protocol_version: cls._handlers_by_version[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn cls._handlers_by_version\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "ques_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n\treturn cls(map(Bookmark.from_raw, values))\n\n\n", "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.append(Bookmark.from_raw(value))\n\treturn bookmarks\n\n\n", "def from_raw_values(cls, values):\n\treturn cls(values)\n\n\n", "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tbookmarks._values = values\n\treturn bookmarks\n\n\t@classmethod\n\tdef from_file(cls, filename):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a file.\n\n", "def from_raw_values(cls, values):\n\tbm = cls()\n\tbm._from_raw_values(values)\n\treturn bm\n\n\n", "def from_raw_values(cls, values):\n\tcls._check_type(\"values\", values, Iterable)\n\treturn cls([cls.Bookmark.from_raw_value(value) for value in values])\n\n", "def from_raw_values(cls, values):\n\treturn cls(values)\n\n\n", "def from_raw_values(cls, values):\n\treturn cls(values)\n\n", "def from_raw_values(cls, values):\n\tbookmarks = {}\n\tfor value in values:\n\t\tb = cls._parse_raw_value(value)\n\t\tif b is not None:\n\t\t\tbookmarks[b[0]] = b\n\treturn cls(bookmarks)\n\n", "def from_raw_values(cls, values):\n\tvalues = list(values)\n\tif len(values) == 0:\n\t\treturn cls()\n\tif values[0] == b'BOOKMARKS\\n':\n\t\tvalues.pop(0)\n\tbookmarks = cls()\n\tfor value in values:\n\t\tvalue = value.decode('ascii')\n\t\tname, url = value.split('\\t')\n\t\tbookmarks.add(name, url)\n\treturn bookmarks\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n\treturn cls(map(Bookmark.from_raw, values))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmarks.append(Bookmark.from_raw(value))\n\treturn bookmarks\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\treturn cls(values)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tbookmarks._values = values\n\treturn bookmarks\n\n\t@classmethod\n\tdef from_file(cls, filename):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a file.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_raw_values(cls, values):\n\tbm = cls()\n\tbm._from_raw_values(values)\n\treturn bm\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tcls._check_type(\"values\", values, Iterable)\n\treturn cls([cls.Bookmark.from_raw_value(value) for value in values])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\treturn cls(values)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\treturn cls(values)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tbookmarks = {}\n\tfor value in values:\n\t\tb = cls._parse_raw_value(value)\n\t\tif b is not None:\n\t\t\tbookmarks[b[0]] = b\n\treturn cls(bookmarks)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tvalues = list(values)\n\tif len(values) == 0:\n\t\treturn cls()\n\tif values[0] == b'BOOKMARKS\\n':\n\t\tvalues.pop(0)\n\tbookmarks = cls()\n\tfor value in values:\n\t\tvalue = value.decode('ascii')\n\t\tname, url = value.split('\\t')\n\t\tbookmarks.add(name, url)\n\treturn bookmarks\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "ques_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (unicode, str)):\n\t\treturn (seq, unicode)\n\t# return (seq, type(seq))\n\tif isinstance(seq, (list, tuple)):\n\t\treturn (seq, type(seq))\n\tif isinstance(seq, Buffer):\n\t\treturn (seq.getvalue(), unicode)\n\tif hasattr(seq, 'read'):\n\t\tif bufsize is None:\n\t\t\tbufsize = 4096\n\t\ts = seq.read(bufsize)\n\t\tif not s:\n\t\t\treturn (u'', unicode)\n\t\tseq.seek(-len(s), 1)\n\t\treturn (s, unicode)\n\tif hasattr(seq, '__iter__'):\n\t\treturn (seq, type(seq))\n\traise TypeError(\n\t\t\t'argument must be a sequence or buffer object'\n\t\t\t)\n\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, _seq_types):\n\t\treturn (seq, type(seq))\n\tif hasattr(seq, '__getitem__'):\n\t\tif bufsize is None:\n\t\t\treturn (seq, _seq_types[0])\n\t\telse:\n\t\t\treturn (iter(seq), _seq_types[0])\n\traise TypeError('invalid sequence type: %r' % type(seq))\n\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (tuple, list)):\n\t\treturn (seq, type(seq))\n\tif isinstance(seq, str):\n\t\treturn (seq, str)\n\tif isinstance(seq, (bytes, bytearray)):\n\t\treturn (seq, bytes)\n\tif isinstance(seq, memoryview):\n\t\t# Sequence is read-only\n\t\treturn (seq, memoryview)\n\tif bufsize is not None:\n\t\treturn (iter(seq), itertools.islice)\n\treturn (iter(seq), type(seq))\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (str, bytearray)):\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq)\n\t\tseq = (seq[pos:pos + bufsize] for pos in range(0, len(seq), bufsize))\n\t\tseq_type = type(seq)\n\telse:\n\t\tseq_type = type(seq)\n\t\tif seq_type is not type(iter(seq)):\n\t\t\tseq = iter(seq)\n\treturn seq, seq_type\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, basestring):\n\t\treturn (seq, 's')\n\ttry:\n\t\tt = type(seq)\n\t\treturn (t(seq), 'm')\n\texcept TypeError:\n\t\tpass\n\ttry:\n\t\treturn (iter(seq), 'i')\n\texcept TypeError:\n\t\tpass\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\treturn (array('b', seq), 'b')\n\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, basestring):\n\t\treturn seq, type(seq)\n\telse:\n\t\treturn tuple(seq), type(seq)\n\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, Sequence):\n\t\treturn (seq, type(seq))\n\telse:\n\t\treturn (list(seq), list)\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif bufsize is None:\n\t\tbufsize = _default_bufsize\n\tif isinstance(seq, basestring):\n\t\tif len(seq) == 1:\n\t\t\t# Treat single characters as a string\n\t\t\treturn (seq, str)\n\t\t# Multi-character strings are treated as sequences\n\t\treturn (seq, (str, unicode))\n\ttry:\n\t\t# Try to get an iterator\n\t\titer(seq)\n\texcept TypeError:\n\t\t# Not iterable, so treat as a single item\n\t\treturn (seq, object)\n\telse:\n\t\treturn (seq, (str, unicode))\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif bufsize is None:\n\t\tbufsize = 8192\n\tif isinstance(seq, bytes):\n\t\treturn seq, 'b'\n\tif isinstance(seq, bytearray):\n\t\treturn seq, 'B'\n\tif isinstance(seq, str):\n\t\treturn seq, 'u'\n\tif isinstance(seq, (list, tuple)):\n\t\treturn seq, 'c'\n\tif isinstance(seq, io.RawIOBase):\n\t\tif not hasattr(seq, 'write'):\n\t\t\traise TypeError(\"'%s' object is not writable\" % type(seq).__name__)\n\t\tif bufsize < 0:\n\t\t\traise ValueError(\"invalid buffer size\")\n\t\treturn seq, 'r'\n\traise TypeError(\"'%s' object is not iterable\" % type(seq).__name__)\n\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (str, bytes)):\n\t\treturn seq, (str, bytes)\n\telse:\n\t\ttry:\n\t\t\treturn _get_seq_with_type(iter(seq), bufsize)\n\t\texcept TypeError:\n\t\t\tpass\n\t\ttry:\n\t\t\treturn _get_seq_with_type(buffer(seq), bufsize)\n\t\texcept TypeError:\n\t\t\tpass\n\treturn seq, None\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (unicode, str)):\n\t\treturn (seq, unicode)\n\t# return (seq, type(seq))\n\tif isinstance(seq, (list, tuple)):\n\t\treturn (seq, type(seq))\n\tif isinstance(seq, Buffer):\n\t\treturn (seq.getvalue(), unicode)\n\tif hasattr(seq, 'read'):\n\t\tif bufsize is None:\n\t\t\tbufsize = 4096\n\t\ts = seq.read(bufsize)\n\t\tif not s:\n\t\t\treturn (u'', unicode)\n\t\tseq.seek(-len(s), 1)\n\t\treturn (s, unicode)\n\tif hasattr(seq, '__iter__'):\n\t\treturn (seq, type(seq))\n\traise TypeError(\n\t\t\t'argument must be a sequence or buffer object'\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, _seq_types):\n\t\treturn (seq, type(seq))\n\tif hasattr(seq, '__getitem__'):\n\t\tif bufsize is None:\n\t\t\treturn (seq, _seq_types[0])\n\t\telse:\n\t\t\treturn (iter(seq), _seq_types[0])\n\traise TypeError('invalid sequence type: %r' % type(seq))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (tuple, list)):\n\t\treturn (seq, type(seq))\n\tif isinstance(seq, str):\n\t\treturn (seq, str)\n\tif isinstance(seq, (bytes, bytearray)):\n\t\treturn (seq, bytes)\n\tif isinstance(seq, memoryview):\n\t\t# Sequence is read-only\n\t\treturn (seq, memoryview)\n\tif bufsize is not None:\n\t\treturn (iter(seq), itertools.islice)\n\treturn (iter(seq), type(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (str, bytearray)):\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq)\n\t\tseq = (seq[pos:pos + bufsize] for pos in range(0, len(seq), bufsize))\n\t\tseq_type = type(seq)\n\telse:\n\t\tseq_type = type(seq)\n\t\tif seq_type is not type(iter(seq)):\n\t\t\tseq = iter(seq)\n\treturn seq, seq_type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, basestring):\n\t\treturn (seq, 's')\n\ttry:\n\t\tt = type(seq)\n\t\treturn (t(seq), 'm')\n\texcept TypeError:\n\t\tpass\n\ttry:\n\t\treturn (iter(seq), 'i')\n\texcept TypeError:\n\t\tpass\n\tif bufsize is None:\n\t\tbufsize = len(seq)\n\treturn (array('b', seq), 'b')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, basestring):\n\t\treturn seq, type(seq)\n\telse:\n\t\treturn tuple(seq), type(seq)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, Sequence):\n\t\treturn (seq, type(seq))\n\telse:\n\t\treturn (list(seq), list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif bufsize is None:\n\t\tbufsize = _default_bufsize\n\tif isinstance(seq, basestring):\n\t\tif len(seq) == 1:\n\t\t\t# Treat single characters as a string\n\t\t\treturn (seq, str)\n\t\t# Multi-character strings are treated as sequences\n\t\treturn (seq, (str, unicode))\n\ttry:\n\t\t# Try to get an iterator\n\t\titer(seq)\n\texcept TypeError:\n\t\t# Not iterable, so treat as a single item\n\t\treturn (seq, object)\n\telse:\n\t\treturn (seq, (str, unicode))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif bufsize is None:\n\t\tbufsize = 8192\n\tif isinstance(seq, bytes):\n\t\treturn seq, 'b'\n\tif isinstance(seq, bytearray):\n\t\treturn seq, 'B'\n\tif isinstance(seq, str):\n\t\treturn seq, 'u'\n\tif isinstance(seq, (list, tuple)):\n\t\treturn seq, 'c'\n\tif isinstance(seq, io.RawIOBase):\n\t\tif not hasattr(seq, 'write'):\n\t\t\traise TypeError(\"'%s' object is not writable\" % type(seq).__name__)\n\t\tif bufsize < 0:\n\t\t\traise ValueError(\"invalid buffer size\")\n\t\treturn seq, 'r'\n\traise TypeError(\"'%s' object is not iterable\" % type(seq).__name__)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (str, bytes)):\n\t\treturn seq, (str, bytes)\n\telse:\n\t\ttry:\n\t\t\treturn _get_seq_with_type(iter(seq), bufsize)\n\t\texcept TypeError:\n\t\t\tpass\n\t\ttry:\n\t\t\treturn _get_seq_with_type(buffer(seq), bufsize)\n\t\texcept TypeError:\n\t\t\tpass\n\treturn seq, None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "ques_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\t\tif other is None:\n\t\t\tif recompute or self._scale is None:\n\t\t\t\tself._scale = self.integral()\n\t\t\treturn self._scale\n\t\telse:\n\t\t\tif self._scale == 0:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"can't rescale a histogram with scale of 0\"\n\t\t\t\t)\n\t\t\treturn self.scale(other=None) / other\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif not self.scale_computed:\n\t\t\tself.scale_computed = True\n\t\t\tself.scale = self.integral()\n\t\treturn self.scale\n\telse:\n\t\tif not self.scale_computed or recompute:\n\t\t\tself.scale = self.integral()\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"scale of histogram {} is zero, can't rescale\".format(self)\n\t\t\t)\n\t\tself.scale_computed = True\n\t\tself.rescale(other / self.scale)\n\t\tself.scale = other\n\t\treturn self\n\n\tdef rescale(self, factor):\n\t\tself.values = factor * self.values\n\t\treturn self\n\n\tdef norm(self, other=None, recompute=False):\n\t\t\"\"\"\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\t# get scale\n\t\tif self.scale_computed and not recompute:\n\t\t\treturn self.scale\n\t\telse:\n\t\t\tscale = self.compute_scale()\n\t\t\tself.scale_computed = True\n\t\t\tself.scale = scale\n\t\t\treturn scale\n\telse:\n\t\t# set scale\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"can't rescale a histogram with zero scale\"\n\t\t\t)\n\t\telse:\n\t\t\tself.a /= self.scale / other\n\t\t\tself.scale = other\n\t\t\tself.scale_computed = True\n\t\t\treturn self\n\n\tdef normalize(self, recompute=False):\n\t\"\"\"\n\tScale the histogram to unity.\n\n", "def scale(self, other=None, recompute=False):\n\tif self._scale is None:\n\t\tself._scale = self.integral()\n\t\tself._scale_computed = True\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"can't rescale histogram with zero scale\")\n\tif recompute:\n\t\tself._scale_computed = False\n\tself.multiply(other / self._scale)\n\tself._scale = other\n\tself._scale_computed = True\n\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif (self.scale is None) or recompute:\n\t\t\tself.scale = sum(self.data)\n\t\treturn self.scale\n\tif self.scale == 0:\n\t\traise LenaValueError(\n\t\t\t\"Cannot rescale a histogram with zero scale\"\n\t\t)\n\tself.data = [x * float(other) / self.scale for x in self.data]\n\tself.scale = float(other)\n\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif (self.scale is None) or recompute:\n\t\t\tself.scale = sum(self.hist)\n\t\treturn self.scale\n\tif self.scale == 0.:\n\t\traise LenaValueError(\"scale is zero\")\n\tself.hist = self.hist * (other / self.scale)\n\tself.scale = other\n\n\tdef scale_up(self, other=None):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self.integral()\n\t\treturn self._scale\n\telse:\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"can't rescale histogram with zero scale\"\n\t\t\t)\n\t\tself.rescale(other / self._scale)\n\t\tself._scale = other\n\n\tdef integral(self):\n\t\t\"\"\"Compute integral of the histogram.\"\"\"\n\t\treturn self.hist.sum()\n\n\tdef _check_pos_hist(self):\n\t\t\"\"\"Check if histogram is positive.\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self.scale_computed and not recompute:\n\t\t\treturn self.scale\n\t\tscale = self.integral()\n\t\tif recompute:\n\t\t\tself.scale = scale\n\t\treturn scale\n\tif not isinstance(other, float):\n\t\traise TypeError(\"Scale must be a float. Got %s\" % type(other))\n\tif self.scale == 0:\n\t\traise LenaValueError(\"Histogram has zero scale. Can't rescale.\")\n\tself.scale *= other / self.scale\n\n\tdef fill(self, value, weight=1):\n\t\t\"\"\"\n\t\tFill the histogram with *value* and *weight*.\n\t\tIf *value* is a sequence, it is treated as a list\n\t\tof values to fill.\n\t\t\"\"\"\n\t\tif isinstance(value, (list, tuple)):\n\t\t\tfor val in value:\n\t\t\t\tself.fill(val, weight)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tself.data[value] += weight\n\t\n", "def scale(self, other=None, recompute=False):\n\t\tif other is None:\n\t\t\tif recompute or self._scale is None:\n\t\t\t\tself._scale = self.sum()\n\t\t\treturn self._scale\n\t\telse:\n\t\t\tif not self._scale:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"can't rescale histogram with scale equal to zero\"\n\t\t\t\t)\n\t\t\tself *= other / self._scale\n\t\t\tself._scale = other\n\n\tdef _get_bins(self, bins=None):\n\t\tif bins is None:\n\t\t\treturn self._bins\n\t\telse:\n\t\t\treturn self._get_bins_from_bins(bins)\n\n\tdef _get_bins_from_bins(self, bins):\n\t\tif not isinstance(bins, (tuple, list)):\n\t\t\tbins = tuple(bins)\n\n\t\tif len(bins) != self.n_dimensions:\n\t\t\traise LenaValueError(\n\t\t\t\t\"bins must have length equal to\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute:\n\t\t\tself._scale = None\n\t\treturn self._scale or self._compute_scale()\n\telif isinstance(other, float):\n\t\tif self.scale() == 0:\n\t\t\traise LenaValueError('can\\'t rescale empty histogram')\n\t\tself.values = self.values * other / self.scale()\n\t\tself._scale = other\n\t\treturn other\n\telse:\n\t\traise LenaTypeError(\n\t\t\t\"other should be float or None, but it is {}\".format(other)\n\t\t)\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\t\tif other is None:\n\t\t\tif recompute or self._scale is None:\n\t\t\t\tself._scale = self.integral()\n\t\t\treturn self._scale\n\t\telse:\n\t\t\tif self._scale == 0:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"can't rescale a histogram with scale of 0\"\n\t\t\t\t)\n\t\t\treturn self.scale(other=None) / other\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif not self.scale_computed:\n\t\t\tself.scale_computed = True\n\t\t\tself.scale = self.integral()\n\t\treturn self.scale\n\telse:\n\t\tif not self.scale_computed or recompute:\n\t\t\tself.scale = self.integral()\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"scale of histogram {} is zero, can't rescale\".format(self)\n\t\t\t)\n\t\tself.scale_computed = True\n\t\tself.rescale(other / self.scale)\n\t\tself.scale = other\n\t\treturn self\n\n\tdef rescale(self, factor):\n\t\tself.values = factor * self.values\n\t\treturn self\n\n\tdef norm(self, other=None, recompute=False):\n\t\t\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\t# get scale\n\t\tif self.scale_computed and not recompute:\n\t\t\treturn self.scale\n\t\telse:\n\t\t\tscale = self.compute_scale()\n\t\t\tself.scale_computed = True\n\t\t\tself.scale = scale\n\t\t\treturn scale\n\telse:\n\t\t# set scale\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"can't rescale a histogram with zero scale\"\n\t\t\t)\n\t\telse:\n\t\t\tself.a /= self.scale / other\n\t\t\tself.scale = other\n\t\t\tself.scale_computed = True\n\t\t\treturn self\n\n\tdef normalize(self, recompute=False):\n\t\"\"\"\n\tScale the histogram to unity.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif self._scale is None:\n\t\tself._scale = self.integral()\n\t\tself._scale_computed = True\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"can't rescale histogram with zero scale\")\n\tif recompute:\n\t\tself._scale_computed = False\n\tself.multiply(other / self._scale)\n\tself._scale = other\n\tself._scale_computed = True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif (self.scale is None) or recompute:\n\t\t\tself.scale = sum(self.data)\n\t\treturn self.scale\n\tif self.scale == 0:\n\t\traise LenaValueError(\n\t\t\t\"Cannot rescale a histogram with zero scale\"\n\t\t)\n\tself.data = [x * float(other) / self.scale for x in self.data]\n\tself.scale = float(other)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif (self.scale is None) or recompute:\n\t\t\tself.scale = sum(self.hist)\n\t\treturn self.scale\n\tif self.scale == 0.:\n\t\traise LenaValueError(\"scale is zero\")\n\tself.hist = self.hist * (other / self.scale)\n\tself.scale = other\n\n\tdef scale_up(self, other=None):\n\t\"\"\"\n\tCompute or set scale (integral of the histogram).\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self.integral()\n\t\treturn self._scale\n\telse:\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"can't rescale histogram with zero scale\"\n\t\t\t)\n\t\tself.rescale(other / self._scale)\n\t\tself._scale = other\n\n\tdef integral(self):\n\t\t\"\"\"Compute integral of the histogram.\"\"\"\n\t\treturn self.hist.sum()\n\n\tdef _check_pos_hist(self):\n\t\t\"\"\"Check if histogram is positive.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self.scale_computed and not recompute:\n\t\t\treturn self.scale\n\t\tscale = self.integral()\n\t\tif recompute:\n\t\t\tself.scale = scale\n\t\treturn scale\n\tif not isinstance(other, float):\n\t\traise TypeError(\"Scale must be a float. Got %s\" % type(other))\n\tif self.scale == 0:\n\t\traise LenaValueError(\"Histogram has zero scale. Can't rescale.\")\n\tself.scale *= other / self.scale\n\n\tdef fill(self, value, weight=1):\n\t\t\"\"\"\n\t\tFill the histogram with *value* and *weight*.\n\t\tIf *value* is a sequence, it is treated as a list\n\t\tof values to fill.\n\t\t\"\"\"\n\t\tif isinstance(value, (list, tuple)):\n\t\t\tfor val in value:\n\t\t\t\tself.fill(val, weight)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tself.data[value] += weight\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t\tif other is None:\n\t\t\tif recompute or self._scale is None:\n\t\t\t\tself._scale = self.sum()\n\t\t\treturn self._scale\n\t\telse:\n\t\t\tif not self._scale:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"can't rescale histogram with scale equal to zero\"\n\t\t\t\t)\n\t\t\tself *= other / self._scale\n\t\t\tself._scale = other\n\n\tdef _get_bins(self, bins=None):\n\t\tif bins is None:\n\t\t\treturn self._bins\n\t\telse:\n\t\t\treturn self._get_bins_from_bins(bins)\n\n\tdef _get_bins_from_bins(self, bins):\n\t\tif not isinstance(bins, (tuple, list)):\n\t\t\tbins = tuple(bins)\n\n\t\tif len(bins) != self.n_dimensions:\n\t\t\traise LenaValueError(\n\t\t\t\t\"bins must have length equal to\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute:\n\t\t\tself._scale = None\n\t\treturn self._scale or self._compute_scale()\n\telif isinstance(other, float):\n\t\tif self.scale() == 0:\n\t\t\traise LenaValueError('can\\'t rescale empty histogram')\n\t\tself.values = self.values * other / self.scale()\n\t\tself._scale = other\n\t\treturn other\n\telse:\n\t\traise LenaTypeError(\n\t\t\t\"other should be float or None, but it is {}\".format(other)\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "ques_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n\t# get scale\n\tif other is None:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\"scale is unknown\")\n\t\treturn self.scale\n\t# set scale\n\telif len(self.fields) == 0:\n\t\traise LenaValueError(\"graph has no coordinates\")\n\t# set scale\n\tif self.scale is None:\n\t\traise LenaValueError(\"scale is unknown\")\n\tscale_coord = self.fields[-1] # last coordinate\n\t# rescale errors\n\tfor error in self.errors:\n\t\tscale_error = error.fields[-1] # last coordinate\n\t\terror *= other / self.scale\n\t\terror.fields[-1] = scale_error\n\t# rescale graph\n\tself *= other / self.scale\n\tself.fields[-1] = scale_coord\n\tself.scale = other\n\treturn self\n\n\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\ttry:\n\t\tother = float(other)\n\texcept:\n\t\traise LenaTypeError(\n\t\t\t\"expected a number, got {}\".format(type(other))\n\t\t)\n\tif self._scale == 0.:\n\t\traise LenaValueError(\"can't rescale graph with zero scale\")\n\tif self._scale is None:\n\t\traise LenaValueError(\"can't rescale graph with unknown scale\")\n\tnew_fields = {}\n\tfor name, field in self.fields.items():\n\t\tnew_fields[name] = field.rescale(other / self._scale)\n\treturn self.__class__(new_fields)\n\n", "def scale(self, other=None):\n\treturn self._scale(other)\n\n\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif other == 0:\n\t\traise LenaValueError(\"cannot rescale to zero\")\n\t# get all fields\n\tfields = self.fields(False)\n\t# get scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\t# get new scale\n\tif other is None:\n\t\treturn self._scale\n\tif other == 0:\n\t\traise LenaValueError(\"cannot rescale to zero\")\n\t# get all fields\n\tfields = self.fields(False)\n\t# get scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\t# get new scale\n\tnew_scale = other\n\t# rescale all fields\n\tfor field in fields:\n\t\t# get old value\n\t\told_value = self.value(field)\n\t\t# get new value\n\t\tnew_value = new_scale * old_value / self._scale\n\t\t# set new value\n\t\tself.set_\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\telif isinstance(other, numbers.Real):\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"cannot rescale: scale is unknown\")\n\t\tscale_factor = other / self._scale\n\t\tself._scale = other\n\t\tself._rescale_coord(scale_factor)\n\t\tself._rescale_errors(scale_factor)\n\t\tself._rescale_values(scale_factor)\n\t\tself._rescale_edges(scale_factor)\n\telse:\n\t\traise LenaTypeError(\n\t\t\t\"scale must be a number, not {!r}\".format(other)\n\t\t)\n\n\tdef _rescale_coord(self, scale_factor):\n\t\t\"\"\"Rescale the last coordinate.\"\"\"\n\t\tif self._coord:\n\t\t\tself._coord[-1] *= scale_factor\n\n\tdef _rescale_errors(self, scale_factor):\n\t\t\"\"\"Rescale errors.\"\"\"\n\t\tif self._errors:\n\t\t\tself\n", "def scale(self, other=None):\n\t# if other is not None and other != self.scale:\n\tif other is not None:\n\t\t# if self.scale is not None:\n\t\tscale_factor = other / self.scale\n\t\t# else:\n\t\t# \traise LenaValueError(\n\t\t# \t\t\"can't scale to {}: scale is not defined\".format(other)\n\t\t# \t)\n\t\tfor field in self.fields:\n\t\t\tself[field] = scale_factor * self[field]\n\t\treturn self\n\telse:\n\t\treturn self.scale\n\n\t# def __add__(self, other):\n\t# \t\"\"\"Return a new graph with the same fields as this one.\n\t#\n\t# The new graph is filled with the sum of this graph and *other*.\n\t#\n\t# The sum is performed coordinate-wise.\n\t#\n\t# If the scale of *other* is not the same as the scale of this graph,\n\t# it will be rescaled before the sum.\n\t#\n\t# If the scale of *other* is not defined,\n\t#\n", "def scale(self, other=None):\n\t# pylint: disable=no-self-use\n\tif other is None:\n\t\treturn 1\n\telse:\n\t\treturn other\n\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\n\t\t\t\"can't rescale graph with unknown scale\"\n\t\t)\n\tscale = other / self._scale\n\tfor field in self.fields:\n\t\tif len(field) > 1:\n\t\t\tself[field] *= scale\n\t\t\tself.errors[field] *= scale\n\tself._scale = other\n\n\n", "def scale(self, other=None):\n\t# Get scale\n\tif other is None:\n\t\tif self._scale is None:\n\t\t\traise LenaValueError(\"unknown scale\")\n\t\treturn self._scale\n\t# Set scale\n\t# If the graph has unknown scale, a ValueError is raised\n\tif self._scale is None:\n\t\traise LenaValueError(\"scale is unknown\")\n\t# If the graph has zero scale, a ValueError is raised\n\tif self._scale == 0:\n\t\traise LenaValueError(\"scale is zero\")\n\t# Get the list of coordinates\n\tcoordinates = self.coordinates()\n\t# Get the last coordinate\n\tcoord = coordinates[-1]\n\t# Get the list of errors\n\terrors = self.errors()\n\t# Get the last error\n\terror = errors[-1]\n\t# Get the factor\n\tfactor = other / self._scale\n\t# Rescale the coordinate\n\tself._coordinates[coord] = self._coordinates[coord] * factor\n\t# Rescale the error\n\tself._errors[error] = self._errors[error] * factor\n\t# Set the new scale\n\t\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale\n\tif self.scale == 0:\n\t\traise LenaValueError(\"zero scale\")\n\tif other == 0:\n\t\traise LenaValueError(\"zero scale\")\n\tif self.scale is None:\n\t\traise LenaValueError(\"unknown scale\")\n\tself.scale *= other\n\tfor field in self.fields:\n\t\tfield *= other\n\t\tif self.errors is not None:\n\t\t\tfield[1] *= other\n\treturn self.scale\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n\t# get scale\n\tif other is None:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\"scale is unknown\")\n\t\treturn self.scale\n\t# set scale\n\telif len(self.fields) == 0:\n\t\traise LenaValueError(\"graph has no coordinates\")\n\t# set scale\n\tif self.scale is None:\n\t\traise LenaValueError(\"scale is unknown\")\n\tscale_coord = self.fields[-1] # last coordinate\n\t# rescale errors\n\tfor error in self.errors:\n\t\tscale_error = error.fields[-1] # last coordinate\n\t\terror *= other / self.scale\n\t\terror.fields[-1] = scale_error\n\t# rescale graph\n\tself *= other / self.scale\n\tself.fields[-1] = scale_coord\n\tself.scale = other\n\treturn self\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\ttry:\n\t\tother = float(other)\n\texcept:\n\t\traise LenaTypeError(\n\t\t\t\"expected a number, got {}\".format(type(other))\n\t\t)\n\tif self._scale == 0.:\n\t\traise LenaValueError(\"can't rescale graph with zero scale\")\n\tif self._scale is None:\n\t\traise LenaValueError(\"can't rescale graph with unknown scale\")\n\tnew_fields = {}\n\tfor name, field in self.fields.items():\n\t\tnew_fields[name] = field.rescale(other / self._scale)\n\treturn self.__class__(new_fields)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\treturn self._scale(other)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif other == 0:\n\t\traise LenaValueError(\"cannot rescale to zero\")\n\t# get all fields\n\tfields = self.fields(False)\n\t# get scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\t# get new scale\n\tif other is None:\n\t\treturn self._scale\n\tif other == 0:\n\t\traise LenaValueError(\"cannot rescale to zero\")\n\t# get all fields\n\tfields = self.fields(False)\n\t# get scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\t# get new scale\n\tnew_scale = other\n\t# rescale all fields\n\tfor field in fields:\n\t\t# get old value\n\t\told_value = self.value(field)\n\t\t# get new value\n\t\tnew_value = new_scale * old_value / self._scale\n\t\t# set new value\n\t\tself.set_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\telif isinstance(other, numbers.Real):\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"cannot rescale: scale is unknown\")\n\t\tscale_factor = other / self._scale\n\t\tself._scale = other\n\t\tself._rescale_coord(scale_factor)\n\t\tself._rescale_errors(scale_factor)\n\t\tself._rescale_values(scale_factor)\n\t\tself._rescale_edges(scale_factor)\n\telse:\n\t\traise LenaTypeError(\n\t\t\t\"scale must be a number, not {!r}\".format(other)\n\t\t)\n\n\tdef _rescale_coord(self, scale_factor):\n\t\t\"\"\"Rescale the last coordinate.\"\"\"\n\t\tif self._coord:\n\t\t\tself._coord[-1] *= scale_factor\n\n\tdef _rescale_errors(self, scale_factor):\n\t\t\"\"\"Rescale errors.\"\"\"\n\t\tif self._errors:\n\t\t\tself\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t# if other is not None and other != self.scale:\n\tif other is not None:\n\t\t# if self.scale is not None:\n\t\tscale_factor = other / self.scale\n\t\t# else:\n\t\t# \traise LenaValueError(\n\t\t# \t\t\"can't scale to {}: scale is not defined\".format(other)\n\t\t# \t)\n\t\tfor field in self.fields:\n\t\t\tself[field] = scale_factor * self[field]\n\t\treturn self\n\telse:\n\t\treturn self.scale\n\n\t# def __add__(self, other):\n\t# \t\"\"\"Return a new graph with the same fields as this one.\n\t#\n\t# The new graph is filled with the sum of this graph and *other*.\n\t#\n\t# The sum is performed coordinate-wise.\n\t#\n\t# If the scale of *other* is not the same as the scale of this graph,\n\t# it will be rescaled before the sum.\n\t#\n\t# If the scale of *other* is not defined,\n\t#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t# pylint: disable=no-self-use\n\tif other is None:\n\t\treturn 1\n\telse:\n\t\treturn other\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\traise LenaValueError(\n\t\t\t\"can't rescale graph with unknown scale\"\n\t\t)\n\tscale = other / self._scale\n\tfor field in self.fields:\n\t\tif len(field) > 1:\n\t\t\tself[field] *= scale\n\t\t\tself.errors[field] *= scale\n\tself._scale = other\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t# Get scale\n\tif other is None:\n\t\tif self._scale is None:\n\t\t\traise LenaValueError(\"unknown scale\")\n\t\treturn self._scale\n\t# Set scale\n\t# If the graph has unknown scale, a ValueError is raised\n\tif self._scale is None:\n\t\traise LenaValueError(\"scale is unknown\")\n\t# If the graph has zero scale, a ValueError is raised\n\tif self._scale == 0:\n\t\traise LenaValueError(\"scale is zero\")\n\t# Get the list of coordinates\n\tcoordinates = self.coordinates()\n\t# Get the last coordinate\n\tcoord = coordinates[-1]\n\t# Get the list of errors\n\terrors = self.errors()\n\t# Get the last error\n\terror = errors[-1]\n\t# Get the factor\n\tfactor = other / self._scale\n\t# Rescale the coordinate\n\tself._coordinates[coord] = self._coordinates[coord] * factor\n\t# Rescale the error\n\tself._errors[error] = self._errors[error] * factor\n\t# Set the new scale\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale\n\tif self.scale == 0:\n\t\traise LenaValueError(\"zero scale\")\n\tif other == 0:\n\t\traise LenaValueError(\"zero scale\")\n\tif self.scale is None:\n\t\traise LenaValueError(\"unknown scale\")\n\tself.scale *= other\n\tfor field in self.fields:\n\t\tfield *= other\n\t\tif self.errors is not None:\n\t\t\tfield[1] *= other\n\treturn self.scale\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "ques_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.lower\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.upper\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.lower + bin_.upper) / 2\n\telse:\n\t\traise ValueError(\"unknown get_coordinate\")\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph = graph_module.graph(field_names, scale)\n\tfor bin_ in hist.bins():\n\t\tgraph.add(get_coordinate(bin_), make_value(bin_))\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True:\n\t\tscale = hist.scale\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = hist.get_left_coordinate\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = hist.get_right_coordinate\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = hist.get_middle_coordinate\n\telse:\n\t\traise ValueError(\"get_coordinate\")\n\tgraph = Graph(field_names=field_names, scale=scale)\n\tfor bin_ in hist.iter_bins():\n\t\tgraph.append(get_coordinate(bin_), *make_value(bin_))\n\treturn graph\n\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is None:\n\t\tscale = Unknown()\n\telif scale is True:\n\t\tscale = hist.scale\n\tresult = graph(make_value(bin_)\n\t\tfor bin_ in hist.bins.values()\n\t\tif bin_)\n\tresult.scale = scale\n\tresult.field_names = field_names\n\tresult.get_coordinate = get_coordinate\n\treturn result\n\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tbins = tuple(hist)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\t\tfield_names = field_names[:1]\n\t\tdim = 1\n\telse:\n\t\tdim = len(make_value(bins[0]))\n\t\tif not isinstance(field_names, tuple):\n\t\t\tfield_names = tuple(field_names)\n\t\tif len(field_names) != dim:\n\t\t\traise ValueError(\n\t\t\t\t\"field_names do not match make_value\",\n\t\t\t\tfield_names, make_value)\n\tif get_coordinate == \"left\":\n\t\tcoordinates = [b.left for b in bins]\n\telif get_coordinate == \"right\":\n\t\tcoordinates = [b.right for b in bins]\n\telif get_coordinate == \"middle\":\n\t\tcoordinates = [(b.left + b.right) / 2 for b in bins]\n\telse:\n\t\traise ValueError(\n\t\t\t\"get_coordinate must\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tresult = graph(field_names=field_names)\n\tif scale:\n\t\tresult.scale = hist.scale\n\tresult.plot = hist.plot.__class__(\n\t\thist.plot.name,\n\t\thist.plot.title,\n\t\thist.plot.xlabel,\n\t\thist.plot.ylabel,\n\t\thist.plot.zlabel,\n\t\thist.plot.options,\n\t\thist.plot.subplot,\n\t\thist.plot.subplot_name,\n\t\thist.plot.subplot_title,\n\t\thist.plot.subplot_options,\n\t)\n\tfor bin_ in hist.bins:\n\t\tif make_value is None:\n\t\t\tvalue = bin_.value\n\t\telse:\n\t\t\tvalue = make_value(bin_.value)\n\t\tif isinstance(value, tuple):\n\t\t\tvalue = list(value)\n\t\telse:\n\t\t\tvalue = [value]\n\t\tif get_coordinate == \"left\":\n\t\t\tif isinstance(bin_.x, tuple\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is None:\n\t\tscale = hist.scale\n\telif scale is True:\n\t\tscale = hist.scale\n\telse:\n\t\tscale = scale\n\tbins = hist.bins\n\tassert get_coordinate in (\"left\", \"right\", \"middle\"), get_coordinate\n\tif get_coordinate == \"middle\":\n\t\tbin_coordinate = lambda bin_: bin_.middle\n\telif get_coordinate == \"left\":\n\t\tbin_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tbin_coordinate = lambda bin_: bin_.right\n\telse:\n\t\traise NotImplementedError(get_coordinate)\n\tassert len(field_names) == bins.dimension + 1\n\tgraph = Graph(bins.dimension, field_names, scale)\n\tfor bin_ in bins:\n\t\tgraph.append(bin_coordinate(bin_), make_value(bin_))\n\treturn graph\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True: scale = hist.scale\n\tgraphs = [graph.graph(field_names=field_names, scale=scale) for hist in hist.split()]\n\tfor graph, hist in zip(graphs, hist.split()):\n\t\tfor bin_ in hist.bins:\n\t\t\tgraph.append(bin_.coordinate(get_coordinate), *make_value(bin_))\n\treturn graph.merge(graphs)\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tbins = hist.bins\n\tif isinstance(hist.bins, numpy.ndarray):\n\t\tbins = hist.bins.ravel()\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is True:\n\t\tscale = hist.scale\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Unknown coordinate: {!r}\".format(get_coordinate))\n\tif get_coordinate == \"left\":\n\t\tcoordinate = hist.bin_left_edges\n\telif get_coordinate == \"right\":\n\t\tcoordinate = hist.bin_right_edges\n\telif get_coordinate == \"middle\":\n\t\tcoordinate = (\n\t\t\thist.bin_left_edges + hist.bin_right_edges) / 2\n\treturn graph(\n\t\tnumpy.array(list(map(make_value, bins))),\n\t\tcoordinate,\n\t\tscale=scale,\n\t\tfield_names=field_names\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tcoordinate = hist.bin_left\n\telif get_coordinate == \"right\":\n\t\tcoordinate = hist.bin_right\n\telif get_coordinate == \"middle\":\n\t\tcoordinate = hist.bin_middle\n\telse:\n\t\traise ValueError(\"unknown get_coordinate: \" + repr(get_coordinate))\n\tif scale is True:\n\t\tscale = hist.scale\n\tresult = graph(scale=scale)\n\tresult.field_names = field_names\n\tfor bin_ in hist.bins:\n\t\tresult.append(coordinate(bin_), make_value(bin_))\n\treturn result\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tassert len(field_names) == hist.dimension + (1 if make_value else 0)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is True:\n\t\tscale = hist.scale\n\telif scale is None:\n\t\tscale = \"unknown\"\n\treturn graph(\n\t\tmap(\n\t\t\tlambda bin_:\n\t\t\t\ttuple(\n\t\t\t\t\tchain((bin_.coordinate[get_coordinate]),\n\t\t\t\t\t      make_value(bin_))),\n\t\t\thist.bins\n\t\t),\n\t\tfield_names=field_names,\n\t\tscale=scale\n\t)\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.lower\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.upper\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.lower + bin_.upper) / 2\n\telse:\n\t\traise ValueError(\"unknown get_coordinate\")\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph = graph_module.graph(field_names, scale)\n\tfor bin_ in hist.bins():\n\t\tgraph.add(get_coordinate(bin_), make_value(bin_))\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True:\n\t\tscale = hist.scale\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = hist.get_left_coordinate\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = hist.get_right_coordinate\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = hist.get_middle_coordinate\n\telse:\n\t\traise ValueError(\"get_coordinate\")\n\tgraph = Graph(field_names=field_names, scale=scale)\n\tfor bin_ in hist.iter_bins():\n\t\tgraph.append(get_coordinate(bin_), *make_value(bin_))\n\treturn graph\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is None:\n\t\tscale = Unknown()\n\telif scale is True:\n\t\tscale = hist.scale\n\tresult = graph(make_value(bin_)\n\t\tfor bin_ in hist.bins.values()\n\t\tif bin_)\n\tresult.scale = scale\n\tresult.field_names = field_names\n\tresult.get_coordinate = get_coordinate\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tbins = tuple(hist)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\t\tfield_names = field_names[:1]\n\t\tdim = 1\n\telse:\n\t\tdim = len(make_value(bins[0]))\n\t\tif not isinstance(field_names, tuple):\n\t\t\tfield_names = tuple(field_names)\n\t\tif len(field_names) != dim:\n\t\t\traise ValueError(\n\t\t\t\t\"field_names do not match make_value\",\n\t\t\t\tfield_names, make_value)\n\tif get_coordinate == \"left\":\n\t\tcoordinates = [b.left for b in bins]\n\telif get_coordinate == \"right\":\n\t\tcoordinates = [b.right for b in bins]\n\telif get_coordinate == \"middle\":\n\t\tcoordinates = [(b.left + b.right) / 2 for b in bins]\n\telse:\n\t\traise ValueError(\n\t\t\t\"get_coordinate must\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tresult = graph(field_names=field_names)\n\tif scale:\n\t\tresult.scale = hist.scale\n\tresult.plot = hist.plot.__class__(\n\t\thist.plot.name,\n\t\thist.plot.title,\n\t\thist.plot.xlabel,\n\t\thist.plot.ylabel,\n\t\thist.plot.zlabel,\n\t\thist.plot.options,\n\t\thist.plot.subplot,\n\t\thist.plot.subplot_name,\n\t\thist.plot.subplot_title,\n\t\thist.plot.subplot_options,\n\t)\n\tfor bin_ in hist.bins:\n\t\tif make_value is None:\n\t\t\tvalue = bin_.value\n\t\telse:\n\t\t\tvalue = make_value(bin_.value)\n\t\tif isinstance(value, tuple):\n\t\t\tvalue = list(value)\n\t\telse:\n\t\t\tvalue = [value]\n\t\tif get_coordinate == \"left\":\n\t\t\tif isinstance(bin_.x, tuple\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is None:\n\t\tscale = hist.scale\n\telif scale is True:\n\t\tscale = hist.scale\n\telse:\n\t\tscale = scale\n\tbins = hist.bins\n\tassert get_coordinate in (\"left\", \"right\", \"middle\"), get_coordinate\n\tif get_coordinate == \"middle\":\n\t\tbin_coordinate = lambda bin_: bin_.middle\n\telif get_coordinate == \"left\":\n\t\tbin_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tbin_coordinate = lambda bin_: bin_.right\n\telse:\n\t\traise NotImplementedError(get_coordinate)\n\tassert len(field_names) == bins.dimension + 1\n\tgraph = Graph(bins.dimension, field_names, scale)\n\tfor bin_ in bins:\n\t\tgraph.append(bin_coordinate(bin_), make_value(bin_))\n\treturn graph\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True: scale = hist.scale\n\tgraphs = [graph.graph(field_names=field_names, scale=scale) for hist in hist.split()]\n\tfor graph, hist in zip(graphs, hist.split()):\n\t\tfor bin_ in hist.bins:\n\t\t\tgraph.append(bin_.coordinate(get_coordinate), *make_value(bin_))\n\treturn graph.merge(graphs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tbins = hist.bins\n\tif isinstance(hist.bins, numpy.ndarray):\n\t\tbins = hist.bins.ravel()\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is True:\n\t\tscale = hist.scale\n\tif get_coordinate not in (\"left\", \"right\", \"middle\"):\n\t\traise ValueError(\"Unknown coordinate: {!r}\".format(get_coordinate))\n\tif get_coordinate == \"left\":\n\t\tcoordinate = hist.bin_left_edges\n\telif get_coordinate == \"right\":\n\t\tcoordinate = hist.bin_right_edges\n\telif get_coordinate == \"middle\":\n\t\tcoordinate = (\n\t\t\thist.bin_left_edges + hist.bin_right_edges) / 2\n\treturn graph(\n\t\tnumpy.array(list(map(make_value, bins))),\n\t\tcoordinate,\n\t\tscale=scale,\n\t\tfield_names=field_names\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tcoordinate = hist.bin_left\n\telif get_coordinate == \"right\":\n\t\tcoordinate = hist.bin_right\n\telif get_coordinate == \"middle\":\n\t\tcoordinate = hist.bin_middle\n\telse:\n\t\traise ValueError(\"unknown get_coordinate: \" + repr(get_coordinate))\n\tif scale is True:\n\t\tscale = hist.scale\n\tresult = graph(scale=scale)\n\tresult.field_names = field_names\n\tfor bin_ in hist.bins:\n\t\tresult.append(coordinate(bin_), make_value(bin_))\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tassert len(field_names) == hist.dimension + (1 if make_value else 0)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is True:\n\t\tscale = hist.scale\n\telif scale is None:\n\t\tscale = \"unknown\"\n\treturn graph(\n\t\tmap(\n\t\t\tlambda bin_:\n\t\t\t\ttuple(\n\t\t\t\t\tchain((bin_.coordinate[get_coordinate]),\n\t\t\t\t\t      make_value(bin_))),\n\t\t\thist.bins\n\t\t),\n\t\tfield_names=field_names,\n\t\tscale=scale\n\t)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide %s\" % (candidate, iface.__identifier__),\n\t\t\t\tcandidate)\n\n\tfor name, descriptor in iface.namesAndDescriptions(vtype):\n\t\t_verifyObject(candidate, name, descriptor, iface, vtype)\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not isinstance(iface, InterfaceClass):\n\t\traise TypeError('iface must be an InterfaceClass instance')\n\tif not isinstance(candidate, (InterfaceClass, type)):\n\t\traise TypeError('candidate must be an InterfaceClass instance'\n\t\t\t\t\t\t' or class')\n\tif vtype is None:\n\t\tif isinstance(candidate, InterfaceClass):\n\t\t\tvtype = 'interface'\n\t\telse:\n\t\t\tvtype = 'class'\n\t# Check the interface\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t'%s %r does not claim to provide %s %r'\n\t\t\t% (vtype.capitalize(), candidate, vtype, iface))\n\t# Check the methods\n\tdeclared = set(iface.names(all=True))\n\tfailed = set()\n\tfor name in declared:\n\t\td = getattr(iface, name, None)\n\t\tif not d:\n\t\t\tcontinue\n\t\tif isinstance(d, Method\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(candidate, iface)\n\n\tif vtype is None:\n\t\tvtype = Verifying\n\n\tif isclass(iface):\n\t\tiface = iface.__identifier__\n\n\tiface = getInterface(iface)\n\n\terrors = []\n\n\tfor name, desc in iface.namesAndDescriptions(1):\n\t\tif desc.getName() == '__conform__':\n\t\t\tcontinue\n\t\tc = getattr(candidate, name, _marker)\n\t\tif c is _marker:\n\t\t\terrors.append(AttributeError(name))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tdesc.interface.validateInvariants(c, vtype, tentative)\n\t\t\texcept Exception as e:\n\t\t\t\terrors.append(e)\n\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise MultipleInvalid(*errors\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif isinstance(iface, str):\n\t\tiface = providedBy(iface)\n\tif not isinstance(iface, InterfaceClass):\n\t\traise TypeError(\"The first argument must be an interface.\")\n\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t)\n\n\terrors = []\n\tinvalid_methods = []\n\tinvalid_attributes = []\n\n\t# check interface\n\tfor name in iface.names(all=True):\n\t\tif name.startswith('_'):\n\t\t\tcontinue\n\t\tif name in ('__new__', '__init__', '__call__'):\n\t\t\tcontinue\n\n\t\tdescriptor = getattr(iface, name, None)\n\t\tif descriptor is None:\n\t\t\tcontinue\n\n\t\tcandidate_descriptor = getattr(candidate, name, None)\n\t\tif IDocDescriptor.providedBy(descriptor):\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\treturn False\n\tif not iface.isOrExtends(candidate):\n\t\treturn False\n\tif vtype is None:\n\t\tvtype = VerifyingAdapter\n\tv = vtype(iface, candidate)\n\ttry:\n\t\tv.verify()\n\texcept Invalid as e:\n\t\terrors = e.args[0]\n\t\tif len(errors) == 1:\n\t\t\t# Raise the single error, like before\n\t\t\traise e\n\t\telse:\n\t\t\t# Raise a new Invalid with all errors\n\t\t\traise Invalid(errors)\n\treturn True\n\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide %s\" % (candidate, iface.__identifier__))\n\n\tiface_errors = []\n\tfor name, descriptor in iface.namesAndDescriptions(vtype):\n\t\ttry:\n\t\t\tdescriptor.validateInvariants(candidate)\n\t\texcept Invalid as e:\n\t\t\tiface_errors.append(e)\n\n\tif iface_errors:\n\t\tif len(iface_errors) == 1:\n\t\t\traise iface_errors[0]\n\t\traise Invalid(\n\t\t\t\"%s has multiple errors:\\n%s\" % (\n\t\t\t\tiface.__identifier__,\n\t\t\t\t'\\n'.join(str(e) for e in iface_errors)))\n\n\treturn True\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid('%r does not provide %s' % (candidate, iface.getName()))\n\n\terrors = []\n\tfor name, desc in iface.namesAndDescriptions(vtype):\n\t\tif vtype == 'attributes':\n\t\t\tvalue = getattr(candidate, name, _MARKER)\n\t\t\tif value is _MARKER:\n\t\t\t\terrors.append(Invalid('%r does not define %s' % (candidate, name)))\n\t\t\telif not desc.validate(value):\n\t\t\t\terrors.append(Invalid('%r has an invalid value for %s: %r' % (candidate, name, value)))\n\t\telse:\n\t\t\tvalue = getattr(candidate, name, _MARKER)\n\t\t\tif not callable(value):\n\t\t\t\terrors.append(Invalid('%r does not define %s' % (candidate, name)))\n\t\t\telif not desc.validate\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"{0!r} does not provide {1}\".format(candidate, iface))\n\tif vtype is None:\n\t\tvtype = _verify_types[iface.names()[0]]\n\treturn vtype(iface, candidate)\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"{0!r} does not provide {1!r}\".format(candidate, iface),\n\t\t\tcandidate, iface)\n\treturn _verifyClass(iface, candidate.__class__, candidate, vtype)\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif vtype is None:\n\t\tvtype = _verifyType\n\tverifyObject(iface, candidate)\n\tfor name, desc in iface.namesAndDescriptions(1):\n\t\tif desc.type is not None:\n\t\t\ttry:\n\t\t\t\tvtype(desc, candidate)\n\t\t\texcept Invalid as ex:\n\t\t\t\t# Collect Invalid exceptions and raise them together,\n\t\t\t\t# unless there is only one, in which case raise it alone.\n\t\t\t\tif not hasattr(candidate, '_v_errors'):\n\t\t\t\t\tcandidate._v_errors = []\n\t\t\t\tcandidate._v_errors.append(ex)\n\t\t\t\tif len(candidate._v_errors) == 1:\n\t\t\t\t\traise\n\tif getattr(candidate, '_v_errors', None) is not None:\n\t\t# There were multiple errors; raise them together\n\t\traise Invalid(candidate, iface, candidate._v_errors)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide %s\" % (candidate, iface.__identifier__),\n\t\t\t\tcandidate)\n\n\tfor name, descriptor in iface.namesAndDescriptions(vtype):\n\t\t_verifyObject(candidate, name, descriptor, iface, vtype)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not isinstance(iface, InterfaceClass):\n\t\traise TypeError('iface must be an InterfaceClass instance')\n\tif not isinstance(candidate, (InterfaceClass, type)):\n\t\traise TypeError('candidate must be an InterfaceClass instance'\n\t\t\t\t\t\t' or class')\n\tif vtype is None:\n\t\tif isinstance(candidate, InterfaceClass):\n\t\t\tvtype = 'interface'\n\t\telse:\n\t\t\tvtype = 'class'\n\t# Check the interface\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t'%s %r does not claim to provide %s %r'\n\t\t\t% (vtype.capitalize(), candidate, vtype, iface))\n\t# Check the methods\n\tdeclared = set(iface.names(all=True))\n\tfailed = set()\n\tfor name in declared:\n\t\td = getattr(iface, name, None)\n\t\tif not d:\n\t\t\tcontinue\n\t\tif isinstance(d, Method\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(candidate, iface)\n\n\tif vtype is None:\n\t\tvtype = Verifying\n\n\tif isclass(iface):\n\t\tiface = iface.__identifier__\n\n\tiface = getInterface(iface)\n\n\terrors = []\n\n\tfor name, desc in iface.namesAndDescriptions(1):\n\t\tif desc.getName() == '__conform__':\n\t\t\tcontinue\n\t\tc = getattr(candidate, name, _marker)\n\t\tif c is _marker:\n\t\t\terrors.append(AttributeError(name))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tdesc.interface.validateInvariants(c, vtype, tentative)\n\t\t\texcept Exception as e:\n\t\t\t\terrors.append(e)\n\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise MultipleInvalid(*errors\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif isinstance(iface, str):\n\t\tiface = providedBy(iface)\n\tif not isinstance(iface, InterfaceClass):\n\t\traise TypeError(\"The first argument must be an interface.\")\n\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t)\n\n\terrors = []\n\tinvalid_methods = []\n\tinvalid_attributes = []\n\n\t# check interface\n\tfor name in iface.names(all=True):\n\t\tif name.startswith('_'):\n\t\t\tcontinue\n\t\tif name in ('__new__', '__init__', '__call__'):\n\t\t\tcontinue\n\n\t\tdescriptor = getattr(iface, name, None)\n\t\tif descriptor is None:\n\t\t\tcontinue\n\n\t\tcandidate_descriptor = getattr(candidate, name, None)\n\t\tif IDocDescriptor.providedBy(descriptor):\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\treturn False\n\tif not iface.isOrExtends(candidate):\n\t\treturn False\n\tif vtype is None:\n\t\tvtype = VerifyingAdapter\n\tv = vtype(iface, candidate)\n\ttry:\n\t\tv.verify()\n\texcept Invalid as e:\n\t\terrors = e.args[0]\n\t\tif len(errors) == 1:\n\t\t\t# Raise the single error, like before\n\t\t\traise e\n\t\telse:\n\t\t\t# Raise a new Invalid with all errors\n\t\t\traise Invalid(errors)\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide %s\" % (candidate, iface.__identifier__))\n\n\tiface_errors = []\n\tfor name, descriptor in iface.namesAndDescriptions(vtype):\n\t\ttry:\n\t\t\tdescriptor.validateInvariants(candidate)\n\t\texcept Invalid as e:\n\t\t\tiface_errors.append(e)\n\n\tif iface_errors:\n\t\tif len(iface_errors) == 1:\n\t\t\traise iface_errors[0]\n\t\traise Invalid(\n\t\t\t\"%s has multiple errors:\\n%s\" % (\n\t\t\t\tiface.__identifier__,\n\t\t\t\t'\\n'.join(str(e) for e in iface_errors)))\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid('%r does not provide %s' % (candidate, iface.getName()))\n\n\terrors = []\n\tfor name, desc in iface.namesAndDescriptions(vtype):\n\t\tif vtype == 'attributes':\n\t\t\tvalue = getattr(candidate, name, _MARKER)\n\t\t\tif value is _MARKER:\n\t\t\t\terrors.append(Invalid('%r does not define %s' % (candidate, name)))\n\t\t\telif not desc.validate(value):\n\t\t\t\terrors.append(Invalid('%r has an invalid value for %s: %r' % (candidate, name, value)))\n\t\telse:\n\t\t\tvalue = getattr(candidate, name, _MARKER)\n\t\t\tif not callable(value):\n\t\t\t\terrors.append(Invalid('%r does not define %s' % (candidate, name)))\n\t\t\telif not desc.validate\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\"{0!r} does not provide {1}\".format(candidate, iface))\n\tif vtype is None:\n\t\tvtype = _verify_types[iface.names()[0]]\n\treturn vtype(iface, candidate)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"{0!r} does not provide {1!r}\".format(candidate, iface),\n\t\t\tcandidate, iface)\n\treturn _verifyClass(iface, candidate.__class__, candidate, vtype)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif vtype is None:\n\t\tvtype = _verifyType\n\tverifyObject(iface, candidate)\n\tfor name, desc in iface.namesAndDescriptions(1):\n\t\tif desc.type is not None:\n\t\t\ttry:\n\t\t\t\tvtype(desc, candidate)\n\t\t\texcept Invalid as ex:\n\t\t\t\t# Collect Invalid exceptions and raise them together,\n\t\t\t\t# unless there is only one, in which case raise it alone.\n\t\t\t\tif not hasattr(candidate, '_v_errors'):\n\t\t\t\t\tcandidate._v_errors = []\n\t\t\t\tcandidate._v_errors.append(ex)\n\t\t\t\tif len(candidate._v_errors) == 1:\n\t\t\t\t\traise\n\tif getattr(candidate, '_v_errors', None) is not None:\n\t\t# There were multiple errors; raise them together\n\t\traise Invalid(candidate, iface, candidate._v_errors)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not claim to provide %r\" % (candidate, iface)\n\t\t)\n\n\terrors = _verifyObject(iface, candidate, tentative=tentative)\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise Invalid(errors)\n\treturn True\n\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\tf\"{candidate} does not implement {iface.__identifier__!r}\",\n\t\t\tcandidate,\n\t\t\tiface,\n\t\t\t)\n\n\terrors = []\n\n\tfor method_name in iface.names(all=True):\n\t\tmethod = getattr(iface, method_name, None)\n\t\tif not isinstance(method, Method):\n\t\t\tcontinue\n\t\timpl_method = getattr(candidate, method_name, None)\n\t\tif impl_method is None:\n\t\t\terrors.append(\n\t\t\t\tInvalid(f\"{candidate} does not define {method_name!r}\",\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tmethod_name,\n\t\t\t\t\t\t))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tcheckObject(method, impl_method, tentative=tentative)\n\t\t\texcept Invalid\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\t# This is not an error if we are just trying to see if an\n\t\t# object might provide an interface\n\t\traise Invalid(\"%r does not claim to provide %s.\" % (\n\t\t\tcandidate,\n\t\t\tiface.__name__,\n\t\t))\n\n\tif not tentative and not _check_method_signatures(iface, candidate):\n\t\traise Invalid(\"Signature mismatch for %r\" % candidate)\n\n\tif not _check_attributes(iface, candidate):\n\t\traise Invalid(\"Missing attributes in %r\" % candidate)\n\n\treturn True\n\n", "def verifyObject(iface, candidate, tentative=False):\n\treturn verify.verifyObject(iface, candidate, tentative)\n\n\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%s\\n\\t%s does not claim to provide %s\" %\n\t\t\t(_getProxiedObject(candidate).__class__, candidate, iface)\n\t\t)\n\n\terrors = []\n\n\tfor name, descriptor in iface.namesAndDescriptions(1):\n\t\tif descriptor.get('readonly', False) and descriptor.get('writeable', False):\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tvalue = getattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\terrors.append(name)\n\t\t\tcontinue\n\n\t\tif isinstance(descriptor, Method):\n\t\t\tif not callable(value):\n\t\t\t\terrors.append(name)\n\t\t\t\tcontinue\n\n\t\t\tif descriptor.get('optional', False):\n\t\t\t\tcontinue\n\n\t\t\ttry:\n\t\t\t\tif not isinstance(descriptor, BoundMethod):\n\t\t\t\t\t# We can't do\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not claim to provide %s\" % (candidate, iface.__identifier__)\n\t\t)\n\n\terrors = []\n\tfor name, spec in iface.namesAndDescriptions():\n\t\tif isinstance(spec, Method):\n\t\t\tactual = getattr(candidate, name, None)\n\t\t\tif actual is None:\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t'%s.%s is missing' % (candidate, name),\n\t\t\t\t\t\tspec,\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\ttry:\n\t\t\t\tvalidateInvariants(\n\t\t\t\t\tiface, candidate, name, actual, spec.positional, spec.required\n\t\t\t\t)\n\t\t\texcept:  # noqa: E722\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\n", "def verifyObject(iface, candidate, tentative=False):\n\terrors = []\n\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(Invalid(\n\t\t\t\t\"%r does not provide %r\" % (candidate, iface)))\n\n\tfor name, descriptor in iface.namesAndDescriptions(all=True):\n\t\tif isinstance(descriptor, Method):\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\"%r does not define method %r\" % (candidate, name)))\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tcheckObject(candidate, descriptor, name)\n\t\t\t\texcept Invalid as error:\n\t\t\t\t\terrors.append(error)\n\n\t\telif isinstance(descriptor, Attribute):\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\"%r does not define attribute %r\" % (candidate, name)))\n\n\tif errors:\n\n", "def verifyObject(iface, candidate, tentative=False):\n\treturn verifyObject(iface, candidate, tentative=tentative)\n\n", "def verifyObject(iface, candidate, tentative=False):\n\terrors = []\n\t# First, make sure the candidate claims to provide the interface.\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(Invalid(\"%r does not claim to provide %s\" % (candidate, iface.__name__)))\n\t# Now, make sure the candidate defines the necessary methods.\n\tfor name, spec in iface.namesAndDescriptions():\n\t\t# we only care about methods\n\t\tif spec.__class__ is Method:\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(Invalid(\"%r does not define %s required by %s\" % (candidate, name, iface.__name__)))\n\t\t\telse:\n\t\t\t\t# Make sure the method has the correct signature\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\t\ttry:\n\t\t\t\t\t# Try to check the signature. It's not always possible\n\t\t\t\t\t# to do this.\n\t\t\t\t\tspec.bind(method)\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t_inv = _InvalidInterface(\n\t\tmsg=_(\"The object %r does not correctly implement %r.\\n\") % (candidate, iface)\n\t)\n\n\tif not tentative and not iface.providedBy(candidate):\n\t\t_inv.append(_InvalidInterface(\n\t\t\tmsg=_(\"%r does not implement %r.\") % (candidate, iface)\n\t\t))\n\n\t# Check for methods\n\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\tif not IAttribute.providedBy(method):\n\t\t\tcontinue\n\t\tcandidate_method = getattr(candidate, name, _marker)\n\t\tif candidate_method is _marker:\n\t\t\t_inv.append(_InvalidInterface(\n\t\t\t\tmsg=_(\"%r does not define %r.\") % (candidate, name),\n\t\t\t\tname=name\n\t\t\t))\n\t\t\tcontinue\n\n\t\tif not isinstance(candidate_method, collections_abc.Callable):\n\t\t\t_inv.append(_InvalidInterface(\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not claim to provide %r\" % (candidate, iface)\n\t\t)\n\n\terrors = _verifyObject(iface, candidate, tentative=tentative)\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise Invalid(errors)\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\tf\"{candidate} does not implement {iface.__identifier__!r}\",\n\t\t\tcandidate,\n\t\t\tiface,\n\t\t\t)\n\n\terrors = []\n\n\tfor method_name in iface.names(all=True):\n\t\tmethod = getattr(iface, method_name, None)\n\t\tif not isinstance(method, Method):\n\t\t\tcontinue\n\t\timpl_method = getattr(candidate, method_name, None)\n\t\tif impl_method is None:\n\t\t\terrors.append(\n\t\t\t\tInvalid(f\"{candidate} does not define {method_name!r}\",\n\t\t\t\t\t\tcandidate,\n\t\t\t\t\t\tiface,\n\t\t\t\t\t\tmethod_name,\n\t\t\t\t\t\t))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tcheckObject(method, impl_method, tentative=tentative)\n\t\t\texcept Invalid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\t# This is not an error if we are just trying to see if an\n\t\t# object might provide an interface\n\t\traise Invalid(\"%r does not claim to provide %s.\" % (\n\t\t\tcandidate,\n\t\t\tiface.__name__,\n\t\t))\n\n\tif not tentative and not _check_method_signatures(iface, candidate):\n\t\traise Invalid(\"Signature mismatch for %r\" % candidate)\n\n\tif not _check_attributes(iface, candidate):\n\t\traise Invalid(\"Missing attributes in %r\" % candidate)\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\treturn verify.verifyObject(iface, candidate, tentative)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%s\\n\\t%s does not claim to provide %s\" %\n\t\t\t(_getProxiedObject(candidate).__class__, candidate, iface)\n\t\t)\n\n\terrors = []\n\n\tfor name, descriptor in iface.namesAndDescriptions(1):\n\t\tif descriptor.get('readonly', False) and descriptor.get('writeable', False):\n\t\t\tcontinue\n\n\t\ttry:\n\t\t\tvalue = getattr(candidate, name)\n\t\texcept AttributeError:\n\t\t\terrors.append(name)\n\t\t\tcontinue\n\n\t\tif isinstance(descriptor, Method):\n\t\t\tif not callable(value):\n\t\t\t\terrors.append(name)\n\t\t\t\tcontinue\n\n\t\t\tif descriptor.get('optional', False):\n\t\t\t\tcontinue\n\n\t\t\ttry:\n\t\t\t\tif not isinstance(descriptor, BoundMethod):\n\t\t\t\t\t# We can't do\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not claim to provide %s\" % (candidate, iface.__identifier__)\n\t\t)\n\n\terrors = []\n\tfor name, spec in iface.namesAndDescriptions():\n\t\tif isinstance(spec, Method):\n\t\t\tactual = getattr(candidate, name, None)\n\t\t\tif actual is None:\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\t\t\t'%s.%s is missing' % (candidate, name),\n\t\t\t\t\t\tspec,\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\n\t\t\ttry:\n\t\t\t\tvalidateInvariants(\n\t\t\t\t\tiface, candidate, name, actual, spec.positional, spec.required\n\t\t\t\t)\n\t\t\texcept:  # noqa: E722\n\t\t\t\terrors.append(\n\t\t\t\t\t(\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\terrors = []\n\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(Invalid(\n\t\t\t\t\"%r does not provide %r\" % (candidate, iface)))\n\n\tfor name, descriptor in iface.namesAndDescriptions(all=True):\n\t\tif isinstance(descriptor, Method):\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\"%r does not define method %r\" % (candidate, name)))\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tcheckObject(candidate, descriptor, name)\n\t\t\t\texcept Invalid as error:\n\t\t\t\t\terrors.append(error)\n\n\t\telif isinstance(descriptor, Attribute):\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\"%r does not define attribute %r\" % (candidate, name)))\n\n\tif errors:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\treturn verifyObject(iface, candidate, tentative=tentative)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\terrors = []\n\t# First, make sure the candidate claims to provide the interface.\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(Invalid(\"%r does not claim to provide %s\" % (candidate, iface.__name__)))\n\t# Now, make sure the candidate defines the necessary methods.\n\tfor name, spec in iface.namesAndDescriptions():\n\t\t# we only care about methods\n\t\tif spec.__class__ is Method:\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(Invalid(\"%r does not define %s required by %s\" % (candidate, name, iface.__name__)))\n\t\t\telse:\n\t\t\t\t# Make sure the method has the correct signature\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\t\ttry:\n\t\t\t\t\t# Try to check the signature. It's not always possible\n\t\t\t\t\t# to do this.\n\t\t\t\t\tspec.bind(method)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t_inv = _InvalidInterface(\n\t\tmsg=_(\"The object %r does not correctly implement %r.\\n\") % (candidate, iface)\n\t)\n\n\tif not tentative and not iface.providedBy(candidate):\n\t\t_inv.append(_InvalidInterface(\n\t\t\tmsg=_(\"%r does not implement %r.\") % (candidate, iface)\n\t\t))\n\n\t# Check for methods\n\tfor name, method in iface.namesAndDescriptions(all=True):\n\t\tif not IAttribute.providedBy(method):\n\t\t\tcontinue\n\t\tcandidate_method = getattr(candidate, name, _marker)\n\t\tif candidate_method is _marker:\n\t\t\t_inv.append(_InvalidInterface(\n\t\t\t\tmsg=_(\"%r does not define %r.\") % (candidate, name),\n\t\t\t\tname=name\n\t\t\t))\n\t\t\tcontinue\n\n\t\tif not isinstance(candidate_method, collections_abc.Callable):\n\t\t\t_inv.append(_InvalidInterface(\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "ques_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, Interface):\n\t\traise TypeError(\"iface must be an instance of Interface\")\n\n\tif isinstance(candidate, Interface):\n\t\treturn candidate.__class__ == iface.__class__ and \\\n\t\t\tcandidate.__name__ == iface.__name__ and \\\n\t\t\tcandidate.__args__ == iface.__args__ and \\\n\t\t\tcandidate.__parent__ == iface.__parent__\n\telif isinstance(candidate, type):\n\t\ttry:\n\t\t\treturn issubclass(candidate, iface)\n\t\texcept Exception:\n\t\t\treturn False\n\n\treturn verifyObject(iface, candidate, tentative)\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not issubclass(candidate, iface):\n\t\traise TypeError(\"%s is not a subclass of %s\" % (candidate, iface))\n\tfor name, attr in iface.__dict__.iteritems():\n\t\tif isinstance(attr, types.FunctionType):\n\t\t\ttry:\n\t\t\t\tcattr = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\traise TypeError(\"%s lacks method %s.%s\" % (candidate, iface, name))\n\t\t\tif not isinstance(cattr, types.FunctionType):\n\t\t\t\traise TypeError(\"%s.%s is not a function\" % (candidate, name))\n\n\t\t\ttry:\n\t\t\t\tverifySignature(attr, cattr, tentative=tentative)\n\t\t\texcept TypeError as e:\n\t\t\t\traise TypeError(\"%s.%s has incorrect signature: %s\" % (candidate, name, e))\n\n", "def verifyClass(iface, candidate, tentative=False):\n\t#\n\t#  The candidate might be an object, or a class.  If it's\n\t#  an object, we verify the object's class.\n\t#\n\tif hasattr(candidate, '__class__'):\n\t\tcandidate = candidate.__class__\n\t#\n\t#  The candidate might be a subclass of the interface,\n\t#  or it might be a classic class that implements it.\n\t#  Either way, we get a list of methods and attributes\n\t#  implemented by the interface.\n\t#\n\tifaces = list(getInterfaces(iface))\n\tif not ifaces:\n\t\tif not tentative:\n\t\t\traise VerificationError(\n\t\t\t\t\"%s does not define any interface\" %\n\t\t\t\trepr(iface))\n\t\treturn False\n\t#\n\t#  Now we loop through each method and attribute\n\t#  implemented by the interface, and verify that\n\t#  the candidate provides it.\n\t#\n\tfor iface in ifaces:\n\t\tfor name in iface.__dict__.keys():\n\t\t\n", "def verifyClass(iface, candidate, tentative=False):\n\tif iface is None:\n\t\treturn True\n\n\tif (\n\t\tisinstance(candidate, type)\n\t\tand issubclass(candidate, iface)\n\t\tor isinstance(candidate, iface)\n\t):\n\t\treturn True\n\n\tif tentative:\n\t\treturn False\n\n\tmsg = (\n\t\t\"The {0} {1} must be a subclass of {2}.\".format(\n\t\t\tiface.__class__.__name__, candidate.__name__, iface.__name__\n\t\t)\n\t)\n\traise TypeError(msg)\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif isinstance(iface, type):\n\t\tif not issubclass(candidate, iface):\n\t\t\tif tentative:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\traise TypeError(\"%s is not a subclass of %s\" % (candidate, iface))\n\telse:\n\t\tif not implements(candidate, iface):\n\t\t\tif tentative:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\treturn True\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif candidate is None:\n\t\tif not tentative:\n\t\t\traise TypeError(\"Expected instance that provides %s, got None\" % iface)\n\t\treturn False\n\tif not isinstance(candidate, InterfaceClass):\n\t\tif not tentative:\n\t\t\traise TypeError(\"Expected instance that provides %s, got %s\" % (iface, candidate.__class__))\n\t\treturn False\n\tif not candidate.__implements__(iface):\n\t\tif not tentative:\n\t\t\traise TypeError(\"Expected instance that provides %s, got instance that provides %s\" % (iface, candidate.__implements__()))\n\t\treturn False\n\treturn True\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif iface == candidate:\n\t\treturn True\n\tif iface in candidate.__mro__:\n\t\treturn True\n\tif not tentative:\n\t\traise TypeError('%s is not a subclass of %s' % (candidate.__name__, iface.__name__))\n\treturn False\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif iface is None:\n\t\treturn True\n\telif isinstance(iface, type):\n\t\tif isinstance(candidate, iface):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(iface, basestring):\n\t\tif not isinstance(candidate, type):\n\t\t\treturn False\n\t\tif iface in candidate.__dict__:\n\t\t\t# The iface was defined on the class, so we can check it\n\t\t\treturn True\n\t\telif not tentative:\n\t\t\t# We need to check the MRO\n\t\t\tfor c in candidate.mro():\n\t\t\t\tif iface in c.__dict__:\n\t\t\t\t\treturn True\n\t\t\treturn False\n\t\telse:\n\t\t\t# We are not sure if the method will be available,\n\t\t\t# so we have to assume that it will be\n\t\t\treturn True\n\telif callable(iface):\n\t\tif iface in candidate.__dict__.values():\n\t\t\treturn\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not (isinstance(candidate, type) and issubclass(candidate, iface)):\n\t\traise TypeError(\"class %s is not a subclass of %s\" % (candidate, iface))\n\tif not tentative and not isinstance(candidate, InterfaceClass):\n\t\traise TypeError(\"class %s is not an InterfaceClass\" % candidate)\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif iface.implementedBy(candidate):\n\t\treturn True\n\tif not tentative:\n\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, Interface):\n\t\traise TypeError(\"iface must be an instance of Interface\")\n\n\tif isinstance(candidate, Interface):\n\t\treturn candidate.__class__ == iface.__class__ and \\\n\t\t\tcandidate.__name__ == iface.__name__ and \\\n\t\t\tcandidate.__args__ == iface.__args__ and \\\n\t\t\tcandidate.__parent__ == iface.__parent__\n\telif isinstance(candidate, type):\n\t\ttry:\n\t\t\treturn issubclass(candidate, iface)\n\t\texcept Exception:\n\t\t\treturn False\n\n\treturn verifyObject(iface, candidate, tentative)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not issubclass(candidate, iface):\n\t\traise TypeError(\"%s is not a subclass of %s\" % (candidate, iface))\n\tfor name, attr in iface.__dict__.iteritems():\n\t\tif isinstance(attr, types.FunctionType):\n\t\t\ttry:\n\t\t\t\tcattr = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\traise TypeError(\"%s lacks method %s.%s\" % (candidate, iface, name))\n\t\t\tif not isinstance(cattr, types.FunctionType):\n\t\t\t\traise TypeError(\"%s.%s is not a function\" % (candidate, name))\n\n\t\t\ttry:\n\t\t\t\tverifySignature(attr, cattr, tentative=tentative)\n\t\t\texcept TypeError as e:\n\t\t\t\traise TypeError(\"%s.%s has incorrect signature: %s\" % (candidate, name, e))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t#\n\t#  The candidate might be an object, or a class.  If it's\n\t#  an object, we verify the object's class.\n\t#\n\tif hasattr(candidate, '__class__'):\n\t\tcandidate = candidate.__class__\n\t#\n\t#  The candidate might be a subclass of the interface,\n\t#  or it might be a classic class that implements it.\n\t#  Either way, we get a list of methods and attributes\n\t#  implemented by the interface.\n\t#\n\tifaces = list(getInterfaces(iface))\n\tif not ifaces:\n\t\tif not tentative:\n\t\t\traise VerificationError(\n\t\t\t\t\"%s does not define any interface\" %\n\t\t\t\trepr(iface))\n\t\treturn False\n\t#\n\t#  Now we loop through each method and attribute\n\t#  implemented by the interface, and verify that\n\t#  the candidate provides it.\n\t#\n\tfor iface in ifaces:\n\t\tfor name in iface.__dict__.keys():\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif iface is None:\n\t\treturn True\n\n\tif (\n\t\tisinstance(candidate, type)\n\t\tand issubclass(candidate, iface)\n\t\tor isinstance(candidate, iface)\n\t):\n\t\treturn True\n\n\tif tentative:\n\t\treturn False\n\n\tmsg = (\n\t\t\"The {0} {1} must be a subclass of {2}.\".format(\n\t\t\tiface.__class__.__name__, candidate.__name__, iface.__name__\n\t\t)\n\t)\n\traise TypeError(msg)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif isinstance(iface, type):\n\t\tif not issubclass(candidate, iface):\n\t\t\tif tentative:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\traise TypeError(\"%s is not a subclass of %s\" % (candidate, iface))\n\telse:\n\t\tif not implements(candidate, iface):\n\t\t\tif tentative:\n\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif candidate is None:\n\t\tif not tentative:\n\t\t\traise TypeError(\"Expected instance that provides %s, got None\" % iface)\n\t\treturn False\n\tif not isinstance(candidate, InterfaceClass):\n\t\tif not tentative:\n\t\t\traise TypeError(\"Expected instance that provides %s, got %s\" % (iface, candidate.__class__))\n\t\treturn False\n\tif not candidate.__implements__(iface):\n\t\tif not tentative:\n\t\t\traise TypeError(\"Expected instance that provides %s, got instance that provides %s\" % (iface, candidate.__implements__()))\n\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif iface == candidate:\n\t\treturn True\n\tif iface in candidate.__mro__:\n\t\treturn True\n\tif not tentative:\n\t\traise TypeError('%s is not a subclass of %s' % (candidate.__name__, iface.__name__))\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif iface is None:\n\t\treturn True\n\telif isinstance(iface, type):\n\t\tif isinstance(candidate, iface):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\telif isinstance(iface, basestring):\n\t\tif not isinstance(candidate, type):\n\t\t\treturn False\n\t\tif iface in candidate.__dict__:\n\t\t\t# The iface was defined on the class, so we can check it\n\t\t\treturn True\n\t\telif not tentative:\n\t\t\t# We need to check the MRO\n\t\t\tfor c in candidate.mro():\n\t\t\t\tif iface in c.__dict__:\n\t\t\t\t\treturn True\n\t\t\treturn False\n\t\telse:\n\t\t\t# We are not sure if the method will be available,\n\t\t\t# so we have to assume that it will be\n\t\t\treturn True\n\telif callable(iface):\n\t\tif iface in candidate.__dict__.values():\n\t\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not (isinstance(candidate, type) and issubclass(candidate, iface)):\n\t\traise TypeError(\"class %s is not a subclass of %s\" % (candidate, iface))\n\tif not tentative and not isinstance(candidate, InterfaceClass):\n\t\traise TypeError(\"class %s is not an InterfaceClass\" % candidate)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif iface.implementedBy(candidate):\n\t\treturn True\n\tif not tentative:\n\t\traise TypeError(\"%s does not implement %s\" % (candidate, iface))\n\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "ques_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n\t# XXX: This is a hack.  We should not be doing this here.\n\t#      We should do it in the compiler instead.\n\tfor x in bases:\n\t\tmcs = x.__class__.mro()\n\t\tif mcs != x.__class__.__mro__ and len(mcs) > 1:\n\t\t\tbreak\n\telse:\n\t\treturn type\n\n\t# If there is only one base, use its metaclass.\n\tif len(bases) == 1:\n\t\tmc = bases[0].__class__\n\t\tif explicit_mc is not None and issubclass(mc, explicit_mc):\n\t\t\treturn explicit_mc\n\t\tif mc is not object:\n\t\t\treturn mc\n\n\t# Otherwise, we need to find the metaclass of the least derived base\n\t# in bases that is compatible with all of the others.\n\tmetaclasses = [b.__class__ for b in bases]\n\t#print \"metaclasses=%r\" % metaclasses\n\tfor mc in metaclasses:\n\t\tif mc is object\n", "def determineMetaclass(bases, explicit_mc=None):\n\t# Determine if any base has a metaclass\n\tmetaclass = None\n\tfor base in bases:\n\t\tif hasattr(base, '__metaclass__'):\n\t\t\tmetaclass = base.__metaclass__\n\t\t\tbreak\n\t\n\t# If no base has a metaclass, use type\n\tif metaclass is None:\n\t\tmetaclass = type\n\t\n\t# If explicit __metaclass__ is supplied, use it\n\tif explicit_mc is not None:\n\t\tmetaclass = explicit_mc\n\t\n\treturn metaclass\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\tmcs = explicit_mc\n\telse:\n\t\tmcs = []\n\t\tfor base in bases:\n\t\t\tmcs.append(getattr(base, '__metaclass__', None))\n\t\tif len(mcs) == 1:\n\t\t\treturn mcs[0]\n\t\telif len(mcs) > 1:\n\t\t\tfor mc in mcs[1:]:\n\t\t\t\tif mc is not mcs[0]:\n\t\t\t\t\traise TypeError(\"Incompatible metaclasses %s and %s\" %\n\t\t\t\t\t\t\t\t\t(mcs[0], mc))\n\t\t\treturn mcs[0]\n\t\telse:\n\t\t\treturn type\n\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tmetaclasses = [type(base) for base in bases]\n\tmetaclasses.append(type)\n\tif explicit_mc:\n\t\tmetaclasses.append(explicit_mc)\n\treturn _find_common_metaclass(metaclasses)\n\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t# If no explicit metaclass, look for one in the bases\n\tif explicit_mc is None:\n\t\t# Look for a base with a metaclass\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\t# We have a metaclass, use it\n\t\t\t\treturn base.__metaclass__\n\t\t\telif type(base) is ObjectType:\n\t\t\t\t# We have a new-style class, use its metaclass\n\t\t\t\treturn base.__class__\n\t\t# Nothing found\n\t\treturn None\n\n\t# We have an explicit metaclass, use it\n\treturn explicit_mc\n\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tmcs = []\n\tfor b in bases:\n\t\tmcs.append(getattr(b, \"__metaclass__\", None))\n\tif explicit_mc is not None:\n\t\tmcs.append(explicit_mc)\n\treturn _determineMetaclass(mcs)\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\t# explicit metaclass, return\n\t\treturn explicit_mc\n\n\t# find metaclasses in bases\n\tmcs = [getattr(b, '__metaclass__', None) for b in bases]\n\n\t# get the non-None metaclasses\n\tnon_none_mcs = [mc for mc in mcs if mc is not None]\n\n\tif len(non_none_mcs) > 1:\n\t\t# multiple non-None metaclasses in bases, return the first\n\t\treturn non_none_mcs[0]\n\telif len(non_none_mcs) == 1:\n\t\t# 1 non-None metaclass, return it\n\t\treturn non_none_mcs[0]\n\telse:\n\t\t# no non-None metaclasses in bases, return type\n\t\treturn type\n\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t#metaclass = None\n\t#if explicit_mc is not None:\n\t#\tmetaclass = explicit_mc\n\t#else:\n\t#\tfor base in bases:\n\t#\t\tif hasattr(base, \"__metaclass__\"):\n\t#\t\t\tmetaclass = getattr(base, \"__metaclass__\")\n\t#\t\t\tbreak\n\t#\tif metaclass is None:\n\t#\t\tif hasattr(types, \"ClassType\"):\n\t#\t\t\tmetaclass = (types.ClassType, type)\n\t#\t\telse:\n\t#\t\t\tmetaclass = (type,)\n\t#return metaclass\n\n\t# NOTE: The above is the code from Python 2.6.  Here is the code from\n\t# Python 2.7 for comparison.\n\tmetaclass = None\n\tif explicit_mc is not None:\n\t\tmetaclass = explicit_mc\n\telse:\n\t\tfor base in bases:\n\t\t\t# All bases must be classes (otherwise, we cannot determine a\n\t\t\t#\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t# check if any of the bases have a metaclass\n\tfor b in bases:\n\t\tif hasattr(b, '__class__'):\n\t\t\tif not isinstance(b, (type, types.ClassType)):\n\t\t\t\t# b is not a class, so we call its __class__\n\t\t\t\tb = b.__class__\n\t\t\tif issubclass(b, typetype):\n\t\t\t\treturn b\n\t# get the first base\n\tb = bases[0]\n\tif hasattr(b, '__class__'):\n\t\tb = b.__class__\n\t# now check all bases for their __class__\n\tfor b in bases[1:]:\n\t\tif hasattr(b, '__class__'):\n\t\t\tb = b.__class__\n\t\tif b is not bases[0]:\n\t\t\treturn typetype\n\treturn type(bases[0])\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\tif not isinstance(explicit_mc, basestring):\n\t\t\treturn explicit_mc\n\t\telse:\n\t\t\treturn _resolve_name(explicit_mc)\n\n\t# we have bases\n\telif bases:\n\t\tmetaclasses = set(type(b) for b in bases)\n\t\tmetaclasses.add(type)\n\t\tif len(metaclasses) > 1:\n\t\t\texplicit_bases = [b for b in bases if type(b) is not type]\n\t\t\tif not explicit_bases:\n\t\t\t\traise TypeError(\"a __metaclass__ is not defined\")\n\t\t\telse:\n\t\t\t\traise TypeError(\"metaclass conflict: \"\n\t\t\t\t\t\t\"the metaclass of a derived class \"\n\t\t\t\t\t\t\"must be a (non-strict) subclass \"\n\t\t\t\t\t\t\"of the metaclasses of all its bases\")\n\t\telse:\n\t\t\t[metaclass] = metaclasses\n\t\t\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t# XXX: This is a hack.  We should not be doing this here.\n\t#      We should do it in the compiler instead.\n\tfor x in bases:\n\t\tmcs = x.__class__.mro()\n\t\tif mcs != x.__class__.__mro__ and len(mcs) > 1:\n\t\t\tbreak\n\telse:\n\t\treturn type\n\n\t# If there is only one base, use its metaclass.\n\tif len(bases) == 1:\n\t\tmc = bases[0].__class__\n\t\tif explicit_mc is not None and issubclass(mc, explicit_mc):\n\t\t\treturn explicit_mc\n\t\tif mc is not object:\n\t\t\treturn mc\n\n\t# Otherwise, we need to find the metaclass of the least derived base\n\t# in bases that is compatible with all of the others.\n\tmetaclasses = [b.__class__ for b in bases]\n\t#print \"metaclasses=%r\" % metaclasses\n\tfor mc in metaclasses:\n\t\tif mc is object\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t# Determine if any base has a metaclass\n\tmetaclass = None\n\tfor base in bases:\n\t\tif hasattr(base, '__metaclass__'):\n\t\t\tmetaclass = base.__metaclass__\n\t\t\tbreak\n\t\n\t# If no base has a metaclass, use type\n\tif metaclass is None:\n\t\tmetaclass = type\n\t\n\t# If explicit __metaclass__ is supplied, use it\n\tif explicit_mc is not None:\n\t\tmetaclass = explicit_mc\n\t\n\treturn metaclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\tmcs = explicit_mc\n\telse:\n\t\tmcs = []\n\t\tfor base in bases:\n\t\t\tmcs.append(getattr(base, '__metaclass__', None))\n\t\tif len(mcs) == 1:\n\t\t\treturn mcs[0]\n\t\telif len(mcs) > 1:\n\t\t\tfor mc in mcs[1:]:\n\t\t\t\tif mc is not mcs[0]:\n\t\t\t\t\traise TypeError(\"Incompatible metaclasses %s and %s\" %\n\t\t\t\t\t\t\t\t\t(mcs[0], mc))\n\t\t\treturn mcs[0]\n\t\telse:\n\t\t\treturn type\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tmetaclasses = [type(base) for base in bases]\n\tmetaclasses.append(type)\n\tif explicit_mc:\n\t\tmetaclasses.append(explicit_mc)\n\treturn _find_common_metaclass(metaclasses)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t# If no explicit metaclass, look for one in the bases\n\tif explicit_mc is None:\n\t\t# Look for a base with a metaclass\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\t# We have a metaclass, use it\n\t\t\t\treturn base.__metaclass__\n\t\t\telif type(base) is ObjectType:\n\t\t\t\t# We have a new-style class, use its metaclass\n\t\t\t\treturn base.__class__\n\t\t# Nothing found\n\t\treturn None\n\n\t# We have an explicit metaclass, use it\n\treturn explicit_mc\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tmcs = []\n\tfor b in bases:\n\t\tmcs.append(getattr(b, \"__metaclass__\", None))\n\tif explicit_mc is not None:\n\t\tmcs.append(explicit_mc)\n\treturn _determineMetaclass(mcs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\t# explicit metaclass, return\n\t\treturn explicit_mc\n\n\t# find metaclasses in bases\n\tmcs = [getattr(b, '__metaclass__', None) for b in bases]\n\n\t# get the non-None metaclasses\n\tnon_none_mcs = [mc for mc in mcs if mc is not None]\n\n\tif len(non_none_mcs) > 1:\n\t\t# multiple non-None metaclasses in bases, return the first\n\t\treturn non_none_mcs[0]\n\telif len(non_none_mcs) == 1:\n\t\t# 1 non-None metaclass, return it\n\t\treturn non_none_mcs[0]\n\telse:\n\t\t# no non-None metaclasses in bases, return type\n\t\treturn type\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t#metaclass = None\n\t#if explicit_mc is not None:\n\t#\tmetaclass = explicit_mc\n\t#else:\n\t#\tfor base in bases:\n\t#\t\tif hasattr(base, \"__metaclass__\"):\n\t#\t\t\tmetaclass = getattr(base, \"__metaclass__\")\n\t#\t\t\tbreak\n\t#\tif metaclass is None:\n\t#\t\tif hasattr(types, \"ClassType\"):\n\t#\t\t\tmetaclass = (types.ClassType, type)\n\t#\t\telse:\n\t#\t\t\tmetaclass = (type,)\n\t#return metaclass\n\n\t# NOTE: The above is the code from Python 2.6.  Here is the code from\n\t# Python 2.7 for comparison.\n\tmetaclass = None\n\tif explicit_mc is not None:\n\t\tmetaclass = explicit_mc\n\telse:\n\t\tfor base in bases:\n\t\t\t# All bases must be classes (otherwise, we cannot determine a\n\t\t\t#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\t# check if any of the bases have a metaclass\n\tfor b in bases:\n\t\tif hasattr(b, '__class__'):\n\t\t\tif not isinstance(b, (type, types.ClassType)):\n\t\t\t\t# b is not a class, so we call its __class__\n\t\t\t\tb = b.__class__\n\t\t\tif issubclass(b, typetype):\n\t\t\t\treturn b\n\t# get the first base\n\tb = bases[0]\n\tif hasattr(b, '__class__'):\n\t\tb = b.__class__\n\t# now check all bases for their __class__\n\tfor b in bases[1:]:\n\t\tif hasattr(b, '__class__'):\n\t\t\tb = b.__class__\n\t\tif b is not bases[0]:\n\t\t\treturn typetype\n\treturn type(bases[0])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\tif not isinstance(explicit_mc, basestring):\n\t\t\treturn explicit_mc\n\t\telse:\n\t\t\treturn _resolve_name(explicit_mc)\n\n\t# we have bases\n\telif bases:\n\t\tmetaclasses = set(type(b) for b in bases)\n\t\tmetaclasses.add(type)\n\t\tif len(metaclasses) > 1:\n\t\t\texplicit_bases = [b for b in bases if type(b) is not type]\n\t\t\tif not explicit_bases:\n\t\t\t\traise TypeError(\"a __metaclass__ is not defined\")\n\t\t\telse:\n\t\t\t\traise TypeError(\"metaclass conflict: \"\n\t\t\t\t\t\t\"the metaclass of a derived class \"\n\t\t\t\t\t\t\"must be a (non-strict) subclass \"\n\t\t\t\t\t\t\"of the metaclasses of all its bases\")\n\t\telse:\n\t\t\t[metaclass] = metaclasses\n\t\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "ques_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n\t\t\t\n\tdef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n", "def pop(self, key, default=__marker):\n\ttry:\n\t\tvalue = self[key]\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\tdel self[key]\n\treturn value\n\n", "def pop(self, key, default=__marker):\n\tif key in self:\n\t\tvalue = super(dict, self).__getitem__(key)\n\t\tdel self[key]\n\t\treturn value\n\tif default is self.__marker:\n\t\traise KeyError(key)\n\treturn default\n\n\n", "def pop(self, key, default=__marker):\n\tdef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n", "def pop(self, key, default=__marker):\n\tif key not in self:\n\t\tif default is self.__marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\tself[key], result = self.__marker, self[key]\n\treturn result\n\n\n", "def pop(self, key, default=__marker):\n\tpass\n\n", "def pop(self, key, default=__marker):\n\tpass\n\t\n", "def pop(self, key, default=__marker):\n\tif self.has_key(key):\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\tif default is self.__marker:\n\t\traise KeyError(key)\n\treturn default\n\n", "def pop(self, key, default=__marker):\n\ttry:\n\t\tself.__delitem__(key)\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\t\n", "def pop(self, key, default=__marker):\n\tpass\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n\t\t\t\n\tdef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\ttry:\n\t\tvalue = self[key]\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\tdel self[key]\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tif key in self:\n\t\tvalue = super(dict, self).__getitem__(key)\n\t\tdel self[key]\n\t\treturn value\n\tif default is self.__marker:\n\t\traise KeyError(key)\n\treturn default\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tdef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tif key not in self:\n\t\tif default is self.__marker:\n\t\t\traise KeyError(key)\n\t\treturn default\n\tself[key], result = self.__marker, self[key]\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tif self.has_key(key):\n\t\tv = self[key]\n\t\tdel self[key]\n\t\treturn v\n\tif default is self.__marker:\n\t\traise KeyError(key)\n\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\ttry:\n\t\tself.__delitem__(key)\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "ques_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tpair = self._queue.remove()\n\tdel self._mapping[pair[0]]\n\tif not self:\n\t\tself._reset()\n\treturn pair\n\n\n", "def popitem(self):\n\tif self.root:\n\t\tkey, value = self.root.min()\n\t\tdel self[key]\n\t\treturn key, value\n\n\n", "def popitem(self):\n\tif self:\n\t\tkey, _ = self.__root.left.pop_max()\n\t\treturn (key, self.__map.pop(key)[1])\n\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "def popitem(self):\n\tif self.root is None:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\tkey, _ = self.root.key, self.root.value\n\tself.discard(key)\n\treturn key, _\n\n\n\tdef __repr__(self):\n\t\treturn '%s(%r, maxsize=%d, currsize=%d)' % (\n\t\t\tself.__class__.__name__,\n\t\t\tlist(self.items()),\n\t\t\tself.maxsize,\n\t\t\tself.currsize,\n\t\t\t)\n\n\n\tdef __getstate__(self):\n\t\treturn self.__dict__.copy()\n\n\n\tdef __setstate__(self, d):\n\t\tself.__dict__.update(d)\n\n\n\tdef __contains__(self, key):\n\t\treturn key in self.mapping\n\n\n\tdef __len__(self):\n\t\treturn len(self.mapping)\n\n\n\tdef _remove(self, node):\n\t\t\"\"\"\n", "def popitem(self):\n\tif self._count:\n\t\tkey, _ = self._removeLRU()\n\t\treturn (key, self._cache_pop(key))\n\traise KeyError('%s is empty' % self.__class__.__name__)\n\n\n", "def popitem(self):\n\ttry:\n\t\treturn self.pop(next(iter(self)))\n\texcept KeyError:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\texcept StopIteration:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\telse:\n\t\tvalue = self.pop(key)\n\t\treturn key, value\n\n\tdef __repr__(self):\n\t\tif not self:\n\t\t\treturn '%s()' % (self.__class__.__name__,)\n\t\treturn '%s(%r)' % (self.__class__.__name__, list(self.items()))\n\n\tdef copy(self):\n\t\treturn self.__class__(self)\n\n\t@classmethod\n\tdef fromkeys(cls, iterable, value=None):\n\t\td = cls()\n\t\tfor key in iterable:\n\t\t\td[key] = value\n\t\treturn d\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, OrderedFrequencyDict):\n\t\t\treturn len(self)==len(other) and \\\n\t\t\t\tall(\n", "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\t_, key = self._heap[0]\n\tif len(self) == 1:\n\t\tself._heap.pop()\n\t\tdel self._counter[key]\n\t\treturn key, self._cache.pop(key)\n\tvalue = self._cache.pop(key)\n\tself._counter.pop(key)\n\tlast_key, _ = self._heap.pop()\n\tdel self._counter[last_key]\n\tif key is last_key:\n\t\treturn key, value\n\tself._cache[last_key] = self._cache.pop(key)\n\tself._fix(0, self._counter[last_key], last_key)\n\treturn key, value\n\n", "def popitem(self):\n\tif self.map:\n\t\tkey, _ = self.heap[0]\n\t\treturn key, self.pop(key)\n\n\n", "def popitem(self):\n\tif self._queue:\n\t\tkey = self._queue.pop()\n\t\tvalue = dict.pop(self, key)\n\t\treturn key, value\n\telse:\n\t\traise KeyError('dictionary is empty')\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tpair = self._queue.remove()\n\tdel self._mapping[pair[0]]\n\tif not self:\n\t\tself._reset()\n\treturn pair\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.root:\n\t\tkey, value = self.root.min()\n\t\tdel self[key]\n\t\treturn key, value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey, _ = self.__root.left.pop_max()\n\t\treturn (key, self.__map.pop(key)[1])\n\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.root is None:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\tkey, _ = self.root.key, self.root.value\n\tself.discard(key)\n\treturn key, _\n\n\n\tdef __repr__(self):\n\t\treturn '%s(%r, maxsize=%d, currsize=%d)' % (\n\t\t\tself.__class__.__name__,\n\t\t\tlist(self.items()),\n\t\t\tself.maxsize,\n\t\t\tself.currsize,\n\t\t\t)\n\n\n\tdef __getstate__(self):\n\t\treturn self.__dict__.copy()\n\n\n\tdef __setstate__(self, d):\n\t\tself.__dict__.update(d)\n\n\n\tdef __contains__(self, key):\n\t\treturn key in self.mapping\n\n\n\tdef __len__(self):\n\t\treturn len(self.mapping)\n\n\n\tdef _remove(self, node):\n\t\t\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self._count:\n\t\tkey, _ = self._removeLRU()\n\t\treturn (key, self._cache_pop(key))\n\traise KeyError('%s is empty' % self.__class__.__name__)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\treturn self.pop(next(iter(self)))\n\texcept KeyError:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\texcept StopIteration:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\telse:\n\t\tvalue = self.pop(key)\n\t\treturn key, value\n\n\tdef __repr__(self):\n\t\tif not self:\n\t\t\treturn '%s()' % (self.__class__.__name__,)\n\t\treturn '%s(%r)' % (self.__class__.__name__, list(self.items()))\n\n\tdef copy(self):\n\t\treturn self.__class__(self)\n\n\t@classmethod\n\tdef fromkeys(cls, iterable, value=None):\n\t\td = cls()\n\t\tfor key in iterable:\n\t\t\td[key] = value\n\t\treturn d\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, OrderedFrequencyDict):\n\t\t\treturn len(self)==len(other) and \\\n\t\t\t\tall(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\t_, key = self._heap[0]\n\tif len(self) == 1:\n\t\tself._heap.pop()\n\t\tdel self._counter[key]\n\t\treturn key, self._cache.pop(key)\n\tvalue = self._cache.pop(key)\n\tself._counter.pop(key)\n\tlast_key, _ = self._heap.pop()\n\tdel self._counter[last_key]\n\tif key is last_key:\n\t\treturn key, value\n\tself._cache[last_key] = self._cache.pop(key)\n\tself._fix(0, self._counter[last_key], last_key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.map:\n\t\tkey, _ = self.heap[0]\n\t\treturn key, self.pop(key)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self._queue:\n\t\tkey = self._queue.pop()\n\t\tvalue = dict.pop(self, key)\n\t\treturn key, value\n\telse:\n\t\traise KeyError('dictionary is empty')\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "ques_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tkey = next(reversed(self))\n\treturn (key, self.pop(key))\n\n", "def popitem(self):\n\ttry:\n\t\tkey = self._queue.pop()\n\texcept IndexError:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self._cache[key]\n\tdel self._cache[key]\n\treturn (key, value)\n\n", "def popitem(self):\n\tif self:\n\t\tkey, _ = self.end[PREV]\n\t\tself.pop(key)\n\t\treturn key, self._prune()\n\n\n", "def popitem(self):\n\twith self.lock:\n\t\ttry:\n\t\t\tkey = next(iter(self.container))\n\t\texcept StopIteration:\n\t\t\traise KeyError('container is empty.')\n\t\telse:\n\t\t\tvalue = self.container.pop(key)\n\t\t\tself.counter.pop(key)\n\t\t\treturn key, value\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet `value` for `key` and move `key` to end of used list.\n\t\t\"\"\"\n\t\texpire = self.expire\n\t\twith self.lock:\n\t\t\tif key in self.container:\n\t\t\t\tdel self[key]\n\t\t\tend = self.end\n\t\t\tcurr_time = time.time()\n\t\t\tself.container[key] = value\n\t\t\tself.counter[key] = [curr_time, end]\n\t\t\tend += 1\n\t\t\tself.end = end\n\t\t\tif expire is not None:\n\t\t\t\theapq.he\n", "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tkey, _ = self.end[PREV]\n\tself.pop(key)\n\treturn key, self[key]\n\n", "def popitem(self):\n\tif self:\n\t    key = next(reversed(self))\n\t    return (key, self.pop(key))\n\telse:\n\t    raise KeyError('dictionary is empty')\n\n", "def popitem(self):\n\tif not self.cache:\n\t\traise KeyError('cache is empty')\n\n\t(key, _value), = self.cache.popitem(last=True)\n\tdel self.mapping[key]\n\treturn key, _value\n\n\n", "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tkey = next(reversed(self))\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "def popitem(self):\n\tif self:\n\t\tkey = self.end[self.PREV]\n\t\tself.detach(key)\n\t\treturn (key, self.__dict__.pop(key))\n\n", "def popitem(self):\n\tif self:\n\t\tkey, _ = self.end[PREV].popitem()\n\t\tdel self[key]\n\t\treturn key, self._pop(self.end[PREV])\n\n\traise KeyError('dictionary is empty')\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tkey = next(reversed(self))\n\treturn (key, self.pop(key))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = self._queue.pop()\n\texcept IndexError:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self._cache[key]\n\tdel self._cache[key]\n\treturn (key, value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey, _ = self.end[PREV]\n\t\tself.pop(key)\n\t\treturn key, self._prune()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\twith self.lock:\n\t\ttry:\n\t\t\tkey = next(iter(self.container))\n\t\texcept StopIteration:\n\t\t\traise KeyError('container is empty.')\n\t\telse:\n\t\t\tvalue = self.container.pop(key)\n\t\t\tself.counter.pop(key)\n\t\t\treturn key, value\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSet `value` for `key` and move `key` to end of used list.\n\t\t\"\"\"\n\t\texpire = self.expire\n\t\twith self.lock:\n\t\t\tif key in self.container:\n\t\t\t\tdel self[key]\n\t\t\tend = self.end\n\t\t\tcurr_time = time.time()\n\t\t\tself.container[key] = value\n\t\t\tself.counter[key] = [curr_time, end]\n\t\t\tend += 1\n\t\t\tself.end = end\n\t\t\tif expire is not None:\n\t\t\t\theapq.he\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tkey, _ = self.end[PREV]\n\tself.pop(key)\n\treturn key, self[key]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t    key = next(reversed(self))\n\t    return (key, self.pop(key))\n\telse:\n\t    raise KeyError('dictionary is empty')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self.cache:\n\t\traise KeyError('cache is empty')\n\n\t(key, _value), = self.cache.popitem(last=True)\n\tdel self.mapping[key]\n\treturn key, _value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tkey = next(reversed(self))\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey = self.end[self.PREV]\n\t\tself.detach(key)\n\t\treturn (key, self.__dict__.pop(key))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey, _ = self.end[PREV].popitem()\n\t\tdel self[key]\n\t\treturn key, self._pop(self.end[PREV])\n\n\traise KeyError('dictionary is empty')\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "ques_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n\tif self.__root:\n\t\tkey, value = self.__root.key, self.__root.value\n\t\tself.__root = self.__root.next\n\t\tself.__map.pop(key, None)\n\t\treturn key, value\n\traise KeyError('popitem(): dictionary is empty')\n\n", "def popitem(self):\n\tpass\n\t\n", "def popitem(self):\n\tif self:\n\t\tkey, value = self.end[0]\n\t\tdel self[key]\n\t\treturn key, value\n\traise KeyError('dictionary is empty')\n\n", "def popitem(self):\n\ttry:\n\t\tkey, value = self.__data.popitem()\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\telse:\n\t\tdel self.__position[key]\n\t\treturn (key, value)\n\n\n", "def popitem(self):\n\tif self.data:\n\t\treturn self.data.popitem()\n\telse:\n\t\traise KeyError('empty')\n\n", "def popitem(self):\n\ttry:\n\t\tkey, value = self.__data.popitem()\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\tself.__queue.remove(key)\n\treturn (key, value)\n\n", "def popitem(self):\n\tif self.head is None:\n\t\traise KeyError('dictionary is empty')\n\t(key, value) = self.head\n\tdel self[key]\n\treturn key, value\n\n\n", "def popitem(self):\n\t# Pop the last item from the list of ordered keys (most recently used)\n\tk = self.orderedkeys.pop()\n\t# Remove the key from the dictionary of items\n\tv = self.items.pop(k)\n\t# Return the pair\n\treturn (k, v)\n\t\n", "def popitem(self):\n\tif self.root is None:\n\t\traise KeyError('dictionary is empty')\n\tif self.root.left is None and self.root.right is None:\n\t\tkey, value = self.root.key, self.root.value\n\t\tself.root = None\n\t\tself.size = 0\n\t\treturn key, value\n\tstk = [self.root]\n\twhile stk[-1].next is not None:\n\t\tstk.append(stk[-1].next)\n\tlast = stk.pop()\n\tif stk:\n\t\tstk[-1].next = None\n\telse:\n\t\tself.root = None\n\tself.size -= 1\n\treturn last.key, last.value\n\n", "def popitem(self):\n\ttry:\n\t\tkey = self.__queue.pop()\n\texcept IndexError:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self.__dict.pop(key)\n\treturn key, value\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\tif self.__root:\n\t\tkey, value = self.__root.key, self.__root.value\n\t\tself.__root = self.__root.next\n\t\tself.__map.pop(key, None)\n\t\treturn key, value\n\traise KeyError('popitem(): dictionary is empty')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey, value = self.end[0]\n\t\tdel self[key]\n\t\treturn key, value\n\traise KeyError('dictionary is empty')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey, value = self.__data.popitem()\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\telse:\n\t\tdel self.__position[key]\n\t\treturn (key, value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.data:\n\t\treturn self.data.popitem()\n\telse:\n\t\traise KeyError('empty')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey, value = self.__data.popitem()\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\tself.__queue.remove(key)\n\treturn (key, value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.head is None:\n\t\traise KeyError('dictionary is empty')\n\t(key, value) = self.head\n\tdel self[key]\n\treturn key, value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t# Pop the last item from the list of ordered keys (most recently used)\n\tk = self.orderedkeys.pop()\n\t# Remove the key from the dictionary of items\n\tv = self.items.pop(k)\n\t# Return the pair\n\treturn (k, v)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.root is None:\n\t\traise KeyError('dictionary is empty')\n\tif self.root.left is None and self.root.right is None:\n\t\tkey, value = self.root.key, self.root.value\n\t\tself.root = None\n\t\tself.size = 0\n\t\treturn key, value\n\tstk = [self.root]\n\twhile stk[-1].next is not None:\n\t\tstk.append(stk[-1].next)\n\tlast = stk.pop()\n\tif stk:\n\t\tstk[-1].next = None\n\telse:\n\t\tself.root = None\n\tself.size -= 1\n\treturn last.key, last.value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = self.__queue.pop()\n\texcept IndexError:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self.__dict.pop(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "ques_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\n\tkey = self._randrange()\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "def popitem(self):\n\t\n\t# Pop a random key and return a (key, value) pair.\n\tkey = self.popitem_key()\n\treturn (key, dict.__getitem__(self, key))\n\t\n", "def popitem(self):\n\t(key, value) = self.__dict__.popitem()\n\tself.__dict__[key] = value\n\treturn (key, value)\n\n", "def popitem(self):\n\t# We do two pop(0)'s here so that the modulo operation can\n\t# occur in the (presumably faster) Python code rather than as\n\t# part of an lua command.\n\ti = self.__getitem__(0)\n\tself.__delitem__(0)\n\treturn (i, self.__getitem__(i))\n", "def popitem(self):\n\t# We do two popitem()'s without checking the length\n\t# for performance (to avoid two dict lookups). The\n\t# check is done in the finally block.\n\te = self._entry_finder.popitem()[1]\n\tresult = (e.key, e.value)\n\tdel self._entry_finder[e.key]\n\tself._num_entries -= 1\n\ttry:\n\t\tself._queue.remove(e)\n\texcept ValueError:\n\t\traise KeyError('popitem(): dictionary is empty')\n\tassert self._num_entries >= 0\n\te.key = e.value = None\n\treturn result\n\n", "def popitem(self):\n\treturn self.__super.popitem()\n\n\n", "def popitem(self):\n\t# Randomly choose a key\n\tkey = random.choice(self.keys())\n\n\t# Remove the key/value pair\n\tvalue = self[key]\n\tdel self[key]\n\n\treturn key, value\n\n", "def popitem(self):\n\t# We do two popitem()'s without checking the length\n\t# for speed. The (slight) risk is a KeyError.\n\n\tdict_key = self.dict.popitem()[0]\n\tentry = self.dict[dict_key]\n\tlist_key = entry.key\n\tlist_value = self.list.pop(list_key)[1]\n\tdel self.dict[dict_key]\n\n\t# Return a (key, value) pair\n\treturn (list_value, dict_key)\n\n", "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\tobj = self._map.pop(self._random.choice(list(self._map.keys())))\n\treturn (obj[0], obj[1])\n\n", "def popitem(self):\n\tkey = self.popitem_key()\n\tvalue = self.pop(key)\n\treturn key, value\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\n\tkey = self._randrange()\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\n\t# Pop a random key and return a (key, value) pair.\n\tkey = self.popitem_key()\n\treturn (key, dict.__getitem__(self, key))\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t(key, value) = self.__dict__.popitem()\n\tself.__dict__[key] = value\n\treturn (key, value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t# We do two pop(0)'s here so that the modulo operation can\n\t# occur in the (presumably faster) Python code rather than as\n\t# part of an lua command.\n\ti = self.__getitem__(0)\n\tself.__delitem__(0)\n\treturn (i, self.__getitem__(i))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t# We do two popitem()'s without checking the length\n\t# for performance (to avoid two dict lookups). The\n\t# check is done in the finally block.\n\te = self._entry_finder.popitem()[1]\n\tresult = (e.key, e.value)\n\tdel self._entry_finder[e.key]\n\tself._num_entries -= 1\n\ttry:\n\t\tself._queue.remove(e)\n\texcept ValueError:\n\t\traise KeyError('popitem(): dictionary is empty')\n\tassert self._num_entries >= 0\n\te.key = e.value = None\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self.__super.popitem()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t# Randomly choose a key\n\tkey = random.choice(self.keys())\n\n\t# Remove the key/value pair\n\tvalue = self[key]\n\tdel self[key]\n\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t# We do two popitem()'s without checking the length\n\t# for speed. The (slight) risk is a KeyError.\n\n\tdict_key = self.dict.popitem()[0]\n\tentry = self.dict[dict_key]\n\tlist_key = entry.key\n\tlist_value = self.list.pop(list_key)[1]\n\tdel self.dict[dict_key]\n\n\t# Return a (key, value) pair\n\treturn (list_value, dict_key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\tobj = self._map.pop(self._random.choice(list(self._map.keys())))\n\treturn (obj[0], obj[1])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tkey = self.popitem_key()\n\tvalue = self.pop(key)\n\treturn key, value\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "ques_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'^(?P<in_key>[^:]+):(?P<in_val>(?:[^:]+:)*[^:]+)$')\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t# Create the in-style parameter regular expression.\n\t# The regex is:\n\t#   (?P<name>\\w+)\\s*in\\s*(?P<values>.+?)\n\t# with the \"re.VERBOSE\" flag.\n\treturn re.compile(r'''\n\t\t(?P<name>\\w+)\t\t# Parameter name.\n\t\t\\s*in\\s*\t\t\t# The \"in\" keyword.\n\t\t(?P<values>.+?)\t\t# The list of values.\n\t\t''', re.VERBOSE)\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'{}(?P<param>\\w+)\\s*in\\s*{}'.format(self._param_open, self._param_close))\n\n\tdef _create_param_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the param-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn self._in_regex\n\n\t@_create_in_regex.setter\n\tdef _create_in_regex(self, value: Pattern) -> None:\n\t\t\"\"\"\n\t\tSet the in-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn self.__in_re\n\tdef _create_in_regex_string(self) -> str:\n\t\treturn self.__in_re_string\n\tdef _create_in_regex_string_fmt(self) -> str:\n\t\treturn self.__in_re_string_fmt\n\tdef _create_in_regex_string_fmt_compiled(self) -> Pattern:\n\t\treturn self.__in_re_string_fmt_compiled\n\tdef _create_in_regex_string_fmt_re(self) -> Pattern:\n\t\treturn self.__in_re_string_fmt_re\n\tdef _create_in_regex_string_fmt_re_compiled(self) -> Pattern:\n\t\treturn self.__in_re_string_fmt_re_compiled\n\tdef _create_in_regex_string_fmt_re_str(self) -> str:\n\t\treturn self.__in_re_string_fmt_re_str\n\tdef _create_in_regex\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r\"{0}(?P<param>[^\\s]+)\\s+in\\s+(?P<iterable>[^{}\\s]+)\\s*(?P<rest>.*)\".format(self._param_re.pattern), re.VERBOSE)\n\n\tdef _create_named_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the named parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(self.IN_REGEX_STR.format(self.name))\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'\\{(?P<param>\\w+):(?P<type>(?:(?!:).)+)(?::(?P<value>[^}]+))?\\}')\n\t#\n\t##########################################################################\n\n\t##########################################################################\n\t#\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(\n\t\t\tr'%s(?P<%s>%s)' % (\n\t\t\t\tself._regex_start,\n\t\t\t\tself._regex_name,\n\t\t\t\tself._regex_in,\n\t\t\t),\n\t\t\tflags=re.IGNORECASE,\n\t\t)\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(\n\t\t\t\trf\"\\A{self.__id_regex.pattern}\\Z\",\n\t\t\t\tself.__id_regex.flags)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the out-style parameter regular expression.\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'^(?P<in_key>[^:]+):(?P<in_val>(?:[^:]+:)*[^:]+)$')\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t# Create the in-style parameter regular expression.\n\t# The regex is:\n\t#   (?P<name>\\w+)\\s*in\\s*(?P<values>.+?)\n\t# with the \"re.VERBOSE\" flag.\n\treturn re.compile(r'''\n\t\t(?P<name>\\w+)\t\t# Parameter name.\n\t\t\\s*in\\s*\t\t\t# The \"in\" keyword.\n\t\t(?P<values>.+?)\t\t# The list of values.\n\t\t''', re.VERBOSE)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'{}(?P<param>\\w+)\\s*in\\s*{}'.format(self._param_open, self._param_close))\n\n\tdef _create_param_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the param-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn self._in_regex\n\n\t@_create_in_regex.setter\n\tdef _create_in_regex(self, value: Pattern) -> None:\n\t\t\"\"\"\n\t\tSet the in-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn self.__in_re\n\tdef _create_in_regex_string(self) -> str:\n\t\treturn self.__in_re_string\n\tdef _create_in_regex_string_fmt(self) -> str:\n\t\treturn self.__in_re_string_fmt\n\tdef _create_in_regex_string_fmt_compiled(self) -> Pattern:\n\t\treturn self.__in_re_string_fmt_compiled\n\tdef _create_in_regex_string_fmt_re(self) -> Pattern:\n\t\treturn self.__in_re_string_fmt_re\n\tdef _create_in_regex_string_fmt_re_compiled(self) -> Pattern:\n\t\treturn self.__in_re_string_fmt_re_compiled\n\tdef _create_in_regex_string_fmt_re_str(self) -> str:\n\t\treturn self.__in_re_string_fmt_re_str\n\tdef _create_in_regex\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r\"{0}(?P<param>[^\\s]+)\\s+in\\s+(?P<iterable>[^{}\\s]+)\\s*(?P<rest>.*)\".format(self._param_re.pattern), re.VERBOSE)\n\n\tdef _create_named_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the named parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(self.IN_REGEX_STR.format(self.name))\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'\\{(?P<param>\\w+):(?P<type>(?:(?!:).)+)(?::(?P<value>[^}]+))?\\}')\n\t#\n\t##########################################################################\n\n\t##########################################################################\n\t#\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(\n\t\t\tr'%s(?P<%s>%s)' % (\n\t\t\t\tself._regex_start,\n\t\t\t\tself._regex_name,\n\t\t\t\tself._regex_in,\n\t\t\t),\n\t\t\tflags=re.IGNORECASE,\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(\n\t\t\t\trf\"\\A{self.__id_regex.pattern}\\Z\",\n\t\t\t\tself.__id_regex.flags)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "ques_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n\tif self._converter is None:\n\t\tself._converter = _converting._Converter(self)\n\treturn self._converter\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(\n\t\t\t\t\t\t\tself.parameter_style,\n\t\t\t\t\t\t\tself.parameter_style_converter,\n\t\t\t\t\t\t\tself.parameter_style_converter_kwargs,\n\t\t\t\t\t\t\tself.parameter_style_converter_args\n\t\t\t\t\t\t)\n\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._param_style, self._param_prefix, self._param_suffix, self._quote_char, self._escape_char)\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._parameter_style)\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self.parameter_style)\n\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._parameter_style)\n\n", "def _create_converter(self) -> _converting._Converter:\n\tif self.__converter is None:\n\t\tself.__converter = _converting._Converter(self.__style, self.__parameter_style, self.__parameter_style_modifier)\n\treturn self.__converter\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self.parameter_style, self._parameter_names, self._metadata)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n\tif self._converter is None:\n\t\tself._converter = _converting._Converter(self)\n\treturn self._converter\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(\n\t\t\t\t\t\t\tself.parameter_style,\n\t\t\t\t\t\t\tself.parameter_style_converter,\n\t\t\t\t\t\t\tself.parameter_style_converter_kwargs,\n\t\t\t\t\t\t\tself.parameter_style_converter_args\n\t\t\t\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._param_style, self._param_prefix, self._param_suffix, self._quote_char, self._escape_char)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._parameter_style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self.parameter_style)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._parameter_style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\tif self.__converter is None:\n\t\tself.__converter = _converting._Converter(self.__style, self.__parameter_style, self.__parameter_style_modifier)\n\treturn self.__converter\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self.parameter_style, self._parameter_names, self._metadata)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "ques_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\treturn self._parse(dt_str, **self.parserinfo.isoparse)\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\treturn self._parser.isoparse(dt_str)\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tif isinstance(dt_str, basestring):\n\t\tdt_str = StringIO(dt_str)\n\treturn ParserInfo._parse(dt_str, self.parserinfo, self.default)\n\n\n", "def isoparse(self, dt_str):\n\tpass\n\t\n\t\n\n", "def isoparse(self, dt_str):\n\tpass\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\treturn self._parse(dt_str, **self.parserinfo.isoparse)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\treturn self._parser.isoparse(dt_str)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tif isinstance(dt_str, basestring):\n\t\tdt_str = StringIO(dt_str)\n\treturn ParserInfo._parse(dt_str, self.parserinfo, self.default)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "ques_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tnow = _datetime.datetime.now()\n\tif default is None:\n\t\tdefault = now\n\telif isinstance(default, _datetime.date) and not isinstance(default, _datetime.datetime):\n\t\tdefault = _datetime.datetime.combine(default, _datetime.time(0, 0))\n\telif not isinstance(default, _datetime.datetime):\n\t\traise TypeError(\"Default must be datetime, date or None\")\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\tif res is None:\n\t\traise ParserError(\"unknown string format\")\n\tif res.tzname() is not None:\n\t\tif tzinfos is not None:\n\t\t\ttzname = res.tzname()\n\t\t\ttzinfo = tzinfos.get(tzname)\n\t\t\tif tzinfo is None:\n\t\t\t\ttzinfo, _ = get_timezone_name(tzname)\n\t\t\tif tzinfo is None:\n\t\t\t\ttz\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tself.check_timezone_awareness(default)\n\t# If timestr is unicode, make it a str (for backwards compatibility)\n\tif isinstance(timestr, unicode):\n\t\ttimestr = timestr.encode(self.default_unicode_type)\n\n\tif not isinstance(timestr, str):\n\t\traise TypeError(\"Parser must be a string or character stream, \"\n\t\t\t\t\t\t\"not '%s'\" % type(timestr).__name__)\n\n\tif default is None:\n\t\tdefault = self.default\n\n\t# Create a copy of the default so we can safely manipulate it\n\tret = default.replace(tzinfo=None)\n\n\t# NOTE: This is not the same as ret is default, because ret could be\n\t# default's class instead of default itself\n\tif ret is default:\n\t\tret = ret.__class__.now()\n\n\t# If the default has no timezone, we should ignore tzinfos\n\tif ret.tzinfo is None:\n\t\tignoretz = True\n\n\t# If ignoretz is True\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, strtypes):\n\t\ttimestr = timestr.strip()\n\telse:\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"{0!r}\".format(type(timestr)))\n\n\tif not timestr:\n\t\traise ParserError(\"Empty string\")\n\n\t# If the string is in ISO8601 format, then we can just use\n\t# dateutil's parser and return the results.\n\tif isoparse(timestr):\n\t\treturn isoparse(timestr)\n\n\t# If the string is in RFC822 format, then we can just use\n\t# dateutil's parser and return the results.\n\tif parsedate_tz(timestr):\n\t\treturn parse_datetime_rfc822(timestr)\n\n\t# If the string is in RFC2822 format, then we can just use\n\t# dateutil's parser and return the results.\n\tif parsedate_tz(timestr):\n\t\treturn parse_datetime_rfc2822(\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, \"\n\t\t\t\"not '%s'\" % type(timestr).__name__)\n\n\tresult, tokens = self._parse(timestr, **kwargs)\n\n\tif result is None:\n\t\traise ParserError(\"Unable to parse string \"\n\t\t\t\"'%s'\" % timestr)\n\n\tif default is not None and not isinstance(default, datetime.datetime):\n\t\traise TypeError(\"If provided, default must be a datetime instance\")\n\n\tif default is None:\n\t\tdt = datetime.datetime(*result[:6])\n\telse:\n\t\tdt = default.replace(**dict(zip(self.KEYS, result[:6])))\n\n\tif result[6] is None:\n\t\ttz = None\n\telse:\n\t\ttz = tzoffset(None, result[6])\n\n\tif not ignoretz and tz is not None:\n\t\tif tzinfos is not None:\n\t\t\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \" + type(timestr).__name__)\n\tif default is None or not hasattr(default, \"tzinfo\") or default.tzinfo is None:\n\t\tdefault_tzinfo = None\n\telse:\n\t\tdefault_tzinfo = default.tzinfo\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\tif res is None:\n\t\traise ParserError(\"Unable to parse date string %r\" % timestr)\n\tif ignoredtz:\n\t\tres = res.replace(tzinfo=None)\n\telif tzinfos is not None:\n\t\ttzinfo = tzinfos.get(res.tzname())\n\t\tif tzinfo is not None:\n\t\t\tres = res.replace(tzinfo=tzinfo)\n\telif default_tzinfo is not None:\n\t\tres = res.replace(tzinfo=default_tzinfo)\n\tif default is not None\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, basestring):\n\t\tif not timestr:\n\t\t\traise ValueError('empty string')\n\t\tresult, skipped_tokens = self._parse(timestr, **kwargs)\n\t\tif result is None:\n\t\t\traise ParserError(\"Unable to parse date string %r\" % timestr)\n\t\tif default is None:\n\t\t\tdefault = datetime(1, 1, 1, tzinfo=self.default_tzinfo)\n\t\tdefault = default.replace(\n\t\t\tyear=result.year or default.year,\n\t\t\tmonth=result.month or default.month,\n\t\t\tday=result.day or default.day,\n\t\t\thour=result.hour or default.hour,\n\t\t\tminute=result.minute or default.minute,\n\t\t\tsecond=result.second or default.second,\n\t\t\tmicrosecond=result.microsecond or default.microsecond)\n\t\tif result.tzname is not None and not ignoretz:\n\t\t\tif tzinfos is\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif default is None:\n\t\tdefault = datetime.datetime(datetime.MINYEAR, 1, 1, tzinfo=self.default_tzinfo)\n\telif isinstance(default, datetime.datetime):\n\t\tif default.tzinfo is None:\n\t\t\tdefault = default.replace(tzinfo=self.default_tzinfo)\n\telse:\n\t\traise TypeError(\"Default must be datetime or None\")\n\n\tif isinstance(timestr, basestring):\n\t\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\telse:\n\t\traise TypeError(\"Parser must be a string or character stream\")\n\n\tif res is None:\n\t\traise ParserError(\"Unknown string format:\", timestr)\n\n\ttry:\n\t\treturn self._build_naive(res, default, tzinfos, ignoretz)\n\texcept (ValueError, OverflowError):\n\t\traise ParserError(\"Invalid or unhandled string format:\", timestr)\n\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t# If no default is provided, the replace behavior is disabled.\n\tif default is None:\n\t\tdefault = _default_time(self.default_timezone)\n\n\t# Check if we have a default time.\n\thave_default_time = True\n\tif default.hour == 0 and default.minute == 0 and default.second == 0 and default.microsecond == 0:\n\t\thave_default_time = False\n\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\n\t# If we have a default time, and we didn't find a time in the string,\n\t# replace it with the default time.\n\tif not have_default_time and res.hour == 0 and res.minute == 0 and res.second == 0 and res.microsecond == 0:\n\t\tres = res.replace(hour=default.hour, minute=default.minute, second=default.second, microsecond=default.microsecond)\n\n\t# If we have a default date, and we didn't find a date in the string,\n\t# replace it with the default date.\n\tif res.year ==\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(default, datetime.datetime) and default.tzinfo:\n\t\tdefault = default.replace(tzinfo=None)\n\tret = self._parse(timestr, **kwargs)\n\tif ret is None:\n\t\traise ParserError(\"Unknown string format: %r\" % timestr)\n\t(errcode, year, month, day, hour, minute,\n\tsecond, microsecond, tzoffset, tzname) = ret\n\tif year == 0:\n\t\tyear = default.year if default else 1900\n\tif month == 0:\n\t\tmonth = default.month if default else 1\n\tif day == 0:\n\t\tday = default.day if default else 1\n\tif hour == tzoffset == tzname == None:\n\t\tif not default:\n\t\t\traise ParserError(\"No time supplied\")\n\t\thour = default.hour\n\t\tminute = default.minute\n\t\tsecond = default.second\n\t\tmicrosecond = default.microsecond\n\t\ttzoffset = default.tzinfo.utcoffset(None).seconds \\\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, unicode):\n\t\t# Convert unicode string to bytes using default encoding\n\t\ttimestr = timestr.encode(self.DefaultEncoding)\n\telif not isinstance(timestr, str):\n\t\traise TypeError(\"Parser must be a string or character stream, \"\n\t\t\t\t\t\t\"not \" + type(timestr).__name__)\n\n\tif self.parserinfo.yearfirst:\n\t\tkwargs.setdefault('yearfirst', True)\n\tif self.parserinfo.dayfirst:\n\t\tkwargs.setdefault('dayfirst', True)\n\n\tret = self._parse(timestr, **kwargs)\n\tif ret is None:\n\t\traise ParserError(\"Unable to parse date string %r\" % timestr)\n\n\tret, skip_tokens = ret\n\tif self.parserinfo.fuzzy:\n\t\tret = datetime(*ret[:6])\n\n\tif default is not None:\n\t\tif isinstance(default, datetime):\n\t\t\tdefault = default.tim\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tnow = _datetime.datetime.now()\n\tif default is None:\n\t\tdefault = now\n\telif isinstance(default, _datetime.date) and not isinstance(default, _datetime.datetime):\n\t\tdefault = _datetime.datetime.combine(default, _datetime.time(0, 0))\n\telif not isinstance(default, _datetime.datetime):\n\t\traise TypeError(\"Default must be datetime, date or None\")\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\tif res is None:\n\t\traise ParserError(\"unknown string format\")\n\tif res.tzname() is not None:\n\t\tif tzinfos is not None:\n\t\t\ttzname = res.tzname()\n\t\t\ttzinfo = tzinfos.get(tzname)\n\t\t\tif tzinfo is None:\n\t\t\t\ttzinfo, _ = get_timezone_name(tzname)\n\t\t\tif tzinfo is None:\n\t\t\t\ttz\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tself.check_timezone_awareness(default)\n\t# If timestr is unicode, make it a str (for backwards compatibility)\n\tif isinstance(timestr, unicode):\n\t\ttimestr = timestr.encode(self.default_unicode_type)\n\n\tif not isinstance(timestr, str):\n\t\traise TypeError(\"Parser must be a string or character stream, \"\n\t\t\t\t\t\t\"not '%s'\" % type(timestr).__name__)\n\n\tif default is None:\n\t\tdefault = self.default\n\n\t# Create a copy of the default so we can safely manipulate it\n\tret = default.replace(tzinfo=None)\n\n\t# NOTE: This is not the same as ret is default, because ret could be\n\t# default's class instead of default itself\n\tif ret is default:\n\t\tret = ret.__class__.now()\n\n\t# If the default has no timezone, we should ignore tzinfos\n\tif ret.tzinfo is None:\n\t\tignoretz = True\n\n\t# If ignoretz is True\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, strtypes):\n\t\ttimestr = timestr.strip()\n\telse:\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"{0!r}\".format(type(timestr)))\n\n\tif not timestr:\n\t\traise ParserError(\"Empty string\")\n\n\t# If the string is in ISO8601 format, then we can just use\n\t# dateutil's parser and return the results.\n\tif isoparse(timestr):\n\t\treturn isoparse(timestr)\n\n\t# If the string is in RFC822 format, then we can just use\n\t# dateutil's parser and return the results.\n\tif parsedate_tz(timestr):\n\t\treturn parse_datetime_rfc822(timestr)\n\n\t# If the string is in RFC2822 format, then we can just use\n\t# dateutil's parser and return the results.\n\tif parsedate_tz(timestr):\n\t\treturn parse_datetime_rfc2822(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, \"\n\t\t\t\"not '%s'\" % type(timestr).__name__)\n\n\tresult, tokens = self._parse(timestr, **kwargs)\n\n\tif result is None:\n\t\traise ParserError(\"Unable to parse string \"\n\t\t\t\"'%s'\" % timestr)\n\n\tif default is not None and not isinstance(default, datetime.datetime):\n\t\traise TypeError(\"If provided, default must be a datetime instance\")\n\n\tif default is None:\n\t\tdt = datetime.datetime(*result[:6])\n\telse:\n\t\tdt = default.replace(**dict(zip(self.KEYS, result[:6])))\n\n\tif result[6] is None:\n\t\ttz = None\n\telse:\n\t\ttz = tzoffset(None, result[6])\n\n\tif not ignoretz and tz is not None:\n\t\tif tzinfos is not None:\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \" + type(timestr).__name__)\n\tif default is None or not hasattr(default, \"tzinfo\") or default.tzinfo is None:\n\t\tdefault_tzinfo = None\n\telse:\n\t\tdefault_tzinfo = default.tzinfo\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\tif res is None:\n\t\traise ParserError(\"Unable to parse date string %r\" % timestr)\n\tif ignoredtz:\n\t\tres = res.replace(tzinfo=None)\n\telif tzinfos is not None:\n\t\ttzinfo = tzinfos.get(res.tzname())\n\t\tif tzinfo is not None:\n\t\t\tres = res.replace(tzinfo=tzinfo)\n\telif default_tzinfo is not None:\n\t\tres = res.replace(tzinfo=default_tzinfo)\n\tif default is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, basestring):\n\t\tif not timestr:\n\t\t\traise ValueError('empty string')\n\t\tresult, skipped_tokens = self._parse(timestr, **kwargs)\n\t\tif result is None:\n\t\t\traise ParserError(\"Unable to parse date string %r\" % timestr)\n\t\tif default is None:\n\t\t\tdefault = datetime(1, 1, 1, tzinfo=self.default_tzinfo)\n\t\tdefault = default.replace(\n\t\t\tyear=result.year or default.year,\n\t\t\tmonth=result.month or default.month,\n\t\t\tday=result.day or default.day,\n\t\t\thour=result.hour or default.hour,\n\t\t\tminute=result.minute or default.minute,\n\t\t\tsecond=result.second or default.second,\n\t\t\tmicrosecond=result.microsecond or default.microsecond)\n\t\tif result.tzname is not None and not ignoretz:\n\t\t\tif tzinfos is\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif default is None:\n\t\tdefault = datetime.datetime(datetime.MINYEAR, 1, 1, tzinfo=self.default_tzinfo)\n\telif isinstance(default, datetime.datetime):\n\t\tif default.tzinfo is None:\n\t\t\tdefault = default.replace(tzinfo=self.default_tzinfo)\n\telse:\n\t\traise TypeError(\"Default must be datetime or None\")\n\n\tif isinstance(timestr, basestring):\n\t\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\telse:\n\t\traise TypeError(\"Parser must be a string or character stream\")\n\n\tif res is None:\n\t\traise ParserError(\"Unknown string format:\", timestr)\n\n\ttry:\n\t\treturn self._build_naive(res, default, tzinfos, ignoretz)\n\texcept (ValueError, OverflowError):\n\t\traise ParserError(\"Invalid or unhandled string format:\", timestr)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t# If no default is provided, the replace behavior is disabled.\n\tif default is None:\n\t\tdefault = _default_time(self.default_timezone)\n\n\t# Check if we have a default time.\n\thave_default_time = True\n\tif default.hour == 0 and default.minute == 0 and default.second == 0 and default.microsecond == 0:\n\t\thave_default_time = False\n\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\n\t# If we have a default time, and we didn't find a time in the string,\n\t# replace it with the default time.\n\tif not have_default_time and res.hour == 0 and res.minute == 0 and res.second == 0 and res.microsecond == 0:\n\t\tres = res.replace(hour=default.hour, minute=default.minute, second=default.second, microsecond=default.microsecond)\n\n\t# If we have a default date, and we didn't find a date in the string,\n\t# replace it with the default date.\n\tif res.year ==\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(default, datetime.datetime) and default.tzinfo:\n\t\tdefault = default.replace(tzinfo=None)\n\tret = self._parse(timestr, **kwargs)\n\tif ret is None:\n\t\traise ParserError(\"Unknown string format: %r\" % timestr)\n\t(errcode, year, month, day, hour, minute,\n\tsecond, microsecond, tzoffset, tzname) = ret\n\tif year == 0:\n\t\tyear = default.year if default else 1900\n\tif month == 0:\n\t\tmonth = default.month if default else 1\n\tif day == 0:\n\t\tday = default.day if default else 1\n\tif hour == tzoffset == tzname == None:\n\t\tif not default:\n\t\t\traise ParserError(\"No time supplied\")\n\t\thour = default.hour\n\t\tminute = default.minute\n\t\tsecond = default.second\n\t\tmicrosecond = default.microsecond\n\t\ttzoffset = default.tzinfo.utcoffset(None).seconds \\\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, unicode):\n\t\t# Convert unicode string to bytes using default encoding\n\t\ttimestr = timestr.encode(self.DefaultEncoding)\n\telif not isinstance(timestr, str):\n\t\traise TypeError(\"Parser must be a string or character stream, \"\n\t\t\t\t\t\t\"not \" + type(timestr).__name__)\n\n\tif self.parserinfo.yearfirst:\n\t\tkwargs.setdefault('yearfirst', True)\n\tif self.parserinfo.dayfirst:\n\t\tkwargs.setdefault('dayfirst', True)\n\n\tret = self._parse(timestr, **kwargs)\n\tif ret is None:\n\t\traise ParserError(\"Unable to parse date string %r\" % timestr)\n\n\tret, skip_tokens = ret\n\tif self.parserinfo.fuzzy:\n\t\tret = datetime(*ret[:6])\n\n\tif default is not None:\n\t\tif isinstance(default, datetime):\n\t\t\tdefault = default.tim\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n\t\tassert(dt.tzinfo is self)\n\t\tif dt.tzinfo is None:\n\t\t\traise TypeError(\"fromutc() requires a naive datetime\")\n\t\tif self._utcoffset(dt) is None:\n\t\t\traise TypeError(\"fromutc() requires a non-folded datetime\")\n\t\treturn self._fromutc(dt)\n\n\t@abstractmethod\n\tdef _fromutc(self, dt):\n\t\t\"\"\"\n\t\tInternal method. Given a timezone-aware datetime in a given timezone,\n\t\tcalculates a timezone-aware datetime in a new timezone.\n\t\t\"\"\"\n\t\tpass\n\n\tdef normalize(self, dt):\n\t\t\"\"\"\n\t\tGiven a timezone-aware datetime, calculate a new timezone-aware datetime\n\t\tin the same timezone, but with the fold attribute set correctly.\n\t\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\traise TypeError(\"normalize() requires a naive datetime\")\n\t\tif dt.tzinfo is self\n", "def fromutc(self, dt):\n\t\t_check_utc_offset(\"dt\", dt.tzinfo.utcoffset(dt))\n\t\t\n\t\tif dt.tzinfo is self:\n\t\t\treturn dt\n\t\t\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"dt must be timezone aware\")\n\t\t\n\t\tutc_epoch = dt.replace(tzinfo=self._tzinfo._utc).timestamp()\n\t\t\n\t\tif self.is_ambiguous(utc_epoch):\n\t\t\tfold = self.fold\n\t\t\t\n\t\t\t# If we're in a fold, use the end of the interval.\n\t\t\tif self.fold:\n\t\t\t\tutc_epoch = self._tzinfo._utc_transition_times[self._tzinfo._transition_info[utc_epoch]]\n\t\t\t\n\t\t\t# If we're in a fold, use the end of the interval.\n\t\t\tif self.fold:\n\t\t\t\tutc_epoch = self._tzinfo._\n", "def fromutc(self, dt):\n\tif not isinstance(dt, datetime.datetime):\n\t\traise TypeError(\"fromutc() requires a datetime.datetime\")\n\tif not dt.tzinfo:\n\t\traise ValueError(\"fromutc() requires a timezone-aware datetime\")\n\tif dt.tzinfo != self:\n\t\traise ValueError(\"fromutc() requires a datetime in this timezone\")\n\tdt = dt.replace(tzinfo=None)\n\tutc_offset = self._utcoffset\n\tstd_offset = self._stdoffset\n\tdst = datetime.timedelta(seconds=std_offset) - datetime.timedelta(seconds=utc_offset)\n\tdt = dt + datetime.timedelta(seconds=utc_offset)\n\toffset = std_offset\n\tif self._tzinfos:\n\t\ttzinfo = self._tzinfos.get(dt)\n\t\tif tzinfo:\n\t\t\toffset = tzinfo['offset']\n\t\t\tdst = tzinfo['dst']\n\tdt = dt\n", "def fromutc(self, dt):\n\t\tif dt.tzinfo is None:\n\t\t\traise TypeError(\"Expected a timezone-aware datetime, got a naive datetime.\")\n\t\tif dt.tzinfo is self:\n\t\t\treturn dt\n\t\treturn self.localize(dt, is_dst=None)\n\n\tdef tzname(self, dt):\n\t\treturn self._tznames[self.zone]\n\n\tdef dst(self, dt):\n\t\treturn timedelta(hours=self.dst(dt))\n\n\tdef __repr__(self):\n\t\treturn \"pytz.FixedOffset({0})\".format(self.zone)\n\n\n", "def fromutc(self, dt):\n\tif not dt.tzinfo:\n\t\traise ValueError('fromutc: dt is naive')\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tdt = dt.astimezone(self)\n\t# determine if the datetime is ambiguous, and in a \"fold\" state\n\t# (e.g. if it's the first occurrence, chronologically, of the\n\t# ambiguous datetime).\n\tdt = self._find_fold(dt)\n\treturn dt\n\n\n", "def fromutc(self, dt):\n\tif dt.tzinfo is self:\n\t\traise ValueError(\"dt.tzinfo is self\")\n\t# Convert datetime to UTC, then to new timezone.\n\tdt = dt.astimezone(pytz.utc).astimezone(self)\n\n\t# Now we know we have a timezone-aware datetime object in the\n\t# local timezone. If it's ambiguous, we need to determine whether\n\t# it's in a \"fold\" or not.\n\tif dt.tzinfo._ambiguous:\n\t\t# This is the one time we know we have an unambiguous\n\t\t# datetime. We can then use this to calculate the fold\n\t\t# status.\n\t\tdt = dt.replace(fold=1)\n\n\treturn dt\n\n\n", "def fromutc(self, dt):\n\tassert dt.tzinfo == self\n\n\t# We need to determine whether or not this datetime is in the fold\n\t# for this timezone.\n\tif self._utc_transition_times and dt in self._transition_info:\n\t\t# We know that we have a fold, and we know that the datetime\n\t\t# is in a fold.\n\t\tfold = True\n\telse:\n\t\t# We know that we don't have a fold, or we know that the\n\t\t# datetime is not in a fold.\n\t\tfold = False\n\n\tutc_dt = dt.replace(tzinfo=self._utc_transition_time)\n\treturn self._tzinfos[self._transition_info[utc_dt]].fromutc(utc_dt, fold)\n\n", "def fromutc(self, dt):\n\tdt_fold = dt.fold\n\tdt_utcoffset = dt.utcoffset()\n\tdt_timestamp = dt.timestamp()\n\tdt_tzname = dt.tzname()\n\tdt_date = dt.date()\n\n\t# convert to UTC\n\tdt_utc = dt.astimezone(pytz.utc)\n\n\t# convert to new timezone\n\tdt_tz = self.localize(dt_utc.replace(tzinfo=None), is_dst=None)\n\n\tif dt_fold:\n\t\t# check if a fold transition has occurred\n\t\tif dt_tz.timestamp() < dt_timestamp:\n\t\t\t# fold transition occurred - use first instance\n\t\t\tdt_tz = dt_tz.replace(fold=0)\n\t\telse:\n\t\t\t# no fold transition occurred - use second instance\n\t\t\tdt_tz = dt_tz.replace(fold=1)\n\n\t# reset timezone name and UTC offset\n\tdt_tz = dt_tz.replace(tz\n", "def fromutc(self, dt):\n\tif not isinstance(dt, datetime):\n\t\traise ValueError(\"dt must be a datetime.datetime instance\")\n\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"dt must be a timezone-aware datetime.datetime\")\n\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tdt_tzinfo = self._tzinfo_cache.get(dt.tzinfo)\n\tif not dt_tzinfo:\n\t\tdt_tzinfo = self._tzinfo_cache[dt.tzinfo] = self._tzinfo_from_tz(dt.tzinfo)\n\n\tdt = dt.astimezone(dt_tzinfo)\n\tlocal_dt = dt.astimezone(self._tz)\n\t# If the datetime is ambiguous, we need to determine whether this is\n\t# the first or second occurrence.\n\tif local_dt.replace(tzinfo=None) in self._dst_transition_times:\n\t\tlocal_dt_first = self._dst_transition_times[local_dt.replace(tzinfo=None\n", "def fromutc(self, dt):\n\t\tif dt.tzinfo is not self:\n\t\t\traise ValueError(\"dt.tzinfo is not self\")\n\n\t\tdst_change = self._utc_transition_times.searchsorted(dt, side='left')\n\t\tif dst_change > 0 and self._utc_transition_times[dst_change - 1] > dt:\n\t\t\tdst_change -= 1\n\n\t\tif dst_change < len(self._utc_transition_times) and self._utc_transition_times[dst_change] == dt:\n\t\t\t# This datetime is ambiguous, and in the \"fold\" state.\n\t\t\tdst_offset = self._transition_info[dst_change][0]\n\t\t\tfold = 1\n\t\telse:\n\t\t\t# This datetime is unambiguous, or ambiguous but not in the\n\t\t\t# \"fold\" state.\n\t\t\tdst_offset = self._transition_info[dst_change][0]\n\t\t\tfold = 0\n\n\t\tutc_offset =\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n\t\tassert(dt.tzinfo is self)\n\t\tif dt.tzinfo is None:\n\t\t\traise TypeError(\"fromutc() requires a naive datetime\")\n\t\tif self._utcoffset(dt) is None:\n\t\t\traise TypeError(\"fromutc() requires a non-folded datetime\")\n\t\treturn self._fromutc(dt)\n\n\t@abstractmethod\n\tdef _fromutc(self, dt):\n\t\t\"\"\"\n\t\tInternal method. Given a timezone-aware datetime in a given timezone,\n\t\tcalculates a timezone-aware datetime in a new timezone.\n\t\t\"\"\"\n\t\tpass\n\n\tdef normalize(self, dt):\n\t\t\"\"\"\n\t\tGiven a timezone-aware datetime, calculate a new timezone-aware datetime\n\t\tin the same timezone, but with the fold attribute set correctly.\n\t\t\"\"\"\n\t\tif dt.tzinfo is None:\n\t\t\traise TypeError(\"normalize() requires a naive datetime\")\n\t\tif dt.tzinfo is self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\t_check_utc_offset(\"dt\", dt.tzinfo.utcoffset(dt))\n\t\t\n\t\tif dt.tzinfo is self:\n\t\t\treturn dt\n\t\t\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"dt must be timezone aware\")\n\t\t\n\t\tutc_epoch = dt.replace(tzinfo=self._tzinfo._utc).timestamp()\n\t\t\n\t\tif self.is_ambiguous(utc_epoch):\n\t\t\tfold = self.fold\n\t\t\t\n\t\t\t# If we're in a fold, use the end of the interval.\n\t\t\tif self.fold:\n\t\t\t\tutc_epoch = self._tzinfo._utc_transition_times[self._tzinfo._transition_info[utc_epoch]]\n\t\t\t\n\t\t\t# If we're in a fold, use the end of the interval.\n\t\t\tif self.fold:\n\t\t\t\tutc_epoch = self._tzinfo._\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tif not isinstance(dt, datetime.datetime):\n\t\traise TypeError(\"fromutc() requires a datetime.datetime\")\n\tif not dt.tzinfo:\n\t\traise ValueError(\"fromutc() requires a timezone-aware datetime\")\n\tif dt.tzinfo != self:\n\t\traise ValueError(\"fromutc() requires a datetime in this timezone\")\n\tdt = dt.replace(tzinfo=None)\n\tutc_offset = self._utcoffset\n\tstd_offset = self._stdoffset\n\tdst = datetime.timedelta(seconds=std_offset) - datetime.timedelta(seconds=utc_offset)\n\tdt = dt + datetime.timedelta(seconds=utc_offset)\n\toffset = std_offset\n\tif self._tzinfos:\n\t\ttzinfo = self._tzinfos.get(dt)\n\t\tif tzinfo:\n\t\t\toffset = tzinfo['offset']\n\t\t\tdst = tzinfo['dst']\n\tdt = dt\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\tif dt.tzinfo is None:\n\t\t\traise TypeError(\"Expected a timezone-aware datetime, got a naive datetime.\")\n\t\tif dt.tzinfo is self:\n\t\t\treturn dt\n\t\treturn self.localize(dt, is_dst=None)\n\n\tdef tzname(self, dt):\n\t\treturn self._tznames[self.zone]\n\n\tdef dst(self, dt):\n\t\treturn timedelta(hours=self.dst(dt))\n\n\tdef __repr__(self):\n\t\treturn \"pytz.FixedOffset({0})\".format(self.zone)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tif not dt.tzinfo:\n\t\traise ValueError('fromutc: dt is naive')\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tdt = dt.astimezone(self)\n\t# determine if the datetime is ambiguous, and in a \"fold\" state\n\t# (e.g. if it's the first occurrence, chronologically, of the\n\t# ambiguous datetime).\n\tdt = self._find_fold(dt)\n\treturn dt\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tif dt.tzinfo is self:\n\t\traise ValueError(\"dt.tzinfo is self\")\n\t# Convert datetime to UTC, then to new timezone.\n\tdt = dt.astimezone(pytz.utc).astimezone(self)\n\n\t# Now we know we have a timezone-aware datetime object in the\n\t# local timezone. If it's ambiguous, we need to determine whether\n\t# it's in a \"fold\" or not.\n\tif dt.tzinfo._ambiguous:\n\t\t# This is the one time we know we have an unambiguous\n\t\t# datetime. We can then use this to calculate the fold\n\t\t# status.\n\t\tdt = dt.replace(fold=1)\n\n\treturn dt\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tassert dt.tzinfo == self\n\n\t# We need to determine whether or not this datetime is in the fold\n\t# for this timezone.\n\tif self._utc_transition_times and dt in self._transition_info:\n\t\t# We know that we have a fold, and we know that the datetime\n\t\t# is in a fold.\n\t\tfold = True\n\telse:\n\t\t# We know that we don't have a fold, or we know that the\n\t\t# datetime is not in a fold.\n\t\tfold = False\n\n\tutc_dt = dt.replace(tzinfo=self._utc_transition_time)\n\treturn self._tzinfos[self._transition_info[utc_dt]].fromutc(utc_dt, fold)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tdt_fold = dt.fold\n\tdt_utcoffset = dt.utcoffset()\n\tdt_timestamp = dt.timestamp()\n\tdt_tzname = dt.tzname()\n\tdt_date = dt.date()\n\n\t# convert to UTC\n\tdt_utc = dt.astimezone(pytz.utc)\n\n\t# convert to new timezone\n\tdt_tz = self.localize(dt_utc.replace(tzinfo=None), is_dst=None)\n\n\tif dt_fold:\n\t\t# check if a fold transition has occurred\n\t\tif dt_tz.timestamp() < dt_timestamp:\n\t\t\t# fold transition occurred - use first instance\n\t\t\tdt_tz = dt_tz.replace(fold=0)\n\t\telse:\n\t\t\t# no fold transition occurred - use second instance\n\t\t\tdt_tz = dt_tz.replace(fold=1)\n\n\t# reset timezone name and UTC offset\n\tdt_tz = dt_tz.replace(tz\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tif not isinstance(dt, datetime):\n\t\traise ValueError(\"dt must be a datetime.datetime instance\")\n\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"dt must be a timezone-aware datetime.datetime\")\n\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tdt_tzinfo = self._tzinfo_cache.get(dt.tzinfo)\n\tif not dt_tzinfo:\n\t\tdt_tzinfo = self._tzinfo_cache[dt.tzinfo] = self._tzinfo_from_tz(dt.tzinfo)\n\n\tdt = dt.astimezone(dt_tzinfo)\n\tlocal_dt = dt.astimezone(self._tz)\n\t# If the datetime is ambiguous, we need to determine whether this is\n\t# the first or second occurrence.\n\tif local_dt.replace(tzinfo=None) in self._dst_transition_times:\n\t\tlocal_dt_first = self._dst_transition_times[local_dt.replace(tzinfo=None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\tif dt.tzinfo is not self:\n\t\t\traise ValueError(\"dt.tzinfo is not self\")\n\n\t\tdst_change = self._utc_transition_times.searchsorted(dt, side='left')\n\t\tif dst_change > 0 and self._utc_transition_times[dst_change - 1] > dt:\n\t\t\tdst_change -= 1\n\n\t\tif dst_change < len(self._utc_transition_times) and self._utc_transition_times[dst_change] == dt:\n\t\t\t# This datetime is ambiguous, and in the \"fold\" state.\n\t\t\tdst_offset = self._transition_info[dst_change][0]\n\t\t\tfold = 1\n\t\telse:\n\t\t\t# This datetime is unambiguous, or ambiguous but not in the\n\t\t\t# \"fold\" state.\n\t\t\tdst_offset = self._transition_info[dst_change][0]\n\t\t\tfold = 0\n\n\t\tutc_offset =\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "ques_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\tif not dt.tzinfo:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\treturn dt if dt.tzinfo is not None else dt.replace(tzinfo=tzinfo)\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo = tzinfo)\n\telse:\n\t\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt is None or dt.tzinfo is not None:\n\t\treturn dt\n\treturn dt.replace(tzinfo=tzinfo)\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif not dt.tzinfo:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\treturn dt if dt.tzinfo is not None else dt.replace(tzinfo=tzinfo)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo = tzinfo)\n\telse:\n\t\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt is None or dt.tzinfo is not None:\n\t\treturn dt\n\treturn dt.replace(tzinfo=tzinfo)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "ques_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tif not isinstance(before, bytes):\n\t\t\traise TypeError(\"'before' must be bytes\")\n\t\tif not isinstance(after, bytes):\n\t\t\traise TypeError(\"'after' must be bytes\")\n\t\tself._cut_chars[0] = before\n\t\tself._cut_chars[1] = after\n\n\tdef get_cut_chars(self) -> Tuple[bytes, bytes]:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tif type(before) != bytes:\n\t\traise TypeError(\"before must be bytes\")\n\tif type(after) != bytes:\n\t\traise TypeError(\"after must be bytes\")\n\n\tself._cut_chars_before = before\n\tself._cut_chars_after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself._cut_before = before\n\tself._cut_after = after\n\tself._reset()\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\traise NotImplementedError()\n\n\tdef set_keep_unmatched_chunks(self, keep: bool) -> None:\n\t\"\"\"\n\tSet whether to keep unmatched chunks.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_before = before\n\tself.cut_after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\t\tself.cut_after_regex = re.compile(after)\n\n\tdef split_file(self, file_name: str, chunk_size: int = 104857600) -> bytes:\n\t\t\"\"\"\n\t\tSplit a file into chunks.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_before = before\n\tself.cut_after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself._before = before\n\t\tself._after = after\n\t\n\tdef set_split_size(self, split_size: int) -> None:\n\t\"\"\"\n\tSet the number of bytes to read before splitting the file.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself._cut_chars = (before, after)\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself._before = before\n\tself._after = after\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tif not isinstance(before, bytes):\n\t\t\traise TypeError(\"'before' must be bytes\")\n\t\tif not isinstance(after, bytes):\n\t\t\traise TypeError(\"'after' must be bytes\")\n\t\tself._cut_chars[0] = before\n\t\tself._cut_chars[1] = after\n\n\tdef get_cut_chars(self) -> Tuple[bytes, bytes]:\n\t\t\"\"\"\n\t\tGet the bytes used to delimit slice points.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tif type(before) != bytes:\n\t\traise TypeError(\"before must be bytes\")\n\tif type(after) != bytes:\n\t\traise TypeError(\"after must be bytes\")\n\n\tself._cut_chars_before = before\n\tself._cut_chars_after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself._cut_before = before\n\tself._cut_after = after\n\tself._reset()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\traise NotImplementedError()\n\n\tdef set_keep_unmatched_chunks(self, keep: bool) -> None:\n\t\"\"\"\n\tSet whether to keep unmatched chunks.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_before = before\n\tself.cut_after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\t\tself.cut_after_regex = re.compile(after)\n\n\tdef split_file(self, file_name: str, chunk_size: int = 104857600) -> bytes:\n\t\t\"\"\"\n\t\tSplit a file into chunks.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_before = before\n\tself.cut_after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself._before = before\n\t\tself._after = after\n\t\n\tdef set_split_size(self, split_size: int) -> None:\n\t\"\"\"\n\tSet the number of bytes to read before splitting the file.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself._cut_chars = (before, after)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself._before = before\n\tself._after = after\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "ques_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: RequestType):\n\ttry:\n\t\trequest.message.unpack_payload()\n\texcept ValueError:\n\t\treturn None\n\tif request.message.type == MessageType.LEGACY_SHARE:\n\t\treturn LegacyRequest()\n\tif request.message.payload.get('type') == 'Profile':\n\t\treturn ProfileRequest()\n\tif request.message.payload.get('type') == 'StatusMessage':\n\t\treturn StatusMessageRequest()\n\tif request.message.payload.get('type') == 'Reshare':\n\t\treturn ReshareRequest()\n\tif request.message.payload.get('type') == 'Retraction':\n\t\treturn RetractionRequest()\n\treturn None\n\n\n", "def identify_request(request: RequestType):\n\tif request.message is not None:\n\t\tif request.message.method == \"public\":\n\t\t\treturn request.message.payload\n\t\tif request.message.method == \"private\":\n\t\t\treturn request.message.payload\n\tif request.payload is not None:\n\t\treturn request.payload\n\tif request.legacy is not None:\n\t\treturn request.legacy\n\treturn None\n\n\n", "def identify_request(request: RequestType):\n\titems = request.get('items', [])\n\tif items:\n\t\tfor item in items:\n\t\t\tif isinstance(item, dict):\n\t\t\t\tif 'public' in item:\n\t\t\t\t\tif 'diaspora' in item['public']:\n\t\t\t\t\t\treturn 'public'\n\t\t\t\tif 'private' in item:\n\t\t\t\t\tif 'diaspora' in item['private']:\n\t\t\t\t\t\treturn 'private'\n\t\treturn 'legacy'\n\treturn None\n\n\n", "def identify_request(request: RequestType):\n\ttry:\n\t\tmessage = Message.from_dict(request.data, request.sender)\n\t\tif message is not False:\n\t\t\treturn message\n\texcept Exception:\n\t\tpass\n\n\ttry:\n\t\tmessage = EncryptedMessage.from_dict(request.data, request.sender)\n\t\tif message is not False:\n\t\t\treturn message\n\texcept Exception:\n\t\tpass\n\n\ttry:\n\t\tmessage = LegacyMessage.from_dict(request.data, request.sender)\n\t\tif message is not False:\n\t\t\treturn message\n\texcept Exception:\n\t\tpass\n\n\treturn False\n", "def identify_request(request: RequestType):\n\tif 'message' in request.json:\n\t\treturn 'legacy'\n\tif 'public' in request.json:\n\t\treturn 'public'\n\tif 'private' in request.json:\n\t\treturn 'private'\n\treturn None\n\n\n", "def identify_request(request: RequestType):\n\ttry:\n\t\tif request.message.get(\"type\", None) == \"public\":\n\t\t\treturn request.message.get(\"payload\", None)\n\texcept AttributeError:\n\t\tpass\n\ttry:\n\t\tif request.message.get(\"type\", None) == \"private\":\n\t\t\treturn request.message.get(\"payload\", None)\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn request.message\n\texcept AttributeError:\n\t\tpass\n\n\treturn None\n\n\n", "def identify_request(request: RequestType):\n\tif \"public\" in request:\n\t\treturn \"public\"\n\tif \"private\" in request:\n\t\treturn \"private\"\n\tif \"encrypted_magic_envelope\" in request:\n\t\treturn \"legacy\"\n\treturn None\n\n\n", "def identify_request(request: RequestType):\n\tif \"public_message\" in request:\n\t\treturn \"public_message\"\n\tif \"private_message\" in request:\n\t\treturn \"private_message\"\n\tif \"legacy_payload\" in request:\n\t\treturn \"legacy_payload\"\n\treturn None\n\n\n", "def identify_request(request: RequestType):\n\tif is_diaspora_public_message(request):\n\t\treturn DiasporaPublicMessageRequest\n\tif is_diaspora_private_message(request):\n\t\treturn DiasporaPrivateMessageRequest\n\tif is_diaspora_legacy_payload(request):\n\t\treturn DiasporaLegacyPayloadRequest\n\treturn None\n", "def identify_request(request: RequestType):\n\tif request.type == \"public\":\n\t\treturn identify_public_request(request)\n\telif request.type == \"private\":\n\t\treturn identify_private_request(request)\n\telse:\n\t\treturn False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType):\n\ttry:\n\t\trequest.message.unpack_payload()\n\texcept ValueError:\n\t\treturn None\n\tif request.message.type == MessageType.LEGACY_SHARE:\n\t\treturn LegacyRequest()\n\tif request.message.payload.get('type') == 'Profile':\n\t\treturn ProfileRequest()\n\tif request.message.payload.get('type') == 'StatusMessage':\n\t\treturn StatusMessageRequest()\n\tif request.message.payload.get('type') == 'Reshare':\n\t\treturn ReshareRequest()\n\tif request.message.payload.get('type') == 'Retraction':\n\t\treturn RetractionRequest()\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif request.message is not None:\n\t\tif request.message.method == \"public\":\n\t\t\treturn request.message.payload\n\t\tif request.message.method == \"private\":\n\t\t\treturn request.message.payload\n\tif request.payload is not None:\n\t\treturn request.payload\n\tif request.legacy is not None:\n\t\treturn request.legacy\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\titems = request.get('items', [])\n\tif items:\n\t\tfor item in items:\n\t\t\tif isinstance(item, dict):\n\t\t\t\tif 'public' in item:\n\t\t\t\t\tif 'diaspora' in item['public']:\n\t\t\t\t\t\treturn 'public'\n\t\t\t\tif 'private' in item:\n\t\t\t\t\tif 'diaspora' in item['private']:\n\t\t\t\t\t\treturn 'private'\n\t\treturn 'legacy'\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\ttry:\n\t\tmessage = Message.from_dict(request.data, request.sender)\n\t\tif message is not False:\n\t\t\treturn message\n\texcept Exception:\n\t\tpass\n\n\ttry:\n\t\tmessage = EncryptedMessage.from_dict(request.data, request.sender)\n\t\tif message is not False:\n\t\t\treturn message\n\texcept Exception:\n\t\tpass\n\n\ttry:\n\t\tmessage = LegacyMessage.from_dict(request.data, request.sender)\n\t\tif message is not False:\n\t\t\treturn message\n\texcept Exception:\n\t\tpass\n\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif 'message' in request.json:\n\t\treturn 'legacy'\n\tif 'public' in request.json:\n\t\treturn 'public'\n\tif 'private' in request.json:\n\t\treturn 'private'\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\ttry:\n\t\tif request.message.get(\"type\", None) == \"public\":\n\t\t\treturn request.message.get(\"payload\", None)\n\texcept AttributeError:\n\t\tpass\n\ttry:\n\t\tif request.message.get(\"type\", None) == \"private\":\n\t\t\treturn request.message.get(\"payload\", None)\n\texcept AttributeError:\n\t\tpass\n\n\ttry:\n\t\treturn request.message\n\texcept AttributeError:\n\t\tpass\n\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif \"public\" in request:\n\t\treturn \"public\"\n\tif \"private\" in request:\n\t\treturn \"private\"\n\tif \"encrypted_magic_envelope\" in request:\n\t\treturn \"legacy\"\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif \"public_message\" in request:\n\t\treturn \"public_message\"\n\tif \"private_message\" in request:\n\t\treturn \"private_message\"\n\tif \"legacy_payload\" in request:\n\t\treturn \"legacy_payload\"\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif is_diaspora_public_message(request):\n\t\treturn DiasporaPublicMessageRequest\n\tif is_diaspora_private_message(request):\n\t\treturn DiasporaPrivateMessageRequest\n\tif is_diaspora_legacy_payload(request):\n\t\treturn DiasporaLegacyPayloadRequest\n\treturn None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif request.type == \"public\":\n\t\treturn identify_public_request(request)\n\telif request.type == \"private\":\n\t\treturn identify_private_request(request)\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "ques_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: RequestType) -> bool:\n\tif request.headers.get('User-Agent', '').startswith('Matrix'):\n\t\treturn True\n\n\tif request.headers.get('Authorization', '').startswith('Bearer '):\n\t\treturn True\n\n\tif 'Content-Type' in request.headers and request.headers['Content-Type'] == 'application/json':\n\t\treturn True\n\n\tif 'content-type' in request.headers and request.headers['content-type'] == 'application/json':\n\t\treturn True\n\n\tif 'Content-type' in request.headers and request.headers['Content-type'] == 'application/json':\n\t\treturn True\n\n\tif 'content-Type' in request.headers and request.headers['content-Type'] == 'application/json':\n\t\treturn True\n\n\treturn False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\tif request.headers.get(\"Authorization\") and \"Bearer\" in request.headers[\"Authorization\"]:\n\t\treturn True\n\telif request.query.get(\"access_token\"):\n\t\treturn True\n\treturn False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn \"matrix-org\" in request.headers.get(\"User-Agent\", \"\")\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn request.headers.get(\"User-Agent\", \"\").startswith(\"Matrix\")\n\n\n", "def identify_request(request: RequestType) -> bool:\n\tif request.method in [\"GET\", \"POST\"] and request.headers.get(\"User-Agent\") == \"Matrix/0.0.1\":\n\t\tif request.path.startswith(\"/_matrix/\"):\n\t\t\treturn True\n\treturn False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\tif request.headers.get(\"Authorization\") and \"Bearer\" in request.headers.get(\"Authorization\"):\n\t\treturn True\n\tif request.headers.get(\"User-Agent\") and \"Matrix\" in request.headers.get(\"User-Agent\"):\n\t\treturn True\n\tif request.headers.get(\"X-Matrix-Request-ID\"):\n\t\treturn True\n\tif request.headers.get(\"X-Matrix-Client\"):\n\t\treturn True\n\treturn False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn \"Content-Type\" in request.headers and request.headers[\"Content-Type\"] == \"application/json\"\n", "def identify_request(request: RequestType) -> bool:\n\tif \"HTTP_ORIGIN\" not in request.META:\n\t\treturn False\n\tif request.META[\"HTTP_ORIGIN\"].startswith(\"https://matrix.to\"):\n\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\t# FIXME: this is a really hacky way of doing it\n\t# Matrix is supposed to send its user-agent, but the server doesn't seem to send it\n\treturn request.headers.get('X-Matrix-Client-Is') is not None\n", "def identify_request(request: RequestType) -> bool:\n\ttry:\n\t\tif request.headers.get(\"User-Agent\", \"\").startswith(\"Python-Matrix-Client\"):\n\t\t\treturn True\n\t\telif request.headers.get(\"Origin\", \"\").startswith(\"https://matrix.org\"):\n\t\t\treturn True\n\t\telif request.headers.get(\"Origin\", \"\").startswith(\"https://matrix.to\"):\n\t\t\treturn True\n\t\telif request.headers.get(\"Origin\", \"\").startswith(\"https://riot.im\"):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif request.headers.get('User-Agent', '').startswith('Matrix'):\n\t\treturn True\n\n\tif request.headers.get('Authorization', '').startswith('Bearer '):\n\t\treturn True\n\n\tif 'Content-Type' in request.headers and request.headers['Content-Type'] == 'application/json':\n\t\treturn True\n\n\tif 'content-type' in request.headers and request.headers['content-type'] == 'application/json':\n\t\treturn True\n\n\tif 'Content-type' in request.headers and request.headers['Content-type'] == 'application/json':\n\t\treturn True\n\n\tif 'content-Type' in request.headers and request.headers['content-Type'] == 'application/json':\n\t\treturn True\n\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif request.headers.get(\"Authorization\") and \"Bearer\" in request.headers[\"Authorization\"]:\n\t\treturn True\n\telif request.query.get(\"access_token\"):\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn \"matrix-org\" in request.headers.get(\"User-Agent\", \"\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn request.headers.get(\"User-Agent\", \"\").startswith(\"Matrix\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif request.method in [\"GET\", \"POST\"] and request.headers.get(\"User-Agent\") == \"Matrix/0.0.1\":\n\t\tif request.path.startswith(\"/_matrix/\"):\n\t\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif request.headers.get(\"Authorization\") and \"Bearer\" in request.headers.get(\"Authorization\"):\n\t\treturn True\n\tif request.headers.get(\"User-Agent\") and \"Matrix\" in request.headers.get(\"User-Agent\"):\n\t\treturn True\n\tif request.headers.get(\"X-Matrix-Request-ID\"):\n\t\treturn True\n\tif request.headers.get(\"X-Matrix-Client\"):\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn \"Content-Type\" in request.headers and request.headers[\"Content-Type\"] == \"application/json\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif \"HTTP_ORIGIN\" not in request.META:\n\t\treturn False\n\tif request.META[\"HTTP_ORIGIN\"].startswith(\"https://matrix.to\"):\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\t# FIXME: this is a really hacky way of doing it\n\t# Matrix is supposed to send its user-agent, but the server doesn't seem to send it\n\treturn request.headers.get('X-Matrix-Client-Is') is not None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\ttry:\n\t\tif request.headers.get(\"User-Agent\", \"\").startswith(\"Python-Matrix-Client\"):\n\t\t\treturn True\n\t\telif request.headers.get(\"Origin\", \"\").startswith(\"https://matrix.org\"):\n\t\t\treturn True\n\t\telif request.headers.get(\"Origin\", \"\").startswith(\"https://matrix.to\"):\n\t\t\treturn True\n\t\telif request.headers.get(\"Origin\", \"\").startswith(\"https://riot.im\"):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "ques_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n\n", "def format_dt(dt):\n\treturn dt.strftime(\"%Y%m%d%H%M%S\")\n\n\n", "def format_dt(dt):\n\treturn dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n", "def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S')\n\n", "def format_dt(dt):\n\treturn '%sT%s' % (dt.strftime('%Y-%m-%d'), dt.strftime('%H:%M:%S'))\n\n", "def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n\n", "def format_dt(dt):\n\treturn dt.strftime('%Y%m%dT%H%M%S')\n\n\n", "def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n", "def format_dt(dt):\n\treturn strftime(dt, \"%Y%m%d%H%M%S\")\n\n", "def format_dt(dt):\n\treturn dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S+00:00')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn dt.strftime(\"%Y%m%d%H%M%S\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%S')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn '%sT%s' % (dt.strftime('%Y-%m-%d'), dt.strftime('%H:%M:%S'))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%d %H:%M:%S')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn dt.strftime('%Y%m%dT%H%M%S')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn dt.strftime('%Y-%m-%dT%H:%M:%SZ')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn strftime(dt, \"%Y%m%d%H%M%S\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "ques_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tin_code = False\n\tfor word in text.split():\n\t\tif word == \"```\":\n\t\t\tin_code = not in_code\n\t\telif not in_code:\n\t\t\tif word.startswith(\"#\"):\n\t\t\t\ttag = word[1:]\n\t\t\t\ttags.add(tag)\n\t\t\t\tif replacer is not None:\n\t\t\t\t\ttext = text.replace(word, replacer(tag))\n\treturn tags, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif not text:\n\t\treturn set(), text\n\n\tmatch_tags = re.findall(r'`(.+?)`', text)\n\tmatch_tags += re.findall(r'(?<!`)([\\w-]+)(?!`)', text)\n\ttags = set(match_tags)\n\tif replacer:\n\t\ttext = re.sub(r'(?<!`)\\b([\\w-]+)\\b(?!`)', lambda m: replacer(m.group(1)), text)\n\treturn tags, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tcode_block = False\n\n\tlines = []\n\tfor line in text.splitlines():\n\t\tif not code_block:\n\t\t\tif line.strip().startswith(CODE_BLOCK_START):\n\t\t\t\tcode_block = True\n\t\t\telse:\n\t\t\t\tfor tag in TAG_RE.findall(line):\n\t\t\t\t\ttags.add(tag)\n\t\t\t\t\tif replacer:\n\t\t\t\t\t\tline = re.sub(r'\\B#{0}\\b'.format(re.escape(tag)), lambda m: replacer(m.group(0)), line)\n\n\t\tif code_block:\n\t\t\tif line.strip().startswith(CODE_BLOCK_END):\n\t\t\t\tcode_block = False\n\t\tlines.append(line)\n\n\treturn tags, '\\n'.join(lines)\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\t# Ignore tags inside code blocks\n\ttext = re.sub(r'```(.*?)```', '', text, flags=re.DOTALL)\n\n\t# Find tags\n\ttags = set(re.findall(TAG_REGEX, text))\n\ttags = set(map(lambda x: x.lower(), tags))\n\n\t# Replace tags in text\n\ttext = re.sub(TAG_REGEX, lambda x: replacer(x.group(0)), text)\n\n\treturn tags, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t# Warn if the API is used incorrectly.\n\tif not isinstance(text, str):\n\t\traise TypeError(\"find_tags(text) expected a string, got {}\".format(type(text)))\n\n\t# The regex will match tags as long as they don't contain a colon (:).\n\t# It will also match tags that are inside backticks.\n\t# Note: This will not match tags that are inside code blocks.\n\t#       This will match tags that are inside inline code.\n\t#       This will match tags that are inside backticks in code blocks.\n\t#       This will not match tags in backticks inside inline code.\n\t#       This will match tags in backticks inside backticks in code blocks.\n\t#       This will match tags in backticks inside backticks in inline code.\n\t#       This will match tags in backticks in backticks in backticks in code blocks.\n\t#       This will match tags in backticks in backticks in backticks in inline code.\n\t#       This will match tags in backticks in backticks in backticks in back\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\t# scan for code blocks\n\tcode_blocks = []\n\tfor match in RE_CODE_BLOCK.finditer(text):\n\t\tcode_blocks.append(match.group())\n\t\t# replace with some placeholder text\n\t\ttext = text.replace(match.group(), '\\n')\n\t# now scan for tags\n\tfor match in RE_TAG.finditer(text):\n\t\tif match.group().lower() not in code_blocks:\n\t\t\t# found a tag\n\t\t\ttags.add(match.group().lower())\n\t\t\t# replace with replacer\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(match.group(), replacer(match.group()))\n\treturn tags, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\tdef _replace_tag(tag):\n\t\treturn replacer(tag.group(1))\n\n\t# Find all the tags\n\ttags = set()\n\tfor match in re.finditer(r\"\\B#(\\w+)\", text):\n\t\ttags.add(match.group(1))\n\n\t# Split the text into code blocks and plain text blocks\n\tcode_block = False\n\tcode_blocks = []\n\tplain_text = []\n\tcurrent_block = []\n\tfor line in text.splitlines():\n\t\tif not code_block and line.startswith(\"```\"):\n\t\t\tcode_block = True\n\t\t\tcurrent_block = []\n\t\telif code_block and line.startswith(\"```\"):\n\t\t\tcode_block = False\n\t\t\tcode_blocks.append(\"\\n\".join(current_block))\n\t\telse:\n\t\t\tif code_block:\n\t\t\t\tcurrent_block.append(line)\n\t\t\telse:\n\t\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\ttags = set()\n\t# Split into lines\n\tlines = text.split('\\n')\n\t# Keep track of the line index for replacement\n\tline_index = 0\n\t# Keep track of whether we're inside a code block\n\tin_code = False\n\tfor line in lines:\n\t\t# Check if a code block is starting or ending\n\t\tif not in_code and line.startswith('```'):\n\t\t\tin_code = True\n\t\telif in_code and line.startswith('```'):\n\t\t\tin_code = False\n\n\t\tif not in_code:\n\t\t\t# The tag regex\n\t\t\tmatch = re.search(TAG_REGEX, line)\n\t\t\tif match is None:\n\t\t\t\t# No tag\n\t\t\t\tline_index += 1\n\t\t\t\tcontinue\n\n\t\t\t# Found a tag, replace it with the replacer\n\t\t\tnew_line = replacer(match.group(2))\n\t\t\t\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t# we use a raw string for this, because we don't want backslashes to\n\t# be interpreted as escape characters.\n\ttag_pattern = r\"(\\s|^|\\b|#)[#+]\\w*\\b\"\n\tregex = re.compile(tag_pattern, re.IGNORECASE)\n\tmatches = regex.findall(text)\n\ttags = {i[1:].lower() for i in matches}\n\tif replacer:\n\t\ttext = regex.sub(lambda x: replacer(x.group()), text)\n\treturn tags, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tcode_blocks = find_code_blocks(text)\n\n\tfor tag in TAG_REGEX.findall(text):\n\t\tif replacer:\n\t\t\ttext = text.replace(tag, replacer(tag))\n\t\tif not tag_in_code_block(tag, code_blocks):\n\t\t\ttags.add(tag)\n\treturn tags, text\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tin_code = False\n\tfor word in text.split():\n\t\tif word == \"```\":\n\t\t\tin_code = not in_code\n\t\telif not in_code:\n\t\t\tif word.startswith(\"#\"):\n\t\t\t\ttag = word[1:]\n\t\t\t\ttags.add(tag)\n\t\t\t\tif replacer is not None:\n\t\t\t\t\ttext = text.replace(word, replacer(tag))\n\treturn tags, text\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif not text:\n\t\treturn set(), text\n\n\tmatch_tags = re.findall(r'`(.+?)`', text)\n\tmatch_tags += re.findall(r'(?<!`)([\\w-]+)(?!`)', text)\n\ttags = set(match_tags)\n\tif replacer:\n\t\ttext = re.sub(r'(?<!`)\\b([\\w-]+)\\b(?!`)', lambda m: replacer(m.group(1)), text)\n\treturn tags, text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tcode_block = False\n\n\tlines = []\n\tfor line in text.splitlines():\n\t\tif not code_block:\n\t\t\tif line.strip().startswith(CODE_BLOCK_START):\n\t\t\t\tcode_block = True\n\t\t\telse:\n\t\t\t\tfor tag in TAG_RE.findall(line):\n\t\t\t\t\ttags.add(tag)\n\t\t\t\t\tif replacer:\n\t\t\t\t\t\tline = re.sub(r'\\B#{0}\\b'.format(re.escape(tag)), lambda m: replacer(m.group(0)), line)\n\n\t\tif code_block:\n\t\t\tif line.strip().startswith(CODE_BLOCK_END):\n\t\t\t\tcode_block = False\n\t\tlines.append(line)\n\n\treturn tags, '\\n'.join(lines)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\t# Ignore tags inside code blocks\n\ttext = re.sub(r'```(.*?)```', '', text, flags=re.DOTALL)\n\n\t# Find tags\n\ttags = set(re.findall(TAG_REGEX, text))\n\ttags = set(map(lambda x: x.lower(), tags))\n\n\t# Replace tags in text\n\ttext = re.sub(TAG_REGEX, lambda x: replacer(x.group(0)), text)\n\n\treturn tags, text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t# Warn if the API is used incorrectly.\n\tif not isinstance(text, str):\n\t\traise TypeError(\"find_tags(text) expected a string, got {}\".format(type(text)))\n\n\t# The regex will match tags as long as they don't contain a colon (:).\n\t# It will also match tags that are inside backticks.\n\t# Note: This will not match tags that are inside code blocks.\n\t#       This will match tags that are inside inline code.\n\t#       This will match tags that are inside backticks in code blocks.\n\t#       This will not match tags in backticks inside inline code.\n\t#       This will match tags in backticks inside backticks in code blocks.\n\t#       This will match tags in backticks inside backticks in inline code.\n\t#       This will match tags in backticks in backticks in backticks in code blocks.\n\t#       This will match tags in backticks in backticks in backticks in inline code.\n\t#       This will match tags in backticks in backticks in backticks in back\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\t# scan for code blocks\n\tcode_blocks = []\n\tfor match in RE_CODE_BLOCK.finditer(text):\n\t\tcode_blocks.append(match.group())\n\t\t# replace with some placeholder text\n\t\ttext = text.replace(match.group(), '\\n')\n\t# now scan for tags\n\tfor match in RE_TAG.finditer(text):\n\t\tif match.group().lower() not in code_blocks:\n\t\t\t# found a tag\n\t\t\ttags.add(match.group().lower())\n\t\t\t# replace with replacer\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(match.group(), replacer(match.group()))\n\treturn tags, text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\tdef _replace_tag(tag):\n\t\treturn replacer(tag.group(1))\n\n\t# Find all the tags\n\ttags = set()\n\tfor match in re.finditer(r\"\\B#(\\w+)\", text):\n\t\ttags.add(match.group(1))\n\n\t# Split the text into code blocks and plain text blocks\n\tcode_block = False\n\tcode_blocks = []\n\tplain_text = []\n\tcurrent_block = []\n\tfor line in text.splitlines():\n\t\tif not code_block and line.startswith(\"```\"):\n\t\t\tcode_block = True\n\t\t\tcurrent_block = []\n\t\telif code_block and line.startswith(\"```\"):\n\t\t\tcode_block = False\n\t\t\tcode_blocks.append(\"\\n\".join(current_block))\n\t\telse:\n\t\t\tif code_block:\n\t\t\t\tcurrent_block.append(line)\n\t\t\telse:\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\ttags = set()\n\t# Split into lines\n\tlines = text.split('\\n')\n\t# Keep track of the line index for replacement\n\tline_index = 0\n\t# Keep track of whether we're inside a code block\n\tin_code = False\n\tfor line in lines:\n\t\t# Check if a code block is starting or ending\n\t\tif not in_code and line.startswith('```'):\n\t\t\tin_code = True\n\t\telif in_code and line.startswith('```'):\n\t\t\tin_code = False\n\n\t\tif not in_code:\n\t\t\t# The tag regex\n\t\t\tmatch = re.search(TAG_REGEX, line)\n\t\t\tif match is None:\n\t\t\t\t# No tag\n\t\t\t\tline_index += 1\n\t\t\t\tcontinue\n\n\t\t\t# Found a tag, replace it with the replacer\n\t\t\tnew_line = replacer(match.group(2))\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\t# we use a raw string for this, because we don't want backslashes to\n\t# be interpreted as escape characters.\n\ttag_pattern = r\"(\\s|^|\\b|#)[#+]\\w*\\b\"\n\tregex = re.compile(tag_pattern, re.IGNORECASE)\n\tmatches = regex.findall(text)\n\ttags = {i[1:].lower() for i in matches}\n\tif replacer:\n\t\ttext = regex.sub(lambda x: replacer(x.group()), text)\n\treturn tags, text\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tcode_blocks = find_code_blocks(text)\n\n\tfor tag in TAG_REGEX.findall(text):\n\t\tif replacer:\n\t\t\ttext = text.replace(tag, replacer(tag))\n\t\tif not tag_in_code_block(tag, code_blocks):\n\t\t\ttags.add(tag)\n\treturn tags, text\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "ques_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n\t\n\t# Convert url links to a-tags\n\ttext = re.sub(r'(https?://[^\\s]+?)', r'<a href=\"\\1\" target=\"_blank\" rel=\"nofollow\">\\1</a>', text)\n\t\n\t# Convert email addresses to mailto: links\n\ttext = re.sub(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6})', r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"nofollow\">\\1</a>', text)\n\t\n\t# Convert newlines to <br> tags\n\ttext = re.sub(r'\\n', '<br>', text)\n\t\n\t# Convert tabs to &nbsp;\n\ttext = re.sub(r'\\t', '&nbsp;' * 4, text)\n\t\n\treturn text\n\n", "def process_text_links(text):\n\tlink_pattern = re.compile(r'\\[(.+?)\\]\\((.+?)\\)')\n\tlink_replacement = r'<a href=\"\\2\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>'\n\ttext = link_pattern.sub(link_replacement, text)\n\treturn text\n\n\n", "def process_text_links(text):\n\tfor link in LINK_REGEXP.finditer(text):\n\t\turl = link.group('url')\n\t\tif '@' in url:\n\t\t\turl = 'mailto:' + url\n\t\telif not url.startswith('/') and not url.startswith('#') and not url.startswith('http'):\n\t\t\turl = 'http://' + url\n\t\ttext = text.replace(link.group(), '<a href=\"%s\">%s</a>' % (url, link.group('text')), 1)\n\treturn text\n\n\n", "def process_text_links(text):\n\t# Linkify text links\n\ttext = re.sub(r'(https?|ftp)://[^\\s/$.?#].[^\\s]*', r'<a href=\"\\1://\\2\">\\1://\\2</a>', text)\n\t# Linkify \"text\" in \"text\":link\n\ttext = re.sub(r'\"([^\"]+)\"\\s*:\\s*([^\\s]+)()', r'<a href=\"\\2\">\\1</a>', text)\n\t# Linkify text in text:link\n\ttext = re.sub(r'([^\\s]+)\\s*:\\s*([^\\s]+)()', r'<a href=\"\\2\">\\1</a>', text)\n\treturn text\n\n", "def process_text_links(text):\n\turl_pattern = re.compile(r'\\(?https?://[-A-Za-z0-9+&@#/%?=~_()|!:,.;]*[-A-Za-z0-9+&@#/%=~_()|]')\n\ttext = url_pattern.sub(make_url_link, text)\n\treturn text\n\n", "def process_text_links(text):\n\t# Add target=\"_blank\" to all links\n\ttext = re.sub(r'<a ([^>]*)href=\"([^\"]*)\"([^>]*)>', r'<a \\1href=\"\\2\" target=\"_blank\"\\3>', text)\n\n\t# Add rel=\"nofollow\" to all links\n\ttext = re.sub(r'<a ([^>]*)href=\"([^\"]*)\"([^>]*)>', r'<a \\1href=\"\\2\" rel=\"nofollow\"\\3>', text)\n\n\t# Linkify textual links\n\ttext = re.sub(r'(https?://[^\\s<>\"]*[^\\s<>\"\\.])', r'<a href=\"\\1\">\\1</a>', text)\n\ttext = re.sub(r'([^\\/])(www\\.[^\\s<>\"]*[^\\s<>\"\\.])', r'\\1<a href=\"http://\\2\">\\2</a>', text)\n\n\t# Linkify email addresses\n\ttext = re.sub(r'([\n", "def process_text_links(text):\n\t# Linkify links in text\n\ttext = linkify(text)\n\n\t# Add attributes to links\n\tsoup = BeautifulSoup(text, 'html.parser')\n\tfor link in soup.find_all('a'):\n\t\tlink['target'] = '_blank'\n\t\tlink['rel'] = 'noopener'\n\n\treturn str(soup)\n\n\n", "def process_text_links(text):\n\ttext = re.sub(r'(https?://[^\\s]+)', r'<a href=\"\\1\" rel=\"nofollow\">\\1</a>', text)\n\ttext = re.sub(r'(www\\.[^\\s]+)', r'<a href=\"http://\\1\" rel=\"nofollow\">\\1</a>', text)\n\ttext = re.sub(r'([^\\s]+@[^\\s]+)', r'<a href=\"mailto:\\1\">\\1</a>', text)\n\treturn text\n\n", "def process_text_links(text):\n\ttext = re.sub(r'(<a[^>]*?href=\")([^\"]+?)(\"[^>]*?>)([^<]+?)(</a>)',\n\t\tr'\\1\\2\\3<span class=\"link-text\">\\4</span>\\5', text)\n\ttext = re.sub(r'(<a[^>]*?>)([^<]+?)(</a>)',\n\t\tr'\\1<span class=\"link-text\">\\2</span>\\3', text)\n\ttext = re.sub(r'(<a[^>]*?)(>)([^<]+?)(</a>)',\n\t\tr'\\1 class=\"link-text\"\\2\\3\\4', text)\n\ttext = re.sub(r'(<a[^>]*?href=\")([^\"]+?)(\"[^>]*?>)([^<]+?)(</a>)',\n\t\tr'\\1\\2\\3<span class=\"link-text\">\\4</span>\\5', text)\n\ttext =\n", "def process_text_links(text):\n\ttext = text.replace('\\n', '<br/>')\n\ttext = text.replace('\\r', '')\n\ttext = text.replace('\\t', '&nbsp;' * 4)\n\n\t# linkify textual links\n\ttext = re.sub(r'(https?://[^\\s]+)', r'<a href=\"\\1\" target=\"_blank\">\\1</a>', text)\n\n\treturn text\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n\t\n\t# Convert url links to a-tags\n\ttext = re.sub(r'(https?://[^\\s]+?)', r'<a href=\"\\1\" target=\"_blank\" rel=\"nofollow\">\\1</a>', text)\n\t\n\t# Convert email addresses to mailto: links\n\ttext = re.sub(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,6})', r'<a href=\"mailto:\\1\" target=\"_blank\" rel=\"nofollow\">\\1</a>', text)\n\t\n\t# Convert newlines to <br> tags\n\ttext = re.sub(r'\\n', '<br>', text)\n\t\n\t# Convert tabs to &nbsp;\n\ttext = re.sub(r'\\t', '&nbsp;' * 4, text)\n\t\n\treturn text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\tlink_pattern = re.compile(r'\\[(.+?)\\]\\((.+?)\\)')\n\tlink_replacement = r'<a href=\"\\2\" target=\"_blank\" rel=\"noopener noreferrer\">\\1</a>'\n\ttext = link_pattern.sub(link_replacement, text)\n\treturn text\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\tfor link in LINK_REGEXP.finditer(text):\n\t\turl = link.group('url')\n\t\tif '@' in url:\n\t\t\turl = 'mailto:' + url\n\t\telif not url.startswith('/') and not url.startswith('#') and not url.startswith('http'):\n\t\t\turl = 'http://' + url\n\t\ttext = text.replace(link.group(), '<a href=\"%s\">%s</a>' % (url, link.group('text')), 1)\n\treturn text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t# Linkify text links\n\ttext = re.sub(r'(https?|ftp)://[^\\s/$.?#].[^\\s]*', r'<a href=\"\\1://\\2\">\\1://\\2</a>', text)\n\t# Linkify \"text\" in \"text\":link\n\ttext = re.sub(r'\"([^\"]+)\"\\s*:\\s*([^\\s]+)()', r'<a href=\"\\2\">\\1</a>', text)\n\t# Linkify text in text:link\n\ttext = re.sub(r'([^\\s]+)\\s*:\\s*([^\\s]+)()', r'<a href=\"\\2\">\\1</a>', text)\n\treturn text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\turl_pattern = re.compile(r'\\(?https?://[-A-Za-z0-9+&@#/%?=~_()|!:,.;]*[-A-Za-z0-9+&@#/%=~_()|]')\n\ttext = url_pattern.sub(make_url_link, text)\n\treturn text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t# Add target=\"_blank\" to all links\n\ttext = re.sub(r'<a ([^>]*)href=\"([^\"]*)\"([^>]*)>', r'<a \\1href=\"\\2\" target=\"_blank\"\\3>', text)\n\n\t# Add rel=\"nofollow\" to all links\n\ttext = re.sub(r'<a ([^>]*)href=\"([^\"]*)\"([^>]*)>', r'<a \\1href=\"\\2\" rel=\"nofollow\"\\3>', text)\n\n\t# Linkify textual links\n\ttext = re.sub(r'(https?://[^\\s<>\"]*[^\\s<>\"\\.])', r'<a href=\"\\1\">\\1</a>', text)\n\ttext = re.sub(r'([^\\/])(www\\.[^\\s<>\"]*[^\\s<>\"\\.])', r'\\1<a href=\"http://\\2\">\\2</a>', text)\n\n\t# Linkify email addresses\n\ttext = re.sub(r'([\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t# Linkify links in text\n\ttext = linkify(text)\n\n\t# Add attributes to links\n\tsoup = BeautifulSoup(text, 'html.parser')\n\tfor link in soup.find_all('a'):\n\t\tlink['target'] = '_blank'\n\t\tlink['rel'] = 'noopener'\n\n\treturn str(soup)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\ttext = re.sub(r'(https?://[^\\s]+)', r'<a href=\"\\1\" rel=\"nofollow\">\\1</a>', text)\n\ttext = re.sub(r'(www\\.[^\\s]+)', r'<a href=\"http://\\1\" rel=\"nofollow\">\\1</a>', text)\n\ttext = re.sub(r'([^\\s]+@[^\\s]+)', r'<a href=\"mailto:\\1\">\\1</a>', text)\n\treturn text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\ttext = re.sub(r'(<a[^>]*?href=\")([^\"]+?)(\"[^>]*?>)([^<]+?)(</a>)',\n\t\tr'\\1\\2\\3<span class=\"link-text\">\\4</span>\\5', text)\n\ttext = re.sub(r'(<a[^>]*?>)([^<]+?)(</a>)',\n\t\tr'\\1<span class=\"link-text\">\\2</span>\\3', text)\n\ttext = re.sub(r'(<a[^>]*?)(>)([^<]+?)(</a>)',\n\t\tr'\\1 class=\"link-text\"\\2\\3\\4', text)\n\ttext = re.sub(r'(<a[^>]*?href=\")([^\"]+?)(\"[^>]*?>)([^<]+?)(</a>)',\n\t\tr'\\1\\2\\3<span class=\"link-text\">\\4</span>\\5', text)\n\ttext =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\ttext = text.replace('\\n', '<br/>')\n\ttext = text.replace('\\r', '')\n\ttext = text.replace('\\t', '&nbsp;' * 4)\n\n\t# linkify textual links\n\ttext = re.sub(r'(https?://[^\\s]+)', r'<a href=\"\\1\" target=\"_blank\">\\1</a>', text)\n\n\treturn text\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "ques_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\twith urllib.request.urlopen(url, timeout=3) as response:\n\t\t\tcontent_type = response.headers.get_content_type()\n\t\t\treturn content_type\n\texcept urllib.error.URLError:\n\t\treturn None\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\treq = requests.head(url)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n\treturn req.headers.get(\"content-type\", None)\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\twith urllib.request.urlopen(url, timeout=5) as conn:\n\t\t\theaders = conn.info()\n\t\t\tif headers.get_content_type().startswith(\"image/\"):\n\t\t\t\treturn headers.get_content_type()\n\texcept (urllib.error.HTTPError, urllib.error.URLError, ConnectionResetError, TimeoutError):\n\t\tpass\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url)\n\t\tif response.status_code == 200:\n\t\t\treturn response.headers.get('content-type')\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\theaders = requests.head(url, timeout=5, allow_redirects=True).headers\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\treturn headers.get('content-type')\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tcontent_type = requests.head(url, allow_redirects=True).headers.get('content-type')\n\texcept requests.RequestException:\n\t\tlogger.warning(f\"Failed to fetch content type of {url}\")\n\t\treturn None\n\treturn content_type\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\tr = requests.head(url)\n\tif r.status_code == 200:\n\t\treturn r.headers.get(\"content-type\")\n\treturn None\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url)\n\texcept requests.exceptions.RequestException as error:\n\t\t_LOGGER.error(\"Error fetching content type for %s: %s\", url, error)\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\t_LOGGER.error(\"Error fetching content type for %s: %s\", url, response.status_code)\n\t\treturn None\n\n\treturn response.headers.get(\"content-type\")\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, verify=False)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\treturn response.headers.get(\"content-type\")\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url)\n\texcept RequestException:\n\t\tlogger.exception(\"Unable to fetch content type\")\n\t\treturn None\n\n\treturn response.headers.get(\"content-type\") or None\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\twith urllib.request.urlopen(url, timeout=3) as response:\n\t\t\tcontent_type = response.headers.get_content_type()\n\t\t\treturn content_type\n\texcept urllib.error.URLError:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\treq = requests.head(url)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n\treturn req.headers.get(\"content-type\", None)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\twith urllib.request.urlopen(url, timeout=5) as conn:\n\t\t\theaders = conn.info()\n\t\t\tif headers.get_content_type().startswith(\"image/\"):\n\t\t\t\treturn headers.get_content_type()\n\texcept (urllib.error.HTTPError, urllib.error.URLError, ConnectionResetError, TimeoutError):\n\t\tpass\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url)\n\t\tif response.status_code == 200:\n\t\t\treturn response.headers.get('content-type')\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\theaders = requests.head(url, timeout=5, allow_redirects=True).headers\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\treturn headers.get('content-type')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tcontent_type = requests.head(url, allow_redirects=True).headers.get('content-type')\n\texcept requests.RequestException:\n\t\tlogger.warning(f\"Failed to fetch content type of {url}\")\n\t\treturn None\n\treturn content_type\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\tr = requests.head(url)\n\tif r.status_code == 200:\n\t\treturn r.headers.get(\"content-type\")\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url)\n\texcept requests.exceptions.RequestException as error:\n\t\t_LOGGER.error(\"Error fetching content type for %s: %s\", url, error)\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\t_LOGGER.error(\"Error fetching content type for %s: %s\", url, response.status_code)\n\t\treturn None\n\n\treturn response.headers.get(\"content-type\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, verify=False)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\treturn response.headers.get(\"content-type\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url)\n\texcept RequestException:\n\t\tlogger.exception(\"Unable to fetch content type\")\n\t\treturn None\n\n\treturn response.headers.get(\"content-type\") or None\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "ques_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n\tif tag in _STOP_WORDS:\n\t\treturn False\n\tif tag.startswith('-'):\n\t\treturn False\n\tif tag.startswith('_'):\n\t\treturn False\n\tif tag.endswith('_'):\n\t\treturn False\n\tif len(tag) == 1:\n\t\treturn False\n\tif tag.isdigit():\n\t\treturn False\n\tif tag.isupper():\n\t\treturn False\n\treturn True\n\n\n", "def test_tag(tag: str) -> bool:\n\tif tag == \"\":\n\t\treturn False\n\telif tag.startswith(\"@\"):\n\t\treturn False\n\telif tag.startswith(\"#\"):\n\t\treturn False\n\telif tag.startswith(\"http://\"):\n\t\treturn False\n\telif tag.startswith(\"https://\"):\n\t\treturn False\n\telif tag.startswith(\"www\"):\n\t\treturn False\n\telif tag.startswith(\"/\"):\n\t\treturn False\n\telif tag.endswith(\"/\"):\n\t\treturn False\n\telif tag.endswith(\".html\"):\n\t\treturn False\n\telse:\n\t\treturn True\n\n\n", "def test_tag(tag: str) -> bool:\n\treturn (not tag.endswith('_')) and (not tag.endswith('-')) and (not tag.endswith('.')) and (not tag.startswith('-')) and (not tag.startswith('.')) and (not ' ' in tag) and (not '\\t' in tag) and (not '\\n' in tag) and (not '\\r' in tag) and (not '\\v' in tag) and (not '\\f' in tag) and (not '\\0' in tag) and (tag != \"\") and (tag != \" \") and (not '\\\\' in tag) and (not '/' in tag) and (not '(' in tag) and (not ')' in tag) and (not '[' in tag) and (not ']' in tag) and (not '{' in tag) and (not '}' in tag) and (not '`' in tag) and (not '~' in tag) and (not '!' in tag) and (not '@' in tag) and (not '#' in tag) and (not '$' in tag\n", "def test_tag(tag: str) -> bool:\n\tif tag in tag_set:\n\t\treturn True\n\tfor tag_name in tag_set:\n\t\tif tag_name.startswith(tag):\n\t\t\treturn True\n\treturn False\n\n\n", "def test_tag(tag: str) -> bool:\n\treturn tag in TAG_SET\n\n\n", "def test_tag(tag: str) -> bool:\n\treturn re.fullmatch(TAG_PATTERN, tag, re.I) is not None\n\n\n", "def test_tag(tag: str) -> bool:\n\treturn bool(re.fullmatch(r\"^[a-zA-Z0-9_]+$\", tag))\n\n", "def test_tag(tag: str) -> bool:\n\treturn re.fullmatch(r\"^[a-zA-Z][a-zA-Z0-9]*$\", tag) is not None\n\n", "def test_tag(tag: str) -> bool:\n\tif not tag:\n\t\treturn False\n\tfor c in tag:\n\t\tif not c.isalpha():\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\tif not tag.isalpha():\n\t\treturn False\n\tif len(tag) <= 1:\n\t\treturn False\n\tif tag in __stopwords:\n\t\treturn False\n\treturn True\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n\tif tag in _STOP_WORDS:\n\t\treturn False\n\tif tag.startswith('-'):\n\t\treturn False\n\tif tag.startswith('_'):\n\t\treturn False\n\tif tag.endswith('_'):\n\t\treturn False\n\tif len(tag) == 1:\n\t\treturn False\n\tif tag.isdigit():\n\t\treturn False\n\tif tag.isupper():\n\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tif tag == \"\":\n\t\treturn False\n\telif tag.startswith(\"@\"):\n\t\treturn False\n\telif tag.startswith(\"#\"):\n\t\treturn False\n\telif tag.startswith(\"http://\"):\n\t\treturn False\n\telif tag.startswith(\"https://\"):\n\t\treturn False\n\telif tag.startswith(\"www\"):\n\t\treturn False\n\telif tag.startswith(\"/\"):\n\t\treturn False\n\telif tag.endswith(\"/\"):\n\t\treturn False\n\telif tag.endswith(\".html\"):\n\t\treturn False\n\telse:\n\t\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n\treturn (not tag.endswith('_')) and (not tag.endswith('-')) and (not tag.endswith('.')) and (not tag.startswith('-')) and (not tag.startswith('.')) and (not ' ' in tag) and (not '\\t' in tag) and (not '\\n' in tag) and (not '\\r' in tag) and (not '\\v' in tag) and (not '\\f' in tag) and (not '\\0' in tag) and (tag != \"\") and (tag != \" \") and (not '\\\\' in tag) and (not '/' in tag) and (not '(' in tag) and (not ')' in tag) and (not '[' in tag) and (not ']' in tag) and (not '{' in tag) and (not '}' in tag) and (not '`' in tag) and (not '~' in tag) and (not '!' in tag) and (not '@' in tag) and (not '#' in tag) and (not '$' in tag\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tif tag in tag_set:\n\t\treturn True\n\tfor tag_name in tag_set:\n\t\tif tag_name.startswith(tag):\n\t\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\treturn tag in TAG_SET\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\treturn re.fullmatch(TAG_PATTERN, tag, re.I) is not None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\treturn bool(re.fullmatch(r\"^[a-zA-Z0-9_]+$\", tag))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n\treturn re.fullmatch(r\"^[a-zA-Z][a-zA-Z0-9]*$\", tag) is not None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tif not tag:\n\t\treturn False\n\tfor c in tag:\n\t\tif not c.isalpha():\n\t\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tif not tag.isalpha():\n\t\treturn False\n\tif len(tag) <= 1:\n\t\treturn False\n\tif tag in __stopwords:\n\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "ques_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == child.ELEMENT_NODE:\n\t\t\td[child.tagName] = child.toxml()\n\treturn d\n\n", "def xml_children_as_dict(node):\n\tret = {}\n\tfor n in node.childNodes:\n\t\tif n.nodeType == n.ELEMENT_NODE:\n\t\t\tif not n.tagName in ret:\n\t\t\t\tret[n.tagName] = []\n\t\t\tret[n.tagName].append(n)\n\treturn ret\n\n\n\n", "def xml_children_as_dict(node):\n\treturn {child.tag: child for child in node}\n\n\n", "def xml_children_as_dict(node):\n\tresult = {}\n\tfor child in node.getchildren():\n\t\tresult[child.tag] = child\n\treturn result\n", "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.children:\n\t\tif child.tag not in d:\n\t\t\td[child.tag] = [child]\n\t\telse:\n\t\t\td[child.tag].append(child)\n\treturn d\n\n\n", "def xml_children_as_dict(node):\n\tret = {}\n\tfor n in node.childNodes:\n\t\tif n.nodeType == n.ELEMENT_NODE:\n\t\t\tret[n.tagName] = n\n\treturn ret\n\n", "def xml_children_as_dict(node):\n\treturn {c.tag: c for c in node}\n\n", "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == xml.dom.Node.ELEMENT_NODE:\n\t\t\td[child.tagName] = child\n\treturn d\n\n\n", "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.getchildren():\n\t\td[child.tag] = child\n\treturn d\n\n", "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.getchildren():\n\t\td[child.tag] = child.text\n\treturn d\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == child.ELEMENT_NODE:\n\t\t\td[child.tagName] = child.toxml()\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\tret = {}\n\tfor n in node.childNodes:\n\t\tif n.nodeType == n.ELEMENT_NODE:\n\t\t\tif not n.tagName in ret:\n\t\t\t\tret[n.tagName] = []\n\t\t\tret[n.tagName].append(n)\n\treturn ret\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\treturn {child.tag: child for child in node}\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\tresult = {}\n\tfor child in node.getchildren():\n\t\tresult[child.tag] = child\n\treturn result\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.children:\n\t\tif child.tag not in d:\n\t\t\td[child.tag] = [child]\n\t\telse:\n\t\t\td[child.tag].append(child)\n\treturn d\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\tret = {}\n\tfor n in node.childNodes:\n\t\tif n.nodeType == n.ELEMENT_NODE:\n\t\t\tret[n.tagName] = n\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\treturn {c.tag: c for c in node}\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == xml.dom.Node.ELEMENT_NODE:\n\t\t\td[child.tagName] = child\n\treturn d\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.getchildren():\n\t\td[child.tag] = child\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.getchildren():\n\t\td[child.tag] = child.text\n\treturn d\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "ques_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise DiasporaException(\"Sender and entity handle mismatch\")\n\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise exceptions.InvalidPayloadFormatException(\n\t\t\t\"sender with handle '{sender_handle}' is not the same as \"\n\t\t\t\"entity with handle '{entity_handle}'\".format(\n\t\t\t\tsender_handle=sender_handle,\n\t\t\t\tentity_handle=entity_handle\n\t\t\t)\n\t\t)\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise DiasporaException(\n\t\t\t\"Sender handle does not match entity handle!\",\n\t\t\t\"The sender handle and entity handle do not match. This means the sender is trying to spoof a post.\",\n\t\t\t\"The sender handle was: \" + sender_handle + \". The entity handle was: \" + entity_handle + \".\",\n\t\t\t\"Ensure that the sender handle matches the entity handle.\",\n\t\t)\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\tlogger.debug(\"Sender doesn't match entity: %s vs %s\" % (sender_handle, entity_handle))\n\t\traise exceptions.PayloadException(\"Sender doesn't match entity\")\n\n\treturn True\n\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\tlog.error(\"Sender handle (%s) and entity handle (%s) do not match\", sender_handle, entity_handle)\n\t\treturn False\n\treturn True\n\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise AuthError(\n\t\t\t'Sender handle in payload headers differs from entity handle in payload body.'\n\t\t)\n\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise InvalidHandle(\"Payload sender and entity handle do not match\")\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif not sender_handle == entity_handle:\n\t\traise ValidationError(\"Sender and Entity handle mismatch.\")\n\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\treturn sender_handle == entity_handle\n\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle and entity_handle:\n\t\tif sender_handle != entity_handle:\n\t\t\traise ValueError('Payload sender handle and entity handle don\\'t match. '\n\t\t\t\t\t\t\t 'This is a serious error and we must bail.')\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise DiasporaException(\"Sender and entity handle mismatch\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise exceptions.InvalidPayloadFormatException(\n\t\t\t\"sender with handle '{sender_handle}' is not the same as \"\n\t\t\t\"entity with handle '{entity_handle}'\".format(\n\t\t\t\tsender_handle=sender_handle,\n\t\t\t\tentity_handle=entity_handle\n\t\t\t)\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise DiasporaException(\n\t\t\t\"Sender handle does not match entity handle!\",\n\t\t\t\"The sender handle and entity handle do not match. This means the sender is trying to spoof a post.\",\n\t\t\t\"The sender handle was: \" + sender_handle + \". The entity handle was: \" + entity_handle + \".\",\n\t\t\t\"Ensure that the sender handle matches the entity handle.\",\n\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\tlogger.debug(\"Sender doesn't match entity: %s vs %s\" % (sender_handle, entity_handle))\n\t\traise exceptions.PayloadException(\"Sender doesn't match entity\")\n\n\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\tlog.error(\"Sender handle (%s) and entity handle (%s) do not match\", sender_handle, entity_handle)\n\t\treturn False\n\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise AuthError(\n\t\t\t'Sender handle in payload headers differs from entity handle in payload body.'\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle != entity_handle:\n\t\traise InvalidHandle(\"Payload sender and entity handle do not match\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif not sender_handle == entity_handle:\n\t\traise ValidationError(\"Sender and Entity handle mismatch.\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\treturn sender_handle == entity_handle\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle and entity_handle:\n\t\tif sender_handle != entity_handle:\n\t\t\traise ValueError('Payload sender handle and entity handle don\\'t match. '\n\t\t\t\t\t\t\t 'This is a serious error and we must bail.')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "ques_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n\tnodeinfo = {}\n\tnodeinfo['links'] = []\n\tnodeinfo['links'].append({'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0', 'href': url + '/' + (document_path or 'nodeinfo/2.0')})\n\n\treturn nodeinfo\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif not document_path:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\tbase_url = 'https://{}'.format(url)\n\tprotocol_version = '2.0'\n\tservices = [\n\t\t'inbound',\n\t\t'outbound',\n\t\t'push',\n\t\t'c2s',\n\t\t'p2p',\n\t\t'p2p-webrtc-direct',\n\t\t'p2p-webrtc-star',\n\t\t'p2p-websocket-star',\n\t\t'turn',\n\t\t'stun',\n\t\t'storage',\n\t\t'vcard4',\n\t\t'vcard4-push',\n\t\t'vcard4-webpush',\n\t\t'vcard4-explorer',\n\t\t'profile',\n\t\t'profile-push',\n\t\t'profile-webpush',\n\t\t'profile-explorer',\n\t\t'http-file-upload',\n\t\t'http-file-download'\n\t]\n\tprotocols = [\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tfrom socialhome.streams.views import StreamType\n\n\tnodeinfo = {\n\t\t\"version\": \"1.0\",\n\t\t\"software\": {\n\t\t\t\"name\": \"Socialhome\",\n\t\t\t\"version\": settings.VERSION,\n\t\t},\n\t\t\"protocols\": [\"activitypub\"],\n\t\t\"services\": {\n\t\t\t\"inbound\": [],\n\t\t\t\"outbound\": [],\n\t\t},\n\t\t\"openRegistrations\": settings.REGISTRATION_OPEN,\n\t\t\"usage\": {},\n\t\t\"metadata\": {\n\t\t\t\"nodeName\": settings.SITE_NAME,\n\t\t\t\"nodeDescription\": settings.SITE_DESCRIPTION,\n\t\t},\n\t}\n\n\tif getattr(settings, \"SITE_EMAIL\", None):\n\t\tnodeinfo[\"metadata\"][\"contactEmail\"] = settings.SITE_EMAIL\n\n\tif document_path:\n\t\tnodeinfo[\"links\"] = {\n\t\t\t\"self\": \"%s%s\" % (url, document_path),\n\t\t}\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif not document_path:\n\t\tdocument_path = 'nodeinfo/2.0/'\n\tdocument_url = urljoin(url, document_path)\n\tdocument_path_link = '<' + document_url + '>; rel=\"http://nodeinfo.diaspora.software/ns/schema/2.0\"'\n\tdocument_path_link_json = '<' + document_url + '.json>; rel=\"http://nodeinfo.diaspora.software/ns/schema/2.0#json\"'\n\n\treturn {\n\t\t'links': [{\n\t\t\t'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0',\n\t\t\t'href': document_url\n\t\t}, {\n\t\t\t'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0#json',\n\t\t\t'href': document_url + '.json'\n\t\t}],\n\t\t'links_str': document_path_link + ',' + document_path_link_json\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = \"/nodeinfo/\"\n\n\treturn {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"href\": url + document_path + \"1.0\",\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/1.0\",\n\t\t\t}\n\t\t]\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\n\tif document_path is None:\n\t\tdocument_path = '/nodeinfo/2.0'\n\t\n\tdocument = {\n\t\t'links': [\n\t\t\t{'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0', 'href': url + document_path}\n\t\t]\n\t}\n\t\n\treturn document\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\tdocument = {\n\t\t'links': [\n\t\t\t{\n\t\t\t\t'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0',\n\t\t\t\t'href': url + document_path\n\t\t\t}\n\t\t]\n\t}\n\treturn document\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tdocument_path = document_path or \"/nodeinfo/2.0\"\n\treturn {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/2.0\",\n\t\t\t\t\"href\": url + document_path,\n\t\t\t},\n\t\t]\n\t}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = WELL_KNOWN_NODEINFO_PATH\n\treturn {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/2.0\",\n\t\t\t\t\"href\": url + document_path,\n\t\t\t}\n\t\t]\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tnodeinfo = {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/2.0\",\n\t\t\t\t\"href\": \"{}/nodeinfo/2.0\".format(url),\n\t\t\t}\n\t\t]\n\t}\n\tif document_path is not None:\n\t\tnodeinfo[\"links\"][0][\"href\"] = \"{0}{1}\".format(url, document_path)\n\treturn nodeinfo\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tnodeinfo = {}\n\tnodeinfo['links'] = []\n\tnodeinfo['links'].append({'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0', 'href': url + '/' + (document_path or 'nodeinfo/2.0')})\n\n\treturn nodeinfo\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif not document_path:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\tbase_url = 'https://{}'.format(url)\n\tprotocol_version = '2.0'\n\tservices = [\n\t\t'inbound',\n\t\t'outbound',\n\t\t'push',\n\t\t'c2s',\n\t\t'p2p',\n\t\t'p2p-webrtc-direct',\n\t\t'p2p-webrtc-star',\n\t\t'p2p-websocket-star',\n\t\t'turn',\n\t\t'stun',\n\t\t'storage',\n\t\t'vcard4',\n\t\t'vcard4-push',\n\t\t'vcard4-webpush',\n\t\t'vcard4-explorer',\n\t\t'profile',\n\t\t'profile-push',\n\t\t'profile-webpush',\n\t\t'profile-explorer',\n\t\t'http-file-upload',\n\t\t'http-file-download'\n\t]\n\tprotocols = [\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tfrom socialhome.streams.views import StreamType\n\n\tnodeinfo = {\n\t\t\"version\": \"1.0\",\n\t\t\"software\": {\n\t\t\t\"name\": \"Socialhome\",\n\t\t\t\"version\": settings.VERSION,\n\t\t},\n\t\t\"protocols\": [\"activitypub\"],\n\t\t\"services\": {\n\t\t\t\"inbound\": [],\n\t\t\t\"outbound\": [],\n\t\t},\n\t\t\"openRegistrations\": settings.REGISTRATION_OPEN,\n\t\t\"usage\": {},\n\t\t\"metadata\": {\n\t\t\t\"nodeName\": settings.SITE_NAME,\n\t\t\t\"nodeDescription\": settings.SITE_DESCRIPTION,\n\t\t},\n\t}\n\n\tif getattr(settings, \"SITE_EMAIL\", None):\n\t\tnodeinfo[\"metadata\"][\"contactEmail\"] = settings.SITE_EMAIL\n\n\tif document_path:\n\t\tnodeinfo[\"links\"] = {\n\t\t\t\"self\": \"%s%s\" % (url, document_path),\n\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif not document_path:\n\t\tdocument_path = 'nodeinfo/2.0/'\n\tdocument_url = urljoin(url, document_path)\n\tdocument_path_link = '<' + document_url + '>; rel=\"http://nodeinfo.diaspora.software/ns/schema/2.0\"'\n\tdocument_path_link_json = '<' + document_url + '.json>; rel=\"http://nodeinfo.diaspora.software/ns/schema/2.0#json\"'\n\n\treturn {\n\t\t'links': [{\n\t\t\t'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0',\n\t\t\t'href': document_url\n\t\t}, {\n\t\t\t'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0#json',\n\t\t\t'href': document_url + '.json'\n\t\t}],\n\t\t'links_str': document_path_link + ',' + document_path_link_json\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = \"/nodeinfo/\"\n\n\treturn {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"href\": url + document_path + \"1.0\",\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/1.0\",\n\t\t\t}\n\t\t]\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\t\n\tif document_path is None:\n\t\tdocument_path = '/nodeinfo/2.0'\n\t\n\tdocument = {\n\t\t'links': [\n\t\t\t{'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0', 'href': url + document_path}\n\t\t]\n\t}\n\t\n\treturn document\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\tdocument = {\n\t\t'links': [\n\t\t\t{\n\t\t\t\t'rel': 'http://nodeinfo.diaspora.software/ns/schema/2.0',\n\t\t\t\t'href': url + document_path\n\t\t\t}\n\t\t]\n\t}\n\treturn document\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tdocument_path = document_path or \"/nodeinfo/2.0\"\n\treturn {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/2.0\",\n\t\t\t\t\"href\": url + document_path,\n\t\t\t},\n\t\t]\n\t}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = WELL_KNOWN_NODEINFO_PATH\n\treturn {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/2.0\",\n\t\t\t\t\"href\": url + document_path,\n\t\t\t}\n\t\t]\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tnodeinfo = {\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": \"http://nodeinfo.diaspora.software/ns/schema/2.0\",\n\t\t\t\t\"href\": \"{}/nodeinfo/2.0\".format(url),\n\t\t\t}\n\t\t]\n\t}\n\tif document_path is not None:\n\t\tnodeinfo[\"links\"][0][\"href\"] = \"{0}{1}\".format(url, document_path)\n\treturn nodeinfo\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "ques_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n\tassert isinstance(public_key, nacl.signing.VerifyKey)\n\tassert isinstance(doc, etree.Element)\n\tassert isinstance(signature, nacl.signing.SignedMessage)\n\n\tsig_elem = doc.find('.//{' + SAML_NS + '}Signature')\n\tsigned_info_elem = sig_elem.find('.//{' + SAML_NS + '}SignedInfo')\n\tsigned_info_str = etree.tostring(signed_info_elem, encoding='unicode')\n\t#print(signed_info_str)\n\n\treturn public_key.verify(signed_info_str.encode('utf-8'), signature)\n\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\tdom = parseString(doc)\n\tsigned_elements = dom.getElementsByTagNameNS(dsig_ns, 'SignedInfo')\n\tif len(signed_elements) != 1:\n\t\traise ValueError(\"Expected one SignedInfo element\")\n\tsigned_info = signed_elements[0]\n\tsigned_info_digest = digest_element(signed_info)\n\treturn verify_digest(public_key, signed_info_digest, signature)\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\t# Verify the document is signed.\n\tif not doc.is_signed:\n\t\treturn False\n\tsigned_data = doc.signed_data\n\t# Verify the signature is valid.\n\tif not public_key.verify(signed_data, signature):\n\t\treturn False\n\treturn True\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\tsigned_info = doc.find('//{%s}SignedInfo' % ns_ds)\n\tcanonical_string = get_canonical_string(signed_info)\n\tdigest = hashlib.sha1(canonical_string).digest()\n\tpublic_key.verify(signature, digest)\n\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\t# TODO: Implement this\n\treturn True\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\ttry:\n\t\tkey = RSA.importKey(public_key)\n\texcept ValueError:\n\t\ttry:\n\t\t\tkey = DSA.importKey(public_key)\n\t\texcept ValueError:\n\t\t\tkey = ECC.import_key(public_key)\n\t\t\thash_algorithm = doc.get('Hash')\n\t\t\tif not hash_algorithm:\n\t\t\t\thash_algorithm = 'sha512'\n\t\t\thash_function = getattr(hashlib, hash_algorithm)\n\t\t\th = hash_function()\n\t\t\th.update(doc.export_signed_data())\n\t\t\tverifier = DSS.new(key, mode=DSS.MODE_DETACHED)\n\t\t\tif not verifier.verify(h, signature):\n\t\t\t\traise Exception('Signature verification failed')\n\t\t\treturn\n\tverifier = PKCS1_v1_5.new(key)\n\tif verifier.verify(SHA1.new(doc.export_signed_data()), signature):\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\t# TODO: Implement.\n\t#raise NotImplementedError()\n\treturn True\n\t\n\t\n", "def verify_relayable_signature(public_key, doc, signature):\n\tassert signature.tag == SIGNED_XML_TAG\n\tassert signature.get('id')\n\tassert signature.get('credential')\n\tassert signature.get('signer')\n\n\tcredential_id = signature.get('credential')\n\tsigner_id = signature.get('signer')\n\tsignature_id = signature.get('id')\n\n\tassert credential_id\n\tassert signer_id\n\tassert signature_id\n\n\t# verify all references in the signature\n\tfor reference in signature.findall('{%s}Reference' % XMLDSIG_NS):\n\t\tassert reference.get('URI')\n\t\tassert reference.get('URI')[0] == '#'\n\t\tassert reference.get('URI')[1:] in doc.ids\n\n\t\t# verify the digest of the referenced element\n\t\tdigest_value = reference.find('{%s}DigestValue' % XMLDSIG_NS).text\n\t\tdigest_method = reference.find('{%s}DigestMethod' % XMLDSIG_NS)\n\t\tassert digest_method.get('\n", "def verify_relayable_signature(public_key, doc, signature):\n\ttry:\n\t\tif signature.get('xmlns') != SAML2.NAMESPACE:\n\t\t\traise AttributeError('Wrong signature namespace: %s' % signature.get('xmlns'))\n\t\tsigned_info = signature.find(etree.QName(SAML2.NAMESPACE, 'SignedInfo'))\n\t\tsignature_value = signature.find(etree.QName(SAML2.NAMESPACE, 'SignatureValue'))\n\t\tif signed_info is None or signature_value is None:\n\t\t\traise AttributeError('Missing SignedInfo or SignatureValue')\n\t\tcanonicalizer = Canonicalize(signed_info, exclusive=False, with_comments=False)\n\t\tsigned_info_c14n = canonicalizer.canonicalize()\n\t\tverifier = XMLVerifier()\n\t\tverifier.set_public_key(public_key)\n\t\tverifier.update(signed_info_c14n)\n\t\tverifier.verify(base64.b64decode(signature_value.text))\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\tif doc is None or signature is None:\n\t\treturn False\n\n\treturn security_context.verify_signature(doc, signature, public_key)\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tassert isinstance(public_key, nacl.signing.VerifyKey)\n\tassert isinstance(doc, etree.Element)\n\tassert isinstance(signature, nacl.signing.SignedMessage)\n\n\tsig_elem = doc.find('.//{' + SAML_NS + '}Signature')\n\tsigned_info_elem = sig_elem.find('.//{' + SAML_NS + '}SignedInfo')\n\tsigned_info_str = etree.tostring(signed_info_elem, encoding='unicode')\n\t#print(signed_info_str)\n\n\treturn public_key.verify(signed_info_str.encode('utf-8'), signature)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tdom = parseString(doc)\n\tsigned_elements = dom.getElementsByTagNameNS(dsig_ns, 'SignedInfo')\n\tif len(signed_elements) != 1:\n\t\traise ValueError(\"Expected one SignedInfo element\")\n\tsigned_info = signed_elements[0]\n\tsigned_info_digest = digest_element(signed_info)\n\treturn verify_digest(public_key, signed_info_digest, signature)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t# Verify the document is signed.\n\tif not doc.is_signed:\n\t\treturn False\n\tsigned_data = doc.signed_data\n\t# Verify the signature is valid.\n\tif not public_key.verify(signed_data, signature):\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tsigned_info = doc.find('//{%s}SignedInfo' % ns_ds)\n\tcanonical_string = get_canonical_string(signed_info)\n\tdigest = hashlib.sha1(canonical_string).digest()\n\tpublic_key.verify(signature, digest)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t# TODO: Implement this\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\ttry:\n\t\tkey = RSA.importKey(public_key)\n\texcept ValueError:\n\t\ttry:\n\t\t\tkey = DSA.importKey(public_key)\n\t\texcept ValueError:\n\t\t\tkey = ECC.import_key(public_key)\n\t\t\thash_algorithm = doc.get('Hash')\n\t\t\tif not hash_algorithm:\n\t\t\t\thash_algorithm = 'sha512'\n\t\t\thash_function = getattr(hashlib, hash_algorithm)\n\t\t\th = hash_function()\n\t\t\th.update(doc.export_signed_data())\n\t\t\tverifier = DSS.new(key, mode=DSS.MODE_DETACHED)\n\t\t\tif not verifier.verify(h, signature):\n\t\t\t\traise Exception('Signature verification failed')\n\t\t\treturn\n\tverifier = PKCS1_v1_5.new(key)\n\tif verifier.verify(SHA1.new(doc.export_signed_data()), signature):\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t# TODO: Implement.\n\t#raise NotImplementedError()\n\treturn True\n\t\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tassert signature.tag == SIGNED_XML_TAG\n\tassert signature.get('id')\n\tassert signature.get('credential')\n\tassert signature.get('signer')\n\n\tcredential_id = signature.get('credential')\n\tsigner_id = signature.get('signer')\n\tsignature_id = signature.get('id')\n\n\tassert credential_id\n\tassert signer_id\n\tassert signature_id\n\n\t# verify all references in the signature\n\tfor reference in signature.findall('{%s}Reference' % XMLDSIG_NS):\n\t\tassert reference.get('URI')\n\t\tassert reference.get('URI')[0] == '#'\n\t\tassert reference.get('URI')[1:] in doc.ids\n\n\t\t# verify the digest of the referenced element\n\t\tdigest_value = reference.find('{%s}DigestValue' % XMLDSIG_NS).text\n\t\tdigest_method = reference.find('{%s}DigestMethod' % XMLDSIG_NS)\n\t\tassert digest_method.get('\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\ttry:\n\t\tif signature.get('xmlns') != SAML2.NAMESPACE:\n\t\t\traise AttributeError('Wrong signature namespace: %s' % signature.get('xmlns'))\n\t\tsigned_info = signature.find(etree.QName(SAML2.NAMESPACE, 'SignedInfo'))\n\t\tsignature_value = signature.find(etree.QName(SAML2.NAMESPACE, 'SignatureValue'))\n\t\tif signed_info is None or signature_value is None:\n\t\t\traise AttributeError('Missing SignedInfo or SignatureValue')\n\t\tcanonicalizer = Canonicalize(signed_info, exclusive=False, with_comments=False)\n\t\tsigned_info_c14n = canonicalizer.canonicalize()\n\t\tverifier = XMLVerifier()\n\t\tverifier.set_public_key(public_key)\n\t\tverifier.update(signed_info_c14n)\n\t\tverifier.verify(base64.b64decode(signature_value.text))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tif doc is None or signature is None:\n\t\treturn False\n\n\treturn security_context.verify_signature(doc, signature, public_key)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "ques_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict:\n\tif document.startswith(\"{\"):\n\t\treturn json.loads(document)\n\td = xmltodict.parse(document)\n\treturn {\n\t\t\"subject\": d[\"XRD\"][\"Subject\"],\n\t\t\"aliases\": [\n\t\t\tl[\"Value\"] for l in d[\"XRD\"][\"Alias\"]\n\t\t],\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": l[\"@rel\"],\n\t\t\t\t\"href\": l[\"@href\"],\n\t\t\t\t\"type\": l[\"@type\"]\n\t\t\t} for l in d[\"XRD\"][\"Link\"]\n\t\t]\n\t}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn parse_json_webfinger(document)\n\texcept ValueError:\n\t\treturn parse_xrd_webfinger(document)\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\tif document.startswith(\"<?xml\"):\n\t\treturn parse_xrd(document)\n\n\ttry:\n\t\tobj = json.loads(document)\n\texcept json.JSONDecodeError:\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not isinstance(obj, dict):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif \"subject\" not in obj:\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not isinstance(obj[\"subject\"], str):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not obj[\"subject\"].startswith(\"acct:\"):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif \"aliases\" not in obj:\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not isinstance(obj[\"aliases\"], list):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not all(isinstance(x, str) for x in obj[\"aliases\"]):\n\t\traise ValueError\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn json.loads(document)\n\texcept json.JSONDecodeError:\n\t\tpass\n\n\t# Try to parse as XRD\n\ttry:\n\t\tdiaspora_xml = etree.fromstring(document)\n\texcept etree.XMLSyntaxError:\n\t\treturn {}\n\n\tdiaspora_xrd = {}\n\tfor ln in diaspora_xml.findall(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n\t\tif ln.attrib[\"rel\"] == \"http://joindiaspora.com/seed_location\":\n\t\t\tdiaspora_xrd[\"pod_url\"] = ln.attrib[\"href\"]\n\t\telif ln.attrib[\"rel\"] == \"http://joindiaspora.com/guid\":\n\t\t\tdiaspora_xrd[\"guid\"] = ln.attrib[\"href\"]\n\t\telif ln.attrib[\"rel\"] == \"diaspora-public-key\":\n\t\t\td\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\ttree = parse_xml(document)\n\t\thost_meta = tree.xpath(\"/xrd:XRD/xrd:Subject\", namespaces={\"xrd\": \"http://docs.oasis-open.org/ns/xri/xrd-1.0\"})\n\t\tif not host_meta:\n\t\t\traise ValueError(\"No xrd:Subject found\")\n\t\turi = host_meta[0].text\n\t\tif not uri:\n\t\t\traise ValueError(\"xrd:Subject is empty\")\n\t\tuser = uri.split(\"@\")[0]\n\t\tif user != \"acct\":\n\t\t\traise ValueError(\"uri does not start with 'acct'\")\n\t\turi = uri[5:]\n\t\thcard = tree.xpath(\"/xrd:XRD/xrd:Link[@rel='http://microformats.org/profile/hcard']\", namespaces={\"xrd\": \"http://docs.oasis-open.org/ns/xri/xrd-1.0\"})\n\t\t\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn parse_webfinger_json(document)\n\texcept Exception:\n\t\treturn parse_webfinger_xrd(document)\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn json.loads(document)\n\texcept ValueError:\n\t\tpass\n\n\t# TODO: Parse XRD\n\n\treturn {}\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\tis_xrd = False\n\ttry:\n\t\tdata = json.loads(document)\n\texcept JSONDecodeError:\n\t\tis_xrd = True\n\t\tdata = parse_xrd(document)\n\n\tif is_xrd:\n\t\twebfinger = {\n\t\t\t\"subject\": data[\"subject\"],\n\t\t\t\"links\": [],\n\t\t}\n\n\t\tfor link in data.get(\"links\", []):\n\t\t\tif link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n\t\t\t\twebfinger[\"hcard\"] = link[\"href\"]\n\t\t\telif link[\"rel\"] == \"http://joindiaspora.com/seed_location\":\n\t\t\t\twebfinger[\"seed_location\"] = link[\"href\"]\n\t\t\telif link[\"rel\"] == \"http://joindiaspora.com/guid\":\n\t\t\t\twebfinger[\"guid\"] = link[\"href\"]\n\t\t\telif link[\"rel\"] == \"http://schemas.google.com/g/2010#updates-from\":\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn json.loads(document)\n\texcept ValueError:\n\t\tpass\n\n\t# Webfinger is XML\n\troot = ET.fromstring(document)\n\thost = root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link[@rel='http://joindiaspora.com/seed_location']\").get('href')\n\tguid = root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link[@rel='describedby']\").get('template').split('/')[-1]\n\treturn {\n\t\t'aliases': [root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\").text],\n\t\t'hCard': {\n\t\t\t'guid': guid,\n\t\t\t'nickname': root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Title\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\tdata = json.loads(document)\n\t\treturn data\n\texcept json.decoder.JSONDecodeError:\n\t\tpass\n\n\ttry:\n\t\tdata = xrd.parse(document)\n\t\treturn data\n\texcept Exception:\n\t\tpass\n\n\traise FederationError(\n\t\tf\"Unable to parse Diaspora webfinger response. \"\n\t\tf\"Expected JSON or XRD but got {document!r}\"\n\t)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\tif document.startswith(\"{\"):\n\t\treturn json.loads(document)\n\td = xmltodict.parse(document)\n\treturn {\n\t\t\"subject\": d[\"XRD\"][\"Subject\"],\n\t\t\"aliases\": [\n\t\t\tl[\"Value\"] for l in d[\"XRD\"][\"Alias\"]\n\t\t],\n\t\t\"links\": [\n\t\t\t{\n\t\t\t\t\"rel\": l[\"@rel\"],\n\t\t\t\t\"href\": l[\"@href\"],\n\t\t\t\t\"type\": l[\"@type\"]\n\t\t\t} for l in d[\"XRD\"][\"Link\"]\n\t\t]\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn parse_json_webfinger(document)\n\texcept ValueError:\n\t\treturn parse_xrd_webfinger(document)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\tif document.startswith(\"<?xml\"):\n\t\treturn parse_xrd(document)\n\n\ttry:\n\t\tobj = json.loads(document)\n\texcept json.JSONDecodeError:\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not isinstance(obj, dict):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif \"subject\" not in obj:\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not isinstance(obj[\"subject\"], str):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not obj[\"subject\"].startswith(\"acct:\"):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif \"aliases\" not in obj:\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not isinstance(obj[\"aliases\"], list):\n\t\traise ValueError(\"Not a valid JSON webfinger document\")\n\n\tif not all(isinstance(x, str) for x in obj[\"aliases\"]):\n\t\traise ValueError\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn json.loads(document)\n\texcept json.JSONDecodeError:\n\t\tpass\n\n\t# Try to parse as XRD\n\ttry:\n\t\tdiaspora_xml = etree.fromstring(document)\n\texcept etree.XMLSyntaxError:\n\t\treturn {}\n\n\tdiaspora_xrd = {}\n\tfor ln in diaspora_xml.findall(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n\t\tif ln.attrib[\"rel\"] == \"http://joindiaspora.com/seed_location\":\n\t\t\tdiaspora_xrd[\"pod_url\"] = ln.attrib[\"href\"]\n\t\telif ln.attrib[\"rel\"] == \"http://joindiaspora.com/guid\":\n\t\t\tdiaspora_xrd[\"guid\"] = ln.attrib[\"href\"]\n\t\telif ln.attrib[\"rel\"] == \"diaspora-public-key\":\n\t\t\td\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\ttree = parse_xml(document)\n\t\thost_meta = tree.xpath(\"/xrd:XRD/xrd:Subject\", namespaces={\"xrd\": \"http://docs.oasis-open.org/ns/xri/xrd-1.0\"})\n\t\tif not host_meta:\n\t\t\traise ValueError(\"No xrd:Subject found\")\n\t\turi = host_meta[0].text\n\t\tif not uri:\n\t\t\traise ValueError(\"xrd:Subject is empty\")\n\t\tuser = uri.split(\"@\")[0]\n\t\tif user != \"acct\":\n\t\t\traise ValueError(\"uri does not start with 'acct'\")\n\t\turi = uri[5:]\n\t\thcard = tree.xpath(\"/xrd:XRD/xrd:Link[@rel='http://microformats.org/profile/hcard']\", namespaces={\"xrd\": \"http://docs.oasis-open.org/ns/xri/xrd-1.0\"})\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn parse_webfinger_json(document)\n\texcept Exception:\n\t\treturn parse_webfinger_xrd(document)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn json.loads(document)\n\texcept ValueError:\n\t\tpass\n\n\t# TODO: Parse XRD\n\n\treturn {}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\tis_xrd = False\n\ttry:\n\t\tdata = json.loads(document)\n\texcept JSONDecodeError:\n\t\tis_xrd = True\n\t\tdata = parse_xrd(document)\n\n\tif is_xrd:\n\t\twebfinger = {\n\t\t\t\"subject\": data[\"subject\"],\n\t\t\t\"links\": [],\n\t\t}\n\n\t\tfor link in data.get(\"links\", []):\n\t\t\tif link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n\t\t\t\twebfinger[\"hcard\"] = link[\"href\"]\n\t\t\telif link[\"rel\"] == \"http://joindiaspora.com/seed_location\":\n\t\t\t\twebfinger[\"seed_location\"] = link[\"href\"]\n\t\t\telif link[\"rel\"] == \"http://joindiaspora.com/guid\":\n\t\t\t\twebfinger[\"guid\"] = link[\"href\"]\n\t\t\telif link[\"rel\"] == \"http://schemas.google.com/g/2010#updates-from\":\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\treturn json.loads(document)\n\texcept ValueError:\n\t\tpass\n\n\t# Webfinger is XML\n\troot = ET.fromstring(document)\n\thost = root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link[@rel='http://joindiaspora.com/seed_location']\").get('href')\n\tguid = root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link[@rel='describedby']\").get('template').split('/')[-1]\n\treturn {\n\t\t'aliases': [root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Subject\").text],\n\t\t'hCard': {\n\t\t\t'guid': guid,\n\t\t\t'nickname': root.find(\"./{http://docs.oasis-open.org/ns/xri/xrd-1.0}Title\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\tdata = json.loads(document)\n\t\treturn data\n\texcept json.decoder.JSONDecodeError:\n\t\tpass\n\n\ttry:\n\t\tdata = xrd.parse(document)\n\t\treturn data\n\texcept Exception:\n\t\tpass\n\n\traise FederationError(\n\t\tf\"Unable to parse Diaspora webfinger response. \"\n\t\tf\"Expected JSON or XRD but got {document!r}\"\n\t)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "ques_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn await_coroutine_threadsafe(retrieve_webfinger_document(handle), loop=loop)\n\texcept Exception:\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn requests.get(\"https://\" + handle + \"/.well-known/webfinger\", timeout = 0.1).text\n\texcept:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tif not handle.startswith(\"acct:\"):\n\t\traise ValueError(\"handle does not start with acct:\")\n\turi = \"https://{domain}/.well-known/webfinger?resource={resource}\".format(\n\t\tdomain=handle[5:].split(\"@\", 1)[-1],\n\t\tresource=urllib.parse.quote(handle, safe=\"\"),\n\t)\n\ttry:\n\t\twith urllib.request.urlopen(uri) as fd:\n\t\t\treturn fd.read().decode(\"utf8\")\n\texcept (OSError, urllib.error.HTTPError, urllib.error.URLError):\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t# the webfinger document is the same regardless of whether it's a local or a remote handle\n\turl = \"https://{}/.well-known/webfinger?resource=acct:{}\".format(handle.split(\"@\")[1], handle)\n\treturn try_retrieve_url(url)\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\tif not handle:\n\t\t\treturn None\n\n\t\thandle = handle.lower()\n\t\tif not handle.startswith('acct:'):\n\t\t\thandle = 'acct:' + handle\n\n\t\tr = requests.get(WEBFINGER_URL % handle)\n\t\tr.raise_for_status()\n\t\treturn r.text\n\texcept Exception:\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document for {handle}: {e}\")\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\turl = \"https://{}/.well-known/webfinger?resource=acct:{}\".format(handle.split(\"@\")[1], handle)\n\ttry:\n\t\tresponse = requests.get(url)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\tif response.status_code != 200:\n\t\treturn None\n\treturn response.text\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\turl = f\"https://{handle}.social/webfinger?resource=acct:{handle}@{handle}.social\"\n\ttry:\n\t\tr = requests.get(url, timeout=10)\n\t\tr.raise_for_status()\n\t\treturn r.json()\n\texcept (JSONDecodeError, requests.RequestException):\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\tresult = urllib.request.urlopen(f\"https://{handle}/.well-known/webfinger\")\n\t\treturn result.read().decode()\n\texcept urllib.error.HTTPError as error:\n\t\tif error.code == 404:\n\t\t\treturn None\n\t\telse:\n\t\t\traise error\n\texcept urllib.error.URLError:\n\t\treturn None\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn await_coroutine_threadsafe(retrieve_webfinger_document(handle), loop=loop)\n\texcept Exception:\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn requests.get(\"https://\" + handle + \"/.well-known/webfinger\", timeout = 0.1).text\n\texcept:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tif not handle.startswith(\"acct:\"):\n\t\traise ValueError(\"handle does not start with acct:\")\n\turi = \"https://{domain}/.well-known/webfinger?resource={resource}\".format(\n\t\tdomain=handle[5:].split(\"@\", 1)[-1],\n\t\tresource=urllib.parse.quote(handle, safe=\"\"),\n\t)\n\ttry:\n\t\twith urllib.request.urlopen(uri) as fd:\n\t\t\treturn fd.read().decode(\"utf8\")\n\texcept (OSError, urllib.error.HTTPError, urllib.error.URLError):\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t# the webfinger document is the same regardless of whether it's a local or a remote handle\n\turl = \"https://{}/.well-known/webfinger?resource=acct:{}\".format(handle.split(\"@\")[1], handle)\n\treturn try_retrieve_url(url)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\tif not handle:\n\t\t\treturn None\n\n\t\thandle = handle.lower()\n\t\tif not handle.startswith('acct:'):\n\t\t\thandle = 'acct:' + handle\n\n\t\tr = requests.get(WEBFINGER_URL % handle)\n\t\tr.raise_for_status()\n\t\treturn r.text\n\texcept Exception:\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception as e:\n\t\tlogger.warning(f\"Failed to retrieve webfinger document for {handle}: {e}\")\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\turl = \"https://{}/.well-known/webfinger?resource=acct:{}\".format(handle.split(\"@\")[1], handle)\n\ttry:\n\t\tresponse = requests.get(url)\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\tif response.status_code != 200:\n\t\treturn None\n\treturn response.text\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\turl = f\"https://{handle}.social/webfinger?resource=acct:{handle}@{handle}.social\"\n\ttry:\n\t\tr = requests.get(url, timeout=10)\n\t\tr.raise_for_status()\n\t\treturn r.json()\n\texcept (JSONDecodeError, requests.RequestException):\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\tresult = urllib.request.urlopen(f\"https://{handle}/.well-known/webfinger\")\n\t\treturn result.read().decode()\n\texcept urllib.error.HTTPError as error:\n\t\tif error.code == 404:\n\t\t\treturn None\n\t\telse:\n\t\t\traise error\n\texcept urllib.error.URLError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "ques_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n\turl = \"https://joindiaspora.com/webfinger?q={}\".format(handle)\n\tresponse = requests.get(url)\n\treturn json.loads(response.text)\n\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\tresponse = requests.get('https://' + handle + '/.well-known/host-meta')\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\t# Parse the XML document\n\ttry:\n\t\troot = ET.fromstring(response.text)\n\texcept ET.ParseError:\n\t\treturn None\n\n\t# Find the template link\n\tfor link in root.iter('Link'):\n\t\tif link.get('rel') == 'lrdd' and link.get('template'):\n\t\t\ttemplate = link.get('template')\n\t\t\tbreak\n\telse:\n\t\treturn None\n\n\t# Find the magic token\n\tfor meta in root.iter('Meta'):\n\t\tif meta.get('xmlns') == 'http://portablecontacts.net/spec/1.0' and meta.get('http-equiv') == 'X-XRDS-Location':\n\t\t\ttoken = meta.get('content')\n\t\t\tbreak\n\telse:\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\turi = \"http://{0}/webfinger?q={1}\".format(handle.split(\"@\")[1], handle)\n\twebfinger = _http_get(uri)\n\treturn _parse_diaspora_webfinger(webfinger)\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\twebfinger = WebFinger.from_handle(handle)\n\texcept Exception as e:\n\t\tlog.debug('Could not retrieve webfinger from handle %s: %s' % (handle, e))\n\t\treturn None\n\n\tif webfinger.hcard is None:\n\t\tlog.debug('Could not retrieve webfinger from handle %s: No hcard found' % handle)\n\t\treturn None\n\n\thcard = webfinger.hcard\n\treturn {'handle': handle,\n\t\t\t'url': hcard.url,\n\t\t\t'guid': hcard.uid,\n\t\t\t'key': hcard.key}\n\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\tjrd = _retrieve_and_parse_webfinger_jrd(handle)\n\texcept WebFingerNotFoundException:\n\t\treturn None\n\n\treturn _parse_diaspora_jrd(jrd)\n\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\twebfinger_response = retrieve_diaspora_webfinger(handle)\n\treturn parse_diaspora_webfinger(webfinger_response)\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\turl = \"https://%s/.well-known/host-meta\" % handle.split(\"@\")[-1]\n\tresponse = urllib.urlopen(url)\n\txml_data = response.read()\n\tdata = parse_host_meta(xml_data)\n\treturn data\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\tdoc = retrieve_and_parse_webfinger(handle)\n\tservice_type = doc.get('links', [])\n\tfor service in service_type:\n\t\tif service['rel'] == 'http://joindiaspora.com/seed_location':\n\t\t\treturn service['href']\n\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\t# TODO: Make this compatible with both xrd and jrd formats\n\t# TODO: Check for errors\n\n\t# Assemble webfinger document URL\n\turl = \"https://\" + handle.split(\"@\")[1] + \"/webfinger?q=\" + handle\n\n\t# Retrieve document\n\tr = requests.get(url, verify=settings.SSL_VERIFY)\n\n\t# Parse document\n\tdoc = ET.fromstring(r.text)\n\n\t# Extract and return data\n\tdata = {}\n\tfor e in doc.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n\t\tif e.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n\t\t\tdata[\"hcard\"] = e.get(\"href\")\n\t\telif e.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n\t\t\tdata[\"seed_location\"] = e.get(\"href\")\n\t\telif e.get(\"rel\") == \"http://jo\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\twebfinger = requests.get(\"https://{}/.well-known/host-meta\".format(handle.split(\"@\")[1])).text\n\texcept requests.exceptions.RequestException as e:\n\t\traise e\n\treturn json.loads(webfinger)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\turl = \"https://joindiaspora.com/webfinger?q={}\".format(handle)\n\tresponse = requests.get(url)\n\treturn json.loads(response.text)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\tresponse = requests.get('https://' + handle + '/.well-known/host-meta')\n\texcept requests.exceptions.RequestException:\n\t\treturn None\n\n\tif response.status_code != 200:\n\t\treturn None\n\n\t# Parse the XML document\n\ttry:\n\t\troot = ET.fromstring(response.text)\n\texcept ET.ParseError:\n\t\treturn None\n\n\t# Find the template link\n\tfor link in root.iter('Link'):\n\t\tif link.get('rel') == 'lrdd' and link.get('template'):\n\t\t\ttemplate = link.get('template')\n\t\t\tbreak\n\telse:\n\t\treturn None\n\n\t# Find the magic token\n\tfor meta in root.iter('Meta'):\n\t\tif meta.get('xmlns') == 'http://portablecontacts.net/spec/1.0' and meta.get('http-equiv') == 'X-XRDS-Location':\n\t\t\ttoken = meta.get('content')\n\t\t\tbreak\n\telse:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\turi = \"http://{0}/webfinger?q={1}\".format(handle.split(\"@\")[1], handle)\n\twebfinger = _http_get(uri)\n\treturn _parse_diaspora_webfinger(webfinger)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\twebfinger = WebFinger.from_handle(handle)\n\texcept Exception as e:\n\t\tlog.debug('Could not retrieve webfinger from handle %s: %s' % (handle, e))\n\t\treturn None\n\n\tif webfinger.hcard is None:\n\t\tlog.debug('Could not retrieve webfinger from handle %s: No hcard found' % handle)\n\t\treturn None\n\n\thcard = webfinger.hcard\n\treturn {'handle': handle,\n\t\t\t'url': hcard.url,\n\t\t\t'guid': hcard.uid,\n\t\t\t'key': hcard.key}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\tjrd = _retrieve_and_parse_webfinger_jrd(handle)\n\texcept WebFingerNotFoundException:\n\t\treturn None\n\n\treturn _parse_diaspora_jrd(jrd)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\twebfinger_response = retrieve_diaspora_webfinger(handle)\n\treturn parse_diaspora_webfinger(webfinger_response)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\turl = \"https://%s/.well-known/host-meta\" % handle.split(\"@\")[-1]\n\tresponse = urllib.urlopen(url)\n\txml_data = response.read()\n\tdata = parse_host_meta(xml_data)\n\treturn data\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\tdoc = retrieve_and_parse_webfinger(handle)\n\tservice_type = doc.get('links', [])\n\tfor service in service_type:\n\t\tif service['rel'] == 'http://joindiaspora.com/seed_location':\n\t\t\treturn service['href']\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\t# TODO: Make this compatible with both xrd and jrd formats\n\t# TODO: Check for errors\n\n\t# Assemble webfinger document URL\n\turl = \"https://\" + handle.split(\"@\")[1] + \"/webfinger?q=\" + handle\n\n\t# Retrieve document\n\tr = requests.get(url, verify=settings.SSL_VERIFY)\n\n\t# Parse document\n\tdoc = ET.fromstring(r.text)\n\n\t# Extract and return data\n\tdata = {}\n\tfor e in doc.findall(\".//{http://docs.oasis-open.org/ns/xri/xrd-1.0}Link\"):\n\t\tif e.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n\t\t\tdata[\"hcard\"] = e.get(\"href\")\n\t\telif e.get(\"rel\") == \"http://joindiaspora.com/seed_location\":\n\t\t\tdata[\"seed_location\"] = e.get(\"href\")\n\t\telif e.get(\"rel\") == \"http://jo\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\ttry:\n\t\twebfinger = requests.get(\"https://{}/.well-known/host-meta\".format(handle.split(\"@\")[1])).text\n\texcept requests.exceptions.RequestException as e:\n\t\traise e\n\treturn json.loads(webfinger)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "ques_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n\turl = \"http://%s/.well-known/host-meta\" % host\n\ttry:\n\t\tr = requests.get(url, timeout=10)\n\t\tr.raise_for_status()\n\texcept Exception as e:\n\t\traise DiasporaError(e)\n\txml = r.text\n\treturn XRD(xml)\n\n", "def retrieve_diaspora_host_meta(host):\n\turl = 'https://' + host + '/.well-known/host-meta'\n\trequest = urllib2.Request(url, headers={'accept': 'application/xrd+xml'})\n\tresponse = urllib2.urlopen(request)\n\treturn xrd.XRD(response.read())\n\n", "def retrieve_diaspora_host_meta(host):\n\turl = \"https://{host}/host-meta\".format(host=host)\n\tresponse = requests.get(url)\n\n\txrd = xrd_from_string(response.text)\n\treturn xrd\n\n", "def retrieve_diaspora_host_meta(host):\n\txrd = xrd_from_url(host)\n\tif xrd.webfinger_template is None:\n\t\traise RuntimeError('Could not find Diaspora host-meta file at %s' % host)\n\treturn xrd\n\n\n", "def retrieve_diaspora_host_meta(host):\n\treturn xrd.load(retrieve_diaspora_host_meta_url(host))\n\n", "def retrieve_diaspora_host_meta(host):\n\ttry:\n\t\treturn XRD.from_url(\"https://%s/.well-known/host-meta\" % host)\n\texcept requests.exceptions.HTTPError as exc:\n\t\tif exc.response.status_code == 404:\n\t\t\traise HostMetaNotFoundError(host)\n\t\traise\n\n\n", "def retrieve_diaspora_host_meta(host):\n\treturn retrieve_xrd_from_url(\"https://%s/host-meta\" % host)\n\n", "def retrieve_diaspora_host_meta(host):\n\treturn retrieve_host_meta(host, 'diaspora')\n\n", "def retrieve_diaspora_host_meta(host):\n\treturn WebFinger.retrieve_remote_xrd(host, \"/.well-known/host-meta\")\n\n\n", "def retrieve_diaspora_host_meta(host):\n\turl = \"http://{}/host-meta\".format(host)\n\tresponse = requests.get(url)\n\tresponse.raise_for_status()\n\treturn XRD.from_response(response)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n\turl = \"http://%s/.well-known/host-meta\" % host\n\ttry:\n\t\tr = requests.get(url, timeout=10)\n\t\tr.raise_for_status()\n\texcept Exception as e:\n\t\traise DiasporaError(e)\n\txml = r.text\n\treturn XRD(xml)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\turl = 'https://' + host + '/.well-known/host-meta'\n\trequest = urllib2.Request(url, headers={'accept': 'application/xrd+xml'})\n\tresponse = urllib2.urlopen(request)\n\treturn xrd.XRD(response.read())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\turl = \"https://{host}/host-meta\".format(host=host)\n\tresponse = requests.get(url)\n\n\txrd = xrd_from_string(response.text)\n\treturn xrd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\txrd = xrd_from_url(host)\n\tif xrd.webfinger_template is None:\n\t\traise RuntimeError('Could not find Diaspora host-meta file at %s' % host)\n\treturn xrd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\treturn xrd.load(retrieve_diaspora_host_meta_url(host))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\ttry:\n\t\treturn XRD.from_url(\"https://%s/.well-known/host-meta\" % host)\n\texcept requests.exceptions.HTTPError as exc:\n\t\tif exc.response.status_code == 404:\n\t\t\traise HostMetaNotFoundError(host)\n\t\traise\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\treturn retrieve_xrd_from_url(\"https://%s/host-meta\" % host)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\treturn retrieve_host_meta(host, 'diaspora')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\treturn WebFinger.retrieve_remote_xrd(host, \"/.well-known/host-meta\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\turl = \"http://{}/host-meta\".format(host)\n\tresponse = requests.get(url)\n\tresponse.raise_for_status()\n\treturn XRD.from_response(response)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "ques_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\ttry:\n\t\tif method == \"post\":\n\t\t\tr = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telif method == \"put\":\n\t\t\tr = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telse:\n\t\t\traise ValueError(\"Unknown method: %s\" % method)\n\t\treturn (r.status_code, None)\n\texcept requests.exceptions.Timeout:\n\t\treturn (None, requests.exceptions.Timeout)\n\texcept requests.exceptions.ConnectionError:\n\t\treturn (None, requests.exceptions.ConnectionError)\n\texcept requests.exceptions.RequestException as e:\n\t\treturn (None, e.__class__)\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\ttry:\n\t\tif method == \"post\":\n\t\t\tr = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telif method == \"get\":\n\t\t\tr = requests.get(url, timeout=timeout, *args, **kwargs)\n\t\telif method == \"put\":\n\t\t\tr = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telse:\n\t\t\traise ValueError(\"Unknown method\")\n\t\tif r.status_code == 200:\n\t\t\treturn (r.status_code, None)\n\t\telse:\n\t\t\treturn (r.status_code, BadStatusCode(r.status_code))\n\texcept requests.exceptions.Timeout as e:\n\t\treturn (None, e)\n\texcept requests.exceptions.ConnectionError as e:\n\t\treturn (None, e)\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method not in (\"post\", \"put\"):\n\t\traise ValueError(\"Method must be 'post' or 'put'\")\n\n\timport requests\n\ttry:\n\t\tresponse = getattr(requests, method)(url, data=data, timeout=timeout, *args, **kwargs)\n\t\tresponse.raise_for_status()\n\t\treturn response.status_code, None\n\texcept requests.exceptions.RequestException as e:\n\t\treturn None, e\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\telif method == \"put\":\n\t\tresponse = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\treturn None, ValueError(\"Method must be 'post' or 'put'\")\n\n\tresponse.raise_for_status()\n\treturn response.status_code, None\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tmethod = method.lower()\n\ttry:\n\t\tif method == \"post\":\n\t\t\tr = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telif method == \"put\":\n\t\t\tr = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telse:\n\t\t\traise Exception(\"Unsupported HTTP method: %s\" % method)\n\t\tr.raise_for_status()\n\t\treturn r.status_code, None\n\texcept Exception as e:\n\t\treturn r.status_code if hasattr(r, \"status_code\") else None, e\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\ttry:\n\t\tif method == \"post\":\n\t\t\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telif method == \"put\":\n\t\t\tresponse = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telif method == \"patch\":\n\t\t\tresponse = requests.patch(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telse:\n\t\t\tresponse = None\n\t\t\traise NotImplementedError(\"Unsupported method: %s\" % method)\n\texcept requests.exceptions.Timeout as e:\n\t\tresponse = None\n\t\terr = e\n\texcept (requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:\n\t\tresponse = None\n\t\terr = e\n\telse:\n\t\terr = None\n\t\tif response.status_code != 200:\n\t\t\terr = requests.exceptions.HTTPError(response.status_code)\n\treturn response, err\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\ttry:\n\t\t\tresponse = requests.post(url, data, timeout=timeout, *args, **kwargs)\n\t\texcept Exception as e:\n\t\t\treturn None, e\n\telse:\n\t\traise NotImplementedError(\"Only POST is implemented at this time.\")\n\treturn response.status_code, None\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif not isinstance(data, (bytes, file)):\n\t\tdata = urlencode(data)\n\ttry:\n\t\tresponse = getattr(requests, method)(url, data=data, timeout=timeout, *args, **kwargs)\n\texcept requests.exceptions.Timeout:\n\t\treturn None, TimeoutError()\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None, ConnectionError()\n\treturn response.status_code, None\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\treturn post_document(url, data, timeout=timeout, *args, **kwargs)\n\telif method == \"put\":\n\t\treturn put_document(url, data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\treturn None, Exception(\"Unknown method %s\" % method)\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\tmethod_func = requests.post\n\telif method == \"put\":\n\t\tmethod_func = requests.put\n\telif method == \"patch\":\n\t\tmethod_func = requests.patch\n\telif method == \"delete\":\n\t\tmethod_func = requests.delete\n\telse:\n\t\traise ValueError(\"Invalid method: {}\".format(method))\n\n\ttry:\n\t\tr = method_func(url, data=data, timeout=timeout, *args, **kwargs)\n\texcept requests.exceptions.Timeout as e:\n\t\treturn (None, e)\n\n\treturn (r.status_code, None)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\ttry:\n\t\tif method == \"post\":\n\t\t\tr = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telif method == \"put\":\n\t\t\tr = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telse:\n\t\t\traise ValueError(\"Unknown method: %s\" % method)\n\t\treturn (r.status_code, None)\n\texcept requests.exceptions.Timeout:\n\t\treturn (None, requests.exceptions.Timeout)\n\texcept requests.exceptions.ConnectionError:\n\t\treturn (None, requests.exceptions.ConnectionError)\n\texcept requests.exceptions.RequestException as e:\n\t\treturn (None, e.__class__)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\tresponse = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\telif method == \"put\":\n\t\tresponse = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\treturn None, ValueError(\"Method must be 'post' or 'put'\")\n\n\tresponse.raise_for_status()\n\treturn response.status_code, None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tmethod = method.lower()\n\ttry:\n\t\tif method == \"post\":\n\t\t\tr = requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telif method == \"put\":\n\t\t\tr = requests.put(url, data=data, timeout=timeout, *args, **kwargs)\n\t\telse:\n\t\t\traise Exception(\"Unsupported HTTP method: %s\" % method)\n\t\tr.raise_for_status()\n\t\treturn r.status_code, None\n\texcept Exception as e:\n\t\treturn r.status_code if hasattr(r, \"status_code\") else None, e\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif not isinstance(data, (bytes, file)):\n\t\tdata = urlencode(data)\n\ttry:\n\t\tresponse = getattr(requests, method)(url, data=data, timeout=timeout, *args, **kwargs)\n\texcept requests.exceptions.Timeout:\n\t\treturn None, TimeoutError()\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None, ConnectionError()\n\treturn response.status_code, None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\treturn post_document(url, data, timeout=timeout, *args, **kwargs)\n\telif method == \"put\":\n\t\treturn put_document(url, data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\treturn None, Exception(\"Unknown method %s\" % method)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "ques_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\n\tif not key in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "def dict_insert(dic, val, key, *keys):\n\tif not isinstance(dic, dict):\n\t\traise TypeError('expected dict')\n\n\tdic = dic.copy()\n\tnested = dic.setdefault(key, {})\n\tif keys:\n\t\tdict_insert(nested, val, *keys)\n\telse:\n\t\tnested[key] = val\n\n\treturn dic\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tkey_chain = [key] + list(keys)\n\tif len(key_chain) > 1:\n\t\tdic[key_chain[0]] = dict_insert(dic.get(key_chain[0], {}), val, *key_chain[1:])\n\telse:\n\t\tdic[key_chain[0]] = val\n\treturn dic\n\n", "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tfor k in keys:\n\t\tdic = dic.setdefault(k, {})\n\tdic[key] = val\n\treturn dic\n\n", "def dict_insert(dic, val, key, *keys):\n\tkey = keys and '.'.join([key] + list(keys)) or key\n\treturn dict_set(dic, key, val)\n\n", "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tsubdic = dic.setdefault(key, {})\n\tdict_insert(subdic, val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\n\tif not key in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, keys[0], *keys[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not isinstance(dic, dict):\n\t\traise TypeError('expected dict')\n\n\tdic = dic.copy()\n\tnested = dic.setdefault(key, {})\n\tif keys:\n\t\tdict_insert(nested, val, *keys)\n\telse:\n\t\tnested[key] = val\n\n\treturn dic\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tkey_chain = [key] + list(keys)\n\tif len(key_chain) > 1:\n\t\tdic[key_chain[0]] = dict_insert(dic.get(key_chain[0], {}), val, *key_chain[1:])\n\telse:\n\t\tdic[key_chain[0]] = val\n\treturn dic\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tfor k in keys:\n\t\tdic = dic.setdefault(k, {})\n\tdic[key] = val\n\treturn dic\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tkey = keys and '.'.join([key] + list(keys)) or key\n\treturn dict_set(dic, key, val)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tsubdic = dic.setdefault(key, {})\n\tdict_insert(subdic, val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "ques_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n\tglobal settings_paths\n\t\n\tif spec_option:\n\t\tsettings_paths.append(spec_option)\n\t\treturn spec_option\n\telse:\n\t\tfor s in settings_dirs:\n\t\t\tsettings_paths.append(os.path.join(s, 'settings.ini'))\n\n\t\treturn settings_paths\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tif not isinstance(spec_option, IniType):\n\t\traise IniError(\"list_of_file_names: spec_option must be an IniType\")\n\tif spec_option.type_name != \"string\":\n\t\traise IniError(\"list_of_file_names:: spec_option must be a string\")\n\n\tlist_of_file_names_type = IniType(\"list_of_file_names\", \"list_of_file_names\", None)\n\n\tdef list_of_file_names_validator(value, spec, section_name, option_name):\n\t\t\"\"\"\n\t\tList of file names validator\n\t\t\"\"\"\n\t\tif not isinstance(value, list):\n\t\t\traise IniError(\"value must be a list\")\n\n\t\tfor filename in value:\n\t\t\tspec_option.validate(filename, spec, section_name, option_name)\n\n\t\treturn value\n\n\tlist_of_file_names_type.validate = list_of_file_names_validator\n\n\tdef list_of_file_names_conver\n", "def list_of_file_names(settings_dirs, spec_option):\n\tif not spec_option:\n\t\treturn []\n\n\tfiles_to_read = []\n\tfor dir_path in settings_dirs:\n\t\tfor file_name in os.listdir(dir_path):\n\t\t\tif file_name.endswith(spec_option):\n\t\t\t\tfiles_to_read.append(os.path.join(dir_path, file_name))\n\treturn files_to_read\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn settings_dirs + \"/\" + spec_option + \".ini\"\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tfilenames = []\n\tfor dirname in settings_dirs:\n\t\tfor f in os.listdir(dirname):\n\t\t\tif f.endswith(spec_option + \".ini\"):\n\t\t\t\tfilenames.append(os.path.join(dirname, f))\n\treturn filenames\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tif isinstance(settings_dirs, str):\n\t\tsettings_dirs = [settings_dirs]\n\t\n\t# Get the list of files contained in each directory\n\tfiles_list = []\n\tfor settings_dir in settings_dirs:\n\t\tfor file_name in os.listdir(settings_dir):\n\t\t\tfull_path = os.path.join(settings_dir, file_name)\n\t\t\tfiles_list.append(full_path)\n\t\n\t# Return the list of files\n\treturn files_list\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tfile_names = []\n\tfor settings_dir in settings_dirs:\n\t\tif os.path.exists(settings_dir):\n\t\t\tfor file_name in os.listdir(settings_dir):\n\t\t\t\tif file_name.endswith('.ini'):\n\t\t\t\t\tif spec_option:\n\t\t\t\t\t\tif spec_option in file_name:\n\t\t\t\t\t\t\tfile_names.append(os.path.join(settings_dir, file_name))\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile_names.append(os.path.join(settings_dir, file_name))\n\treturn file_names\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tini_type = new_type('file_list', 'list')\n\tini_type.add_property(new_property('path', 'string'))\n\tini_type.add_property(new_property('recursive', 'boolean', 'false'))\n\tini_type.add_property(new_property('exclude', 'string'))\n\tini_type.add_property(new_property('include', 'string'))\n\tini_type.add_property(new_property('extension', 'string'))\n\tini_type.add_property(new_property('ignore_hidden', 'boolean', 'true'))\n\tini_type.add_property(new_property('full_path', 'boolean', 'false'))\n\tspec_option.type = ini_type\n\treturn ini_type\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tif not isinstance(settings_dirs, list):\n\t\traise Exception(\"settings_dirs must be a list\")\n\n\tif not isinstance(spec_option, str):\n\t\traise Exception(\"spec_option must be a string\")\n\n\toptions_list = []\n\tfor settings_dir in settings_dirs:\n\t\toptions_list.append(os.path.join(settings_dir, spec_option))\n\n\treturn options_list\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\t\n\tfile_names = []\n\tfor settings_dir in settings_dirs:\n\t\tfor file_name in os.listdir(settings_dir):\n\t\t\tif file_name.endswith('.ini'):\n\t\t\t\tfile_names.append(file_name)\n\t\t\t\t\n\treturn file_names\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tglobal settings_paths\n\t\n\tif spec_option:\n\t\tsettings_paths.append(spec_option)\n\t\treturn spec_option\n\telse:\n\t\tfor s in settings_dirs:\n\t\t\tsettings_paths.append(os.path.join(s, 'settings.ini'))\n\n\t\treturn settings_paths\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tif not isinstance(spec_option, IniType):\n\t\traise IniError(\"list_of_file_names: spec_option must be an IniType\")\n\tif spec_option.type_name != \"string\":\n\t\traise IniError(\"list_of_file_names:: spec_option must be a string\")\n\n\tlist_of_file_names_type = IniType(\"list_of_file_names\", \"list_of_file_names\", None)\n\n\tdef list_of_file_names_validator(value, spec, section_name, option_name):\n\t\t\"\"\"\n\t\tList of file names validator\n\t\t\"\"\"\n\t\tif not isinstance(value, list):\n\t\t\traise IniError(\"value must be a list\")\n\n\t\tfor filename in value:\n\t\t\tspec_option.validate(filename, spec, section_name, option_name)\n\n\t\treturn value\n\n\tlist_of_file_names_type.validate = list_of_file_names_validator\n\n\tdef list_of_file_names_conver\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tif not spec_option:\n\t\treturn []\n\n\tfiles_to_read = []\n\tfor dir_path in settings_dirs:\n\t\tfor file_name in os.listdir(dir_path):\n\t\t\tif file_name.endswith(spec_option):\n\t\t\t\tfiles_to_read.append(os.path.join(dir_path, file_name))\n\treturn files_to_read\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn settings_dirs + \"/\" + spec_option + \".ini\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tfilenames = []\n\tfor dirname in settings_dirs:\n\t\tfor f in os.listdir(dirname):\n\t\t\tif f.endswith(spec_option + \".ini\"):\n\t\t\t\tfilenames.append(os.path.join(dirname, f))\n\treturn filenames\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tif isinstance(settings_dirs, str):\n\t\tsettings_dirs = [settings_dirs]\n\t\n\t# Get the list of files contained in each directory\n\tfiles_list = []\n\tfor settings_dir in settings_dirs:\n\t\tfor file_name in os.listdir(settings_dir):\n\t\t\tfull_path = os.path.join(settings_dir, file_name)\n\t\t\tfiles_list.append(full_path)\n\t\n\t# Return the list of files\n\treturn files_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tfile_names = []\n\tfor settings_dir in settings_dirs:\n\t\tif os.path.exists(settings_dir):\n\t\t\tfor file_name in os.listdir(settings_dir):\n\t\t\t\tif file_name.endswith('.ini'):\n\t\t\t\t\tif spec_option:\n\t\t\t\t\t\tif spec_option in file_name:\n\t\t\t\t\t\t\tfile_names.append(os.path.join(settings_dir, file_name))\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile_names.append(os.path.join(settings_dir, file_name))\n\treturn file_names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tini_type = new_type('file_list', 'list')\n\tini_type.add_property(new_property('path', 'string'))\n\tini_type.add_property(new_property('recursive', 'boolean', 'false'))\n\tini_type.add_property(new_property('exclude', 'string'))\n\tini_type.add_property(new_property('include', 'string'))\n\tini_type.add_property(new_property('extension', 'string'))\n\tini_type.add_property(new_property('ignore_hidden', 'boolean', 'true'))\n\tini_type.add_property(new_property('full_path', 'boolean', 'false'))\n\tspec_option.type = ini_type\n\treturn ini_type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tif not isinstance(settings_dirs, list):\n\t\traise Exception(\"settings_dirs must be a list\")\n\n\tif not isinstance(spec_option, str):\n\t\traise Exception(\"spec_option must be a string\")\n\n\toptions_list = []\n\tfor settings_dir in settings_dirs:\n\t\toptions_list.append(os.path.join(settings_dir, spec_option))\n\n\treturn options_list\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\t\n\tfile_names = []\n\tfor settings_dir in settings_dirs:\n\t\tfor file_name in os.listdir(settings_dir):\n\t\t\tif file_name.endswith('.ini'):\n\t\t\t\tfile_names.append(file_name)\n\t\t\t\t\n\treturn file_names\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "ques_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n\n", "def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n", "def ansible_config_manager(cls):\n\treturn AnsibleConfigManager(cls.ansible_config_path)\n\n\n", "def ansible_config_manager(cls):\n\tif not cls._ansible_config_manager:\n\t\tcls._ansible_config_manager = AnsibleConfigManager(\n\t\t\tcls.config_dir,\n\t\t\tcls.config_file,\n\t\t\tcls.config_section\n\t\t)\n\treturn cls._ansible_config_manager\n\n\n", "def ansible_config_manager(cls):\n\treturn cls.ansible_config_manager\n\n", "def ansible_config_manager(cls):\n\treturn cls.ansibleConfigManager\n\n", "def ansible_config_manager(cls):\n\treturn cls.config_manager\n\n", "def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n", "def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n", "def ansible_config_manager(cls):\n\treturn cls._ansible_config_manager\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn AnsibleConfigManager(cls.ansible_config_path)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\tif not cls._ansible_config_manager:\n\t\tcls._ansible_config_manager = AnsibleConfigManager(\n\t\t\tcls.config_dir,\n\t\t\tcls.config_file,\n\t\t\tcls.config_section\n\t\t)\n\treturn cls._ansible_config_manager\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls.ansible_config_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls.ansibleConfigManager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls.config_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls.__ansible_config_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._ansible_config_manager\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "ques_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "def workspace_manager(cls):\n\t\n\treturn cls.__workspace_manager\n\n", "def workspace_manager(cls):\n\ttry:\n\t\treturn cls._workspace_manager\n\texcept AttributeError:\n\t\treturn None\n\n", "def workspace_manager(cls):\n\treturn cls._workspace_manager\n", "def workspace_manager(cls):\n\tif not cls.__workspace_manager:\n\t\tcls.__workspace_manager = WorkspaceManager()\n\treturn cls.__workspace_manager\n\n", "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n\n", "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "def workspace_manager(cls):\n\tif cls.__workspace_manager is None:\n\t\tcls.__workspace_manager = WorkspaceManager()\n\treturn cls.__workspace_manager\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\t\n\treturn cls.__workspace_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\ttry:\n\t\treturn cls._workspace_manager\n\texcept AttributeError:\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._workspace_manager\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\tif not cls.__workspace_manager:\n\t\tcls.__workspace_manager = WorkspaceManager()\n\treturn cls.__workspace_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls.__workspace_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\tif cls.__workspace_manager is None:\n\t\tcls.__workspace_manager = WorkspaceManager()\n\treturn cls.__workspace_manager\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "ques_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n\treturn cls.__instance__.__plugins_manager__\n\n", "def plugins_manager(cls):\n\tif not cls.plugin_manager:\n\t\tcls.plugin_manager = PluginManager()\n\treturn cls.plugin_manager\n\n\n", "def plugins_manager(cls):\n\treturn cls._plugin_manager\n\n", "def plugins_manager(cls):\n\tif cls.__plugins is None:\n\t\tcls.__plugins = plugins.PluginManager()\n\treturn cls.__plugins\n\n", "def plugins_manager(cls):\n\tif cls.__plugins_manager is None:\n\t\tcls.__plugins_manager = PluginManager()\n\t\tcls.__plugins_manager.setPluginPlaces([\"plugins\"])\n\t\tcls.__plugins_manager.collectPlugins()\n\treturn cls.__plugins_manager\n\n", "def plugins_manager(cls):\n\treturn cls._plugins_manager\n\n", "def plugins_manager(cls):\n\tif not hasattr(cls, '_plugins_manager'):\n\t\tcls._plugins_manager = PluginManager(\n\t\t\tplugin_info_ext='plugin',\n\t\t\tplugin_info_dir=cls.plugins_dir,\n\t\t\tplugin_info_ext_list=['py'],\n\t\t\tplugin_package='plugins')\n\treturn cls._plugins_manager\n\n", "def plugins_manager(cls):\n\tif cls._plugins_manager is None:\n\t\tcls._plugins_manager = PluginManager()\n\treturn cls._plugins_manager\n\n", "def plugins_manager(cls):\n\tglobal __plugins_manager__\n\tif __plugins_manager__ is None:\n\t\t__plugins_manager__ = PluginManager()\n\treturn __plugins_manager__\n\n", "def plugins_manager(cls):\n\treturn cls.__plugins_manager\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n\treturn cls.__instance__.__plugins_manager__\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tif not cls.plugin_manager:\n\t\tcls.plugin_manager = PluginManager()\n\treturn cls.plugin_manager\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._plugin_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tif cls.__plugins is None:\n\t\tcls.__plugins = plugins.PluginManager()\n\treturn cls.__plugins\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tif cls.__plugins_manager is None:\n\t\tcls.__plugins_manager = PluginManager()\n\t\tcls.__plugins_manager.setPluginPlaces([\"plugins\"])\n\t\tcls.__plugins_manager.collectPlugins()\n\treturn cls.__plugins_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._plugins_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tif not hasattr(cls, '_plugins_manager'):\n\t\tcls._plugins_manager = PluginManager(\n\t\t\tplugin_info_ext='plugin',\n\t\t\tplugin_info_dir=cls.plugins_dir,\n\t\t\tplugin_info_ext_list=['py'],\n\t\t\tplugin_package='plugins')\n\treturn cls._plugins_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tif cls._plugins_manager is None:\n\t\tcls._plugins_manager = PluginManager()\n\treturn cls._plugins_manager\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tglobal __plugins_manager__\n\tif __plugins_manager__ is None:\n\t\t__plugins_manager__ = PluginManager()\n\treturn __plugins_manager__\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls.__plugins_manager\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "ques_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\t\tif spec_data is None:\n\t\t\traise IRValidatorException(\"Spec file is empty\")\n\t\tcls.validate_spec_structure(spec_data)\n\texcept Exception as ex:\n\t\traise IRValidatorException(\"Cannot validate spec file: \" + str(ex))\n\treturn spec_data\n\n\n", "def validate_from_content(cls, spec_content=None):\n\tspec_data = cls._load_data_from_content(spec_content)\n\tcls._validate_name(spec_data)\n\tcls._validate_version(spec_data)\n\tcls._validate_description(spec_data)\n\tcls._validate_author(spec_data)\n\tcls._validate_maintainer(spec_data)\n\tcls._validate_license(spec_data)\n\tcls._validate_url(spec_data)\n\tcls._validate_build(spec_data)\n\tcls._validate_architecture(spec_data)\n\tcls._validate_type(spec_data)\n\tcls._validate_vm_type(spec_data)\n\tcls._validate_vm_image(spec_data)\n\tcls._validate_vm_system_image(spec_data)\n\tcls._validate_vm_user_data(spec_data)\n\tcls._validate_vm_meta_data(spec_data)\n\tcls._validate\n", "def validate_from_content(cls, spec_content=None):\n\tdata = yaml.load(spec_content)\n\tif not data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! No data found!')\n\tif 'name' not in data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Name is missing!')\n\tif not data['name']:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Name is empty!')\n\tif 'version' not in data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Version is missing!')\n\tif not data['version']:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Version is empty!')\n\tif 'description' not in data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Description is missing!')\n\tif not data['description']:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Description is empty!')\n\tif 'container' not in data:\n\t\traise IRValidatorException(\n\t\n", "def validate_from_content(cls, spec_content=None):\n\tspec_dict = load_yaml_string(spec_content)\n\tif 'name' not in spec_dict:\n\t\traise IRValidatorException(\n\t\t\t'name is missing from spec file')\n\tif 'version' not in spec_dict:\n\t\traise IRValidatorException(\n\t\t\t'version is missing from spec file')\n\treturn spec_dict\n\n\n", "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Spec content is empty\")\n\n\tspec_data = yaml.load(spec_content)\n\n\treturn cls.validate_from_dict(spec_data)\n\n\n", "def validate_from_content(cls, spec_content=None):\n\tspec_data = yaml.load(spec_content, Loader=yaml.SafeLoader)\n\n\tif not spec_data:\n\t\traise IRValidatorException(\"Spec file is empty\")\n\n\tif not cls.check_field_exists(spec_data, \"version\"):\n\t\traise IRValidatorException(\"'version' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"name\"):\n\t\traise IRValidatorException(\"'name' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"description\"):\n\t\traise IRValidatorException(\"'description' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"author\"):\n\t\traise IRValidatorException(\"'author' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"input\"):\n\t\traise IRValidatorException(\"'input' field is missing in spec file\")\n\n\tif not cls.\n", "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Missing spec file content\")\n\n\t# load YAML data\n\ttry:\n\t\tdata = yaml.load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\n\t\t\t\"Unable to parse YAML spec file: %s\" % str(exc))\n\n\treturn cls.validate_from_dict(data)\n\n\n", "def validate_from_content(cls, spec_content=None):\n\ttry:\n\t\tspec_data=cls.load_from_content(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException(str(e))\n\n\tif not isinstance(spec_data, dict):\n\t\traise IRValidatorException(\n\t\t\t'Internal error: expected data to be a dictionary, got {0}'.format(type(spec_data)))\n\n\tif 'name' not in spec_data:\n\t\traise IRValidatorException('Missing spec name')\n\n\tif 'version' not in spec_data:\n\t\traise IRValidatorException('Missing spec version')\n\n\tif 'type' not in spec_data:\n\t\traise IRValidatorException('Missing spec type')\n\n\tif 'namespace' not in spec_data:\n\t\traise IRValidatorException('Missing spec namespace')\n\n\tif 'description' not in spec_data:\n\t\traise IRValidatorException('Missing spec description')\n\n\tif 'versions' not in spec_data:\n\t\traise IRValidatorException('Missing spec versions')\n\n\tif spec_data['type']=='app':\n", "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Cannot validate empty spec content\")\n\n\tdata = yaml.load(spec_content)\n\tif 'id' not in data:\n\t\traise IRValidatorException(\"'id' field is missing\")\n\tif 'type' not in data:\n\t\traise IRValidatorException(\"'type' field is missing\")\n\tif 'name' not in data:\n\t\traise IRValidatorException(\"'name' field is missing\")\n\tif 'description' not in data:\n\t\traise IRValidatorException(\"'description' field is missing\")\n\tif 'version' not in data:\n\t\traise IRValidatorException(\"'version' field is missing\")\n\tif 'state' not in data:\n\t\traise IRValidatorException(\"'state' field is missing\")\n\tif 'template' not in data:\n\t\traise IRValidatorException(\"'template' field is missing\")\n\n\treturn data\n\n\n", "def validate_from_content(cls, spec_content=None):\n\ttry:\n\t\tif not spec_content:\n\t\t\traise IRValidatorException(\"spec content is empty\")\n\t\tspec_data = yaml.safe_load(spec_content)\n\t\tif not spec_data:\n\t\t\traise IRValidatorException(\"spec content is empty\")\n\t\treturn spec_data\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Cannot parse spec content: {}\".format(exc))\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n\ttry:\n\t\tspec_data = yaml.safe_load(spec_content)\n\t\tif spec_data is None:\n\t\t\traise IRValidatorException(\"Spec file is empty\")\n\t\tcls.validate_spec_structure(spec_data)\n\texcept Exception as ex:\n\t\traise IRValidatorException(\"Cannot validate spec file: \" + str(ex))\n\treturn spec_data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tspec_data = cls._load_data_from_content(spec_content)\n\tcls._validate_name(spec_data)\n\tcls._validate_version(spec_data)\n\tcls._validate_description(spec_data)\n\tcls._validate_author(spec_data)\n\tcls._validate_maintainer(spec_data)\n\tcls._validate_license(spec_data)\n\tcls._validate_url(spec_data)\n\tcls._validate_build(spec_data)\n\tcls._validate_architecture(spec_data)\n\tcls._validate_type(spec_data)\n\tcls._validate_vm_type(spec_data)\n\tcls._validate_vm_image(spec_data)\n\tcls._validate_vm_system_image(spec_data)\n\tcls._validate_vm_user_data(spec_data)\n\tcls._validate_vm_meta_data(spec_data)\n\tcls._validate\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tdata = yaml.load(spec_content)\n\tif not data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! No data found!')\n\tif 'name' not in data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Name is missing!')\n\tif not data['name']:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Name is empty!')\n\tif 'version' not in data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Version is missing!')\n\tif not data['version']:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Version is empty!')\n\tif 'description' not in data:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Description is missing!')\n\tif not data['description']:\n\t\traise IRValidatorException(\n\t\t\t'Invalid spec file! Description is empty!')\n\tif 'container' not in data:\n\t\traise IRValidatorException(\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tspec_dict = load_yaml_string(spec_content)\n\tif 'name' not in spec_dict:\n\t\traise IRValidatorException(\n\t\t\t'name is missing from spec file')\n\tif 'version' not in spec_dict:\n\t\traise IRValidatorException(\n\t\t\t'version is missing from spec file')\n\treturn spec_dict\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Spec content is empty\")\n\n\tspec_data = yaml.load(spec_content)\n\n\treturn cls.validate_from_dict(spec_data)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tspec_data = yaml.load(spec_content, Loader=yaml.SafeLoader)\n\n\tif not spec_data:\n\t\traise IRValidatorException(\"Spec file is empty\")\n\n\tif not cls.check_field_exists(spec_data, \"version\"):\n\t\traise IRValidatorException(\"'version' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"name\"):\n\t\traise IRValidatorException(\"'name' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"description\"):\n\t\traise IRValidatorException(\"'description' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"author\"):\n\t\traise IRValidatorException(\"'author' field is missing in spec file\")\n\n\tif not cls.check_field_exists(spec_data, \"input\"):\n\t\traise IRValidatorException(\"'input' field is missing in spec file\")\n\n\tif not cls.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Missing spec file content\")\n\n\t# load YAML data\n\ttry:\n\t\tdata = yaml.load(spec_content)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\n\t\t\t\"Unable to parse YAML spec file: %s\" % str(exc))\n\n\treturn cls.validate_from_dict(data)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\ttry:\n\t\tspec_data=cls.load_from_content(spec_content)\n\texcept Exception as e:\n\t\traise IRValidatorException(str(e))\n\n\tif not isinstance(spec_data, dict):\n\t\traise IRValidatorException(\n\t\t\t'Internal error: expected data to be a dictionary, got {0}'.format(type(spec_data)))\n\n\tif 'name' not in spec_data:\n\t\traise IRValidatorException('Missing spec name')\n\n\tif 'version' not in spec_data:\n\t\traise IRValidatorException('Missing spec version')\n\n\tif 'type' not in spec_data:\n\t\traise IRValidatorException('Missing spec type')\n\n\tif 'namespace' not in spec_data:\n\t\traise IRValidatorException('Missing spec namespace')\n\n\tif 'description' not in spec_data:\n\t\traise IRValidatorException('Missing spec description')\n\n\tif 'versions' not in spec_data:\n\t\traise IRValidatorException('Missing spec versions')\n\n\tif spec_data['type']=='app':\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"Cannot validate empty spec content\")\n\n\tdata = yaml.load(spec_content)\n\tif 'id' not in data:\n\t\traise IRValidatorException(\"'id' field is missing\")\n\tif 'type' not in data:\n\t\traise IRValidatorException(\"'type' field is missing\")\n\tif 'name' not in data:\n\t\traise IRValidatorException(\"'name' field is missing\")\n\tif 'description' not in data:\n\t\traise IRValidatorException(\"'description' field is missing\")\n\tif 'version' not in data:\n\t\traise IRValidatorException(\"'version' field is missing\")\n\tif 'state' not in data:\n\t\traise IRValidatorException(\"'state' field is missing\")\n\tif 'template' not in data:\n\t\traise IRValidatorException(\"'template' field is missing\")\n\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\ttry:\n\t\tif not spec_content:\n\t\t\traise IRValidatorException(\"spec content is empty\")\n\t\tspec_data = yaml.safe_load(spec_content)\n\t\tif not spec_data:\n\t\t\traise IRValidatorException(\"spec content is empty\")\n\t\treturn spec_data\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Cannot parse spec content: {}\".format(exc))\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "ques_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\treturn {}\n\n\tif not os.path.exists(yaml_file):\n\t\traise IRValidatorException('Can not find file: {}'.format(yaml_file))\n\n\tyaml_data = None\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_data = yaml.load(f.read())\n\texcept Exception as e:\n\t\traise IRValidatorException('Can not load YAML file: {}'.format(e))\n\n\tif yaml_data is None:\n\t\traise IRValidatorException('YAML file is empty: {}'.format(yaml_file))\n\n\ttry:\n\t\tcls.validate(yaml_data)\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException('{file}: {error}'.format(file=yaml_file, error=e))\n\n\treturn yaml_data\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file:\n\t\traise IRValidatorException(\"YAML File not set\")\n\t\n\tdata = None\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tdata = yaml.load(stream)\n\texcept (IOError, OSError) as e:\n\t\traise IRValidatorException(\"YAML File not found: \"+str(e))\n\t\n\tvalidator = cls(data)\n\tvalidator.validate()\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\tyaml_data = cls.load_from_file(yaml_file)\n\tif yaml_data is None:\n\t\treturn None\n\n\tcls.validate_data(yaml_data)\n\treturn yaml_data\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tdata = yaml.load(stream)\n\t\t\tcls.validate(data)\n\t\treturn data\n\texcept (FileNotFoundError, yaml.YAMLError):\n\t\traise IRValidatorException(\"Invalid YAML file\")\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tdata = yaml.load(stream)\n\t\t\t\t# Load data from yaml file\n\t\t\t\tcls.validate(data)\n\t\t\t\treturn data\n\t\t\texcept yaml.YAMLError as ex:\n\t\t\t\tLOG.error(ex)\n\t\t\t\traise Exception(ex)\n\texcept IOError as ex:\n\t\tLOG.error(ex)\n\t\traise Exception(ex)\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tdata_dict = cls.load_from_file(yaml_file)\n\tcls.validate(data_dict)\n\treturn data_dict\n\n\t@classmethod\n\tdef load_from_file(cls, yaml_file=None):\n\t\t\"\"\"\n\t\tLoads a YAML file\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file or not os.path.isfile(yaml_file):\n\t\traise IRValidatorException('YAML file not found: %s' % yaml_file)\n\twith open(yaml_file) as f:\n\t\tyaml_data = yaml.load(f)\n\t\treturn cls.validate_from_dict(yaml_data)\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\traise IRValidatorException(\"Missing YAML file\")\n\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream, Loader=yaml.FullLoader)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"Error loading YAML file: \" + yaml_file + \" \" + str(exc))\n\n\treturn cls.validate(data)\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\tyaml_file = cls.yaml_file_path\n\n\twith io.open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tprint(exc)\n\t\t\traise IRValidatorException(exc)\n\n\treturn data\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\tyaml_file = cls.yaml_file\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = yaml.load(f)\n\texcept IOError as e:\n\t\tmsg = 'Error reading file %s: %s' % (yaml_file, e)\n\t\traise IRValidatorException(msg)\n\texcept yaml.YAMLError as e:\n\t\tmsg = 'Invalid YAML syntax in file %s: %s' % (yaml_file, e)\n\t\traise IRValidatorException(msg)\n\tif not data:\n\t\tmsg = 'No data found in file %s' % yaml_file\n\t\traise IRValidatorException(msg)\n\telse:\n\t\tcls.validate_data(data)\n\t\treturn data\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\treturn {}\n\n\tif not os.path.exists(yaml_file):\n\t\traise IRValidatorException('Can not find file: {}'.format(yaml_file))\n\n\tyaml_data = None\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tyaml_data = yaml.load(f.read())\n\texcept Exception as e:\n\t\traise IRValidatorException('Can not load YAML file: {}'.format(e))\n\n\tif yaml_data is None:\n\t\traise IRValidatorException('YAML file is empty: {}'.format(yaml_file))\n\n\ttry:\n\t\tcls.validate(yaml_data)\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException('{file}: {error}'.format(file=yaml_file, error=e))\n\n\treturn yaml_data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file:\n\t\traise IRValidatorException(\"YAML File not set\")\n\t\n\tdata = None\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tdata = yaml.load(stream)\n\texcept (IOError, OSError) as e:\n\t\traise IRValidatorException(\"YAML File not found: \"+str(e))\n\t\n\tvalidator = cls(data)\n\tvalidator.validate()\n\treturn data\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tyaml_data = cls.load_from_file(yaml_file)\n\tif yaml_data is None:\n\t\treturn None\n\n\tcls.validate_data(yaml_data)\n\treturn yaml_data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tdata = yaml.load(stream)\n\t\t\tcls.validate(data)\n\t\treturn data\n\texcept (FileNotFoundError, yaml.YAMLError):\n\t\traise IRValidatorException(\"Invalid YAML file\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\ttry:\n\t\t\t\tdata = yaml.load(stream)\n\t\t\t\t# Load data from yaml file\n\t\t\t\tcls.validate(data)\n\t\t\t\treturn data\n\t\t\texcept yaml.YAMLError as ex:\n\t\t\t\tLOG.error(ex)\n\t\t\t\traise Exception(ex)\n\texcept IOError as ex:\n\t\tLOG.error(ex)\n\t\traise Exception(ex)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tdata_dict = cls.load_from_file(yaml_file)\n\tcls.validate(data_dict)\n\treturn data_dict\n\n\t@classmethod\n\tdef load_from_file(cls, yaml_file=None):\n\t\t\"\"\"\n\t\tLoads a YAML file\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file or not os.path.isfile(yaml_file):\n\t\traise IRValidatorException('YAML file not found: %s' % yaml_file)\n\twith open(yaml_file) as f:\n\t\tyaml_data = yaml.load(f)\n\t\treturn cls.validate_from_dict(yaml_data)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\traise IRValidatorException(\"Missing YAML file\")\n\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream, Loader=yaml.FullLoader)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"Error loading YAML file: \" + yaml_file + \" \" + str(exc))\n\n\treturn cls.validate(data)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\tyaml_file = cls.yaml_file_path\n\n\twith io.open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tprint(exc)\n\t\t\traise IRValidatorException(exc)\n\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\tyaml_file = cls.yaml_file\n\ttry:\n\t\twith open(yaml_file, 'r') as f:\n\t\t\tdata = yaml.load(f)\n\texcept IOError as e:\n\t\tmsg = 'Error reading file %s: %s' % (yaml_file, e)\n\t\traise IRValidatorException(msg)\n\texcept yaml.YAMLError as e:\n\t\tmsg = 'Invalid YAML syntax in file %s: %s' % (yaml_file, e)\n\t\traise IRValidatorException(msg)\n\tif not data:\n\t\tmsg = 'No data found in file %s' % yaml_file\n\t\traise IRValidatorException(msg)\n\telse:\n\t\tcls.validate_data(data)\n\t\treturn data\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "ques_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n\tif 'include_groups' not in parser_dict:\n\t\treturn\n\n\tfor group_name in parser_dict['include_groups']:\n\t\tif group_name in self._groups:\n\t\t\tgroup = self._groups[group_name]\n\t\t\tself._include_groups(group)\n\t\t\tparser_dict.update(group)\n\t\t\tdel parser_dict['include_groups']\n\n\n", "def _include_groups(self, parser_dict):\n\tif 'include' in parser_dict:\n\t\tif not isinstance(parser_dict['include'], list):\n\t\t\tparser_dict['include'] = [parser_dict['include']]\n\n\t\t# if an include group is not defined, skip it\n\t\tif not parser_dict['include']:\n\t\t\treturn\n\n\t\t# include the group\n\t\tfor group in parser_dict['include']:\n\t\t\tself.__include_group(group)\n\n\t\t# delete the include directive\n\t\tdel parser_dict['include']\n\n\t# check if groups are defined\n\tif 'groups' in parser_dict:\n\t\t# iterate over groups\n\t\tfor group_name, group in parser_dict['groups'].items():\n\t\t\tself._include_groups(group)\n\n", "def _include_groups(self, parser_dict):\n\tinclude_groups = parser_dict.get('include_groups', None)\n\tif include_groups is None:\n\t\treturn parser_dict\n\telse:\n\t\tdel parser_dict['include_groups']\n\t\tfor include_group in include_groups:\n\t\t\tinclude_group_path = os.path.join(self.include_groups_path, include_group+'.yml')\n\t\t\tif os.path.exists(include_group_path):\n\t\t\t\tinclude_group_dict = self._load_group(include_group_path)\n\t\t\t\tif include_group_dict != None:\n\t\t\t\t\tparser_dict = self._include_groups(include_group_dict)\n\t\t\t\t\tfor key,value in include_group_dict.iteritems():\n\t\t\t\t\t\tif key in parser_dict:\n\t\t\t\t\t\t\tparser_dict[key].update(value)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tparser_dict[key] = value\n\t\t\t\telse:\n\t\t\n", "def _include_groups(self, parser_dict):\n\tif \"include\" in parser_dict:\n\t\tfor include_item in parser_dict[\"include\"]:\n\t\t\tif include_item not in self.group_dict:\n\t\t\t\traise Exception(\"include item not found in group dict: %s\" % include_item)\n\t\t\tparser_dict.update(self.group_dict[include_item])\n\t\tdel parser_dict[\"include\"]\n\treturn parser_dict\n\n", "def _include_groups(self, parser_dict):\n\tif \"include\" in parser_dict:\n\t\tif \"groups\" in parser_dict[\"include\"]:\n\t\t\tfor group in parser_dict[\"include\"][\"groups\"]:\n\t\t\t\tif group not in self.groups_dict:\n\t\t\t\t\traise Exception(\"Group %s not found\" % group)\n\t\t\t\t# merge the group dicts\n\t\t\t\tfor key in self.groups_dict[group]:\n\t\t\t\t\t# if key already exists in the parser_dict, merge the values\n\t\t\t\t\tif key in parser_dict:\n\t\t\t\t\t\tparser_dict[key] += self.groups_dict[group][key]\n\t\t\t\t\telse:\n\t\t\t\t\t\tparser_dict[key] = self.groups_dict[group][key]\n\t\t\t\t# delete the include section\n\t\t\t\tdel parser_dict[\"include\"]\n\n\treturn parser_dict\n\n\n", "def _include_groups(self, parser_dict):\n\tif \"include\" in parser_dict:\n\t\t# If there is an include directive and it is a list,\n\t\t# it must be a list of groups.\n\t\tif isinstance(parser_dict[\"include\"], list):\n\t\t\t# For each group, we need to include all the options defined in the group.\n\t\t\tfor group in parser_dict[\"include\"]:\n\t\t\t\t# If the group is a string, it must be the name of a group.\n\t\t\t\tif isinstance(group, str):\n\t\t\t\t\tif group not in self._groups:\n\t\t\t\t\t\traise ValueError(\"group '{}' does not exist\".format(group))\n\t\t\t\t\t# Merge the group options into the parser.\n\t\t\t\t\tself._include_groups(parser_dict, self._groups[group])\n\t\t\t\t# Otherwise, it must be a dict of options.\n\t\t\t\telif isinstance(group, dict):\n\t\t\t\t\t# Merge the group options into the parser.\n\t\t\t\t\tself._include_groups(parser_\n", "def _include_groups(self, parser_dict):\n\tinclude_dict = parser_dict.get('include_dict', None)\n\tif include_dict is None:\n\t\treturn parser_dict\n\n\tincluded_dicts = []\n\n\tfor inc_dict in include_dict:\n\t\tincluded_dict = self.parser_dicts.get(inc_dict, None)\n\t\tif included_dict is None:\n\t\t\traise Exception('Spec file includes \"%s\" dictionary, but it was not found in the config file' % inc_dict)\n\t\tincluded_dicts.append(included_dict)\n\n\t# Now, merge all the included dicts into the current parser_dict.\n\t# Start with the first included dict.\n\tfor inc_dict in included_dicts:\n\t\tparser_dict = self._merge_dicts(parser_dict, inc_dict)\n\n\treturn parser_dict\n\n\n", "def _include_groups(self, parser_dict):\n\tfor k, v in parser_dict.items():\n\t\tif isinstance(v, dict) and 'include_groups' in v:\n\t\t\tparser_dict[k]['include_groups']['group_list'] = [\n\t\t\t\tself.config_dict[x] for x in parser_dict[k]['include_groups']['group_list']\n\t\t\t]\n\t\t\tparser_dict[k].pop('include_groups')\n\t\t\tparser_dict[k]['groups'] = parser_dict[k]['include_groups']['group_list']\n\t\telif isinstance(v, dict):\n\t\t\tself._include_groups(v)\n\n\n", "def _include_groups(self, parser_dict):\n\tfor group_name, group_dict in parser_dict.items():\n\t\tif group_name == 'include':\n\t\t\tfor sub_group_name, sub_group_dict in group_dict.items():\n\t\t\t\tif sub_group_name == 'dict':\n\t\t\t\t\t# Include the whole dict\n\t\t\t\t\tfor key, value in sub_group_dict.items():\n\t\t\t\t\t\tparser_dict[key] = value\n\t\t\t\telse:\n\t\t\t\t\t# Include a part of the dict\n\t\t\t\t\tparser_dict[sub_group_name] = sub_group_dict\n\t\t\t\t\tfor key, value in sub_group_dict.items():\n\t\t\t\t\t\tif key == 'include':\n\t\t\t\t\t\t\tself._include_groups(sub_group_dict)\n\n\t\telif isinstance(group_dict, dict):\n\t\t\tfor key, value in group_dict.items():\n\t\t\t\tif key == 'include':\n\t\t\t\t\tself._include_groups(group\n", "def _include_groups(self, parser_dict):\n\tif 'include_groups' in parser_dict:\n\t\tfor group_id in parser_dict['include_groups']:\n\t\t\tif group_id in self.groups:\n\t\t\t\tgroup = self.groups[group_id]\n\t\t\t\tfor directive in group.directives:\n\t\t\t\t\tif directive not in parser_dict:\n\t\t\t\t\t\tparser_dict[directive] = group.directives[directive]\n\t\t\t\t\t\tif 'include_groups' in parser_dict[directive]:\n\t\t\t\t\t\t\tself._include_groups(parser_dict[directive])\n\t\t\t\n\t\t\telse:\n\t\t\t\traise RuntimeError(\"Group %s not found in groups dict\" % group_id)\n\t\n\t# remove the include_groups from the dict\n\tdel parser_dict['include_groups']\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n\tif 'include_groups' not in parser_dict:\n\t\treturn\n\n\tfor group_name in parser_dict['include_groups']:\n\t\tif group_name in self._groups:\n\t\t\tgroup = self._groups[group_name]\n\t\t\tself._include_groups(group)\n\t\t\tparser_dict.update(group)\n\t\t\tdel parser_dict['include_groups']\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif 'include' in parser_dict:\n\t\tif not isinstance(parser_dict['include'], list):\n\t\t\tparser_dict['include'] = [parser_dict['include']]\n\n\t\t# if an include group is not defined, skip it\n\t\tif not parser_dict['include']:\n\t\t\treturn\n\n\t\t# include the group\n\t\tfor group in parser_dict['include']:\n\t\t\tself.__include_group(group)\n\n\t\t# delete the include directive\n\t\tdel parser_dict['include']\n\n\t# check if groups are defined\n\tif 'groups' in parser_dict:\n\t\t# iterate over groups\n\t\tfor group_name, group in parser_dict['groups'].items():\n\t\t\tself._include_groups(group)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tinclude_groups = parser_dict.get('include_groups', None)\n\tif include_groups is None:\n\t\treturn parser_dict\n\telse:\n\t\tdel parser_dict['include_groups']\n\t\tfor include_group in include_groups:\n\t\t\tinclude_group_path = os.path.join(self.include_groups_path, include_group+'.yml')\n\t\t\tif os.path.exists(include_group_path):\n\t\t\t\tinclude_group_dict = self._load_group(include_group_path)\n\t\t\t\tif include_group_dict != None:\n\t\t\t\t\tparser_dict = self._include_groups(include_group_dict)\n\t\t\t\t\tfor key,value in include_group_dict.iteritems():\n\t\t\t\t\t\tif key in parser_dict:\n\t\t\t\t\t\t\tparser_dict[key].update(value)\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tparser_dict[key] = value\n\t\t\t\telse:\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif \"include\" in parser_dict:\n\t\tfor include_item in parser_dict[\"include\"]:\n\t\t\tif include_item not in self.group_dict:\n\t\t\t\traise Exception(\"include item not found in group dict: %s\" % include_item)\n\t\t\tparser_dict.update(self.group_dict[include_item])\n\t\tdel parser_dict[\"include\"]\n\treturn parser_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif \"include\" in parser_dict:\n\t\tif \"groups\" in parser_dict[\"include\"]:\n\t\t\tfor group in parser_dict[\"include\"][\"groups\"]:\n\t\t\t\tif group not in self.groups_dict:\n\t\t\t\t\traise Exception(\"Group %s not found\" % group)\n\t\t\t\t# merge the group dicts\n\t\t\t\tfor key in self.groups_dict[group]:\n\t\t\t\t\t# if key already exists in the parser_dict, merge the values\n\t\t\t\t\tif key in parser_dict:\n\t\t\t\t\t\tparser_dict[key] += self.groups_dict[group][key]\n\t\t\t\t\telse:\n\t\t\t\t\t\tparser_dict[key] = self.groups_dict[group][key]\n\t\t\t\t# delete the include section\n\t\t\t\tdel parser_dict[\"include\"]\n\n\treturn parser_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif \"include\" in parser_dict:\n\t\t# If there is an include directive and it is a list,\n\t\t# it must be a list of groups.\n\t\tif isinstance(parser_dict[\"include\"], list):\n\t\t\t# For each group, we need to include all the options defined in the group.\n\t\t\tfor group in parser_dict[\"include\"]:\n\t\t\t\t# If the group is a string, it must be the name of a group.\n\t\t\t\tif isinstance(group, str):\n\t\t\t\t\tif group not in self._groups:\n\t\t\t\t\t\traise ValueError(\"group '{}' does not exist\".format(group))\n\t\t\t\t\t# Merge the group options into the parser.\n\t\t\t\t\tself._include_groups(parser_dict, self._groups[group])\n\t\t\t\t# Otherwise, it must be a dict of options.\n\t\t\t\telif isinstance(group, dict):\n\t\t\t\t\t# Merge the group options into the parser.\n\t\t\t\t\tself._include_groups(parser_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tinclude_dict = parser_dict.get('include_dict', None)\n\tif include_dict is None:\n\t\treturn parser_dict\n\n\tincluded_dicts = []\n\n\tfor inc_dict in include_dict:\n\t\tincluded_dict = self.parser_dicts.get(inc_dict, None)\n\t\tif included_dict is None:\n\t\t\traise Exception('Spec file includes \"%s\" dictionary, but it was not found in the config file' % inc_dict)\n\t\tincluded_dicts.append(included_dict)\n\n\t# Now, merge all the included dicts into the current parser_dict.\n\t# Start with the first included dict.\n\tfor inc_dict in included_dicts:\n\t\tparser_dict = self._merge_dicts(parser_dict, inc_dict)\n\n\treturn parser_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tfor k, v in parser_dict.items():\n\t\tif isinstance(v, dict) and 'include_groups' in v:\n\t\t\tparser_dict[k]['include_groups']['group_list'] = [\n\t\t\t\tself.config_dict[x] for x in parser_dict[k]['include_groups']['group_list']\n\t\t\t]\n\t\t\tparser_dict[k].pop('include_groups')\n\t\t\tparser_dict[k]['groups'] = parser_dict[k]['include_groups']['group_list']\n\t\telif isinstance(v, dict):\n\t\t\tself._include_groups(v)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tfor group_name, group_dict in parser_dict.items():\n\t\tif group_name == 'include':\n\t\t\tfor sub_group_name, sub_group_dict in group_dict.items():\n\t\t\t\tif sub_group_name == 'dict':\n\t\t\t\t\t# Include the whole dict\n\t\t\t\t\tfor key, value in sub_group_dict.items():\n\t\t\t\t\t\tparser_dict[key] = value\n\t\t\t\telse:\n\t\t\t\t\t# Include a part of the dict\n\t\t\t\t\tparser_dict[sub_group_name] = sub_group_dict\n\t\t\t\t\tfor key, value in sub_group_dict.items():\n\t\t\t\t\t\tif key == 'include':\n\t\t\t\t\t\t\tself._include_groups(sub_group_dict)\n\n\t\telif isinstance(group_dict, dict):\n\t\t\tfor key, value in group_dict.items():\n\t\t\t\tif key == 'include':\n\t\t\t\t\tself._include_groups(group\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif 'include_groups' in parser_dict:\n\t\tfor group_id in parser_dict['include_groups']:\n\t\t\tif group_id in self.groups:\n\t\t\t\tgroup = self.groups[group_id]\n\t\t\t\tfor directive in group.directives:\n\t\t\t\t\tif directive not in parser_dict:\n\t\t\t\t\t\tparser_dict[directive] = group.directives[directive]\n\t\t\t\t\t\tif 'include_groups' in parser_dict[directive]:\n\t\t\t\t\t\t\tself._include_groups(parser_dict[directive])\n\t\t\t\n\t\t\telse:\n\t\t\t\traise RuntimeError(\"Group %s not found in groups dict\" % group_id)\n\t\n\t# remove the include_groups from the dict\n\tdel parser_dict['include_groups']\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "ques_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n\t# Assign default values from spec\n\tself.args.update(self.spec['arguments'])\n\n\t# Assign default values from environment\n\tfor key, value in self.spec['arguments'].items():\n\t\tif key not in self.args:\n\t\t\tif key in os.environ:\n\t\t\t\tself.args[key] = os.environ[key]\n\n\t# Assign default values from defaults\n\tfor key, value in self.defaults.items():\n\t\tif key not in self.args:\n\t\t\tself.args[key] = value\n\n\t# Assign default values from global defaults\n\tfor key, value in _GLOBAL_DEFAULTS.items():\n\t\tif key not in self.args:\n\t\t\tself.args[key] = value\n\n", "def get_spec_defaults(self):\n\tif self.spec is None:\n\t\traise ValueError(\"spec is not set\")\n\tif self.name is None:\n\t\traise ValueError(\"name is not set\")\n\n\tself.set_target_profile(self.spec.get(\"target_profile\"))\n\tif self.target_profile is None:\n\t\traise ValueError(\"target_profile is not set\")\n\n\tself.title = self.spec.get(\"title\", self.name.replace(\"_\", \" \").title())\n\tself.categories = self.spec.get(\"categories\", [\"X-MandrivaLinux\"])\n\tself.summary = self.spec.get(\"summary\", \"\")\n\tself.description = self.spec.get(\"description\", \"\")\n\tself.icon = self.spec.get(\"icon\", \"\")\n\tself.url = self.spec.get(\"url\", \"\")\n\tself.keywords = self.spec.get(\"keywords\", [])\n\tself.buildrequires = self.spec.get(\"buildrequires\", [])\n\tself.requires = self.spec.get(\"requires\", [])\n\tself\n", "def get_spec_defaults(self):\n\tresult = self.spec_values.copy()\n\tfor k, v in result.iteritems():\n\t\tif v is None:\n\t\t\tresult[k] = self.get_default_value(k)\n\treturn result\n\n", "def get_spec_defaults(self):\n\tspec_defaults = {}\n\tfor arg, value in six.iteritems(self.arguments):\n\t\tif value is None:\n\t\t\ttry:\n\t\t\t\tspec_defaults[arg] = self.spec[arg]['default']\n\t\t\texcept KeyError:\n\t\t\t\traise HttpError(404, u'Missing argument \"%s\"' % arg)\n\t\telse:\n\t\t\tspec_defaults[arg] = value\n\treturn spec_defaults\n\n", "def get_spec_defaults(self):\n\tif self.spec_args is None:\n\t\tself.spec_args = {}\n\t\tfor arg in self.args:\n\t\t\tif arg.name in self.spec_args:\n\t\t\t\traise InvalidSpec(\n\t\t\t\t\t\"invalid spec: duplicate argument '%s'\" % arg.name)\n\t\t\tif arg.name in self.spec.arg_defaults:\n\t\t\t\tself.spec_args[arg.name] = self.spec.arg_defaults[arg.name]\n\t\t\telif arg.has_default:\n\t\t\t\tself.spec_args[arg.name] = arg.default\n\t\t\telse:\n\t\t\t\traise InvalidSpec(\n\t\t\t\t\t\"invalid spec: argument '%s' has no default\" % arg.name)\n\treturn self.spec_args\n\n", "def get_spec_defaults(self):\n\tif self.args is None:\n\t\tself.args = self.spec.arguments\n\t\tself.spec_defaults = self.spec.default_values\n\t\tif self.spec_defaults is None:\n\t\t\tself.spec_defaults = {}\n\telse:\n\t\tself.spec_defaults = {}\n\tif self.spec_defaults is not None:\n\t\tfor key in self.spec_defaults:\n\t\t\tif not self.args.has_key(key):\n\t\t\t\tself.args[key] = self.spec_defaults[key]\n\t\t\t\tself.arg_names.append(key)\n\t\t\telse:\n\t\t\t\tself.arg_names.append(key)\n\telif self.args is not None:\n\t\tself.arg_names = self.args.keys()\n\t\tself.arg_names.sort()\n\telse:\n\t\tself.arg_names = []\n\tif self.spec.return_type is not None:\n\t\tself.return_type = self.spec.return_\n", "def get_spec_defaults(self):\n\tfor k in self.args:\n\t\tif self.args[k] is None:\n\t\t\tif k in self.spec.defaults:\n\t\t\t\tself.args[k] = self.spec.defaults[k]\n\t\t\telse:\n\t\t\t\traise ValueError(\"Argument '{}' has no default value.\".format(k))\n\n\tdef get_arg_value(self, arg_name, arg_type, arg_value):\n\t\t\"\"\"\n\t\tReturn the value of an argument from the spec.\n\t\t\"\"\"\n\t\tif arg_type == 'bool':\n\t\t\treturn bool(arg_value)\n\t\telif arg_type == 'int':\n\t\t\treturn int(arg_value)\n\t\telif arg_type == 'float':\n\t\t\treturn float(arg_value)\n\t\telif arg_type == 'str':\n\t\t\treturn str(arg_value)\n\t\telif arg_type == 'list':\n\t\t\treturn arg_value.split(',')\n\t\telse:\n\t\t\traise\n", "def get_spec_defaults(self):\n\tif self.spec is not None:\n\t\tif self.spec.has_key('defaults'):\n\t\t\tfor (name, value) in self.spec['defaults'].items():\n\t\t\t\tif not self.args.has_key(name):\n\t\t\t\t\tself.args[name] = value\n\t\tif self.spec.has_key('constants'):\n\t\t\tfor (name, value) in self.spec['constants'].items():\n\t\t\t\tself.args[name] = value\n\t\tif self.spec.has_key('enums'):\n\t\t\tfor (name, value) in self.spec['enums'].items():\n\t\t\t\tself.args[name] = value.keys()[0]\n\n\t# Override with defaults from argparse\n\tfor name, value in self.parser.defaults.items():\n\t\tif not self.args.has_key(name):\n\t\t\tself.args[name] = value\n\n\t# Override with defaults from config\n\tif self.config is not None:\n\t\n", "def get_spec_defaults(self):\n\tres = {}\n\tfor key in self.spec.keys():\n\t\tres[key] = self.spec[key]\n\treturn res\n\n\n", "def get_spec_defaults(self):\n\treturn self.__get_spec_defaults(self.spec)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n\t# Assign default values from spec\n\tself.args.update(self.spec['arguments'])\n\n\t# Assign default values from environment\n\tfor key, value in self.spec['arguments'].items():\n\t\tif key not in self.args:\n\t\t\tif key in os.environ:\n\t\t\t\tself.args[key] = os.environ[key]\n\n\t# Assign default values from defaults\n\tfor key, value in self.defaults.items():\n\t\tif key not in self.args:\n\t\t\tself.args[key] = value\n\n\t# Assign default values from global defaults\n\tfor key, value in _GLOBAL_DEFAULTS.items():\n\t\tif key not in self.args:\n\t\t\tself.args[key] = value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tif self.spec is None:\n\t\traise ValueError(\"spec is not set\")\n\tif self.name is None:\n\t\traise ValueError(\"name is not set\")\n\n\tself.set_target_profile(self.spec.get(\"target_profile\"))\n\tif self.target_profile is None:\n\t\traise ValueError(\"target_profile is not set\")\n\n\tself.title = self.spec.get(\"title\", self.name.replace(\"_\", \" \").title())\n\tself.categories = self.spec.get(\"categories\", [\"X-MandrivaLinux\"])\n\tself.summary = self.spec.get(\"summary\", \"\")\n\tself.description = self.spec.get(\"description\", \"\")\n\tself.icon = self.spec.get(\"icon\", \"\")\n\tself.url = self.spec.get(\"url\", \"\")\n\tself.keywords = self.spec.get(\"keywords\", [])\n\tself.buildrequires = self.spec.get(\"buildrequires\", [])\n\tself.requires = self.spec.get(\"requires\", [])\n\tself\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tresult = self.spec_values.copy()\n\tfor k, v in result.iteritems():\n\t\tif v is None:\n\t\t\tresult[k] = self.get_default_value(k)\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tspec_defaults = {}\n\tfor arg, value in six.iteritems(self.arguments):\n\t\tif value is None:\n\t\t\ttry:\n\t\t\t\tspec_defaults[arg] = self.spec[arg]['default']\n\t\t\texcept KeyError:\n\t\t\t\traise HttpError(404, u'Missing argument \"%s\"' % arg)\n\t\telse:\n\t\t\tspec_defaults[arg] = value\n\treturn spec_defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tif self.spec_args is None:\n\t\tself.spec_args = {}\n\t\tfor arg in self.args:\n\t\t\tif arg.name in self.spec_args:\n\t\t\t\traise InvalidSpec(\n\t\t\t\t\t\"invalid spec: duplicate argument '%s'\" % arg.name)\n\t\t\tif arg.name in self.spec.arg_defaults:\n\t\t\t\tself.spec_args[arg.name] = self.spec.arg_defaults[arg.name]\n\t\t\telif arg.has_default:\n\t\t\t\tself.spec_args[arg.name] = arg.default\n\t\t\telse:\n\t\t\t\traise InvalidSpec(\n\t\t\t\t\t\"invalid spec: argument '%s' has no default\" % arg.name)\n\treturn self.spec_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tif self.args is None:\n\t\tself.args = self.spec.arguments\n\t\tself.spec_defaults = self.spec.default_values\n\t\tif self.spec_defaults is None:\n\t\t\tself.spec_defaults = {}\n\telse:\n\t\tself.spec_defaults = {}\n\tif self.spec_defaults is not None:\n\t\tfor key in self.spec_defaults:\n\t\t\tif not self.args.has_key(key):\n\t\t\t\tself.args[key] = self.spec_defaults[key]\n\t\t\t\tself.arg_names.append(key)\n\t\t\telse:\n\t\t\t\tself.arg_names.append(key)\n\telif self.args is not None:\n\t\tself.arg_names = self.args.keys()\n\t\tself.arg_names.sort()\n\telse:\n\t\tself.arg_names = []\n\tif self.spec.return_type is not None:\n\t\tself.return_type = self.spec.return_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tfor k in self.args:\n\t\tif self.args[k] is None:\n\t\t\tif k in self.spec.defaults:\n\t\t\t\tself.args[k] = self.spec.defaults[k]\n\t\t\telse:\n\t\t\t\traise ValueError(\"Argument '{}' has no default value.\".format(k))\n\n\tdef get_arg_value(self, arg_name, arg_type, arg_value):\n\t\t\"\"\"\n\t\tReturn the value of an argument from the spec.\n\t\t\"\"\"\n\t\tif arg_type == 'bool':\n\t\t\treturn bool(arg_value)\n\t\telif arg_type == 'int':\n\t\t\treturn int(arg_value)\n\t\telif arg_type == 'float':\n\t\t\treturn float(arg_value)\n\t\telif arg_type == 'str':\n\t\t\treturn str(arg_value)\n\t\telif arg_type == 'list':\n\t\t\treturn arg_value.split(',')\n\t\telse:\n\t\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tif self.spec is not None:\n\t\tif self.spec.has_key('defaults'):\n\t\t\tfor (name, value) in self.spec['defaults'].items():\n\t\t\t\tif not self.args.has_key(name):\n\t\t\t\t\tself.args[name] = value\n\t\tif self.spec.has_key('constants'):\n\t\t\tfor (name, value) in self.spec['constants'].items():\n\t\t\t\tself.args[name] = value\n\t\tif self.spec.has_key('enums'):\n\t\t\tfor (name, value) in self.spec['enums'].items():\n\t\t\t\tself.args[name] = value.keys()[0]\n\n\t# Override with defaults from argparse\n\tfor name, value in self.parser.defaults.items():\n\t\tif not self.args.has_key(name):\n\t\t\tself.args[name] = value\n\n\t# Override with defaults from config\n\tif self.config is not None:\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tres = {}\n\tfor key in self.spec.keys():\n\t\tres[key] = self.spec[key]\n\treturn res\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\treturn self.__get_spec_defaults(self.spec)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "ques_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n\treturn self.deprecated_args\n\n\n", "def get_deprecated_args(self):\n\treturn {\n\t\t('--' + self.option_name, '--' + self.deprecated_option_name):\n\t\t\tself.deprecated_option_message\n\t}\n\n", "def get_deprecated_args(self):\n\treturn self.deprecated_args\n\n\tdef get_options(self):\n\t\"\"\"\n\tReturning options which should be passed to the command.\n\t\"\"\"\n\treturn self.options\n\n\tdef get_args(self):\n\t\"\"\"\n\tReturning args which should be passed to the command.\n\t\"\"\"\n\treturn self.args\n\n\tdef get_command(self):\n\t\"\"\"\n\tReturning command.\n\t\"\"\"\n\treturn self.command\n\n\tdef get_command_string(self):\n\t\"\"\"\n\tReturning command string which should be executed.\n\t\"\"\"\n\treturn self.command_string\n\n\tdef get_command_string_args(self):\n\t\"\"\"\n\tReturning command string which should be executed with args.\n\t\"\"\"\n\treturn f'{self.command_string} {self.args}'\n\n\tdef get_command_string_options(self):\n\t\"\"\"\n\tReturning command string which should be executed with options.\n\t\"\"\"\n\treturn f'{self.command_string} {self.options}'\n\n\tdef get_command_string_full(self):\n\t\"\"\"\n\t\n", "def get_deprecated_args(self):\n\treturn {\n\t\t'-r': '--recursive'\n\t}\n\n", "def get_deprecated_args(self):\n\treturn {\n\t\t'qtgui': ['gui'],\n\t\t'qtgui-path': ['gui-path'],\n\t\t'qtgui-debug': ['gui-debug'],\n\t}\n\n\n", "def get_deprecated_args(self):\n\treturn {}\n\n\n", "def get_deprecated_args(self):\n\treturn {\n\t\t\"--no-reload\": \"--no-reload-on-change\",\n\t\t\"--no-livereload\": \"--no-reload-on-change\",\n\t}\n\n\n", "def get_deprecated_args(self):\n\treturn {\n\t\t\"--dev\": [\n\t\t\t\"--dev-server\",\n\t\t\t\"--dev-port\",\n\t\t],\n\t\t\"--dev-server\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-port\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-poll\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-delay\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-exclude\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-include\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-ignore\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-verbose\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-source\n", "def get_deprecated_args(self):\n\treturn {\n\t\t\"xlsx_sheet\": \"excel_sheet\",\n\t\t\"xlsx_sheet_num\": \"excel_sheet_num\",\n\t\t\"xlsx_sheet_name\": \"excel_sheet_name\",\n\t\t\"xlsx_sheet_by_index\": \"excel_sheet_by_index\",\n\t\t\"xlsx_sheet_by_name\": \"excel_sheet_by_name\",\n\t\t\"xlsx_start_row\": \"excel_start_row\",\n\t\t\"xlsx_end_row\": \"excel_end_row\",\n\t\t\"xlsx_start_col\": \"excel_start_col\",\n\t\t\"xlsx_end_col\": \"excel_end_col\",\n\t\t\"xlsx_read_merged_cells\": \"excel_read_merged_cells\",\n\t\t\"xlsx_read_only_size\": \"excel_read_only_size\",\n\t\t\"xlsx_read_all_sheets\": \"excel_read_\n", "def get_deprecated_args(self):\n\treturn {\n\t\t'--dry-run': [\n\t\t\t'--no-commit',\n\t\t],\n\t}\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n\treturn self.deprecated_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {\n\t\t('--' + self.option_name, '--' + self.deprecated_option_name):\n\t\t\tself.deprecated_option_message\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn self.deprecated_args\n\n\tdef get_options(self):\n\t\"\"\"\n\tReturning options which should be passed to the command.\n\t\"\"\"\n\treturn self.options\n\n\tdef get_args(self):\n\t\"\"\"\n\tReturning args which should be passed to the command.\n\t\"\"\"\n\treturn self.args\n\n\tdef get_command(self):\n\t\"\"\"\n\tReturning command.\n\t\"\"\"\n\treturn self.command\n\n\tdef get_command_string(self):\n\t\"\"\"\n\tReturning command string which should be executed.\n\t\"\"\"\n\treturn self.command_string\n\n\tdef get_command_string_args(self):\n\t\"\"\"\n\tReturning command string which should be executed with args.\n\t\"\"\"\n\treturn f'{self.command_string} {self.args}'\n\n\tdef get_command_string_options(self):\n\t\"\"\"\n\tReturning command string which should be executed with options.\n\t\"\"\"\n\treturn f'{self.command_string} {self.options}'\n\n\tdef get_command_string_full(self):\n\t\"\"\"\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {\n\t\t'-r': '--recursive'\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {\n\t\t'qtgui': ['gui'],\n\t\t'qtgui-path': ['gui-path'],\n\t\t'qtgui-debug': ['gui-debug'],\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {\n\t\t\"--no-reload\": \"--no-reload-on-change\",\n\t\t\"--no-livereload\": \"--no-reload-on-change\",\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {\n\t\t\"--dev\": [\n\t\t\t\"--dev-server\",\n\t\t\t\"--dev-port\",\n\t\t],\n\t\t\"--dev-server\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-port\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-poll\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-delay\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-exclude\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-include\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-ignore\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-verbose\": [\n\t\t\t\"--dev\",\n\t\t],\n\t\t\"--dev-watch-source\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {\n\t\t\"xlsx_sheet\": \"excel_sheet\",\n\t\t\"xlsx_sheet_num\": \"excel_sheet_num\",\n\t\t\"xlsx_sheet_name\": \"excel_sheet_name\",\n\t\t\"xlsx_sheet_by_index\": \"excel_sheet_by_index\",\n\t\t\"xlsx_sheet_by_name\": \"excel_sheet_by_name\",\n\t\t\"xlsx_start_row\": \"excel_start_row\",\n\t\t\"xlsx_end_row\": \"excel_end_row\",\n\t\t\"xlsx_start_col\": \"excel_start_col\",\n\t\t\"xlsx_end_col\": \"excel_end_col\",\n\t\t\"xlsx_read_merged_cells\": \"excel_read_merged_cells\",\n\t\t\"xlsx_read_only_size\": \"excel_read_only_size\",\n\t\t\"xlsx_read_all_sheets\": \"excel_read_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\treturn {\n\t\t'--dry-run': [\n\t\t\t'--no-commit',\n\t\t],\n\t}\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "ques_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_arg_names = [\n\t\t\"agent_list\",\n\t\t\"ambari_server_host\",\n\t\t\"ambari_server_port\",\n\t\t\"ambari_server_use_ssl\",\n\t\t\"cluster_name\",\n\t\t\"database_host\",\n\t\t\"database_port\",\n\t\t\"database_name\",\n\t\t\"database_username\",\n\t\t\"database_password\",\n\t\t\"database_index\",\n\t\t\"java_home\",\n\t\t\"jdk_location\",\n\t\t\"ldap_primary_url\",\n\t\t\"ldap_secondary_url\",\n\t\t\"ldap_use_ssl\",\n\t\t\"ldap_username\",\n\t\t\"ldap_password\",\n\t\t\"ldap_base_dn\",\n\t\t\"ldap_dn_attribute\",\n\t\t\"ldap_group_base\",\n\t\t\"ldap_group_object_class\",\n\t\t\"ldap_group_name_attribute\",\n\t\t\"ldap_group_member_attribute\",\n\t\t\"ldap_group\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tself.log.info(\"Validating deprecated arguments\")\n\tfor arg in self.get_deprecated_args():\n\t\tif arg in cli_args or arg in answer_file_args:\n\t\t\tself.log.warning(\"The argument '%s' is deprecated.\" % arg)\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturns a list of deprecated arguments.\n\n\t\t:return: the list of deprecated arguments\n\t\t\"\"\"\n\t\treturn [\"-proxy-host\", \"-proxy-port\", \"-proxy-user\", \"-proxy-password\", \"-proxy-ntlm-domain\"]\n\n\tdef get_ambari_properties(self):\n\t\treturn self.ambari_properties\n\n\tdef get_ambari_version(self):\n\t\treturn self.ambari_version\n\n\tdef get_java_exe_path(self):\n\t\treturn self.java_exe_path\n\n\tdef get_ambari_server_jar_path(self):\n\t\treturn self.ambari_server_jar_path\n\n\tdef get_java_home(self):\n\t\treturn self.\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_arg_dict = {}\n\tdeprecated_arg_dict.update(self.deprecated_arg_dict)\n\tdeprecated_arg_dict.update(self.deprecated_arg_dict_for_file)\n\tfor key in deprecated_arg_dict:\n\t\tif key in cli_args and cli_args[key] is not None:\n\t\t\tprint('WARNING: Argument \"{}\" is deprecated, and will be removed in the future.'.format(key.replace('_', '-')))\n\t\t\tprint('         {}'.format(deprecated_arg_dict[key]))\n\t\t\tprint()\n\t\telif key in answer_file_args and answer_file_args[key] is not None:\n\t\t\tprint('WARNING: Argument \"{}\" is deprecated, and will be removed in the future.'.format(key.replace('_', '-')))\n\t\t\tprint('         {}'.format(deprecated_arg_dict[key]))\n\t\t\tprint()\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t# Validate deprecated cli parameters\n\tfor arg, value in cli_args.items():\n\t\tif value is not None and arg in self.deprecated_args:\n\t\t\tprint(\"WARNING: The argument {0} is deprecated and will be ignored\".format(arg))\n\t# Validate deprecated answer file parameters\n\tfor arg, value in answer_file_args.items():\n\t\tif value is not None and arg in self.deprecated_args:\n\t\t\tprint(\"WARNING: The argument {0} is deprecated and will be ignored\".format(arg))\n\n\treturn\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\tfor arg in cli_args.keys():\n\t\tif arg in self.deprecated_args:\n\t\t\tdeprecated_args[arg] = cli_args[arg]\n\tfor arg in answer_file_args.keys():\n\t\tif arg in self.deprecated_args:\n\t\t\tdeprecated_args[arg] = answer_file_args[arg]\n\n\tif deprecated_args:\n\t\tfor arg in deprecated_args.keys():\n\t\t\tself.log.warning('\\nThe argument {0} is deprecated.\\n'.format(arg))\n\n\t\tif self.pargs.interactive:\n\t\t\tself.log.warning(\n\t\t\t\t'\\nThe arguments {0} will be removed in a future release.\\n'.format(\n\t\t\t\t\tstr(deprecated_args.keys())))\n\n\t\tself.log.debug('The deprecated arguments: {0}'.format(str(deprecated_args)))\n\n\treturn deprecated_args\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tself.deprecated_args = {}\n\tfor arg in cli_args:\n\t\tif arg in constants.deprecated_arguments:\n\t\t\tself.deprecated_args[arg] = cli_args[arg]\n\tfor arg in answer_file_args:\n\t\tif arg in constants.deprecated_arguments:\n\t\t\tself.deprecated_args[arg] = answer_file_args[arg]\n\tif self.deprecated_args:\n\t\tself._print_deprecated_args()\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tfor deprecated_arg in self.deprecated_args:\n\t\tif deprecated_arg in cli_args:\n\t\t\tprint(\"Argument '{}' is deprecated. Please use '{}' instead.\".format(deprecated_arg, self.deprecated_args[deprecated_arg]))\n\t\t\tsys.exit(1)\n\t\tif deprecated_arg in answer_file_args:\n\t\t\tprint(\"Argument '{}' is deprecated. Please use '{}' instead.\".format(deprecated_arg, self.deprecated_args[deprecated_arg]))\n\t\t\tsys.exit(1)\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\tdeprecated_args.update(cli_args)\n\tdeprecated_args.update(answer_file_args)\n\tfor key, value in deprecated_args.items():\n\t\tif value is None:\n\t\t\tcontinue\n\t\tif key in self.deprecated_arguments_list:\n\t\t\tself.print_warning(\"Argument '\" + str(key) + \"' is deprecated and will not be available in future releases.\")\n\n\tdef parse_args(self, args):\n\t\t\"\"\"\n\t\tParses the arguments.\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\t# check deprecated arguments\n\tfor arg in cli_args:\n\t\tif arg in DEPRECATED_ARGUMENTS:\n\t\t\tdeprecated_args[arg] = cli_args[arg]\n\tfor arg in answer_file_args:\n\t\tif arg in DEPRECATED_ARGUMENTS:\n\t\t\tdeprecated_args[arg] = answer_file_args[arg]\n\tif deprecated_args:\n\t\tlog.warning('Deprecated argument(s) detected:')\n\t\tfor arg in deprecated_args:\n\t\t\tlog.warning('  --{} {}'.format(arg, deprecated_args[arg]))\n\t\tlog.warning('Please consult the documentation for the correct usage.')\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = self.get_deprecated_args(cli_args, answer_file_args)\n\tif deprecated_args:\n\t\tself.logger.warning(\"Deprecated arguments were used: \" + \" \".join(deprecated_args))\n\n\treturn deprecated_args\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_arg_names = [\n\t\t\"agent_list\",\n\t\t\"ambari_server_host\",\n\t\t\"ambari_server_port\",\n\t\t\"ambari_server_use_ssl\",\n\t\t\"cluster_name\",\n\t\t\"database_host\",\n\t\t\"database_port\",\n\t\t\"database_name\",\n\t\t\"database_username\",\n\t\t\"database_password\",\n\t\t\"database_index\",\n\t\t\"java_home\",\n\t\t\"jdk_location\",\n\t\t\"ldap_primary_url\",\n\t\t\"ldap_secondary_url\",\n\t\t\"ldap_use_ssl\",\n\t\t\"ldap_username\",\n\t\t\"ldap_password\",\n\t\t\"ldap_base_dn\",\n\t\t\"ldap_dn_attribute\",\n\t\t\"ldap_group_base\",\n\t\t\"ldap_group_object_class\",\n\t\t\"ldap_group_name_attribute\",\n\t\t\"ldap_group_member_attribute\",\n\t\t\"ldap_group\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tself.log.info(\"Validating deprecated arguments\")\n\tfor arg in self.get_deprecated_args():\n\t\tif arg in cli_args or arg in answer_file_args:\n\t\t\tself.log.warning(\"The argument '%s' is deprecated.\" % arg)\n\n\tdef get_deprecated_args(self):\n\t\t\"\"\"\n\t\tReturns a list of deprecated arguments.\n\n\t\t:return: the list of deprecated arguments\n\t\t\"\"\"\n\t\treturn [\"-proxy-host\", \"-proxy-port\", \"-proxy-user\", \"-proxy-password\", \"-proxy-ntlm-domain\"]\n\n\tdef get_ambari_properties(self):\n\t\treturn self.ambari_properties\n\n\tdef get_ambari_version(self):\n\t\treturn self.ambari_version\n\n\tdef get_java_exe_path(self):\n\t\treturn self.java_exe_path\n\n\tdef get_ambari_server_jar_path(self):\n\t\treturn self.ambari_server_jar_path\n\n\tdef get_java_home(self):\n\t\treturn self.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_arg_dict = {}\n\tdeprecated_arg_dict.update(self.deprecated_arg_dict)\n\tdeprecated_arg_dict.update(self.deprecated_arg_dict_for_file)\n\tfor key in deprecated_arg_dict:\n\t\tif key in cli_args and cli_args[key] is not None:\n\t\t\tprint('WARNING: Argument \"{}\" is deprecated, and will be removed in the future.'.format(key.replace('_', '-')))\n\t\t\tprint('         {}'.format(deprecated_arg_dict[key]))\n\t\t\tprint()\n\t\telif key in answer_file_args and answer_file_args[key] is not None:\n\t\t\tprint('WARNING: Argument \"{}\" is deprecated, and will be removed in the future.'.format(key.replace('_', '-')))\n\t\t\tprint('         {}'.format(deprecated_arg_dict[key]))\n\t\t\tprint()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t# Validate deprecated cli parameters\n\tfor arg, value in cli_args.items():\n\t\tif value is not None and arg in self.deprecated_args:\n\t\t\tprint(\"WARNING: The argument {0} is deprecated and will be ignored\".format(arg))\n\t# Validate deprecated answer file parameters\n\tfor arg, value in answer_file_args.items():\n\t\tif value is not None and arg in self.deprecated_args:\n\t\t\tprint(\"WARNING: The argument {0} is deprecated and will be ignored\".format(arg))\n\n\treturn\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\tfor arg in cli_args.keys():\n\t\tif arg in self.deprecated_args:\n\t\t\tdeprecated_args[arg] = cli_args[arg]\n\tfor arg in answer_file_args.keys():\n\t\tif arg in self.deprecated_args:\n\t\t\tdeprecated_args[arg] = answer_file_args[arg]\n\n\tif deprecated_args:\n\t\tfor arg in deprecated_args.keys():\n\t\t\tself.log.warning('\\nThe argument {0} is deprecated.\\n'.format(arg))\n\n\t\tif self.pargs.interactive:\n\t\t\tself.log.warning(\n\t\t\t\t'\\nThe arguments {0} will be removed in a future release.\\n'.format(\n\t\t\t\t\tstr(deprecated_args.keys())))\n\n\t\tself.log.debug('The deprecated arguments: {0}'.format(str(deprecated_args)))\n\n\treturn deprecated_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tself.deprecated_args = {}\n\tfor arg in cli_args:\n\t\tif arg in constants.deprecated_arguments:\n\t\t\tself.deprecated_args[arg] = cli_args[arg]\n\tfor arg in answer_file_args:\n\t\tif arg in constants.deprecated_arguments:\n\t\t\tself.deprecated_args[arg] = answer_file_args[arg]\n\tif self.deprecated_args:\n\t\tself._print_deprecated_args()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tfor deprecated_arg in self.deprecated_args:\n\t\tif deprecated_arg in cli_args:\n\t\t\tprint(\"Argument '{}' is deprecated. Please use '{}' instead.\".format(deprecated_arg, self.deprecated_args[deprecated_arg]))\n\t\t\tsys.exit(1)\n\t\tif deprecated_arg in answer_file_args:\n\t\t\tprint(\"Argument '{}' is deprecated. Please use '{}' instead.\".format(deprecated_arg, self.deprecated_args[deprecated_arg]))\n\t\t\tsys.exit(1)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\tdeprecated_args.update(cli_args)\n\tdeprecated_args.update(answer_file_args)\n\tfor key, value in deprecated_args.items():\n\t\tif value is None:\n\t\t\tcontinue\n\t\tif key in self.deprecated_arguments_list:\n\t\t\tself.print_warning(\"Argument '\" + str(key) + \"' is deprecated and will not be available in future releases.\")\n\n\tdef parse_args(self, args):\n\t\t\"\"\"\n\t\tParses the arguments.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\t# check deprecated arguments\n\tfor arg in cli_args:\n\t\tif arg in DEPRECATED_ARGUMENTS:\n\t\t\tdeprecated_args[arg] = cli_args[arg]\n\tfor arg in answer_file_args:\n\t\tif arg in DEPRECATED_ARGUMENTS:\n\t\t\tdeprecated_args[arg] = answer_file_args[arg]\n\tif deprecated_args:\n\t\tlog.warning('Deprecated argument(s) detected:')\n\t\tfor arg in deprecated_args:\n\t\t\tlog.warning('  --{} {}'.format(arg, deprecated_args[arg]))\n\t\tlog.warning('Please consult the documentation for the correct usage.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = self.get_deprecated_args(cli_args, answer_file_args)\n\tif deprecated_args:\n\t\tself.logger.warning(\"Deprecated arguments were used: \" + \" \".join(deprecated_args))\n\n\treturn deprecated_args\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "ques_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n\toptions = []\n\tif command_name in self.parser_option_specs:\n\t\toptions = self.parser_option_specs[command_name]\n\treturn options\n\n", "def get_parser_option_specs(self, command_name):\n\tif not self.is_command_exists(command_name):\n\t\traise Exception(\"The command %s does not exists\" % command_name)\n\tif not hasattr(self, command_name):\n\t\traise Exception(\"The command %s is not implemented\" % command_name)\n\treturn self.get_command_options(command_name)\n\n", "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn parser.option_list\n\n\tdef get_parser_option(self, command_name, option_name):\n\t\t\"\"\"\n\t\tGets the specified parser option\n\n", "def get_parser_option_specs(self, command_name):\n\tif command_name not in self.parser_options:\n\t\treturn []\n\telse:\n\t\treturn self.parser_options[command_name]\n\n", "def get_parser_option_specs(self, command_name):\n\treturn self._parser._optionals._option_string_actions.keys()\n\n", "def get_parser_option_specs(self, command_name):\n\treturn self.parser.get_option_specs(command_name)\n\n", "def get_parser_option_specs(self, command_name):\n\t\tparser = self.get_parser(command_name)\n\t\treturn parser.option_specs\n\t\n\tdef get_parser_option_names(self, command_name):\n\t\t\"\"\"\n\t\tGets all the options names for the specified command\n\n", "def get_parser_option_specs(self, command_name):\n\tresult = []\n\tfor opt in self.parser.option_list:\n\t\tif opt.dest == command_name:\n\t\t\tresult.append(opt)\n\treturn result\n\n\tdef get_all_options(self):\n\t\t\"\"\"\n\t\tGets all the options for all commands\n\n", "def get_parser_option_specs(self, command_name):\n\toption_specs = []\n\tif command_name in self.parser_option_specs:\n\t\toption_specs = self.parser_option_specs[command_name]\n\treturn option_specs\n\n\tdef get_parser_option_spec_by_long_name(self, command_name, long_name):\n\t\t\"\"\"\n\t\tGets the option spec for the specified command and option long name\n\n", "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn parser.option_list\n\n\tdef get_parser_option_value(self, command_name, option_name):\n\t\t\"\"\"\n\t\tGets the value of the specified option for the specified command\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n\toptions = []\n\tif command_name in self.parser_option_specs:\n\t\toptions = self.parser_option_specs[command_name]\n\treturn options\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tif not self.is_command_exists(command_name):\n\t\traise Exception(\"The command %s does not exists\" % command_name)\n\tif not hasattr(self, command_name):\n\t\traise Exception(\"The command %s is not implemented\" % command_name)\n\treturn self.get_command_options(command_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn parser.option_list\n\n\tdef get_parser_option(self, command_name, option_name):\n\t\t\"\"\"\n\t\tGets the specified parser option\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tif command_name not in self.parser_options:\n\t\treturn []\n\telse:\n\t\treturn self.parser_options[command_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\treturn self._parser._optionals._option_string_actions.keys()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\treturn self.parser.get_option_specs(command_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\t\tparser = self.get_parser(command_name)\n\t\treturn parser.option_specs\n\t\n\tdef get_parser_option_names(self, command_name):\n\t\t\"\"\"\n\t\tGets all the options names for the specified command\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tresult = []\n\tfor opt in self.parser.option_list:\n\t\tif opt.dest == command_name:\n\t\t\tresult.append(opt)\n\treturn result\n\n\tdef get_all_options(self):\n\t\t\"\"\"\n\t\tGets all the options for all commands\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\toption_specs = []\n\tif command_name in self.parser_option_specs:\n\t\toption_specs = self.parser_option_specs[command_name]\n\treturn option_specs\n\n\tdef get_parser_option_spec_by_long_name(self, command_name, long_name):\n\t\t\"\"\"\n\t\tGets the option spec for the specified command and option long name\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn parser.option_list\n\n\tdef get_parser_option_value(self, command_name, option_name):\n\t\t\"\"\"\n\t\tGets the value of the specified option for the specified command\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "ques_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n\t\n\treturn self.get_subcommand_spec(command_name, argument_name)[1]\n\n", "def get_option_spec(self, command_name, argument_name):\n\tif self._options is None:\n\t\tself._options = self._get_options(command_name)\n\t\tself._options_by_name = {}\n\t\tfor option in self._options:\n\t\t\tself._options_by_name[option['name']] = option\n\t\t\tself._options_by_name[option['name'].replace('-', '_')] = option\n\t\t\tself._options_by_name[option['name'].replace('-', '')] = option\n\t\t\tself._options_by_name[option['name'].replace('_', '')] = option\n\treturn self._options_by_name.get(argument_name)\n\n\n", "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\treturn self.options[command_name][argument_name]\n\texcept KeyError:\n\t\treturn None\n\n", "def get_option_spec(self, command_name, argument_name):\n\t\n\tif command_name is None:\n\t\tcommand_name = self.current_command\n\t\n\tif argument_name is None:\n\t\targument_name = self.current_argument\n\t\n\tif command_name is None:\n\t\treturn None\n\t\n\tif argument_name is None:\n\t\treturn None\n\t\n\tif command_name not in self.command_definitions:\n\t\treturn None\n\t\n\tcommand_def = self.command_definitions[command_name]\n\t\n\tif argument_name not in command_def['options']:\n\t\treturn None\n\t\n\treturn command_def['options'][argument_name]\n\t\n", "def get_option_spec(self, command_name, argument_name):\n\t\n\toption_spec = None\n\tcommand = self.get_command(command_name)\n\tif command is not None:\n\t\toption_spec = command.get_option_spec(argument_name)\n\t\t\n\treturn option_spec\n\t\n", "def get_option_spec(self, command_name, argument_name):\n\treturn self.options[command_name][argument_name]\n\n", "def get_option_spec(self, command_name, argument_name):\n\tfor option in self.options:\n\t\tif option.command_name == command_name:\n\t\t\tfor spec in option.get_specs():\n\t\t\t\tif spec.argument_name == argument_name:\n\t\t\t\t\treturn spec\n\n\treturn None\n\n", "def get_option_spec(self, command_name, argument_name):\n\tif command_name not in self._option_specs:\n\t\treturn None\n\tif argument_name not in self._option_specs[command_name]:\n\t\treturn None\n\treturn self._option_specs[command_name][argument_name]\n\n", "def get_option_spec(self, command_name, argument_name):\n\tcommand = self.get_command(command_name)\n\ttry:\n\t\treturn command.get_option_spec(argument_name)\n\texcept KeyError:\n\t\traise UnknownOptionException('Unknown option: %s' % argument_name)\n\n\n", "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\treturn self._option_specs[command_name][argument_name]\n\texcept:\n\t\treturn None\n\n\tdef get_command_line_args(self):\n\t\t\"\"\"\n\t\tGets the command line arguments that were specified.\n\t\t\"\"\"\n\t\treturn self._command_line_args\n\n\tdef get_command_line_values(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the command line values that were specified for the specified option.\n\t\t\"\"\"\n\t\tif command_name == None:\n\t\t\treturn self._global_options.get(argument_name)\n\n\t\ttry:\n\t\t\treturn self._command_line_values[command_name].get(argument_name)\n\t\texcept:\n\t\t\treturn None\n\n\tdef get_command_line_value(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the first command line value that was specified for the specified option.\n\t\t\"\"\"\n\t\tvalues = self.get_command_line_values(command_name, argument_name\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\n\treturn self.get_subcommand_spec(command_name, argument_name)[1]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tif self._options is None:\n\t\tself._options = self._get_options(command_name)\n\t\tself._options_by_name = {}\n\t\tfor option in self._options:\n\t\t\tself._options_by_name[option['name']] = option\n\t\t\tself._options_by_name[option['name'].replace('-', '_')] = option\n\t\t\tself._options_by_name[option['name'].replace('-', '')] = option\n\t\t\tself._options_by_name[option['name'].replace('_', '')] = option\n\treturn self._options_by_name.get(argument_name)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\treturn self.options[command_name][argument_name]\n\texcept KeyError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\n\tif command_name is None:\n\t\tcommand_name = self.current_command\n\t\n\tif argument_name is None:\n\t\targument_name = self.current_argument\n\t\n\tif command_name is None:\n\t\treturn None\n\t\n\tif argument_name is None:\n\t\treturn None\n\t\n\tif command_name not in self.command_definitions:\n\t\treturn None\n\t\n\tcommand_def = self.command_definitions[command_name]\n\t\n\tif argument_name not in command_def['options']:\n\t\treturn None\n\t\n\treturn command_def['options'][argument_name]\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t\n\toption_spec = None\n\tcommand = self.get_command(command_name)\n\tif command is not None:\n\t\toption_spec = command.get_option_spec(argument_name)\n\t\t\n\treturn option_spec\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\treturn self.options[command_name][argument_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tfor option in self.options:\n\t\tif option.command_name == command_name:\n\t\t\tfor spec in option.get_specs():\n\t\t\t\tif spec.argument_name == argument_name:\n\t\t\t\t\treturn spec\n\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tif command_name not in self._option_specs:\n\t\treturn None\n\tif argument_name not in self._option_specs[command_name]:\n\t\treturn None\n\treturn self._option_specs[command_name][argument_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tcommand = self.get_command(command_name)\n\ttry:\n\t\treturn command.get_option_spec(argument_name)\n\texcept KeyError:\n\t\traise UnknownOptionException('Unknown option: %s' % argument_name)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\treturn self._option_specs[command_name][argument_name]\n\texcept:\n\t\treturn None\n\n\tdef get_command_line_args(self):\n\t\t\"\"\"\n\t\tGets the command line arguments that were specified.\n\t\t\"\"\"\n\t\treturn self._command_line_args\n\n\tdef get_command_line_values(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the command line values that were specified for the specified option.\n\t\t\"\"\"\n\t\tif command_name == None:\n\t\t\treturn self._global_options.get(argument_name)\n\n\t\ttry:\n\t\t\treturn self._command_line_values[command_name].get(argument_name)\n\t\texcept:\n\t\t\treturn None\n\n\tdef get_command_line_value(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the first command line value that was specified for the specified option.\n\t\t\"\"\"\n\t\tvalues = self.get_command_line_values(command_name, argument_name\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "ques_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n\treturn [arg for arg in args if arg.startswith('_')]\n\n", "def get_silent_args(self, args):\n\treturn [arg for arg in args if arg.startswith(\"-\")]\n\n", "def get_silent_args(self, args):\n\treturn self.silent_args\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\"\"\"\n\tlist of silenced keyword argument\n\n", "def get_silent_args(self, args):\n\tif self.silent is not None:\n\t\tif not isinstance(self.silent, (list, tuple)):\n\t\t\tself.silent = [self.silent]\n\t\tfor arg in args:\n\t\t\tif arg in self.silent:\n\t\t\t\treturn True\n\t\treturn False\n\treturn False\n\n\n", "def get_silent_args(self, args):\n\tsilent = []\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tsilent.append(arg[2:])\n\treturn silent\n\n\n", "def get_silent_args(self, args):\n\t\tsilent = []\n\t\tfor name, value in args.items():\n\t\t\tif isinstance(value, SilentArgument):\n\t\t\t\tsilent.append(name)\n\t\treturn silent\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\t\"\"\"\n\tlist of silenced keyword arguments\n\n", "def get_silent_args(self, args):\n\t\tsilent = []\n\t\tfor i in range(args.length):\n\t\t\tif args[i] == '--':\n\t\t\t\tsilent.append(args[i+1])\n\t\t\t\ti += 2\n\t\treturn silent\n\n\tdef get_args(self, args):\n\t\t\"\"\"\n\t\tlist of all arguments (except silenced)\n\t\n", "def get_silent_args(self, args):\n\tsilent = []\n\tfor arg in args:\n\t\tif arg.startswith(\"-\"):\n\t\t\tsilent.append(arg)\n\treturn silent\n\n\n", "def get_silent_args(self, args):\n\t\tsilent = []\n\t\tif hasattr(self, 'silent'):\n\t\t\tfor arg in self.silent:\n\t\t\t\tif arg in args:\n\t\t\t\t\tsilent.append(arg)\n\t\treturn silent\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\"\"\"\n\tlist of silenced keyword argument\n\n", "def get_silent_args(self, args):\n\t\tsilent_args = []\n\t\tfor arg in args:\n\t\t\tif arg.startswith(\"--silent-\"):\n\t\t\t\targ = arg.replace(\"--silent-\", \"\")\n\t\t\t\tsilent_args.append(arg)\n\t\treturn silent_args\n\n\tdef get_argument_info(self, arg_name, args):\n\t\t\"\"\"\n\t\tget the info about an argument\n\n\t:param arg_name: The argument name\n\t:param args: The received arguments.\n\t:return: dict, info about the argument\n\t\t\"\"\"\n\t\targ_info = {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith(arg_name):\n\t\t\t\targ_info['value'] = arg.replace(arg_name, \"\")\n\t\t\t\targ_info['present'] = True\n\t\t\t\treturn arg_info\n\t\targ_info['value'] = None\n\t\targ_info['present'] = False\n\t\treturn arg_info\n\n\tdef args_exist(self, args):\n\t\t\"\"\"\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n\treturn [arg for arg in args if arg.startswith('_')]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n\treturn [arg for arg in args if arg.startswith(\"-\")]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n\treturn self.silent_args\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\"\"\"\n\tlist of silenced keyword argument\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\tif self.silent is not None:\n\t\tif not isinstance(self.silent, (list, tuple)):\n\t\t\tself.silent = [self.silent]\n\t\tfor arg in args:\n\t\t\tif arg in self.silent:\n\t\t\t\treturn True\n\t\treturn False\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\tsilent = []\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tsilent.append(arg[2:])\n\treturn silent\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n\t\tsilent = []\n\t\tfor name, value in args.items():\n\t\t\tif isinstance(value, SilentArgument):\n\t\t\t\tsilent.append(name)\n\t\treturn silent\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\t\"\"\"\n\tlist of silenced keyword arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\tsilent = []\n\t\tfor i in range(args.length):\n\t\t\tif args[i] == '--':\n\t\t\t\tsilent.append(args[i+1])\n\t\t\t\ti += 2\n\t\treturn silent\n\n\tdef get_args(self, args):\n\t\t\"\"\"\n\t\tlist of all arguments (except silenced)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\tsilent = []\n\tfor arg in args:\n\t\tif arg.startswith(\"-\"):\n\t\t\tsilent.append(arg)\n\treturn silent\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n\t\tsilent = []\n\t\tif hasattr(self, 'silent'):\n\t\t\tfor arg in self.silent:\n\t\t\t\tif arg in args:\n\t\t\t\t\tsilent.append(arg)\n\t\treturn silent\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\"\"\"\n\tlist of silenced keyword argument\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\tsilent_args = []\n\t\tfor arg in args:\n\t\t\tif arg.startswith(\"--silent-\"):\n\t\t\t\targ = arg.replace(\"--silent-\", \"\")\n\t\t\t\tsilent_args.append(arg)\n\t\treturn silent_args\n\n\tdef get_argument_info(self, arg_name, args):\n\t\t\"\"\"\n\t\tget the info about an argument\n\n\t:param arg_name: The argument name\n\t:param args: The received arguments.\n\t:return: dict, info about the argument\n\t\t\"\"\"\n\t\targ_info = {}\n\t\tfor arg in args:\n\t\t\tif arg.startswith(arg_name):\n\t\t\t\targ_info['value'] = arg.replace(arg_name, \"\")\n\t\t\t\targ_info['present'] = True\n\t\t\t\treturn arg_info\n\t\targ_info['value'] = None\n\t\targ_info['present'] = False\n\t\treturn arg_info\n\n\tdef args_exist(self, args):\n\t\t\"\"\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "ques_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n\tfor arg in self.arguments:\n\t\tif arg.required and not arg.provided(args):\n\t\t\traise TypeError(\"missing required argument: %s\" % arg.name)\n\n", "def validate_requires_args(self, args):\n\treturn True\n\n", "def validate_requires_args(self, args):\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tself.logger.error(\"Argument '%s' not provided.\" % arg)\n\t\t\treturn False\n\treturn True\n\n", "def validate_requires_args(self, args):\n\tif self.required_args is not None:\n\t\tfor arg in self.required_args:\n\t\t\tif args.get(arg) is None:\n\t\t\t\treturn False\n\n\treturn True\n\n", "def validate_requires_args(self, args):\n\tif not os.path.exists(args.input):\n\t\tsys.stderr.write(\"Error: input file '{}' does not exist.\\n\".format(args.input))\n\t\tsys.exit(2)\n\tif not os.path.exists(args.input):\n\t\tsys.stderr.write(\"Error: output directory '{}' does not exist.\\n\".format(args.input))\n\t\tsys.exit(2)\n\treturn True\n\n\n", "def validate_requires_args(self, args):\n\tif not isinstance(args, dict):\n\t\traise ValueError('Invalid arguments. Expected a dictionary, got %s' % type(args))\n\n\tfor arg_name in self.required_args:\n\t\tif not arg_name in args:\n\t\t\traise ValueError('Missing argument %s' % arg_name)\n\n\treturn True\n\n\n", "def validate_requires_args(self, args):\n\tif args.model is None:\n\t\traise ValueError('Model file not provided. Use --model to specify the model path.')\n\treturn args\n\n", "def validate_requires_args(self, args):\n\tif not args:\n\t\targs = sys.argv[1:]\n\n\tif len(args) < self.required_args:\n\t\tself.parser.error(\"missing required arguments\")\n\n\n", "def validate_requires_args(self, args):\n\t# check if the required arguments are provided\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\traise Exception(\"required argument '%s' not provided.\" % arg)\n\n\n", "def validate_requires_args(self, args):\n\tfor arg in self.required_args:\n\t\tif not arg in args:\n\t\t\treturn False\n\treturn True\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n\tfor arg in self.arguments:\n\t\tif arg.required and not arg.provided(args):\n\t\t\traise TypeError(\"missing required argument: %s\" % arg.name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\tself.logger.error(\"Argument '%s' not provided.\" % arg)\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif self.required_args is not None:\n\t\tfor arg in self.required_args:\n\t\t\tif args.get(arg) is None:\n\t\t\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif not os.path.exists(args.input):\n\t\tsys.stderr.write(\"Error: input file '{}' does not exist.\\n\".format(args.input))\n\t\tsys.exit(2)\n\tif not os.path.exists(args.input):\n\t\tsys.stderr.write(\"Error: output directory '{}' does not exist.\\n\".format(args.input))\n\t\tsys.exit(2)\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif not isinstance(args, dict):\n\t\traise ValueError('Invalid arguments. Expected a dictionary, got %s' % type(args))\n\n\tfor arg_name in self.required_args:\n\t\tif not arg_name in args:\n\t\t\traise ValueError('Missing argument %s' % arg_name)\n\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif args.model is None:\n\t\traise ValueError('Model file not provided. Use --model to specify the model path.')\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif not args:\n\t\targs = sys.argv[1:]\n\n\tif len(args) < self.required_args:\n\t\tself.parser.error(\"missing required arguments\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\t# check if the required arguments are provided\n\tfor arg in self.required_args:\n\t\tif arg not in args:\n\t\t\traise Exception(\"required argument '%s' not provided.\" % arg)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tfor arg in self.required_args:\n\t\tif not arg in args:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "ques_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif option.get('required_when') is None:\n\t\t\tcontinue\n\t\tcondition = option['required_when']\n\t\tif self._is_condition_met(condition, command_name, options_spec, args):\n\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t# Filters out all arguments with ``required_when``\n\trequired_when_args = [option for option in options_spec if\n\t\t\t\t\t\t\t\t\t\t\t\toption.get('required_when')]\n\t# Checks if the argument has a ``required_when`` option\n\tconditionally_required_args = []\n\tfor argument in required_when_args:\n\t\t# Gets the argument value\n\t\targ_value = args.get(argument['name'], None)\n\t\t# Checks if the argument has a value\n\t\tif arg_value is not None:\n\t\t\t# Checks if the argument is of type list\n\t\t\tif isinstance(arg_value, list):\n\t\t\t\t# Checks if the list has more than one value\n\t\t\t\tif len(arg_value) > 1:\n\t\t\t\t\t# Raises an exception, since multiple values are not\n\t\t\t\t\t# allowed\n\t\t\t\t\traise CLIError(\n\t\t\t\t\t\t'Multiple values received for argument \\'{}\\'.'.format(\n\t\t\t\t\t\t\t\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_when_arguments = []\n\tfor option in options_spec:\n\t\tif 'required_when' not in option:\n\t\t\tcontinue\n\t\tcondition_name, condition_value = option['required_when'].split('=')\n\t\tif condition_name in args and args[condition_name] == condition_value:\n\t\t\trequired_when_arguments.append(option['name'])\n\treturn required_when_arguments\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t# check if any of the options has a required_when\n\tconditionally_required = []\n\tfor option in options_spec:\n\t\tif not option.required_when:\n\t\t\tcontinue\n\t\t# check if the option is present in the args list\n\t\tif option.name in args:\n\t\t\t# check if the option value matches the \"required when\"\n\t\t\t# condition\n\t\t\tif self._check_required_when(option, args):\n\t\t\t\tconditionally_required.append(option.name)\n\treturn conditionally_required\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\n\tif options_spec is None:\n\t\treturn required_args\n\n\tfor option in options_spec:\n\t\tif option.get('required_when') is None:\n\t\t\tcontinue\n\n\t\tcondition = option['required_when']\n\t\tlhs, rhs = condition.split('=')\n\n\t\tif lhs in args:\n\t\t\tif args[lhs] == rhs:\n\t\t\t\trequired_args.append(option['name'])\n\n\treturn required_args\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif 'required_when' in option and option['required_when'] is not None:\n\t\t\tfor k, v in option['required_when'].items():\n\t\t\t\tif k in args and args[k] == v and option['name'] not in args:\n\t\t\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tif args is None:\n\t\targs = {}\n\tresult = []\n\tfor opt in options_spec:\n\t\tif opt['name'] in args and opt.get('required_when'):\n\t\t\tcondition = opt['required_when']\n\t\t\tcondition_args = condition[0]\n\t\t\tif isinstance(args[opt['name']], list):\n\t\t\t\tsatisfied = [args[opt['name']][i] in condition[1] for i in\n\t\t\t\t\t\t\t range(len(args[opt['name']]))]\n\t\t\telse:\n\t\t\t\tsatisfied = args[opt['name']] in condition[1]\n\t\t\tif satisfied and all([arg in args for arg in condition_args]):\n\t\t\t\tresult.append(opt['name'])\n\treturn result\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\treq_args = []\n\tfor opt in options_spec:\n\t\tif 'required_when' in opt:\n\t\t\tfor condition in opt['required_when']:\n\t\t\t\tif condition[0] and condition[2]:\n\t\t\t\t\tif condition[0] in args and condition[2] in args:\n\t\t\t\t\t\tif args[condition[0]] == condition[1] and \\\n\t\t\t\t\t\t\t\targs[condition[2]] == condition[3]:\n\t\t\t\t\t\t\treq_args.append(opt['name'])\n\t\t\t\telif condition[0] and not condition[2]:\n\t\t\t\t\tif condition[0] in args:\n\t\t\t\t\t\tif args[condition[0]] == condition[1]:\n\t\t\t\t\t\t\treq_args.append(opt['name'])\n\t\t\t\telif not condition[0] and condition[2]:\n\t\t\t\t\tif condition[2] in args:\n\t\t\t\t\t\tif args[condition[2]] == condition[3]:\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tresult = []\n\tfor option in options_spec:\n\t\trequired_when = option.get(\"required_when\")\n\t\tif required_when:\n\t\t\tif required_when == command_name:\n\t\t\t\tresult.append(option[\"name\"])\n\t\t\telif required_when in args:\n\t\t\t\tresult.append(option[\"name\"])\n\treturn result\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option_spec in options_spec:\n\t\tif 'required_when' not in option_spec:\n\t\t\tcontinue\n\n\t\trequired_when_spec = option_spec['required_when']\n\t\tif not self._is_spec_condition_matched(required_when_spec, args):\n\t\t\tcontinue\n\n\t\tconditionally_required_args.append(option_spec['name'])\n\n\treturn conditionally_required_args\n\n\tdef _is_spec_condition_matched(self, spec, args):\n\t\t\"\"\"\n\t\tCheck if ``required_when`` condition is matched.\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif option.get('required_when') is None:\n\t\t\tcontinue\n\t\tcondition = option['required_when']\n\t\tif self._is_condition_met(condition, command_name, options_spec, args):\n\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t# Filters out all arguments with ``required_when``\n\trequired_when_args = [option for option in options_spec if\n\t\t\t\t\t\t\t\t\t\t\t\toption.get('required_when')]\n\t# Checks if the argument has a ``required_when`` option\n\tconditionally_required_args = []\n\tfor argument in required_when_args:\n\t\t# Gets the argument value\n\t\targ_value = args.get(argument['name'], None)\n\t\t# Checks if the argument has a value\n\t\tif arg_value is not None:\n\t\t\t# Checks if the argument is of type list\n\t\t\tif isinstance(arg_value, list):\n\t\t\t\t# Checks if the list has more than one value\n\t\t\t\tif len(arg_value) > 1:\n\t\t\t\t\t# Raises an exception, since multiple values are not\n\t\t\t\t\t# allowed\n\t\t\t\t\traise CLIError(\n\t\t\t\t\t\t'Multiple values received for argument \\'{}\\'.'.format(\n\t\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_when_arguments = []\n\tfor option in options_spec:\n\t\tif 'required_when' not in option:\n\t\t\tcontinue\n\t\tcondition_name, condition_value = option['required_when'].split('=')\n\t\tif condition_name in args and args[condition_name] == condition_value:\n\t\t\trequired_when_arguments.append(option['name'])\n\treturn required_when_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\t# check if any of the options has a required_when\n\tconditionally_required = []\n\tfor option in options_spec:\n\t\tif not option.required_when:\n\t\t\tcontinue\n\t\t# check if the option is present in the args list\n\t\tif option.name in args:\n\t\t\t# check if the option value matches the \"required when\"\n\t\t\t# condition\n\t\t\tif self._check_required_when(option, args):\n\t\t\t\tconditionally_required.append(option.name)\n\treturn conditionally_required\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\n\tif options_spec is None:\n\t\treturn required_args\n\n\tfor option in options_spec:\n\t\tif option.get('required_when') is None:\n\t\t\tcontinue\n\n\t\tcondition = option['required_when']\n\t\tlhs, rhs = condition.split('=')\n\n\t\tif lhs in args:\n\t\t\tif args[lhs] == rhs:\n\t\t\t\trequired_args.append(option['name'])\n\n\treturn required_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif 'required_when' in option and option['required_when'] is not None:\n\t\t\tfor k, v in option['required_when'].items():\n\t\t\t\tif k in args and args[k] == v and option['name'] not in args:\n\t\t\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tif args is None:\n\t\targs = {}\n\tresult = []\n\tfor opt in options_spec:\n\t\tif opt['name'] in args and opt.get('required_when'):\n\t\t\tcondition = opt['required_when']\n\t\t\tcondition_args = condition[0]\n\t\t\tif isinstance(args[opt['name']], list):\n\t\t\t\tsatisfied = [args[opt['name']][i] in condition[1] for i in\n\t\t\t\t\t\t\t range(len(args[opt['name']]))]\n\t\t\telse:\n\t\t\t\tsatisfied = args[opt['name']] in condition[1]\n\t\t\tif satisfied and all([arg in args for arg in condition_args]):\n\t\t\t\tresult.append(opt['name'])\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\treq_args = []\n\tfor opt in options_spec:\n\t\tif 'required_when' in opt:\n\t\t\tfor condition in opt['required_when']:\n\t\t\t\tif condition[0] and condition[2]:\n\t\t\t\t\tif condition[0] in args and condition[2] in args:\n\t\t\t\t\t\tif args[condition[0]] == condition[1] and \\\n\t\t\t\t\t\t\t\targs[condition[2]] == condition[3]:\n\t\t\t\t\t\t\treq_args.append(opt['name'])\n\t\t\t\telif condition[0] and not condition[2]:\n\t\t\t\t\tif condition[0] in args:\n\t\t\t\t\t\tif args[condition[0]] == condition[1]:\n\t\t\t\t\t\t\treq_args.append(opt['name'])\n\t\t\t\telif not condition[0] and condition[2]:\n\t\t\t\t\tif condition[2] in args:\n\t\t\t\t\t\tif args[condition[2]] == condition[3]:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tresult = []\n\tfor option in options_spec:\n\t\trequired_when = option.get(\"required_when\")\n\t\tif required_when:\n\t\t\tif required_when == command_name:\n\t\t\t\tresult.append(option[\"name\"])\n\t\t\telif required_when in args:\n\t\t\t\tresult.append(option[\"name\"])\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option_spec in options_spec:\n\t\tif 'required_when' not in option_spec:\n\t\t\tcontinue\n\n\t\trequired_when_spec = option_spec['required_when']\n\t\tif not self._is_spec_condition_matched(required_when_spec, args):\n\t\t\tcontinue\n\n\t\tconditionally_required_args.append(option_spec['name'])\n\n\treturn conditionally_required_args\n\n\tdef _is_spec_condition_matched(self, spec, args):\n\t\t\"\"\"\n\t\tCheck if ``required_when`` condition is matched.\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n\t\tfor arg, value in args.items():\n\t\t\tif value is not None:\n\t\t\t\tif isinstance(value, str):\n\t\t\t\t\tif len(value) > self.doc[arg]['length']:\n\t\t\t\t\t\tself.logger.debug (f\"Argument {arg} length is longer than {self.doc[arg]['length']}\")\n\t\t\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_datatype_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is of the specified datatype.\n\n", "def validate_length_args(self, args):\n\t\tif args.length < 1:\n\t\t\traise ValueError('The length must be greater than 0.')\n\n\tdef validate_type_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is of type specified.\n\n", "def validate_length_args(self, args):\n\t\treturn self._validate_length(args)\n\n\tdef _validate_length(self, args, validated_args=None, mode=\"length\"):\n\t\t\"\"\"\n\t\tCheck if value of arguments is not longer than length specified.\n\n", "def validate_length_args(self, args):\n\t\tfor arg, value in args.items():\n\t\t\tif hasattr(self.__class__, '_validate_length_%s' % arg):\n\t\t\t\targ_len = getattr(self.__class__, '_validate_length_%s' % arg)\n\t\t\t\tif not (isinstance(value, str) or isinstance(value, unicode)) or not len(value) <= arg_len:\n\t\t\t\t\traise ValueError('Length of %s must be less than %d' % (arg, arg_len))\n\n\tdef validate_args(self, args):\n\t\tfor arg in args:\n\t\t\tif not hasattr(self, arg):\n\t\t\t\traise ValueError('No optional argument named %s' % arg)\n\n\tdef _new_from_json_dict(self, json_dict):\n\t\t\"\"\"\n\tReturns a new instance based on the JSON dict.\n\n", "def validate_length_args(self, args):\n\t\tfor arg in args:\n\t\t\tif arg.name in self.required_args:\n\t\t\t\tif arg.value == None:\n\t\t\t\t\traise ValueError(\"Argument {} is required.\".format(arg.name))\n\t\t\t\tif arg.value > arg.max_length:\n\t\t\t\t\traise ValueError(\"Argument {} has max length {}.\".format(arg.name, arg.max_length))\n\n\tdef validate_type_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments has the right type.\n\n", "def validate_length_args(self, args):\n\t\tfor argument in args:\n\t\t\tif len(str(self.args[argument])) > self.length_args[argument]:\n\t\t\t\traise ValueError('The value of argument %s is longer than the specified length of %s' % (argument, self.length_args[argument]))\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tValidate arguments.\n\n", "def validate_length_args(self, args):\n\t\tfor key in args.keys():\n\t\t\tif args[key] and len(args[key]) > self.length[key]:\n\t\t\t\traise ValueError('Argument %s is too long' % key)\n\n\n", "def validate_length_args(self, args):\n\t\tfor arg in args:\n\t\t\tif args[arg] is not None:\n\t\t\t\tif len(args[arg]) > self.length[arg]:\n\t\t\t\t\traise ValueError(\"Length of argument %s is longer than %s\" % (arg, self.length[arg]))\n\n\tdef validate_exists_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of argument exists in database.\n\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg in self.exists:\n\t\t\t\tif args[arg] is None:\n\t\t\t\t\traise ValueError(\"Value of argument %s is None\" % arg)\n\t\t\t\tif not self.exists[arg].find_one({arg: args[arg]}):\n\t\t\t\t\traise ValueError(\"Value of argument %s does not exist\" % arg)\n\n\tdef validate_unique_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of argument is unique in database.\n\n\t:param args: The received arguments.\n\t\n", "def validate_length_args(self, args):\n\t\tfor item in args:\n\t\t\tvalue = self.get_argument(item)\n\t\t\tlength = self.get_argument(item + '_length')\n\t\t\tif len(value) > int(length):\n\t\t\t\tself.set_status(400)\n\t\t\t\tself.write(json.dumps({'error': 'Value of %s is longer than %s' % (item, length)}))\n\t\t\t\tself.finish()\n\t\t\t\treturn\n\n\tdef validate_args_present(self, args):\n\t\t\"\"\"\n\t\tCheck if all arguments are present.\n\n", "def validate_length_args(self, args):\n\t\tfor argument in args:\n\t\t\tif argument.value is not None:\n\t\t\t\tif len(argument.value) > argument.length:\n\t\t\t\t\traise ValueError(\n\t\t\t\t\t\t'Value of argument {0} must be not longer than {1}'\n\t\t\t\t\t\t\t.format(argument.name, argument.length))\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tAbstract method to validate received arguments.\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n\t\tfor arg, value in args.items():\n\t\t\tif value is not None:\n\t\t\t\tif isinstance(value, str):\n\t\t\t\t\tif len(value) > self.doc[arg]['length']:\n\t\t\t\t\t\tself.logger.debug (f\"Argument {arg} length is longer than {self.doc[arg]['length']}\")\n\t\t\t\t\t\treturn False\n\t\treturn True\n\n\tdef validate_datatype_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is of the specified datatype.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tif args.length < 1:\n\t\t\traise ValueError('The length must be greater than 0.')\n\n\tdef validate_type_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is of type specified.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\treturn self._validate_length(args)\n\n\tdef _validate_length(self, args, validated_args=None, mode=\"length\"):\n\t\t\"\"\"\n\t\tCheck if value of arguments is not longer than length specified.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tfor arg, value in args.items():\n\t\t\tif hasattr(self.__class__, '_validate_length_%s' % arg):\n\t\t\t\targ_len = getattr(self.__class__, '_validate_length_%s' % arg)\n\t\t\t\tif not (isinstance(value, str) or isinstance(value, unicode)) or not len(value) <= arg_len:\n\t\t\t\t\traise ValueError('Length of %s must be less than %d' % (arg, arg_len))\n\n\tdef validate_args(self, args):\n\t\tfor arg in args:\n\t\t\tif not hasattr(self, arg):\n\t\t\t\traise ValueError('No optional argument named %s' % arg)\n\n\tdef _new_from_json_dict(self, json_dict):\n\t\t\"\"\"\n\tReturns a new instance based on the JSON dict.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tfor arg in args:\n\t\t\tif arg.name in self.required_args:\n\t\t\t\tif arg.value == None:\n\t\t\t\t\traise ValueError(\"Argument {} is required.\".format(arg.name))\n\t\t\t\tif arg.value > arg.max_length:\n\t\t\t\t\traise ValueError(\"Argument {} has max length {}.\".format(arg.name, arg.max_length))\n\n\tdef validate_type_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments has the right type.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tfor argument in args:\n\t\t\tif len(str(self.args[argument])) > self.length_args[argument]:\n\t\t\t\traise ValueError('The value of argument %s is longer than the specified length of %s' % (argument, self.length_args[argument]))\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tValidate arguments.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tfor key in args.keys():\n\t\t\tif args[key] and len(args[key]) > self.length[key]:\n\t\t\t\traise ValueError('Argument %s is too long' % key)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tfor arg in args:\n\t\t\tif args[arg] is not None:\n\t\t\t\tif len(args[arg]) > self.length[arg]:\n\t\t\t\t\traise ValueError(\"Length of argument %s is longer than %s\" % (arg, self.length[arg]))\n\n\tdef validate_exists_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of argument exists in database.\n\n\t:param args: The received arguments.\n\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif arg in self.exists:\n\t\t\t\tif args[arg] is None:\n\t\t\t\t\traise ValueError(\"Value of argument %s is None\" % arg)\n\t\t\t\tif not self.exists[arg].find_one({arg: args[arg]}):\n\t\t\t\t\traise ValueError(\"Value of argument %s does not exist\" % arg)\n\n\tdef validate_unique_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of argument is unique in database.\n\n\t:param args: The received arguments.\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tfor item in args:\n\t\t\tvalue = self.get_argument(item)\n\t\t\tlength = self.get_argument(item + '_length')\n\t\t\tif len(value) > int(length):\n\t\t\t\tself.set_status(400)\n\t\t\t\tself.write(json.dumps({'error': 'Value of %s is longer than %s' % (item, length)}))\n\t\t\t\tself.finish()\n\t\t\t\treturn\n\n\tdef validate_args_present(self, args):\n\t\t\"\"\"\n\t\tCheck if all arguments are present.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\t\tfor argument in args:\n\t\t\tif argument.value is not None:\n\t\t\t\tif len(argument.value) > argument.length:\n\t\t\t\t\traise ValueError(\n\t\t\t\t\t\t'Value of argument {0} must be not longer than {1}'\n\t\t\t\t\t\t\t.format(argument.name, argument.length))\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tAbstract method to validate received arguments.\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n\tsuper(Parser, self).validate_choices_args(args)\n\tfor arg in vars(args):\n\t\tif arg in self._choices:\n\t\t\tif getattr(args, arg) not in self._choices[arg]:\n\t\t\t\tself.error(\n\t\t\t\t\t'argument {}: invalid choice: {} (choose from {})'\n\t\t\t\t\t.format(\n\t\t\t\t\t\targ,\n\t\t\t\t\t\tgetattr(args, arg),\n\t\t\t\t\t\tself._choices[arg]\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\n", "def validate_choices_args(self, args):\n\t\tfor arg in self.choices:\n\t\t\tif arg in args:\n\t\t\t\tif args[arg] not in self.choices[arg]:\n\t\t\t\t\traise exceptions.ValidationError(\n\t\t\t\t\t\t\"The value of the argument '{0}' must be one of the following: {1}\".format(\n\t\t\t\t\t\t\targ,\n\t\t\t\t\t\t\t\", \".join(self.choices[arg])\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\n\tdef validate_datetime_args(self, args):\n\t\"\"\"\n\tCheck if value of datetime arguments can be parsed by dateutil.parser.parse.\n\n", "def validate_choices_args(self, args):\n\tfor arg in vars(args):\n\t\tif arg in self.choices.keys():\n\t\t\tif not hasattr(args, arg):\n\t\t\t\tcontinue\n\t\t\tif getattr(args, arg) not in self.choices[arg]:\n\t\t\t\traise argparse.ArgumentTypeError(\"{} is not a valid value for argument {}\".format(getattr(args, arg), arg))\n\n\tdef validate_file_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of file arguments is an existing file.\n\n\t:param args: The received arguments.\n\t\t\"\"\"\n\t\tfor arg in vars(args):\n\t\t\tif arg in self.files:\n\t\t\t\tif not hasattr(args, arg):\n\t\t\t\t\tcontinue\n\t\t\t\tif not os.path.isfile(getattr(args, arg)):\n\t\t\t\t\traise argparse.ArgumentTypeError(\"{} is not a valid file for argument {}\".format(getattr(args, arg), arg))\n\n\tdef validate_directory_args(self, args\n", "def validate_choices_args(self, args):\n\tfor arg_name in args:\n\t\tif arg_name in self.choices:\n\t\t\tif args[arg_name] == None:\n\t\t\t\tcontinue\n\t\t\tif args[arg_name] not in self.choices[arg_name]:\n\t\t\t\traise ValidationError('Argument {} must be one of the following values: {}'.format(arg_name, self.choices[arg_name]))\n\n\n", "def validate_choices_args(self, args):\n\t\tfor arg in args:\n\t\t\tif arg in self.__class__.choices.keys():\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\traise argparse.ArgumentTypeError(\"invalid choice: '%s' (choose from '%s')\" % (arg, \"', '\".join(self.__class__.choices.keys())))\n\n", "def validate_choices_args(self, args):\n\t\tfor key, value in args.items():\n\t\t\tif key in self.choices_args and value not in self.choices_args[key]:\n\t\t\t\traise ValidationError(\n\t\t\t\t\tf\"Argument {key} should be one of {self.choices_args[key]}\")\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tValidate all arguments.\n\n", "def validate_choices_args(self, args):\n\t\tfor arg in self._choices_args:\n\t\t\tif arg not in args:\n\t\t\t\tcontinue\n\t\t\tif args[arg] not in self._choices_args[arg]:\n\t\t\t\traise InvalidArgument('Argument %s has an invalid value %s, allowed are: %s' % (arg, args[arg], self._choices_args[arg]))\n\n\tdef get_args_from_request(self, request):\n\t\t\"\"\"\n\t\tGet the arguments from the request.\n\n", "def validate_choices_args(self, args):\n\tsuper(ArgumentParser, self).validate_choices_args(args)\n\n\tfor action in self._actions:\n\t\tif action.choices is not None:\n\t\t\tchoices = action.choices\n\t\t\tif isinstance(choices, dict):\n\t\t\t\tchoices = choices.keys()\n\t\t\tif action.dest in args and args[action.dest] not in choices:\n\t\t\t\tself.error(_(\"invalid choice: %(arg)s (choose from %(choices)s)\") % {\n\t\t\t\t\t\"arg\": args[action.dest],\n\t\t\t\t\t\"choices\": \", \".join(map(repr, choices))\n\t\t\t\t})\n\n", "def validate_choices_args(self, args):\n\tfor arg in self._actions:\n\t\tif arg.choices is not None and arg.dest in args:\n\t\t\tif args[arg.dest] not in arg.choices:\n\t\t\t\tmsg = 'invalid choice: %r (choose from %s)'\n\t\t\t\traise ArgumentError(arg, msg % (args[arg.dest], arg.choices))\n\n\n", "def validate_choices_args(self, args):\n\tfor arg in self._actions:\n\t\tif arg.choices is not None and args[arg.dest] not in arg.choices:\n\t\t\tself.error(\"argument {}: invalid choice: '{}' (choose from '{}')\".format(arg.dest, args[arg.dest], arg.choices))\n\n\t\tif arg.dest == \"network_type\" and args[arg.dest] == \"hybrid\":\n\t\t\tif args[\"hybrid_type\"] is None:\n\t\t\t\tself.error(\"argument {}: hybrid_type is required when using hybrid network_type\".format(arg.dest))\n\t\t\tif not args[\"hybrid_type\"] in [\"mixed\", \"separate\"]:\n\t\t\t\tself.error(\"argument {}: hybrid_type is not valid. Choose from mixed or separate\".format(arg.dest))\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n\tsuper(Parser, self).validate_choices_args(args)\n\tfor arg in vars(args):\n\t\tif arg in self._choices:\n\t\t\tif getattr(args, arg) not in self._choices[arg]:\n\t\t\t\tself.error(\n\t\t\t\t\t'argument {}: invalid choice: {} (choose from {})'\n\t\t\t\t\t.format(\n\t\t\t\t\t\targ,\n\t\t\t\t\t\tgetattr(args, arg),\n\t\t\t\t\t\tself._choices[arg]\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\tfor arg in self.choices:\n\t\t\tif arg in args:\n\t\t\t\tif args[arg] not in self.choices[arg]:\n\t\t\t\t\traise exceptions.ValidationError(\n\t\t\t\t\t\t\"The value of the argument '{0}' must be one of the following: {1}\".format(\n\t\t\t\t\t\t\targ,\n\t\t\t\t\t\t\t\", \".join(self.choices[arg])\n\t\t\t\t\t\t)\n\t\t\t\t\t)\n\n\tdef validate_datetime_args(self, args):\n\t\"\"\"\n\tCheck if value of datetime arguments can be parsed by dateutil.parser.parse.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg in vars(args):\n\t\tif arg in self.choices.keys():\n\t\t\tif not hasattr(args, arg):\n\t\t\t\tcontinue\n\t\t\tif getattr(args, arg) not in self.choices[arg]:\n\t\t\t\traise argparse.ArgumentTypeError(\"{} is not a valid value for argument {}\".format(getattr(args, arg), arg))\n\n\tdef validate_file_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of file arguments is an existing file.\n\n\t:param args: The received arguments.\n\t\t\"\"\"\n\t\tfor arg in vars(args):\n\t\t\tif arg in self.files:\n\t\t\t\tif not hasattr(args, arg):\n\t\t\t\t\tcontinue\n\t\t\t\tif not os.path.isfile(getattr(args, arg)):\n\t\t\t\t\traise argparse.ArgumentTypeError(\"{} is not a valid file for argument {}\".format(getattr(args, arg), arg))\n\n\tdef validate_directory_args(self, args\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg_name in args:\n\t\tif arg_name in self.choices:\n\t\t\tif args[arg_name] == None:\n\t\t\t\tcontinue\n\t\t\tif args[arg_name] not in self.choices[arg_name]:\n\t\t\t\traise ValidationError('Argument {} must be one of the following values: {}'.format(arg_name, self.choices[arg_name]))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\tfor arg in args:\n\t\t\tif arg in self.__class__.choices.keys():\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\traise argparse.ArgumentTypeError(\"invalid choice: '%s' (choose from '%s')\" % (arg, \"', '\".join(self.__class__.choices.keys())))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\tfor key, value in args.items():\n\t\t\tif key in self.choices_args and value not in self.choices_args[key]:\n\t\t\t\traise ValidationError(\n\t\t\t\t\tf\"Argument {key} should be one of {self.choices_args[key]}\")\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tValidate all arguments.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\tfor arg in self._choices_args:\n\t\t\tif arg not in args:\n\t\t\t\tcontinue\n\t\t\tif args[arg] not in self._choices_args[arg]:\n\t\t\t\traise InvalidArgument('Argument %s has an invalid value %s, allowed are: %s' % (arg, args[arg], self._choices_args[arg]))\n\n\tdef get_args_from_request(self, request):\n\t\t\"\"\"\n\t\tGet the arguments from the request.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tsuper(ArgumentParser, self).validate_choices_args(args)\n\n\tfor action in self._actions:\n\t\tif action.choices is not None:\n\t\t\tchoices = action.choices\n\t\t\tif isinstance(choices, dict):\n\t\t\t\tchoices = choices.keys()\n\t\t\tif action.dest in args and args[action.dest] not in choices:\n\t\t\t\tself.error(_(\"invalid choice: %(arg)s (choose from %(choices)s)\") % {\n\t\t\t\t\t\"arg\": args[action.dest],\n\t\t\t\t\t\"choices\": \", \".join(map(repr, choices))\n\t\t\t\t})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg in self._actions:\n\t\tif arg.choices is not None and arg.dest in args:\n\t\t\tif args[arg.dest] not in arg.choices:\n\t\t\t\tmsg = 'invalid choice: %r (choose from %s)'\n\t\t\t\traise ArgumentError(arg, msg % (args[arg.dest], arg.choices))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg in self._actions:\n\t\tif arg.choices is not None and args[arg.dest] not in arg.choices:\n\t\t\tself.error(\"argument {}: invalid choice: '{}' (choose from '{}')\".format(arg.dest, args[arg.dest], arg.choices))\n\n\t\tif arg.dest == \"network_type\" and args[arg.dest] == \"hybrid\":\n\t\t\tif args[\"hybrid_type\"] is None:\n\t\t\t\tself.error(\"argument {}: hybrid_type is required when using hybrid network_type\".format(arg.dest))\n\t\t\tif not args[\"hybrid_type\"] in [\"mixed\", \"separate\"]:\n\t\t\t\tself.error(\"argument {}: hybrid_type is not valid. Choose from mixed or separate\".format(arg.dest))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n\t\tif not (self.minimum_value <= int(args['value']) <= self.maximum_value):\n\t\t\traise ValueError(\n\t\t\t\t\"The value should be between {} and {}\".format(self.minimum_value, self.maximum_value))\n\n\tdef get_argument(self, arg, arg_type=None):\n\t\t\"\"\"\n\t\tGet the value of an argument.\n\n", "def validate_min_max_args(self, args):\n\t\t# Check if minimum and maximum values are defined\n\t\tif self.minimum_value is not None and self.maximum_value is not None:\n\t\t\t# Check if value of arguments is between minimum and maximum values\n\t\t\tif not self.minimum_value <= args <= self.maximum_value:\n\t\t\t\tpattern = 'argument value must be between {} and {}'\n\t\t\t\tmessage = pattern.format(self.minimum_value, self.maximum_value)\n\t\t\t\traise ArgumentError(message)\n\t\t# Check if minimum value is defined\n\t\telif self.minimum_value is not None:\n\t\t\t# Check if value of arguments is greater than minimum value\n\t\t\tif not args >= self.minimum_value:\n\t\t\t\tpattern = 'argument value must be greater than or equal to {}'\n\t\t\t\tmessage = pattern.format(self.minimum_value)\n\t\t\t\traise ArgumentError(message)\n\t\t# Check if maximum value is defined\n\t\telif self.maximum_value is not None:\n\t\t\t# Check if value of arguments is less than\n", "def validate_min_max_args(self, args):\n\t\tfor arg, min_value, max_value in args:\n\t\t\ttry:\n\t\t\t\tif not min_value <= float(arg) <= max_value:\n\t\t\t\t\traise ValueError\n\t\t\texcept ValueError:\n\t\t\t\traise ArgumentsError(\n\t\t\t\t\t'\\'{}\\' argument is not between {} and {}'.format(arg, min_value, max_value))\n\n\tdef validate_step_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is a multiple of step value.\n\n", "def validate_min_max_args(self, args):\n\t\tfor arg in args:\n\t\t\tif args[arg] < arg.minimum:\n\t\t\t\traise ArgumentOutOfRangeError(arg.name, arg.minimum, arg.maximum)\n\t\t\tif args[arg] > arg.maximum:\n\t\t\t\traise ArgumentOutOfRangeError(arg.name, arg.minimum, arg.maximum)\n\n\tdef validate_default_value(self, args):\n\t\t\"\"\"\n\tCheck if there is a default value for each arguments that are not present.\n\n", "def validate_min_max_args(self, args):\n\t\tfor arg in args:\n\t\t\tvalue = args[arg]\n\t\t\tif arg in self._min_max_args:\n\t\t\t\tif not self._min_max_args[arg][0] <= value <= self._min_max_args[arg][1]:\n\t\t\t\t\traise ValueError(\"Value of argument '{}' is not between minimum and maximum values ({} - {})\".format(arg, self._min_max_args[arg][0], self._min_max_args[arg][1]))\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if all the required arguments are present in the given arguments.\n\n", "def validate_min_max_args(self, args):\n\t\tfor i in args:\n\t\t\tif i.value < i.minimum or i.value > i.maximum:\n\t\t\t\traise ValidationError(\"value of argument {} is not between min: {} and max: {}\".format(i.name, i.minimum, i.maximum))\n\n\tdef validate_min_max_args_in_list(self, args):\n\t\t\"\"\"\n\t\tCheck if values of arguments are between minimum and maximum values.\n\n", "def validate_min_max_args(self, args):\n\t\ttry:\n\t\t\tfor arg in args:\n\t\t\t\tif not (arg.min <= float(arg.value) <= arg.max):\n\t\t\t\t\treturn False\n\t\t\treturn True\n\n\t\texcept ValueError:\n\t\t\treturn False\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tValidate received arguments.\n\n", "def validate_min_max_args(self, args):\n\t\tfor arg in args:\n\t\t\targ_value = args[arg]\n\t\t\targ_min = arg + \"_min\"\n\t\t\targ_max = arg + \"_max\"\n\t\t\tif arg_min in self.__dict__:\n\t\t\t\tif arg_value < self.__dict__[arg_min]:\n\t\t\t\t\traise ValueError(\n\t\t\t\t\t\t\"Argument '{}' must be >= {}.\".format(arg, self.__dict__[arg_min])\n\t\t\t\t\t)\n\t\t\tif arg_max in self.__dict__:\n\t\t\t\tif arg_value > self.__dict__[arg_max]:\n\t\t\t\t\traise ValueError(\n\t\t\t\t\t\t\"Argument '{}' must be <= {}.\".format(arg, self.__dict__[arg_max])\n\t\t\t\t\t)\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is valid.\n\n\t:param args: The received arguments.\n\t\t\"\"\"\n", "def validate_min_max_args(self, args):\n\tfor key, value in args.items():\n\t\tif key in self.min_max_args:\n\t\t\tif not self.min_max_args[key]['min'] <= value <= self.min_max_args[key]['max']:\n\t\t\t\traise InvalidArgumentError('{} must be between {} and {}'.format(key, self.min_max_args[key]['min'], self.min_max_args[key]['max']))\n\n", "def validate_min_max_args(self, args):\n\t\targs = vars(args)\n\t\tfor key, value in args.items():\n\t\t\tif value is not None:\n\t\t\t\tif self.args_info[key][\"min\"] is not None:\n\t\t\t\t\tif value < self.args_info[key][\"min\"]:\n\t\t\t\t\t\traise ValueError(\"The argument %s must be greater than or equal to %s\" % (key, self.args_info[key][\"min\"]))\n\t\t\t\tif self.args_info[key][\"max\"] is not None:\n\t\t\t\t\tif value > self.args_info[key][\"max\"]:\n\t\t\t\t\t\traise ValueError(\"The argument %s must be lower than or equal to %s\" % (key, self.args_info[key][\"max\"]))\n\t\n\tdef validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of arguments is in choices.\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n\t\tif not (self.minimum_value <= int(args['value']) <= self.maximum_value):\n\t\t\traise ValueError(\n\t\t\t\t\"The value should be between {} and {}\".format(self.minimum_value, self.maximum_value))\n\n\tdef get_argument(self, arg, arg_type=None):\n\t\t\"\"\"\n\t\tGet the value of an argument.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\t# Check if minimum and maximum values are defined\n\t\tif self.minimum_value is not None and self.maximum_value is not None:\n\t\t\t# Check if value of arguments is between minimum and maximum values\n\t\t\tif not self.minimum_value <= args <= self.maximum_value:\n\t\t\t\tpattern = 'argument value must be between {} and {}'\n\t\t\t\tmessage = pattern.format(self.minimum_value, self.maximum_value)\n\t\t\t\traise ArgumentError(message)\n\t\t# Check if minimum value is defined\n\t\telif self.minimum_value is not None:\n\t\t\t# Check if value of arguments is greater than minimum value\n\t\t\tif not args >= self.minimum_value:\n\t\t\t\tpattern = 'argument value must be greater than or equal to {}'\n\t\t\t\tmessage = pattern.format(self.minimum_value)\n\t\t\t\traise ArgumentError(message)\n\t\t# Check if maximum value is defined\n\t\telif self.maximum_value is not None:\n\t\t\t# Check if value of arguments is less than\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\tfor arg, min_value, max_value in args:\n\t\t\ttry:\n\t\t\t\tif not min_value <= float(arg) <= max_value:\n\t\t\t\t\traise ValueError\n\t\t\texcept ValueError:\n\t\t\t\traise ArgumentsError(\n\t\t\t\t\t'\\'{}\\' argument is not between {} and {}'.format(arg, min_value, max_value))\n\n\tdef validate_step_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is a multiple of step value.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\tfor arg in args:\n\t\t\tif args[arg] < arg.minimum:\n\t\t\t\traise ArgumentOutOfRangeError(arg.name, arg.minimum, arg.maximum)\n\t\t\tif args[arg] > arg.maximum:\n\t\t\t\traise ArgumentOutOfRangeError(arg.name, arg.minimum, arg.maximum)\n\n\tdef validate_default_value(self, args):\n\t\t\"\"\"\n\tCheck if there is a default value for each arguments that are not present.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\tfor arg in args:\n\t\t\tvalue = args[arg]\n\t\t\tif arg in self._min_max_args:\n\t\t\t\tif not self._min_max_args[arg][0] <= value <= self._min_max_args[arg][1]:\n\t\t\t\t\traise ValueError(\"Value of argument '{}' is not between minimum and maximum values ({} - {})\".format(arg, self._min_max_args[arg][0], self._min_max_args[arg][1]))\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if all the required arguments are present in the given arguments.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\tfor i in args:\n\t\t\tif i.value < i.minimum or i.value > i.maximum:\n\t\t\t\traise ValidationError(\"value of argument {} is not between min: {} and max: {}\".format(i.name, i.minimum, i.maximum))\n\n\tdef validate_min_max_args_in_list(self, args):\n\t\t\"\"\"\n\t\tCheck if values of arguments are between minimum and maximum values.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\ttry:\n\t\t\tfor arg in args:\n\t\t\t\tif not (arg.min <= float(arg.value) <= arg.max):\n\t\t\t\t\treturn False\n\t\t\treturn True\n\n\t\texcept ValueError:\n\t\t\treturn False\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tValidate received arguments.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\tfor arg in args:\n\t\t\targ_value = args[arg]\n\t\t\targ_min = arg + \"_min\"\n\t\t\targ_max = arg + \"_max\"\n\t\t\tif arg_min in self.__dict__:\n\t\t\t\tif arg_value < self.__dict__[arg_min]:\n\t\t\t\t\traise ValueError(\n\t\t\t\t\t\t\"Argument '{}' must be >= {}.\".format(arg, self.__dict__[arg_min])\n\t\t\t\t\t)\n\t\t\tif arg_max in self.__dict__:\n\t\t\t\tif arg_value > self.__dict__[arg_max]:\n\t\t\t\t\traise ValueError(\n\t\t\t\t\t\t\"Argument '{}' must be <= {}.\".format(arg, self.__dict__[arg_max])\n\t\t\t\t\t)\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of arguments is valid.\n\n\t:param args: The received arguments.\n\t\t\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\tfor key, value in args.items():\n\t\tif key in self.min_max_args:\n\t\t\tif not self.min_max_args[key]['min'] <= value <= self.min_max_args[key]['max']:\n\t\t\t\traise InvalidArgumentError('{} must be between {} and {}'.format(key, self.min_max_args[key]['min'], self.min_max_args[key]['max']))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\targs = vars(args)\n\t\tfor key, value in args.items():\n\t\t\tif value is not None:\n\t\t\t\tif self.args_info[key][\"min\"] is not None:\n\t\t\t\t\tif value < self.args_info[key][\"min\"]:\n\t\t\t\t\t\traise ValueError(\"The argument %s must be greater than or equal to %s\" % (key, self.args_info[key][\"min\"]))\n\t\t\t\tif self.args_info[key][\"max\"] is not None:\n\t\t\t\t\tif value > self.args_info[key][\"max\"]:\n\t\t\t\t\t\traise ValueError(\"The argument %s must be lower than or equal to %s\" % (key, self.args_info[key][\"max\"]))\n\t\n\tdef validate_choices_args(self, args):\n\t\"\"\"\n\tCheck if value of arguments is in choices.\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "ques_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t# TODO: create complex type\n\treturn self\n\n\tdef create_subcommand_types(self, subcommand):\n\t\"\"\"\n\tCreate the subcommand types\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t# TODO: we should improve the handling of options and subcommands\n\t# options are not handled the same way as subcommands\n\tcomplex_type_name = type_name\n\tif '.' in complex_type_name:\n\t\tcomplex_type_name = complex_type_name.replace('.', '_', 1)\n\tcomplex_type = ComplexType(\n\t\tname=complex_type_name,\n\t\tdescription=spec_option.get('description', ''),\n\t\tdoc_url=spec_option.get('doc_url', ''),\n\t\tsubcommand=subcommand,\n\t\toption=option_name,\n\t\tis_list=spec_option.get('list', False),\n\t\tis_map=spec_option.get('map', False),\n\t)\n\tif 'properties' in spec_option:\n\t\tfor property_name, property_value in \\\n\t\t\t\tspec_option['properties'].items():\n\t\t\tif 'type' in property_value:\n\t\t\t\tproperty_type = self.create_type(\n\t\t\t\t\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t#debug(\"create_complex_argumet_type: spec_option=\" + str(spec_option))\n\ttype_name = type_name.replace(\".\", \"_\")\n\tif \";\" in spec_option[\"type\"]:\n\t\ttype_list = spec_option[\"type\"].split(\";\")\n\t\ttype_list = [type.strip() for type in type_list]\n\t\ttype_list = [type.replace(\".\", \"_\") for type in type_list]\n\t\ttype_list = [type.lower() for type in type_list]\n\t\tspec_option[\"type\"] = type_list\n\telse:\n\t\tspec_option[\"type\"] = spec_option[\"type\"].replace(\".\", \"_\")\n\t\tspec_option[\"type\"] = spec_option[\"type\"].lower()\n\tspec_option[\"type\"] = spec_option[\"type\"].split(\";\")\n\tspec_option[\"type\"] = [type.strip() for type in spec_option[\"type\"]]\n\n\tif spec_option[\"type\"][0] == \"choice\":\n\t\t#debug(\"create_\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif spec_option.get('type') == 'array':\n\t\tif spec_option.get('subtype') == 'string':\n\t\t\treturn self.create_array_string_type(subcommand, type_name,\n\t\t\t                                         option_name, spec_option)\n\t\telse:\n\t\t\treturn self.create_array_type(subcommand, type_name,\n\t\t\t                                  option_name, spec_option)\n\telse:\n\t\tif spec_option.get('subtype') == 'string':\n\t\t\treturn self.create_string_type(subcommand, type_name,\n\t\t\t                                    option_name, spec_option)\n\t\telse:\n\t\t\treturn self.create_object_type(subcommand, type_name,\n\t\t\t                                    option_name, spec_option)\n\n\tdef create_array_type(self, subcommand, type_name, option_name,\n\t                          spec_option):\n\t\t\"\"\"\n\t\tBuild the array type\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif option_name not in spec_option:\n\t\treturn None\n\ttype_name = subcommand + '::' + type_name\n\t# the type name is already in the types list\n\tif type_name in self.types:\n\t\treturn None\n\tself.types[type_name] = self.create_type_list(subcommand, type_name, spec_option)\n\treturn self.types[type_name]\n\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\ttype_description = spec_option.get('description', '')\n\ttype_description = type_description.replace('\\n', ' ')\n\ttype_description = type_description.replace('\\t', ' ')\n\ttype_description = type_description.replace('\\r', ' ')\n\ttype_description = type_description.replace('\\f', ' ')\n\ttype_description = type_description.replace('\\v', ' ')\n\ttype_description = type_description.replace('\\\\', '\\\\\\\\')\n\ttype_description = type_description.replace('\"', '\\\\\"')\n\ttype_description = type_description.replace(\"'\", \"\\\\'\")\n\ttype_description = type_description.replace('\\a', ' ')\n\ttype_description = type_description.replace('\\b', ' ')\n\ttype_description = type_description.replace('\\x00', ' ')\n\ttype_description = type_description.strip()\n\tif type_description:\n\t\ttype_description = '\\n\"%s\"' % type_description\n\ttype_title = spec\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t# pylint: disable=too-many-locals\n\t# pylint: disable=too-many-statements\n\t# pylint: disable=too-many-branches\n\n\tif spec_option.get(\"type\") == \"object\":\n\t\t# Check if the option is an object\n\t\tproperties = {}\n\t\t# Build the properties\n\t\tfor name, prop in spec_option.get(\"properties\").items():\n\t\t\tproperties[name] = self.create_complex_argumet_type(\n\t\t\t\tsubcommand,\n\t\t\t\ttype_name,\n\t\t\t\tname,\n\t\t\t\tprop)\n\n\t\treturn properties\n\n\telif spec_option.get(\"type\") == \"array\":\n\t\t# Check if the option is an array\n\t\titems = spec_option.get(\"items\")\n\t\tif isinstance(items, dict):\n\t\t\t# Check if the items are an object\n\t\t\tproperties = {}\n\t\t\t# Build the properties\n\t\t\tfor name, prop in items.get(\"properties\").items():\n\t\t\t\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif self.debug:\n\t\tprint(\"create_complex_argumet_type: \" + type_name)\n\n\tif type_name.startswith(\"array:\"):\n\t\treturn self.create_array_argumet_type(subcommand, type_name, option_name,\n\t\t                                      spec_option)\n\telse:\n\t\treturn self.create_object_argumet_type(subcommand, type_name, option_name,\n\t\t                                       spec_option)\n\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tall_type = spec_option.get('type', {})\n\tif isinstance(all_type, list):\n\t\tall_type = all_type[0]\n\n\tif 'type' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'opts' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'arg' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'choice' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'type_name' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\toption = spec_option['option']\n\ttype = spec_option['type']\n\tif type == 'list':\n\t\ttype = spec_option['list_type']\n\t\tif type == 'string':\n\t\t\ttype = 'str'\n\t\telif type == 'integer':\n\t\t\ttype = 'int'\n\t\telif type == 'boolean':\n\t\t\ttype = 'bool'\n\t\telif type == 'list':\n\t\t\traise Exception('Nested lists are not supported')\n\t\telif type == 'dictionary':\n\t\t\traise Exception('Mixed lists and dictionaries are not supported')\n\t\telif type == 'complex':\n\t\t\traise Exception('Mixed lists and complex types are not supported')\n\t\telse:\n\t\t\traise Exception('Unsupported list type %s' % type)\n\telif type == 'dictionary':\n\t\ttype = spec_option['dictionary_key_type']\n\t\tif type == 'string':\n\t\t\ttype = 'str'\n\t\telif type == 'integer':\n\t\t\ttype\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t# TODO: create complex type\n\treturn self\n\n\tdef create_subcommand_types(self, subcommand):\n\t\"\"\"\n\tCreate the subcommand types\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t# TODO: we should improve the handling of options and subcommands\n\t# options are not handled the same way as subcommands\n\tcomplex_type_name = type_name\n\tif '.' in complex_type_name:\n\t\tcomplex_type_name = complex_type_name.replace('.', '_', 1)\n\tcomplex_type = ComplexType(\n\t\tname=complex_type_name,\n\t\tdescription=spec_option.get('description', ''),\n\t\tdoc_url=spec_option.get('doc_url', ''),\n\t\tsubcommand=subcommand,\n\t\toption=option_name,\n\t\tis_list=spec_option.get('list', False),\n\t\tis_map=spec_option.get('map', False),\n\t)\n\tif 'properties' in spec_option:\n\t\tfor property_name, property_value in \\\n\t\t\t\tspec_option['properties'].items():\n\t\t\tif 'type' in property_value:\n\t\t\t\tproperty_type = self.create_type(\n\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t#debug(\"create_complex_argumet_type: spec_option=\" + str(spec_option))\n\ttype_name = type_name.replace(\".\", \"_\")\n\tif \";\" in spec_option[\"type\"]:\n\t\ttype_list = spec_option[\"type\"].split(\";\")\n\t\ttype_list = [type.strip() for type in type_list]\n\t\ttype_list = [type.replace(\".\", \"_\") for type in type_list]\n\t\ttype_list = [type.lower() for type in type_list]\n\t\tspec_option[\"type\"] = type_list\n\telse:\n\t\tspec_option[\"type\"] = spec_option[\"type\"].replace(\".\", \"_\")\n\t\tspec_option[\"type\"] = spec_option[\"type\"].lower()\n\tspec_option[\"type\"] = spec_option[\"type\"].split(\";\")\n\tspec_option[\"type\"] = [type.strip() for type in spec_option[\"type\"]]\n\n\tif spec_option[\"type\"][0] == \"choice\":\n\t\t#debug(\"create_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif spec_option.get('type') == 'array':\n\t\tif spec_option.get('subtype') == 'string':\n\t\t\treturn self.create_array_string_type(subcommand, type_name,\n\t\t\t                                         option_name, spec_option)\n\t\telse:\n\t\t\treturn self.create_array_type(subcommand, type_name,\n\t\t\t                                  option_name, spec_option)\n\telse:\n\t\tif spec_option.get('subtype') == 'string':\n\t\t\treturn self.create_string_type(subcommand, type_name,\n\t\t\t                                    option_name, spec_option)\n\t\telse:\n\t\t\treturn self.create_object_type(subcommand, type_name,\n\t\t\t                                    option_name, spec_option)\n\n\tdef create_array_type(self, subcommand, type_name, option_name,\n\t                          spec_option):\n\t\t\"\"\"\n\t\tBuild the array type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif option_name not in spec_option:\n\t\treturn None\n\ttype_name = subcommand + '::' + type_name\n\t# the type name is already in the types list\n\tif type_name in self.types:\n\t\treturn None\n\tself.types[type_name] = self.create_type_list(subcommand, type_name, spec_option)\n\treturn self.types[type_name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\ttype_description = spec_option.get('description', '')\n\ttype_description = type_description.replace('\\n', ' ')\n\ttype_description = type_description.replace('\\t', ' ')\n\ttype_description = type_description.replace('\\r', ' ')\n\ttype_description = type_description.replace('\\f', ' ')\n\ttype_description = type_description.replace('\\v', ' ')\n\ttype_description = type_description.replace('\\\\', '\\\\\\\\')\n\ttype_description = type_description.replace('\"', '\\\\\"')\n\ttype_description = type_description.replace(\"'\", \"\\\\'\")\n\ttype_description = type_description.replace('\\a', ' ')\n\ttype_description = type_description.replace('\\b', ' ')\n\ttype_description = type_description.replace('\\x00', ' ')\n\ttype_description = type_description.strip()\n\tif type_description:\n\t\ttype_description = '\\n\"%s\"' % type_description\n\ttype_title = spec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\t# pylint: disable=too-many-locals\n\t# pylint: disable=too-many-statements\n\t# pylint: disable=too-many-branches\n\n\tif spec_option.get(\"type\") == \"object\":\n\t\t# Check if the option is an object\n\t\tproperties = {}\n\t\t# Build the properties\n\t\tfor name, prop in spec_option.get(\"properties\").items():\n\t\t\tproperties[name] = self.create_complex_argumet_type(\n\t\t\t\tsubcommand,\n\t\t\t\ttype_name,\n\t\t\t\tname,\n\t\t\t\tprop)\n\n\t\treturn properties\n\n\telif spec_option.get(\"type\") == \"array\":\n\t\t# Check if the option is an array\n\t\titems = spec_option.get(\"items\")\n\t\tif isinstance(items, dict):\n\t\t\t# Check if the items are an object\n\t\t\tproperties = {}\n\t\t\t# Build the properties\n\t\t\tfor name, prop in items.get(\"properties\").items():\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif self.debug:\n\t\tprint(\"create_complex_argumet_type: \" + type_name)\n\n\tif type_name.startswith(\"array:\"):\n\t\treturn self.create_array_argumet_type(subcommand, type_name, option_name,\n\t\t                                      spec_option)\n\telse:\n\t\treturn self.create_object_argumet_type(subcommand, type_name, option_name,\n\t\t                                       spec_option)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tall_type = spec_option.get('type', {})\n\tif isinstance(all_type, list):\n\t\tall_type = all_type[0]\n\n\tif 'type' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'opts' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'arg' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'choice' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\tsubcommand, type_name, option_name, all_type)\n\n\tif 'type_name' in all_type:\n\t\treturn self.create_complex_argumet_type(\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\toption = spec_option['option']\n\ttype = spec_option['type']\n\tif type == 'list':\n\t\ttype = spec_option['list_type']\n\t\tif type == 'string':\n\t\t\ttype = 'str'\n\t\telif type == 'integer':\n\t\t\ttype = 'int'\n\t\telif type == 'boolean':\n\t\t\ttype = 'bool'\n\t\telif type == 'list':\n\t\t\traise Exception('Nested lists are not supported')\n\t\telif type == 'dictionary':\n\t\t\traise Exception('Mixed lists and dictionaries are not supported')\n\t\telif type == 'complex':\n\t\t\traise Exception('Mixed lists and complex types are not supported')\n\t\telse:\n\t\t\traise Exception('Unsupported list type %s' % type)\n\telif type == 'dictionary':\n\t\ttype = spec_option['dictionary_key_type']\n\t\tif type == 'string':\n\t\t\ttype = 'str'\n\t\telif type == 'integer':\n\t\t\ttype\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "ques_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif self.is_control_arg(arg):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif self.is_custom_arg(arg):\n\t\t\traise ValueError(\"Custom args are not supported yet.\")\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.name in self.CONTROL_ARGS:\n\t\t\tcontrol_args[arg.name] = arg.value\n\t\telif arg.name in self.NESTED_ARGS:\n\t\t\tnested_args[arg.name] = arg.value\n\t\telse:\n\t\t\tcustom_args[arg.name] = arg.value\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor (key, value) in args.items():\n\t\tif key.startswith('nested_'):\n\t\t\tnested_args[key] = value\n\t\telif key.startswith('custom_'):\n\t\t\tcustom_args[key] = value\n\t\telse:\n\t\t\tcontrol_args[key] = value\n\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.custom:\n\t\t\tcontrol_args.update(arg.custom)\n\t\telif arg.nested_arg:\n\t\t\tnested_args.update(arg.nested_arg)\n\n\treturn control_args, nested_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith(self.CONTROL_ARGS_PREFIX):\n\t\t\targ = arg.replace(self.CONTROL_ARGS_PREFIX, '')\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg.startswith(self.CUSTOM_ARGS_PREFIX):\n\t\t\targ = arg.replace(self.CUSTOM_ARGS_PREFIX, '')\n\t\t\tcustom_args[arg] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor k, v in args.items():\n\t\tif k in self.CONTROL_ARGS:\n\t\t\tcontrol_args[k] = v\n\t\telif k in self.CUSTOM_ARGS:\n\t\t\tcustom_args[k] = v\n\t\telse:\n\t\t\tnested_args[k] = v\n\treturn control_args, nested_args, custom_args\n\n", "def get_nested_custom_and_control_args(self, args):\n\tnested_args = {}\n\tcontrol_args = {}\n\tfor arg in args:\n\t\tif arg.startswith('custom_'):\n\t\t\tnested_args[arg] = self.module.params[arg]\n\t\telif not arg.startswith('control_'):\n\t\t\tnested_args[arg] = self.module.params[arg]\n\t\telse:\n\t\t\tcontrol_args[arg] = self.module.params[arg]\n\treturn control_args, nested_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--\"):\n\t\t\tif arg.startswith(\"--custom-\"):\n\t\t\t\tkey = arg[9:]\n\t\t\t\tvalue = args[arg]\n\t\t\t\tcustom_args[key] = value\n\t\t\telif arg.startswith(\"--nested-\"):\n\t\t\t\tkey = arg[9:]\n\t\t\t\tvalue = args[arg]\n\t\t\t\tnested_args[key] = value\n\t\t\telse:\n\t\t\t\tkey = arg[2:]\n\t\t\t\tvalue = args[arg]\n\t\t\t\tcontrol_args[key] = value\n\t\telse:\n\t\t\tkey = arg[1:]\n\t\t\tvalue = args[arg]\n\t\t\tcontrol_args[key] = value\n\n\tif control_args['debug']:\n\t\tlog.debug(\"control_args: %s\" % control_args)\n\t\tlog.debug\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tif arg.startswith('--control-'):\n\t\t\t\tcontrol_args[arg[10:]] = args[arg]\n\t\t\telif arg.startswith('--nested-'):\n\t\t\t\tnested_args[arg[9:]] = args[arg]\n\t\t\telif arg.startswith('--custom-'):\n\t\t\t\tcustom_args[arg[9:]] = args[arg]\n\t\t\telif arg.startswith('--'):\n\t\t\t\tcontrol_args[arg[2:]] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg[1:]] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg_name in args:\n\t\targ_val = args[arg_name]\n\t\tif arg_name in self.control_args:\n\t\t\tcontrol_args[arg_name] = arg_val\n\t\telse:\n\t\t\tnested_args[arg_name] = arg_val\n\t\t\tif arg_name in self.custom_args:\n\t\t\t\tcustom_args[arg_name] = arg_val\n\treturn control_args, nested_args, custom_args\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tnested_args = {}\n\tcontrol_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif self.is_control_arg(arg):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif self.is_custom_arg(arg):\n\t\t\traise ValueError(\"Custom args are not supported yet.\")\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.name in self.CONTROL_ARGS:\n\t\t\tcontrol_args[arg.name] = arg.value\n\t\telif arg.name in self.NESTED_ARGS:\n\t\t\tnested_args[arg.name] = arg.value\n\t\telse:\n\t\t\tcustom_args[arg.name] = arg.value\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor (key, value) in args.items():\n\t\tif key.startswith('nested_'):\n\t\t\tnested_args[key] = value\n\t\telif key.startswith('custom_'):\n\t\t\tcustom_args[key] = value\n\t\telse:\n\t\t\tcontrol_args[key] = value\n\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg.custom:\n\t\t\tcontrol_args.update(arg.custom)\n\t\telif arg.nested_arg:\n\t\t\tnested_args.update(arg.nested_arg)\n\n\treturn control_args, nested_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith(self.CONTROL_ARGS_PREFIX):\n\t\t\targ = arg.replace(self.CONTROL_ARGS_PREFIX, '')\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg.startswith(self.CUSTOM_ARGS_PREFIX):\n\t\t\targ = arg.replace(self.CUSTOM_ARGS_PREFIX, '')\n\t\t\tcustom_args[arg] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor k, v in args.items():\n\t\tif k in self.CONTROL_ARGS:\n\t\t\tcontrol_args[k] = v\n\t\telif k in self.CUSTOM_ARGS:\n\t\t\tcustom_args[k] = v\n\t\telse:\n\t\t\tnested_args[k] = v\n\treturn control_args, nested_args, custom_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tnested_args = {}\n\tcontrol_args = {}\n\tfor arg in args:\n\t\tif arg.startswith('custom_'):\n\t\t\tnested_args[arg] = self.module.params[arg]\n\t\telif not arg.startswith('control_'):\n\t\t\tnested_args[arg] = self.module.params[arg]\n\t\telse:\n\t\t\tcontrol_args[arg] = self.module.params[arg]\n\treturn control_args, nested_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"--\"):\n\t\t\tif arg.startswith(\"--custom-\"):\n\t\t\t\tkey = arg[9:]\n\t\t\t\tvalue = args[arg]\n\t\t\t\tcustom_args[key] = value\n\t\t\telif arg.startswith(\"--nested-\"):\n\t\t\t\tkey = arg[9:]\n\t\t\t\tvalue = args[arg]\n\t\t\t\tnested_args[key] = value\n\t\t\telse:\n\t\t\t\tkey = arg[2:]\n\t\t\t\tvalue = args[arg]\n\t\t\t\tcontrol_args[key] = value\n\t\telse:\n\t\t\tkey = arg[1:]\n\t\t\tvalue = args[arg]\n\t\t\tcontrol_args[key] = value\n\n\tif control_args['debug']:\n\t\tlog.debug(\"control_args: %s\" % control_args)\n\t\tlog.debug\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--'):\n\t\t\tif arg.startswith('--control-'):\n\t\t\t\tcontrol_args[arg[10:]] = args[arg]\n\t\t\telif arg.startswith('--nested-'):\n\t\t\t\tnested_args[arg[9:]] = args[arg]\n\t\t\telif arg.startswith('--custom-'):\n\t\t\t\tcustom_args[arg[9:]] = args[arg]\n\t\t\telif arg.startswith('--'):\n\t\t\t\tcontrol_args[arg[2:]] = args[arg]\n\t\t\telse:\n\t\t\t\tnested_args[arg[1:]] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg_name in args:\n\t\targ_val = args[arg_name]\n\t\tif arg_name in self.control_args:\n\t\t\tcontrol_args[arg_name] = arg_val\n\t\telse:\n\t\t\tnested_args[arg_name] = arg_val\n\t\t\tif arg_name in self.custom_args:\n\t\t\t\tcustom_args[arg_name] = arg_val\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "ques_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tif '=' in extra_var:\n\t\t\t\t(key, value) = extra_var.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise AnsibleError(\n\t\t\t\t\t\"Invalid variable '%s' in extra-vars: missing =\" % extra_var\n\t\t\t\t)\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise AnsibleParserError(\"Invalid extra-var: %s. Extra-vars should be key=value\" % extra_var)\n\t\t\telse:\n\t\t\t\tvars_dict[key] = value\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor pair in extra_vars:\n\t\t\tif \"=\" in pair:\n\t\t\t\t(key, value) = pair.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise AnsibleError(\"Invalid extra-var argument: %s\" % pair)\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tif '=' in extra_var:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\t\tif key in vars_dict:\n\t\t\t\t\traise AnsibleError(\n\t\t\t\t\t\t\"variable name '%s' already exists, \"\n\t\t\t\t\t\t\"cannot add another\" % key)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise AnsibleError(\n\t\t\t\t\t\"failure to parse extra-var string: '%s'\" % extra_var)\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars is None:\n\t\treturn\n\n\tfor extra_var in extra_vars:\n\t\tif not extra_var.startswith('@'):\n\t\t\tif extra_var.startswith('{'):\n\t\t\t\textra_var = json.loads(extra_var)\n\t\t\telse:\n\t\t\t\textra_var = parse_extra_vars(extra_var)\n\t\tvars_dict.update(extra_var)\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor extra_var in extra_vars:\n\t\textra_var_key, extra_var_value = extra_var.split(\"=\", 1)\n\t\textra_var_key = extra_var_key.strip()\n\t\textra_var_value = extra_var_value.strip()\n\n\t\tif extra_var_key in vars_dict:\n\t\t\traise ValueError(\"extra-var '{}' is defined multiple times\".format(extra_var_key))\n\n\t\tvars_dict[extra_var_key] = extra_var_value\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor item in extra_vars:\n\t\t\tif '=' in item:\n\t\t\t\t(key, value) = item.split('=', 1)\n\t\t\t\tif isinstance(value, types.StringTypes):\n\t\t\t\t\tvalue = value.decode('utf-8')\n\t\t\t\tvars_dict[key] = value\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif not extra_vars:\n\t\treturn\n\tfor extra_var in extra_vars:\n\t\t# NOTE: extra_vars are of the format 'key=value'.\n\t\t#       We need to split on the first '=' to handle values\n\t\t#       that contain an equal sign.\n\t\tkey, value = extra_var.split(\"=\", 1)\n\t\tkey = key.strip()\n\t\tvalue = value.strip()\n\t\tif key in vars_dict:\n\t\t\tlog.warning(\"Extra-var '{0}' overrides the value of '{0}' defined in the playbook\".format(key))\n\t\tvars_dict[key] = value\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\tvars_dict[key] = value\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tvars_dict = vars_dict or {}\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\textra_var = extra_var.split('=')\n\t\t\tif len(extra_var) == 1:\n\t\t\t\textra_var.append(True)\n\t\t\tvars_dict[extra_var[0]] = extra_var[1]\n\n\treturn vars_dict\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tif '=' in extra_var:\n\t\t\t\t(key, value) = extra_var.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise AnsibleError(\n\t\t\t\t\t\"Invalid variable '%s' in extra-vars: missing =\" % extra_var\n\t\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise AnsibleParserError(\"Invalid extra-var: %s. Extra-vars should be key=value\" % extra_var)\n\t\t\telse:\n\t\t\t\tvars_dict[key] = value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor pair in extra_vars:\n\t\t\tif \"=\" in pair:\n\t\t\t\t(key, value) = pair.split(\"=\", 1)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise AnsibleError(\"Invalid extra-var argument: %s\" % pair)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tif '=' in extra_var:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\t\tif key in vars_dict:\n\t\t\t\t\traise AnsibleError(\n\t\t\t\t\t\t\"variable name '%s' already exists, \"\n\t\t\t\t\t\t\"cannot add another\" % key)\n\t\t\t\tvars_dict[key] = value\n\t\t\telse:\n\t\t\t\traise AnsibleError(\n\t\t\t\t\t\"failure to parse extra-var string: '%s'\" % extra_var)\n\treturn vars_dict\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars is None:\n\t\treturn\n\n\tfor extra_var in extra_vars:\n\t\tif not extra_var.startswith('@'):\n\t\t\tif extra_var.startswith('{'):\n\t\t\t\textra_var = json.loads(extra_var)\n\t\t\telse:\n\t\t\t\textra_var = parse_extra_vars(extra_var)\n\t\tvars_dict.update(extra_var)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars is None:\n\t\treturn vars_dict\n\n\tfor extra_var in extra_vars:\n\t\textra_var_key, extra_var_value = extra_var.split(\"=\", 1)\n\t\textra_var_key = extra_var_key.strip()\n\t\textra_var_value = extra_var_value.strip()\n\n\t\tif extra_var_key in vars_dict:\n\t\t\traise ValueError(\"extra-var '{}' is defined multiple times\".format(extra_var_key))\n\n\t\tvars_dict[extra_var_key] = extra_var_value\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor item in extra_vars:\n\t\t\tif '=' in item:\n\t\t\t\t(key, value) = item.split('=', 1)\n\t\t\t\tif isinstance(value, types.StringTypes):\n\t\t\t\t\tvalue = value.decode('utf-8')\n\t\t\t\tvars_dict[key] = value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif not extra_vars:\n\t\treturn\n\tfor extra_var in extra_vars:\n\t\t# NOTE: extra_vars are of the format 'key=value'.\n\t\t#       We need to split on the first '=' to handle values\n\t\t#       that contain an equal sign.\n\t\tkey, value = extra_var.split(\"=\", 1)\n\t\tkey = key.strip()\n\t\tvalue = value.strip()\n\t\tif key in vars_dict:\n\t\t\tlog.warning(\"Extra-var '{0}' overrides the value of '{0}' defined in the playbook\".format(key))\n\t\tvars_dict[key] = value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\tvars_dict[key] = value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tvars_dict = vars_dict or {}\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\textra_var = extra_var.split('=')\n\t\t\tif len(extra_var) == 1:\n\t\t\t\textra_var.append(True)\n\t\t\tvars_dict[extra_var[0]] = extra_var[1]\n\n\treturn vars_dict\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "ques_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t# NOTE(mrostecki): We use the fact that the default value for extra_vars\n\t# is None, so we can pass it as a parameter to ansible_playbook_cmd\n\t# down below.\n\tif extra_vars is None:\n\t\textra_vars = {}\n\n\t# NOTE(mrostecki): We use the fact that the default value for ansible_args\n\t# is None, so we can pass it as a parameter to ansible_playbook_cmd\n\t# down below.\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# NOTE(mrostecki): Passing ansible_args directly to ansible_playbook_cmd\n\t# will override the default arguments we set there. This is intentional,\n\t# as we want to give the user more control over what arguments will be\n\t# passed to the Ansible CLI.\n\tansible_playbook_cmd = ansible_cmd_builder(\n\t\tir_workspace, ir_plugin,\n\t\tverbose=verbose,\n\t\textra_vars=extra_vars,\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tplaybook_path = os.path.join(ir_plugin.plugin_path, playbook_path)\n\tif not os.path.exists(playbook_path):\n\t\traise ValueError(\"File not found: {0}\".format(playbook_path))\n\n\tif not verbose:\n\t\tverbose = ir_workspace.verbosity\n\n\tif not extra_vars:\n\t\textra_vars = {}\n\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\t# TODO: Add support for ansible-vault\n\t# The following is a list of ansible-playbook arguments that are not\n\t# supported by Infrared. They are not supported for multiple reasons,\n\t# such as:\n\t# - Infrared already has a better way to pass this argument\n\t# - Infrared doesn't support it\n\t# - Infrared doesn't support it yet\n\tunsupported_args = [\n\t\t# Inventory\n\t\t\"inventory\", \"inventory-file\", \"list-hosts\", \"subset\",\n\t\t# Modules\n\t\t\"module-path\",\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\textra_vars = extra_vars or {}\n\tansible_args = ansible_args or {}\n\n\tenv_vars = ir_workspace.get_env_vars()\n\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['inventory'] = ir_workspace.get_inventory()\n\tansible_args['verbosity'] = verbose\n\tansible_args['env'] = env_vars\n\tansible_args['playbook'] = playbook_path\n\tansible_args['become'] = True\n\n\tir_plugin.run_ansible_playbook(**ansible_args)\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not playbook_path:\n\t\traise AnsiblePlaybookError(\"Missing argument playbook_path\")\n\n\tif not isfile(playbook_path):\n\t\traise AnsiblePlaybookError(\n\t\t\t\"playbook_path {} does not exist\".format(playbook_path))\n\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tansible_cmd_args = {\n\t\t\"verbose\": verbose,\n\t\t\"extra-vars\": extra_vars,\n\t\t\"inventory\": ir_workspace.inventory_path,\n\t\t\"config\": ir_workspace.ansible_cfg_path,\n\t\t\"limit\": ir_workspace.ansible_limit,\n\t\t\"tags\": ir_workspace.ansible_tags,\n\t\t\"skip-tags\": ir_workspace.ansible_skip_tags\n\t}\n\tansible_cmd_args.update(ansible_args)\n\n\tansible_cmd_args = dict((k, v) for\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tcmd = [\"ansible-playbook\"]\n\tif verbose:\n\t\tcmd.append(\"-{}\".format(verbose))\n\tif ansible_args:\n\t\tfor k, v in ansible_args.items():\n\t\t\tcmd.append(\"-{}\".format(k))\n\t\t\tcmd.append(v)\n\tcmd.append(playbook_path)\n\tif extra_vars:\n\t\tfor k, v in extra_vars.items():\n\t\t\tcmd.append(\"-e\")\n\t\t\tcmd.append(\"{}={}\".format(k, v))\n\tcmd.append(\"-i\")\n\tcmd.append(ir_plugin.inventory_path)\n\tcmd.append(\"-e\")\n\tcmd.append(\"host_key_checking=False\")\n\tcmd.append(\"-e\")\n\tcmd.append(\"ansible_ssh_common_args='-o StrictHostKeyChecking=no'\")\n\tcmd.append(\"-e\")\n\tcmd.append(\"ansible_ssh_private_key_file={}\".format(\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not isinstance(ir_workspace, ir_ws.IRWorkspace):\n\t\traise TypeError(\"Workspace should be an IRWorkspace object\")\n\n\tif not isinstance(ir_plugin, ir_pl.IRPlugin):\n\t\traise TypeError(\"Plugin should be an IRPlugin object\")\n\n\tif not isinstance(playbook_path, str):\n\t\traise TypeError(\"playbook_path should be a string\")\n\n\tif verbose is not None and not isinstance(verbose, int):\n\t\traise TypeError(\"verbose should be an int\")\n\n\tif extra_vars is not None and not isinstance(extra_vars, dict):\n\t\traise TypeError(\"extra_vars should be a dict\")\n\n\tif ansible_args is not None and not isinstance(ansible_args, dict):\n\t\traise TypeError(\"ansible_args should be a dict\")\n\n\tif not os.path.exists(playbook_path):\n\t\tir_logging.get_logger().error(\"Playbook file %s doesn't exist\" % playbook_path)\n\t\treturn False\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tansible_cmd = \"ansible-playbook\"\n\n\tif verbose is not None:\n\t\tansible_cmd += \" -{}\".format(verbose)\n\n\tif extra_vars is not None:\n\t\tansible_cmd += \" -e {}\".format(json.dumps(extra_vars))\n\n\tif ansible_args is not None:\n\t\tfor k, v in ansible_args.items():\n\t\t\tif isinstance(v, bool):\n\t\t\t\tif v:\n\t\t\t\t\tansible_cmd += \" --{}\".format(k)\n\t\t\telse:\n\t\t\t\tansible_cmd += \" --{}={}\".format(k, v)\n\n\tansible_cmd += \" {}\".format(playbook_path)\n\treturn ir_workspace.run_command(ansible_cmd,\n\t                                ir_plugin.get_playbook_dir())\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not verbose:\n\t\tverbose = _get_verbosity_level(ir_workspace, ir_plugin)\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\t# --extra-vars should be a json string\n\textra_vars = json.dumps(extra_vars)\n\n\tans_args = _ansible_args(verbose, extra_vars, ansible_args)\n\n\tcli = \"{} {} {}\".format(ansible_playbook_bin(), ans_args, playbook_path)\n\t_run_cli(cli, ir_workspace, ir_plugin)\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\targs = []\n\tif verbose is not None:\n\t\targs.append(\"-{}\".format(\"v\" * verbose))\n\tif extra_vars is not None:\n\t\tfor k, v in extra_vars.items():\n\t\t\targs.append(\"-e {}={}\".format(k, v))\n\tif ansible_args is not None:\n\t\tfor k, v in ansible_args.items():\n\t\t\targs.append(\"-{} {}\".format(k, v))\n\targs.append(playbook_path)\n\tir_plugin.run_ansible_playbook(args)\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not isinstance(ir_workspace, ir_workspace.IRWorkspace):\n\t\traise TypeError(\"Argument `ir_workspace` must be of type \"\n\t\t                \"IRWorkspace. Got: '{}'\".format(type(ir_workspace)))\n\tif not isinstance(ir_plugin, ir_plugin.IRPlugin):\n\t\traise TypeError(\"Argument `ir_plugin` must be of type \"\n\t\t                \"IRPlugin. Got: '{}'\".format(type(ir_plugin)))\n\tif not isinstance(playbook_path, str):\n\t\traise TypeError(\"Argument `playbook_path` must be of \"\n\t\t                \"type str. Got: '{}'\".format(type(playbook_path)))\n\tif not isinstance(verbose, int) and verbose is not None:\n\t\traise TypeError(\"Argument `verbose` must be of \"\n\t\t                \"type int. Got: '{}'\".format(type(verbose)))\n\tif not isinstance(extra_vars, dict) and extra_\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\t# NOTE(mrostecki): We use the fact that the default value for extra_vars\n\t# is None, so we can pass it as a parameter to ansible_playbook_cmd\n\t# down below.\n\tif extra_vars is None:\n\t\textra_vars = {}\n\n\t# NOTE(mrostecki): We use the fact that the default value for ansible_args\n\t# is None, so we can pass it as a parameter to ansible_playbook_cmd\n\t# down below.\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\t# NOTE(mrostecki): Passing ansible_args directly to ansible_playbook_cmd\n\t# will override the default arguments we set there. This is intentional,\n\t# as we want to give the user more control over what arguments will be\n\t# passed to the Ansible CLI.\n\tansible_playbook_cmd = ansible_cmd_builder(\n\t\tir_workspace, ir_plugin,\n\t\tverbose=verbose,\n\t\textra_vars=extra_vars,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tplaybook_path = os.path.join(ir_plugin.plugin_path, playbook_path)\n\tif not os.path.exists(playbook_path):\n\t\traise ValueError(\"File not found: {0}\".format(playbook_path))\n\n\tif not verbose:\n\t\tverbose = ir_workspace.verbosity\n\n\tif not extra_vars:\n\t\textra_vars = {}\n\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\t# TODO: Add support for ansible-vault\n\t# The following is a list of ansible-playbook arguments that are not\n\t# supported by Infrared. They are not supported for multiple reasons,\n\t# such as:\n\t# - Infrared already has a better way to pass this argument\n\t# - Infrared doesn't support it\n\t# - Infrared doesn't support it yet\n\tunsupported_args = [\n\t\t# Inventory\n\t\t\"inventory\", \"inventory-file\", \"list-hosts\", \"subset\",\n\t\t# Modules\n\t\t\"module-path\",\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\textra_vars = extra_vars or {}\n\tansible_args = ansible_args or {}\n\n\tenv_vars = ir_workspace.get_env_vars()\n\n\tansible_args['extra_vars'] = extra_vars\n\tansible_args['inventory'] = ir_workspace.get_inventory()\n\tansible_args['verbosity'] = verbose\n\tansible_args['env'] = env_vars\n\tansible_args['playbook'] = playbook_path\n\tansible_args['become'] = True\n\n\tir_plugin.run_ansible_playbook(**ansible_args)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not playbook_path:\n\t\traise AnsiblePlaybookError(\"Missing argument playbook_path\")\n\n\tif not isfile(playbook_path):\n\t\traise AnsiblePlaybookError(\n\t\t\t\"playbook_path {} does not exist\".format(playbook_path))\n\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tansible_cmd_args = {\n\t\t\"verbose\": verbose,\n\t\t\"extra-vars\": extra_vars,\n\t\t\"inventory\": ir_workspace.inventory_path,\n\t\t\"config\": ir_workspace.ansible_cfg_path,\n\t\t\"limit\": ir_workspace.ansible_limit,\n\t\t\"tags\": ir_workspace.ansible_tags,\n\t\t\"skip-tags\": ir_workspace.ansible_skip_tags\n\t}\n\tansible_cmd_args.update(ansible_args)\n\n\tansible_cmd_args = dict((k, v) for\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tcmd = [\"ansible-playbook\"]\n\tif verbose:\n\t\tcmd.append(\"-{}\".format(verbose))\n\tif ansible_args:\n\t\tfor k, v in ansible_args.items():\n\t\t\tcmd.append(\"-{}\".format(k))\n\t\t\tcmd.append(v)\n\tcmd.append(playbook_path)\n\tif extra_vars:\n\t\tfor k, v in extra_vars.items():\n\t\t\tcmd.append(\"-e\")\n\t\t\tcmd.append(\"{}={}\".format(k, v))\n\tcmd.append(\"-i\")\n\tcmd.append(ir_plugin.inventory_path)\n\tcmd.append(\"-e\")\n\tcmd.append(\"host_key_checking=False\")\n\tcmd.append(\"-e\")\n\tcmd.append(\"ansible_ssh_common_args='-o StrictHostKeyChecking=no'\")\n\tcmd.append(\"-e\")\n\tcmd.append(\"ansible_ssh_private_key_file={}\".format(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not isinstance(ir_workspace, ir_ws.IRWorkspace):\n\t\traise TypeError(\"Workspace should be an IRWorkspace object\")\n\n\tif not isinstance(ir_plugin, ir_pl.IRPlugin):\n\t\traise TypeError(\"Plugin should be an IRPlugin object\")\n\n\tif not isinstance(playbook_path, str):\n\t\traise TypeError(\"playbook_path should be a string\")\n\n\tif verbose is not None and not isinstance(verbose, int):\n\t\traise TypeError(\"verbose should be an int\")\n\n\tif extra_vars is not None and not isinstance(extra_vars, dict):\n\t\traise TypeError(\"extra_vars should be a dict\")\n\n\tif ansible_args is not None and not isinstance(ansible_args, dict):\n\t\traise TypeError(\"ansible_args should be a dict\")\n\n\tif not os.path.exists(playbook_path):\n\t\tir_logging.get_logger().error(\"Playbook file %s doesn't exist\" % playbook_path)\n\t\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tansible_cmd = \"ansible-playbook\"\n\n\tif verbose is not None:\n\t\tansible_cmd += \" -{}\".format(verbose)\n\n\tif extra_vars is not None:\n\t\tansible_cmd += \" -e {}\".format(json.dumps(extra_vars))\n\n\tif ansible_args is not None:\n\t\tfor k, v in ansible_args.items():\n\t\t\tif isinstance(v, bool):\n\t\t\t\tif v:\n\t\t\t\t\tansible_cmd += \" --{}\".format(k)\n\t\t\telse:\n\t\t\t\tansible_cmd += \" --{}={}\".format(k, v)\n\n\tansible_cmd += \" {}\".format(playbook_path)\n\treturn ir_workspace.run_command(ansible_cmd,\n\t                                ir_plugin.get_playbook_dir())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not verbose:\n\t\tverbose = _get_verbosity_level(ir_workspace, ir_plugin)\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\t# --extra-vars should be a json string\n\textra_vars = json.dumps(extra_vars)\n\n\tans_args = _ansible_args(verbose, extra_vars, ansible_args)\n\n\tcli = \"{} {} {}\".format(ansible_playbook_bin(), ans_args, playbook_path)\n\t_run_cli(cli, ir_workspace, ir_plugin)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\targs = []\n\tif verbose is not None:\n\t\targs.append(\"-{}\".format(\"v\" * verbose))\n\tif extra_vars is not None:\n\t\tfor k, v in extra_vars.items():\n\t\t\targs.append(\"-e {}={}\".format(k, v))\n\tif ansible_args is not None:\n\t\tfor k, v in ansible_args.items():\n\t\t\targs.append(\"-{} {}\".format(k, v))\n\targs.append(playbook_path)\n\tir_plugin.run_ansible_playbook(args)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not isinstance(ir_workspace, ir_workspace.IRWorkspace):\n\t\traise TypeError(\"Argument `ir_workspace` must be of type \"\n\t\t                \"IRWorkspace. Got: '{}'\".format(type(ir_workspace)))\n\tif not isinstance(ir_plugin, ir_plugin.IRPlugin):\n\t\traise TypeError(\"Argument `ir_plugin` must be of type \"\n\t\t                \"IRPlugin. Got: '{}'\".format(type(ir_plugin)))\n\tif not isinstance(playbook_path, str):\n\t\traise TypeError(\"Argument `playbook_path` must be of \"\n\t\t                \"type str. Got: '{}'\".format(type(playbook_path)))\n\tif not isinstance(verbose, int) and verbose is not None:\n\t\traise TypeError(\"Argument `verbose` must be of \"\n\t\t                \"type int. Got: '{}'\".format(type(verbose)))\n\tif not isinstance(extra_vars, dict) and extra_\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "ques_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tansible_env = dict(os.environ)\n\tansible_env.update(ir_plugin.env_vars)\n\tansible_env[\"ANSIBLE_STDOUT_CALLBACK\"] = \"junit\"\n\n\tif ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_private_key_file\",\n\t\t\tdefault=None) is not None:\n\t\tansible_env[\"ANSIBLE_PRIVATE_KEY_FILE\"] = ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_private_key_file\")\n\n\tif ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_common_args\",\n\t\t\tdefault=None) is not None:\n\t\tansible_env[\"ANSIBLE_SSH_COMMON_ARGS\"] = ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_common_args\")\n\n\tif ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_p\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcli_args.append(\"--extra-vars\")\n\tcli_args.append(json.dumps(vars_dict))\n\tir_workspace.print_debug(\"Running ansible-playbook with args: {}\".format(\n\t\tcli_args))\n\treturn ansible_runner.run(ir_plugin.get_plugin_path(),\n\t\t\t\t\t\t\t  cli_args)\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcli_args['extra_vars'] = vars_dict\n\tir_workspace.run_playbook(cli_args, ir_plugin)\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\ttry:\n\t\timport ansible\n\t\tANSIBLE_VERSION = ansible.__version__\n\texcept ImportError:\n\t\tANSIBLE_VERSION = None\n\n\tif ANSIBLE_VERSION and LooseVersion(ANSIBLE_VERSION) >= LooseVersion('2.4'):\n\t\timport ansible.constants as C\n\n\t\t# disable colors in ansible-playbook output\n\t\tC.ANSIBLE_NOCOLOR = True\n\n\t\t# disable host key checking\n\t\tC.HOST_KEY_CHECKING = False\n\n\t\t# don't record the command being run into the job events\n\t\tC.ANSIBLE_DISPLAY_SKIPPED_HOSTS = False\n\n\t\t# change the default callback to json\n\t\tC.DEFAULT_CALLBACK_PLUGIN = 'json'\n\n\t\t# don't check the filesystem for config file\n\t\tC.ANSIBLE_CONFIG = None\n\n\t\t# enable the lookup plugin cache\n\t\tC.DEFAULT_LOAD_CALLBACK_PLUGIN = 'cache'\n\n\t\t# specify the path to the cache plugin\n\t\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcmd = cli_args\n\tcmd.extend([\n\t\t\"--extra-vars\",\n\t\t\"@{}\".format(ir_plugin.get_vars_file_path(ir_workspace))\n\t])\n\t# TODO: refactor this function\n\tLOG.debug(\"Running command: {}\".format(cmd))\n\tproc = subprocess.Popen(cmd,\n\t\t\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\t\t\t\tuniversal_newlines=True)\n\tstdout, stderr = proc.communicate()\n\tif proc.returncode != 0:\n\t\tLOG.error(\"Error running Ansible command: {}\".format(cmd))\n\t\tLOG.error(stderr)\n\t\tLOG.error(stdout)\n\t\treturn proc.returncode\n\n\t# TODO: move this to a separate function in utils\n\tdef _format_ansible_log_line(line):\n\t\t\"\"\"\n\t\treturns a formatted\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tif vars_dict:\n\t\tcli_args.extend(['--extra-vars',\n\t\t                 json.dumps(vars_dict,\n\t\t                            sort_keys=True,\n\t\t                            indent=4,\n\t\t                            separators=(',', ': '))])\n\n\treturn run_ansible_playbook(cli_args)\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tlog.debug(\"Running playbook: {}\".format(cli_args))\n\n\tcli_args = [str(i) for i in cli_args]\n\tvars_dict = _build_vars(vars_dict, ir_workspace, ir_plugin)\n\tlog.debug(\"Vars dict: {}\".format(vars_dict))\n\n\t# TODO(aschultz): replace this with a call to the cli.py\n\t# TODO(aschultz): add a kwargs to the run_playbook function to pass\n\t#  the vars dict to the ansible-playbook\n\tlog.debug(\"Running the following command: {}\".format(cli_args))\n\tp = subprocess.Popen(cli_args, env=os.environ.copy(),\n\t                     stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tstdout, stderr = p.communicate()\n\tlog.debug(\"Playbook stdout: {}\".format(stdout))\n\treturn stdout, stderr\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tvars_dict = dict_to_ansible_extra_vars(vars_dict)\n\n\tcli_args.append('-e')\n\tcli_args.append(vars_dict)\n\n\tcommand = build_ansible_cli_command(cli_args)\n\n\tir_plugin.logger.debug(\"Executing command: {}\".format(command))\n\n\treturn run_command(command, cwd=ir_workspace.get_workspace_path())\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcmd = cli_args.copy()\n\tcmd.extend(['-e', json.dumps(vars_dict)])\n\tcmd.extend(['-e', 'infrared_workspace={}'.format(ir_workspace.path)])\n\tcmd.extend(['-e', 'infrared_plugin_entry_file={}'.format(\n\t\tir_plugin.entry_file)])\n\n\tcmd.extend(['-e', 'infrared_verbosity={}'.format(cli_args.verbosity)])\n\tcmd.extend(['-e', 'infrared_debug={}'.format(cli_args.debug)])\n\tcmd.extend(['-e', 'infrared_skip_sanity={}'.format(\n\t\tcli_args.skip_sanity)])\n\tcmd.extend(['-e', 'infrared_skip_cleanup={}'.format(\n\t\tcli_args.skip_cleanup)])\n\tcmd.extend(['-e', 'infrared_skip_cleanup={}'.format(\n\t\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tvars_file_path = _generate_vars_file(vars_dict)\n\tcli_args.extend(['--extra-vars', \"@\" + vars_file_path])\n\tcli_args.append(ir_workspace.ansible_inventory)\n\tcli_args.extend(ir_plugin.playbooks)\n\tLOG.debug(\"Running Ansible CLI: %s\", cli_args)\n\treturn ansible_runner.run(private_data_dir=ir_workspace.workspace_path,\n\t                          verbosity=cli_args.verbosity,\n\t                          playbook=cli_args.playbooks[0],\n\t                          inventory=ir_workspace.ansible_inventory,\n\t                          extravars=vars_dict,\n\t                          subset=cli_args.subset,\n\t                          tags=cli_args.tags,\n\t                          skip_tags=cli_args.skip_tags,\n\t                          no_log=cli_args.no_log,\n\t                          check=cli_args.check)\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tansible_env = dict(os.environ)\n\tansible_env.update(ir_plugin.env_vars)\n\tansible_env[\"ANSIBLE_STDOUT_CALLBACK\"] = \"junit\"\n\n\tif ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_private_key_file\",\n\t\t\tdefault=None) is not None:\n\t\tansible_env[\"ANSIBLE_PRIVATE_KEY_FILE\"] = ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_private_key_file\")\n\n\tif ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_common_args\",\n\t\t\tdefault=None) is not None:\n\t\tansible_env[\"ANSIBLE_SSH_COMMON_ARGS\"] = ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_common_args\")\n\n\tif ir_workspace.get_workspace_var(\n\t\t\t\"ansible_ssh_p\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcli_args.append(\"--extra-vars\")\n\tcli_args.append(json.dumps(vars_dict))\n\tir_workspace.print_debug(\"Running ansible-playbook with args: {}\".format(\n\t\tcli_args))\n\treturn ansible_runner.run(ir_plugin.get_plugin_path(),\n\t\t\t\t\t\t\t  cli_args)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcli_args['extra_vars'] = vars_dict\n\tir_workspace.run_playbook(cli_args, ir_plugin)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\ttry:\n\t\timport ansible\n\t\tANSIBLE_VERSION = ansible.__version__\n\texcept ImportError:\n\t\tANSIBLE_VERSION = None\n\n\tif ANSIBLE_VERSION and LooseVersion(ANSIBLE_VERSION) >= LooseVersion('2.4'):\n\t\timport ansible.constants as C\n\n\t\t# disable colors in ansible-playbook output\n\t\tC.ANSIBLE_NOCOLOR = True\n\n\t\t# disable host key checking\n\t\tC.HOST_KEY_CHECKING = False\n\n\t\t# don't record the command being run into the job events\n\t\tC.ANSIBLE_DISPLAY_SKIPPED_HOSTS = False\n\n\t\t# change the default callback to json\n\t\tC.DEFAULT_CALLBACK_PLUGIN = 'json'\n\n\t\t# don't check the filesystem for config file\n\t\tC.ANSIBLE_CONFIG = None\n\n\t\t# enable the lookup plugin cache\n\t\tC.DEFAULT_LOAD_CALLBACK_PLUGIN = 'cache'\n\n\t\t# specify the path to the cache plugin\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcmd = cli_args\n\tcmd.extend([\n\t\t\"--extra-vars\",\n\t\t\"@{}\".format(ir_plugin.get_vars_file_path(ir_workspace))\n\t])\n\t# TODO: refactor this function\n\tLOG.debug(\"Running command: {}\".format(cmd))\n\tproc = subprocess.Popen(cmd,\n\t\t\t\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\t\t\t\tstderr=subprocess.PIPE,\n\t\t\t\t\t\t\tuniversal_newlines=True)\n\tstdout, stderr = proc.communicate()\n\tif proc.returncode != 0:\n\t\tLOG.error(\"Error running Ansible command: {}\".format(cmd))\n\t\tLOG.error(stderr)\n\t\tLOG.error(stdout)\n\t\treturn proc.returncode\n\n\t# TODO: move this to a separate function in utils\n\tdef _format_ansible_log_line(line):\n\t\t\"\"\"\n\t\treturns a formatted\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tif vars_dict:\n\t\tcli_args.extend(['--extra-vars',\n\t\t                 json.dumps(vars_dict,\n\t\t                            sort_keys=True,\n\t\t                            indent=4,\n\t\t                            separators=(',', ': '))])\n\n\treturn run_ansible_playbook(cli_args)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tlog.debug(\"Running playbook: {}\".format(cli_args))\n\n\tcli_args = [str(i) for i in cli_args]\n\tvars_dict = _build_vars(vars_dict, ir_workspace, ir_plugin)\n\tlog.debug(\"Vars dict: {}\".format(vars_dict))\n\n\t# TODO(aschultz): replace this with a call to the cli.py\n\t# TODO(aschultz): add a kwargs to the run_playbook function to pass\n\t#  the vars dict to the ansible-playbook\n\tlog.debug(\"Running the following command: {}\".format(cli_args))\n\tp = subprocess.Popen(cli_args, env=os.environ.copy(),\n\t                     stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\tstdout, stderr = p.communicate()\n\tlog.debug(\"Playbook stdout: {}\".format(stdout))\n\treturn stdout, stderr\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tvars_dict = dict_to_ansible_extra_vars(vars_dict)\n\n\tcli_args.append('-e')\n\tcli_args.append(vars_dict)\n\n\tcommand = build_ansible_cli_command(cli_args)\n\n\tir_plugin.logger.debug(\"Executing command: {}\".format(command))\n\n\treturn run_command(command, cwd=ir_workspace.get_workspace_path())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tcmd = cli_args.copy()\n\tcmd.extend(['-e', json.dumps(vars_dict)])\n\tcmd.extend(['-e', 'infrared_workspace={}'.format(ir_workspace.path)])\n\tcmd.extend(['-e', 'infrared_plugin_entry_file={}'.format(\n\t\tir_plugin.entry_file)])\n\n\tcmd.extend(['-e', 'infrared_verbosity={}'.format(cli_args.verbosity)])\n\tcmd.extend(['-e', 'infrared_debug={}'.format(cli_args.debug)])\n\tcmd.extend(['-e', 'infrared_skip_sanity={}'.format(\n\t\tcli_args.skip_sanity)])\n\tcmd.extend(['-e', 'infrared_skip_cleanup={}'.format(\n\t\tcli_args.skip_cleanup)])\n\tcmd.extend(['-e', 'infrared_skip_cleanup={}'.format(\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tvars_file_path = _generate_vars_file(vars_dict)\n\tcli_args.extend(['--extra-vars', \"@\" + vars_file_path])\n\tcli_args.append(ir_workspace.ansible_inventory)\n\tcli_args.extend(ir_plugin.playbooks)\n\tLOG.debug(\"Running Ansible CLI: %s\", cli_args)\n\treturn ansible_runner.run(private_data_dir=ir_workspace.workspace_path,\n\t                          verbosity=cli_args.verbosity,\n\t                          playbook=cli_args.playbooks[0],\n\t                          inventory=ir_workspace.ansible_inventory,\n\t                          extravars=vars_dict,\n\t                          subset=cli_args.subset,\n\t                          tags=cli_args.tags,\n\t                          skip_tags=cli_args.skip_tags,\n\t                          no_log=cli_args.no_log,\n\t                          check=cli_args.check)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "ques_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n\t# Get parser from the parser dictionary\n\tparser = self.parsers_dict[parser_name]\n\n\t# Now cast the values to correct types\n\tfor arg in values_dict:\n\t\tif arg in parser._option_string_actions:\n\t\t\taction = parser._option_string_actions[arg]\n\t\t\tif action.type:\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[arg] = action.type(values_dict[arg])\n\t\t\t\texcept ValueError as exc:\n\t\t\t\t\traise ValueError(\"Invalid value for '%s': %s.\\n%s\" % (arg, values_dict[arg], exc))\n\n\treturn values_dict\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key in values_dict:\n\t\tif key in self.cli_arg_types:\n\t\t\targ_type = self.cli_arg_types[key]\n\t\t\tif arg_type == int:\n\t\t\t\tvalues_dict[key] = int(values_dict[key])\n\t\t\telif arg_type == float:\n\t\t\t\tvalues_dict[key] = float(values_dict[key])\n\t\t\telif arg_type == bool:\n\t\t\t\tif values_dict[key] in ['True', 'true', '1', 't', 'y', 'yes']:\n\t\t\t\t\tvalues_dict[key] = True\n\t\t\t\telse:\n\t\t\t\t\tvalues_dict[key] = False\n\t\t\telif arg_type == list:\n\t\t\t\tvalues_dict[key] = [v.strip() for v in values_dict[key].split(',')]\n\t\t\telse:\n\t\t\t\traise Exception('Unknown argument type: %s' % arg_type)\n\t\telse:\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tif key in self.non_cli_args[parser_name]:\n\t\t\tif type(self.non_cli_args[parser_name][key]) == int:\n\t\t\t\tvalues_dict[key] = int(value)\n\t\t\telif type(self.non_cli_args[parser_name][key]) == float:\n\t\t\t\tvalues_dict[key] = float(value)\n\t\t\telif type(self.non_cli_args[parser_name][key]) == bool:\n\t\t\t\tif value.lower() == 'true':\n\t\t\t\t\tvalues_dict[key] = True\n\t\t\t\telif value.lower() == 'false':\n\t\t\t\t\tvalues_dict[key] = False\n\t\t\t\telse:\n\t\t\t\t\traise Exception(\"Value of option '{0}' must be either 'true' or 'false'\".format(key))\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tif parser_name == 'main':\n\t\tfor key in self.main_parser.get_conversion_dict():\n\t\t\tif key in values_dict:\n\t\t\t\tvalues_dict[key] = self.main_parser.get_conversion_dict()[key](values_dict[key])\n\telif parser_name == 'virsh':\n\t\tfor key in self.virsh_parser.get_conversion_dict():\n\t\t\tif key in values_dict:\n\t\t\t\tvalues_dict[key] = self.virsh_parser.get_conversion_dict()[key](values_dict[key])\n\telif parser_name == 'ospd':\n\t\tfor key in self.ospd_parser.get_conversion_dict():\n\t\t\tif key in values_dict:\n\t\t\t\tvalues_dict[key] = self.ospd_parser.get_conversion_dict()[key](values_dict[key])\n\telif parser_name == 'ovirt':\n\t\tfor key in self.ovirt_parser.get_con\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key in values_dict:\n\t\tif key in self.arg_names:\n\t\t\tvalues_dict[key] = self.convert_arg(key, values_dict[key])\n\n\t# for key in values_dict:\n\t# \tif key in self.arg_names:\n\t# \t\tvalues_dict[key] = self.convert_arg(key, values_dict[key])\n\n\treturn values_dict\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg_name, arg_value in values_dict.items():\n\t\ttry:\n\t\t\tconverter = getattr(self, '_convert_' + parser_name + '_' + arg_name)\n\t\texcept AttributeError:\n\t\t\tcontinue\n\n\t\tvalues_dict[arg_name] = converter(arg_value)\n\n\treturn values_dict\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t# pylint: disable=unused-argument\n\t# pylint: disable=no-self-use\n\tvalues_dict['start'] = int(values_dict['start'])\n\tvalues_dict['end'] = int(values_dict['end'])\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t# Add here any command specific conversion\n\tif parser_name == 'main':\n\t\tif values_dict['exec_mode'] == 'script':\n\t\t\tif values_dict['script_file'] == '-':\n\t\t\t\tvalues_dict['script_file'] = sys.stdin\n\t\t\telse:\n\t\t\t\tvalues_dict['script_file'] = open(values_dict['script_file'], 'r')\n\t\t\tvalues_dict['script'] = True\n\t\t\tvalues_dict['interactive'] = False\n\n\t# Add here any argument specific conversion\n\tif parser_name != 'main':\n\t\tif values_dict['timeout']:\n\t\t\tvalues_dict['timeout'] = int(values_dict['timeout'])\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tif parser_name not in self._parser_types_dict:\n\t\treturn values_dict\n\n\tfor opt_name, opt_type in self._parser_types_dict[parser_name].items():\n\t\tif values_dict.get(opt_name) is not None:\n\t\t\tvalues_dict[opt_name] = opt_type(values_dict[opt_name])\n\n\treturn values_dict\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg, value in values_dict.items():\n\t\tif parser_name == \"main\" and arg in self._main_args_type:\n\t\t\tvalues_dict[arg] = self._main_args_type[arg](value)\n\n\t\telif parser_name == \"virt-who\" and arg in self._virt_who_args_type:\n\t\t\tvalues_dict[arg] = self._virt_who_args_type[arg](value)\n\n\t\telif parser_name == \"ospd\" and arg in self._ospd_args_type:\n\t\t\tvalues_dict[arg] = self._ospd_args_type[arg](value)\n\n\t\telif parser_name == \"virsh\" and arg in self._virsh_args_type:\n\t\t\tvalues_dict[arg] = self._virsh_args_type[arg](value)\n\n\t\telif parser_name == \"virt-who-password\" and arg in self._virt_who_password_args_type:\n\t\t\tvalues_dict[arg] = self._virt_who_password_\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t# Get parser from the parser dictionary\n\tparser = self.parsers_dict[parser_name]\n\n\t# Now cast the values to correct types\n\tfor arg in values_dict:\n\t\tif arg in parser._option_string_actions:\n\t\t\taction = parser._option_string_actions[arg]\n\t\t\tif action.type:\n\t\t\t\ttry:\n\t\t\t\t\tvalues_dict[arg] = action.type(values_dict[arg])\n\t\t\t\texcept ValueError as exc:\n\t\t\t\t\traise ValueError(\"Invalid value for '%s': %s.\\n%s\" % (arg, values_dict[arg], exc))\n\n\treturn values_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key in values_dict:\n\t\tif key in self.cli_arg_types:\n\t\t\targ_type = self.cli_arg_types[key]\n\t\t\tif arg_type == int:\n\t\t\t\tvalues_dict[key] = int(values_dict[key])\n\t\t\telif arg_type == float:\n\t\t\t\tvalues_dict[key] = float(values_dict[key])\n\t\t\telif arg_type == bool:\n\t\t\t\tif values_dict[key] in ['True', 'true', '1', 't', 'y', 'yes']:\n\t\t\t\t\tvalues_dict[key] = True\n\t\t\t\telse:\n\t\t\t\t\tvalues_dict[key] = False\n\t\t\telif arg_type == list:\n\t\t\t\tvalues_dict[key] = [v.strip() for v in values_dict[key].split(',')]\n\t\t\telse:\n\t\t\t\traise Exception('Unknown argument type: %s' % arg_type)\n\t\telse:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tif key in self.non_cli_args[parser_name]:\n\t\t\tif type(self.non_cli_args[parser_name][key]) == int:\n\t\t\t\tvalues_dict[key] = int(value)\n\t\t\telif type(self.non_cli_args[parser_name][key]) == float:\n\t\t\t\tvalues_dict[key] = float(value)\n\t\t\telif type(self.non_cli_args[parser_name][key]) == bool:\n\t\t\t\tif value.lower() == 'true':\n\t\t\t\t\tvalues_dict[key] = True\n\t\t\t\telif value.lower() == 'false':\n\t\t\t\t\tvalues_dict[key] = False\n\t\t\t\telse:\n\t\t\t\t\traise Exception(\"Value of option '{0}' must be either 'true' or 'false'\".format(key))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tif parser_name == 'main':\n\t\tfor key in self.main_parser.get_conversion_dict():\n\t\t\tif key in values_dict:\n\t\t\t\tvalues_dict[key] = self.main_parser.get_conversion_dict()[key](values_dict[key])\n\telif parser_name == 'virsh':\n\t\tfor key in self.virsh_parser.get_conversion_dict():\n\t\t\tif key in values_dict:\n\t\t\t\tvalues_dict[key] = self.virsh_parser.get_conversion_dict()[key](values_dict[key])\n\telif parser_name == 'ospd':\n\t\tfor key in self.ospd_parser.get_conversion_dict():\n\t\t\tif key in values_dict:\n\t\t\t\tvalues_dict[key] = self.ospd_parser.get_conversion_dict()[key](values_dict[key])\n\telif parser_name == 'ovirt':\n\t\tfor key in self.ovirt_parser.get_con\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key in values_dict:\n\t\tif key in self.arg_names:\n\t\t\tvalues_dict[key] = self.convert_arg(key, values_dict[key])\n\n\t# for key in values_dict:\n\t# \tif key in self.arg_names:\n\t# \t\tvalues_dict[key] = self.convert_arg(key, values_dict[key])\n\n\treturn values_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg_name, arg_value in values_dict.items():\n\t\ttry:\n\t\t\tconverter = getattr(self, '_convert_' + parser_name + '_' + arg_name)\n\t\texcept AttributeError:\n\t\t\tcontinue\n\n\t\tvalues_dict[arg_name] = converter(arg_value)\n\n\treturn values_dict\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t# pylint: disable=unused-argument\n\t# pylint: disable=no-self-use\n\tvalues_dict['start'] = int(values_dict['start'])\n\tvalues_dict['end'] = int(values_dict['end'])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t# Add here any command specific conversion\n\tif parser_name == 'main':\n\t\tif values_dict['exec_mode'] == 'script':\n\t\t\tif values_dict['script_file'] == '-':\n\t\t\t\tvalues_dict['script_file'] = sys.stdin\n\t\t\telse:\n\t\t\t\tvalues_dict['script_file'] = open(values_dict['script_file'], 'r')\n\t\t\tvalues_dict['script'] = True\n\t\t\tvalues_dict['interactive'] = False\n\n\t# Add here any argument specific conversion\n\tif parser_name != 'main':\n\t\tif values_dict['timeout']:\n\t\t\tvalues_dict['timeout'] = int(values_dict['timeout'])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tif parser_name not in self._parser_types_dict:\n\t\treturn values_dict\n\n\tfor opt_name, opt_type in self._parser_types_dict[parser_name].items():\n\t\tif values_dict.get(opt_name) is not None:\n\t\t\tvalues_dict[opt_name] = opt_type(values_dict[opt_name])\n\n\treturn values_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg, value in values_dict.items():\n\t\tif parser_name == \"main\" and arg in self._main_args_type:\n\t\t\tvalues_dict[arg] = self._main_args_type[arg](value)\n\n\t\telif parser_name == \"virt-who\" and arg in self._virt_who_args_type:\n\t\t\tvalues_dict[arg] = self._virt_who_args_type[arg](value)\n\n\t\telif parser_name == \"ospd\" and arg in self._ospd_args_type:\n\t\t\tvalues_dict[arg] = self._ospd_args_type[arg](value)\n\n\t\telif parser_name == \"virsh\" and arg in self._virsh_args_type:\n\t\t\tvalues_dict[arg] = self._virsh_args_type[arg](value)\n\n\t\telif parser_name == \"virt-who-password\" and arg in self._virt_who_password_args_type:\n\t\t\tvalues_dict[arg] = self._virt_who_password_\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "ques_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tflatten_dict = {}\n\n\tfor key, value in plugin_spec.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor inner_key, inner_value in value.items():\n\t\t\t\tflatten_dict[inner_key] = inner_value\n\t\telse:\n\t\t\tflatten_dict[key] = value\n\n\treturn flatten_dict\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tres_dict = {}\n\n\tif plugin_spec:\n\t\tres_dict['name'] = plugin_spec['name']\n\t\tres_dict['description'] = plugin_spec['description']\n\t\tres_dict['version'] = plugin_spec['version']\n\t\tres_dict['image'] = plugin_spec['image']\n\t\tres_dict['template_path'] = os.path.join(plugin_dir, 'template')\n\t\tres_dict['plugin_dir'] = plugin_dir\n\t\tres_dict['requirements'] = plugin_spec.get('requirements', [])\n\t\tres_dict['module_name'] = plugin_spec.get('module', None)\n\t\tres_dict['display_name'] = plugin_spec.get('display_name', None)\n\t\tres_dict['ui_info'] = plugin_spec.get('ui_info', None)\n\t\tres_dict['is_builtin'] = plugin_spec.get('is_builtin', False)\n\n\treturn res_dict\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin.spec')\n\tplugin_spec = parse_plugin_spec(plugin_spec_path)\n\tplugin_spec_flatten = {}\n\tfor key, value in plugin_spec.items():\n\t\tif '.' in key:\n\t\t\tplugin_spec_flatten.update(dict_dot_notation_to_nested_dict(key, value))\n\t\telse:\n\t\t\tplugin_spec_flatten[key] = value\n\treturn plugin_spec_flatten\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\treturn get_plugin_spec_flatten_dict_from_file(os.path.join(plugin_dir, 'plugin.spec'))\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tif not plugin_dir:\n\t\traise ValueError(\"plugin_dir cannot be None\")\n\n\tplugin_spec_file = os.path.join(plugin_dir, PLUGIN_SPEC_FILE_NAME)\n\tif not os.path.exists(plugin_spec_file):\n\t\traise ValueError(\"No plugin spec file found in the given dir: {0}\".format(plugin_dir))\n\n\ttry:\n\t\tplugin_spec = json.load(open(plugin_spec_file))\n\texcept ValueError as e:\n\t\traise ValueError(\"The given plugin spec is not valid json: {0}\".format(e.message))\n\n\treturn _flatten_dict(plugin_spec)\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_path = os.path.join(plugin_dir, PLUGIN_SPEC)\n\tplugin_spec = load_yaml(plugin_spec_path)\n\treturn flatten_dict(plugin_spec)\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tspec_dict = get_plugin_spec(plugin_dir)\n\treturn _flatten_dict(spec_dict)\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tdata = get_plugin_spec(plugin_dir)\n\treturn flatten_dict(data)\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tplugin_spec_dict = {}\n\tif 'plugin' in plugin_spec:\n\t\tplugin_spec_dict.update(plugin_spec['plugin'])\n\tif 'content_types' in plugin_spec:\n\t\tfor ct in plugin_spec['content_types']:\n\t\t\tplugin_spec_dict.update(ct)\n\treturn plugin_spec_dict\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tif plugin_spec is None:\n\t\treturn {}\n\treturn PluginSpec(plugin_spec).flatten()\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tflatten_dict = {}\n\n\tfor key, value in plugin_spec.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor inner_key, inner_value in value.items():\n\t\t\t\tflatten_dict[inner_key] = inner_value\n\t\telse:\n\t\t\tflatten_dict[key] = value\n\n\treturn flatten_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tres_dict = {}\n\n\tif plugin_spec:\n\t\tres_dict['name'] = plugin_spec['name']\n\t\tres_dict['description'] = plugin_spec['description']\n\t\tres_dict['version'] = plugin_spec['version']\n\t\tres_dict['image'] = plugin_spec['image']\n\t\tres_dict['template_path'] = os.path.join(plugin_dir, 'template')\n\t\tres_dict['plugin_dir'] = plugin_dir\n\t\tres_dict['requirements'] = plugin_spec.get('requirements', [])\n\t\tres_dict['module_name'] = plugin_spec.get('module', None)\n\t\tres_dict['display_name'] = plugin_spec.get('display_name', None)\n\t\tres_dict['ui_info'] = plugin_spec.get('ui_info', None)\n\t\tres_dict['is_builtin'] = plugin_spec.get('is_builtin', False)\n\n\treturn res_dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_path = os.path.join(plugin_dir, 'plugin.spec')\n\tplugin_spec = parse_plugin_spec(plugin_spec_path)\n\tplugin_spec_flatten = {}\n\tfor key, value in plugin_spec.items():\n\t\tif '.' in key:\n\t\t\tplugin_spec_flatten.update(dict_dot_notation_to_nested_dict(key, value))\n\t\telse:\n\t\t\tplugin_spec_flatten[key] = value\n\treturn plugin_spec_flatten\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\treturn get_plugin_spec_flatten_dict_from_file(os.path.join(plugin_dir, 'plugin.spec'))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tif not plugin_dir:\n\t\traise ValueError(\"plugin_dir cannot be None\")\n\n\tplugin_spec_file = os.path.join(plugin_dir, PLUGIN_SPEC_FILE_NAME)\n\tif not os.path.exists(plugin_spec_file):\n\t\traise ValueError(\"No plugin spec file found in the given dir: {0}\".format(plugin_dir))\n\n\ttry:\n\t\tplugin_spec = json.load(open(plugin_spec_file))\n\texcept ValueError as e:\n\t\traise ValueError(\"The given plugin spec is not valid json: {0}\".format(e.message))\n\n\treturn _flatten_dict(plugin_spec)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_path = os.path.join(plugin_dir, PLUGIN_SPEC)\n\tplugin_spec = load_yaml(plugin_spec_path)\n\treturn flatten_dict(plugin_spec)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tspec_dict = get_plugin_spec(plugin_dir)\n\treturn _flatten_dict(spec_dict)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tdata = get_plugin_spec(plugin_dir)\n\treturn flatten_dict(data)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tplugin_spec_dict = {}\n\tif 'plugin' in plugin_spec:\n\t\tplugin_spec_dict.update(plugin_spec['plugin'])\n\tif 'content_types' in plugin_spec:\n\t\tfor ct in plugin_spec['content_types']:\n\t\t\tplugin_spec_dict.update(ct)\n\treturn plugin_spec_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\tif plugin_spec is None:\n\t\treturn {}\n\treturn PluginSpec(plugin_spec).flatten()\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "ques_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = self.config_path\n\t\tself.logger.debug(\"CONFIG_PATH set to %s\" % self.config_path)\n\treturn\n\n", "def inject_config(self):\n\tif not os.environ.has_key(Config.CONFIG_ENV_VAR):\n\t\tos.environ[Config.CONFIG_ENV_VAR] = self.config_path\n\t\tprint \"Config path set to \" + self.config_path\n\t\treturn True\n\telse:\n\t\tprint \"Config path is already set to \" + os.environ[Config.CONFIG_ENV_VAR]\n\t\treturn False\n\n", "def inject_config(self):\n\tif not 'CONFIG_PATH' in os.environ:\n\t\tos.environ['CONFIG_PATH'] = self.config_path\n\n", "def inject_config(self):\n\tif not os.getenv('CONFIG_PATH'):\n\t\tos.environ['CONFIG_PATH'] = os.path.abspath(os.path.join(\n\t\t\tos.path.dirname(__file__), '..', '..', '..',\n\t\t\t'tests', 'config'))\n\t\t\n\t\tprint 'CONFIG_PATH', os.getenv('CONFIG_PATH')\n\t\t\n\t\tsys.path.append(os.getenv('CONFIG_PATH'))\n\n\n", "def inject_config(self):\n\tif 'CONFIG' not in os.environ:\n\t\tos.environ['CONFIG'] = os.path.join(self.config_path, 'config.json')\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "def inject_config(self):\n\tif not self.env[\"CONFIG_PATH\"]:\n\t\tself.env[\"CONFIG_PATH\"] = os.path.join(self.env[\"PROJECT_ROOT\"], \"config\")\n\n\tself.env[\"CONFIG_FILE\"] = os.path.join(self.env[\"CONFIG_PATH\"],\n\t\t\t\t\t\t\t\t\t\t   self.env[\"CONFIG_FILE\"])\n\tself.env[\"ENV_FILE\"] = os.path.join(self.env[\"CONFIG_PATH\"],\n\t\t\t\t\t\t\t\t\t\tself.env[\"ENV_FILE\"])\n\n", "def inject_config(self):\n\tif not os.environ.has_key(self.env_var_name):\n\t\tos.environ[self.env_var_name] = self.config_path\n\n", "def inject_config(self):\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = os.path.join(os.path.dirname(__file__), \"..\", \"config.json\")\n\t\t#print(\"INFO: Config path set to: '%s'\" % os.environ['CONFIG_PATH'])\n\telse:\n\t\t#print(\"INFO: Config path already set to: '%s'\" % os.environ['CONFIG_PATH'])\n\t\tpass\n\n\tif 'LOGGING_CONFIG_PATH' not in os.environ:\n\t\tos.environ['LOGGING_CONFIG_PATH'] = os.path.join(os.path.dirname(__file__), \"..\", \"logging.json\")\n\t\t#print(\"INFO: Logging config path set to: '%s'\" % os.environ['LOGGING_CONFIG_PATH'])\n\telse:\n\t\t#print(\"INFO: Logging config path already set to: '%s'\" % os.environ['LOGGING_CONFIG_\n", "def inject_config(self):\n\tif os.environ.get('NEO4J_CONF', None) is None:\n\t\tos.environ['NEO4J_CONF'] = os.path.join(self.server_dir, 'conf')\n\t\tprint(\"injected NEO4J_CONF=\" + os.environ['NEO4J_CONF'])\n\n\n", "def inject_config(self):\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'config.json')\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = self.config_path\n\t\tself.logger.debug(\"CONFIG_PATH set to %s\" % self.config_path)\n\treturn\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not os.environ.has_key(Config.CONFIG_ENV_VAR):\n\t\tos.environ[Config.CONFIG_ENV_VAR] = self.config_path\n\t\tprint \"Config path set to \" + self.config_path\n\t\treturn True\n\telse:\n\t\tprint \"Config path is already set to \" + os.environ[Config.CONFIG_ENV_VAR]\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not 'CONFIG_PATH' in os.environ:\n\t\tos.environ['CONFIG_PATH'] = self.config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not os.getenv('CONFIG_PATH'):\n\t\tos.environ['CONFIG_PATH'] = os.path.abspath(os.path.join(\n\t\t\tos.path.dirname(__file__), '..', '..', '..',\n\t\t\t'tests', 'config'))\n\t\t\n\t\tprint 'CONFIG_PATH', os.getenv('CONFIG_PATH')\n\t\t\n\t\tsys.path.append(os.getenv('CONFIG_PATH'))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif 'CONFIG' not in os.environ:\n\t\tos.environ['CONFIG'] = os.path.join(self.config_path, 'config.json')\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not self.env[\"CONFIG_PATH\"]:\n\t\tself.env[\"CONFIG_PATH\"] = os.path.join(self.env[\"PROJECT_ROOT\"], \"config\")\n\n\tself.env[\"CONFIG_FILE\"] = os.path.join(self.env[\"CONFIG_PATH\"],\n\t\t\t\t\t\t\t\t\t\t   self.env[\"CONFIG_FILE\"])\n\tself.env[\"ENV_FILE\"] = os.path.join(self.env[\"CONFIG_PATH\"],\n\t\t\t\t\t\t\t\t\t\tself.env[\"ENV_FILE\"])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not os.environ.has_key(self.env_var_name):\n\t\tos.environ[self.env_var_name] = self.config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = os.path.join(os.path.dirname(__file__), \"..\", \"config.json\")\n\t\t#print(\"INFO: Config path set to: '%s'\" % os.environ['CONFIG_PATH'])\n\telse:\n\t\t#print(\"INFO: Config path already set to: '%s'\" % os.environ['CONFIG_PATH'])\n\t\tpass\n\n\tif 'LOGGING_CONFIG_PATH' not in os.environ:\n\t\tos.environ['LOGGING_CONFIG_PATH'] = os.path.join(os.path.dirname(__file__), \"..\", \"logging.json\")\n\t\t#print(\"INFO: Logging config path set to: '%s'\" % os.environ['LOGGING_CONFIG_PATH'])\n\telse:\n\t\t#print(\"INFO: Logging config path already set to: '%s'\" % os.environ['LOGGING_CONFIG_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif os.environ.get('NEO4J_CONF', None) is None:\n\t\tos.environ['NEO4J_CONF'] = os.path.join(self.server_dir, 'conf')\n\t\tprint(\"injected NEO4J_CONF=\" + os.environ['NEO4J_CONF'])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif 'CONFIG_PATH' not in os.environ:\n\t\tos.environ['CONFIG_PATH'] = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'config.json')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "ques_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n\tself.logger.debug(\"extending cli\")\n\tparser = root_subparsers.add_parser(\"spec\", help=\"Manage specs.\")\n\tparser.set_defaults(plugin=self)\n\tspec_subparsers = parser.add_subparsers(title=\"spec subcommands\")\n\n\tlist_parser = spec_subparsers.add_parser(\"list\", help=\"List specs.\")\n\tlist_parser.add_argument(\"--active\", dest=\"active\", action=\"store_true\", help=\"Show active specs\")\n\tlist_parser.add_argument(\"--inactive\", dest=\"inactive\", action=\"store_true\", help=\"Show inactive specs\")\n\tlist_parser.add_argument(\"--all\", dest=\"all\", action=\"store_true\", help=\"Show all specs\")\n\tlist_parser.set_defaults(func=self.list)\n\n\tshow_parser = spec_subparsers.add_parser(\"show\", help=\"Show a spec.\")\n\tshow_parser.add_argument(\"spec\", help=\"spec name or spec ID\")\n\tshow_parser.set_\n", "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help='spec cli')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tself.add_config_subparser(subparsers)\n\tself.add_run_subparser(subparsers)\n\tself.add_test_subparser(subparsers)\n\tself.add_test_all_subparser(subparsers)\n\tself.add_benchmark_subparser(subparsers)\n\tself.add_benchmark_all_subparser(subparsers)\n\tself.add_profile_subparser(subparsers)\n\tself.add_profile_all_subparser(subparsers)\n\tself.add_debug_subparser(subparsers)\n\tself.add_debug_all_subparser(subparsers)\n\tself.add_dump_subparser(subparsers)\n\tself.add_dump_all_subparser(subparsers)\n\tself.add_clean_\n", "def extend_cli(self, root_subparsers):\n\tself.logger.debug('Extending cli with the spec options.')\n\tspec_subparser = root_subparsers.add_parser('spec', help='Specification related options.')\n\n\t# Create the spec subcommands\n\tspec_subparsers = spec_subparser.add_subparsers(help='Specification related subcommands.')\n\tspec_subparsers.required = True\n\tspec_subparsers.dest = 'spec_command'\n\n\t# Create the list spec subcommand\n\tlist_spec_parser = spec_subparsers.add_parser('list', help='Lists the specifications.')\n\tlist_spec_parser.add_argument('-a', '--all', action='store_true', help='List all specifications.')\n\tlist_spec_parser.add_argument('-l', '--limit', type=int, help='Limit the number of specifications returned.')\n\tlist_spec_parser.add_argument('-o', '--offset', type=int, help='Offset the list of specifications returned.')\n\tlist_spec_parser\n", "def extend_cli(self, root_subparsers):\n\thelp = 'Runs the spec (specs) in the current directory.'\n\tparser = root_subparsers.add_parser('spec', help=help)\n\tparser.add_argument('--tags', action='append', default=[])\n\tparser.add_argument('--spec', action='append', default=[])\n\tparser.add_argument('--no-color', action='store_true', default=False)\n\tparser.add_argument('--fail-fast', action='store_true', default=False)\n\tparser.add_argument('--spec-dir', default='./spec')\n\tparser.set_defaults(func=self.run_specs)\n\n\n", "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser(self.name, help=self.description)\n\tparser.set_defaults(func=self.__class__)\n\tparser.add_argument('--database', '-d', default='sqlite:///:memory:',\n\t\t\t\t\t\thelp='The database URI to use for the queue')\n\tparser.add_argument('--queue-name', '-q', default='queue',\n\t\t\t\t\t\thelp='The name of the queue to use')\n\tparser.add_argument('--verbose', '-v', action='store_true',\n\t\t\t\t\t\thelp='Enables verbose output')\n\tparser.add_argument('--timeout', '-t', type=int, default=30,\n\t\t\t\t\t\thelp='The maximum time to wait for a result')\n\tparser.add_argument('--max-results', '-m', type=int, default=1,\n\t\t\t\t\t\thelp='The maximum number of results to return')\n\tparser.add_argument('--result-type', '-\n", "def extend_cli(self, root_subparsers):\n\tsubp = root_subparsers.add_parser('spec', help='spec commands')\n\tsubp.add_argument('-D', '--debug', action='store_true', help='enable debug mode', default=False)\n\tsubp.add_argument('-v', '--verbose', action='store_true', help='enable verbose mode', default=False)\n\n\tsubp_subparsers = subp.add_subparsers(dest='spec_command', help='spec commands')\n\n\tsubp_subparsers.add_parser('show', help='show spec')\n\n\tsubp_subparsers.add_parser('update', help='update spec')\n\n\tsubp_subparsers.add_parser('merge', help='merge spec')\n\n\tsubp_subparsers.add_parser('diff', help='diff spec')\n\n\tsubp_subparsers.add_parser('render', help='render spec')\n\n\tsubp_subparsers.add_parser('validate', help='validate spec')\n\n\tsubp_subparsers\n", "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser(\n\t\t'run',\n\t\thelp='run the spec',\n\t\tdescription='Run the spec.')\n\n\tsubparser.add_argument(\n\t\t'--spec',\n\t\thelp='The spec file to run.')\n\n\tsubparser.add_argument(\n\t\t'--output',\n\t\thelp='The path to the output file.')\n\n\tsubparser.add_argument(\n\t\t'--skip-test-cases',\n\t\thelp='A comma separated list of test cases to skip.')\n\n\tsubparser.add_argument(\n\t\t'--only-test-cases',\n\t\thelp='A comma separated list of test cases to only run.')\n\n\tsubparser.set_defaults(func=self.run)\n\n\tsubparser = root_subparsers.add_parser(\n\t\t'list-test-cases',\n\t\thelp='list the test cases in the spec',\n\t\tdescription='List the test cases in the spec.')\n\n\tsubparser.add_argument(\n\t\t'--spec',\n\t\n", "def extend_cli(self, root_subparsers):\n\tcli_parser = root_subparsers.add_parser(\n\t\tself.__class__.__name__,\n\t\taliases=['spec'],\n\t\thelp='Run the spec tests.')\n\tcli_parser.set_defaults(spec=self)\n\tcli_parser.add_argument(\n\t\t'--spec-path',\n\t\thelp='the spec path to run.',\n\t\tdefault=None)\n\tcli_parser.add_argument(\n\t\t'--watch',\n\t\thelp='whether to watch the spec files',\n\t\taction='store_true')\n\tcli_parser.add_argument(\n\t\t'--spec-pattern',\n\t\thelp='the regex pattern to use to filter the spec files',\n\t\tdefault=None)\n\tcli_parser.add_argument(\n\t\t'--no-coverage',\n\t\thelp='whether to run the coverage',\n\t\taction='store_true')\n\tcli_parser.add_argument(\n\t\t'--no-color',\n\t\thelp='whether to use the colors',\n\t\taction\n", "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\tdescription='spec related commands',\n\t\thelp='spec related commands'\n\t)\n\n\tsubparser_spec = subparser.add_subparsers(title='spec', dest='spec_cmd')\n\tsubparser_spec.required = True\n\n\tsubparser_spec_create = subparser_spec.add_parser(\n\t\t'create',\n\t\tdescription='Create a new spec.',\n\t\thelp='Create a new spec.'\n\t)\n\tsubparser_spec_create.add_argument('--name', '-n', default=None, help='Name of the spec.')\n\tsubparser_spec_create.add_argument('--description', '-d', default=None, help='Description of the spec.')\n\tsubparser_spec_create.add_argument('--path', '-p', default=None, help='Path of the spec.')\n\tsubparser_spec_create.add_argument('--size', '-s', default=None, help='Size of the spec.')\n\t\n", "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help='Commands for managing specs.')\n\tparser.set_defaults(func=self.spec_cli_runner)\n\tparser.set_defaults(type='spec')\n\tself.spec_cli(parser)\n\n\treturn parser\n\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n\tself.logger.debug(\"extending cli\")\n\tparser = root_subparsers.add_parser(\"spec\", help=\"Manage specs.\")\n\tparser.set_defaults(plugin=self)\n\tspec_subparsers = parser.add_subparsers(title=\"spec subcommands\")\n\n\tlist_parser = spec_subparsers.add_parser(\"list\", help=\"List specs.\")\n\tlist_parser.add_argument(\"--active\", dest=\"active\", action=\"store_true\", help=\"Show active specs\")\n\tlist_parser.add_argument(\"--inactive\", dest=\"inactive\", action=\"store_true\", help=\"Show inactive specs\")\n\tlist_parser.add_argument(\"--all\", dest=\"all\", action=\"store_true\", help=\"Show all specs\")\n\tlist_parser.set_defaults(func=self.list)\n\n\tshow_parser = spec_subparsers.add_parser(\"show\", help=\"Show a spec.\")\n\tshow_parser.add_argument(\"spec\", help=\"spec name or spec ID\")\n\tshow_parser.set_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help='spec cli')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\n\tself.add_config_subparser(subparsers)\n\tself.add_run_subparser(subparsers)\n\tself.add_test_subparser(subparsers)\n\tself.add_test_all_subparser(subparsers)\n\tself.add_benchmark_subparser(subparsers)\n\tself.add_benchmark_all_subparser(subparsers)\n\tself.add_profile_subparser(subparsers)\n\tself.add_profile_all_subparser(subparsers)\n\tself.add_debug_subparser(subparsers)\n\tself.add_debug_all_subparser(subparsers)\n\tself.add_dump_subparser(subparsers)\n\tself.add_dump_all_subparser(subparsers)\n\tself.add_clean_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tself.logger.debug('Extending cli with the spec options.')\n\tspec_subparser = root_subparsers.add_parser('spec', help='Specification related options.')\n\n\t# Create the spec subcommands\n\tspec_subparsers = spec_subparser.add_subparsers(help='Specification related subcommands.')\n\tspec_subparsers.required = True\n\tspec_subparsers.dest = 'spec_command'\n\n\t# Create the list spec subcommand\n\tlist_spec_parser = spec_subparsers.add_parser('list', help='Lists the specifications.')\n\tlist_spec_parser.add_argument('-a', '--all', action='store_true', help='List all specifications.')\n\tlist_spec_parser.add_argument('-l', '--limit', type=int, help='Limit the number of specifications returned.')\n\tlist_spec_parser.add_argument('-o', '--offset', type=int, help='Offset the list of specifications returned.')\n\tlist_spec_parser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\thelp = 'Runs the spec (specs) in the current directory.'\n\tparser = root_subparsers.add_parser('spec', help=help)\n\tparser.add_argument('--tags', action='append', default=[])\n\tparser.add_argument('--spec', action='append', default=[])\n\tparser.add_argument('--no-color', action='store_true', default=False)\n\tparser.add_argument('--fail-fast', action='store_true', default=False)\n\tparser.add_argument('--spec-dir', default='./spec')\n\tparser.set_defaults(func=self.run_specs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser(self.name, help=self.description)\n\tparser.set_defaults(func=self.__class__)\n\tparser.add_argument('--database', '-d', default='sqlite:///:memory:',\n\t\t\t\t\t\thelp='The database URI to use for the queue')\n\tparser.add_argument('--queue-name', '-q', default='queue',\n\t\t\t\t\t\thelp='The name of the queue to use')\n\tparser.add_argument('--verbose', '-v', action='store_true',\n\t\t\t\t\t\thelp='Enables verbose output')\n\tparser.add_argument('--timeout', '-t', type=int, default=30,\n\t\t\t\t\t\thelp='The maximum time to wait for a result')\n\tparser.add_argument('--max-results', '-m', type=int, default=1,\n\t\t\t\t\t\thelp='The maximum number of results to return')\n\tparser.add_argument('--result-type', '-\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tsubp = root_subparsers.add_parser('spec', help='spec commands')\n\tsubp.add_argument('-D', '--debug', action='store_true', help='enable debug mode', default=False)\n\tsubp.add_argument('-v', '--verbose', action='store_true', help='enable verbose mode', default=False)\n\n\tsubp_subparsers = subp.add_subparsers(dest='spec_command', help='spec commands')\n\n\tsubp_subparsers.add_parser('show', help='show spec')\n\n\tsubp_subparsers.add_parser('update', help='update spec')\n\n\tsubp_subparsers.add_parser('merge', help='merge spec')\n\n\tsubp_subparsers.add_parser('diff', help='diff spec')\n\n\tsubp_subparsers.add_parser('render', help='render spec')\n\n\tsubp_subparsers.add_parser('validate', help='validate spec')\n\n\tsubp_subparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser(\n\t\t'run',\n\t\thelp='run the spec',\n\t\tdescription='Run the spec.')\n\n\tsubparser.add_argument(\n\t\t'--spec',\n\t\thelp='The spec file to run.')\n\n\tsubparser.add_argument(\n\t\t'--output',\n\t\thelp='The path to the output file.')\n\n\tsubparser.add_argument(\n\t\t'--skip-test-cases',\n\t\thelp='A comma separated list of test cases to skip.')\n\n\tsubparser.add_argument(\n\t\t'--only-test-cases',\n\t\thelp='A comma separated list of test cases to only run.')\n\n\tsubparser.set_defaults(func=self.run)\n\n\tsubparser = root_subparsers.add_parser(\n\t\t'list-test-cases',\n\t\thelp='list the test cases in the spec',\n\t\tdescription='List the test cases in the spec.')\n\n\tsubparser.add_argument(\n\t\t'--spec',\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tcli_parser = root_subparsers.add_parser(\n\t\tself.__class__.__name__,\n\t\taliases=['spec'],\n\t\thelp='Run the spec tests.')\n\tcli_parser.set_defaults(spec=self)\n\tcli_parser.add_argument(\n\t\t'--spec-path',\n\t\thelp='the spec path to run.',\n\t\tdefault=None)\n\tcli_parser.add_argument(\n\t\t'--watch',\n\t\thelp='whether to watch the spec files',\n\t\taction='store_true')\n\tcli_parser.add_argument(\n\t\t'--spec-pattern',\n\t\thelp='the regex pattern to use to filter the spec files',\n\t\tdefault=None)\n\tcli_parser.add_argument(\n\t\t'--no-coverage',\n\t\thelp='whether to run the coverage',\n\t\taction='store_true')\n\tcli_parser.add_argument(\n\t\t'--no-color',\n\t\thelp='whether to use the colors',\n\t\taction\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\tdescription='spec related commands',\n\t\thelp='spec related commands'\n\t)\n\n\tsubparser_spec = subparser.add_subparsers(title='spec', dest='spec_cmd')\n\tsubparser_spec.required = True\n\n\tsubparser_spec_create = subparser_spec.add_parser(\n\t\t'create',\n\t\tdescription='Create a new spec.',\n\t\thelp='Create a new spec.'\n\t)\n\tsubparser_spec_create.add_argument('--name', '-n', default=None, help='Name of the spec.')\n\tsubparser_spec_create.add_argument('--description', '-d', default=None, help='Description of the spec.')\n\tsubparser_spec_create.add_argument('--path', '-p', default=None, help='Path of the spec.')\n\tsubparser_spec_create.add_argument('--size', '-s', default=None, help='Size of the spec.')\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help='Commands for managing specs.')\n\tparser.set_defaults(func=self.spec_cli_runner)\n\tparser.set_defaults(type='spec')\n\tself.spec_cli(parser)\n\n\treturn parser\n\n\n\n", "is_pass": false, "return_code": 1}]}
