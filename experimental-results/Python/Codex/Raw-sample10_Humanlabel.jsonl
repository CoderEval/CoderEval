{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "hydrate_time", "docstring": "Hydrator for `Time` and `LocalTime` values.\n\n:param nanoseconds:\n:param tz:\n:return: Time", "ques_id": "62e60f43d76274f8a4026e28", "code": ["def hydrate_time(nanoseconds, tz=None):\n\tmicroseconds = nanoseconds // 1000\n\tif tz:\n\t\tdt = datetime.datetime.fromtimestamp(microseconds / 1e6, tz)\n\telse:\n\t\tdt = datetime.datetime.fromtimestamp(microseconds / 1e6)\n\treturn dt.strftime('%H:%M:%S')\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn nanoseconds\n\tif nanoseconds > 0:\n\t\tnanoseconds = int(nanoseconds)\n\t\treturn datetime.time(\n\t\t\thour=nanoseconds//NANOSECONDS_PER_HOUR,\n\t\t\tminute=(nanoseconds%NANOSECONDS_PER_HOUR)//NANOSECONDS_PER_MINUTE,\n\t\t\tsecond=(nanoseconds%NANOSECONDS_PER_MINUTE)//NANOSECONDS_PER_SECOND,\n\t\t\tmicrosecond=(nanoseconds%NANOSECONDS_PER_SECOND)//NANOSECONDS_PER_MICROSECOND,\n\t\t\ttzinfo=tz\n\t\t)\n\telse:\n\t\treturn None\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\treturn hydrate_datetime(nanoseconds, tz).time()\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tseconds = nanoseconds / 1000000000\n\tif tz:\n\t\treturn datetime.datetime.fromtimestamp(seconds, tz).time().strftime(settings.TIME_FORMAT)\n\telse:\n\t\treturn datetime.datetime.fromtimestamp(seconds).time().strftime(settings.TIME_FORMAT)\n\n", "def hydrate_time(nanoseconds, tz=None):\n\treturn datetime.time(microsecond=(nanoseconds / 1000) % 1000000,\n\t\t\t\t\t\t second=(nanoseconds / 1000000000) % 60,\n\t\t\t\t\t\t minute=(nanoseconds / 60000000000) % 60,\n\t\t\t\t\t\t hour=(nanoseconds / 3600000000000) % 24,\n\t\t\t\t\t\t tzinfo=tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tsec, nano = divmod(nanoseconds, 1000000000)\n\thour, sec = divmod(sec, 3600)\n\tmin, sec = divmod(sec, 60)\n\tusec, nano = divmod(nano, 1000)\n\treturn time(hour, min, sec, usec, nano, tz)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\ttz = timezone.utc\n\treturn time(tzinfo=tz).replace(\n\t\tmicrosecond=(nanoseconds // 1000) % 1000000,\n\t\tsecond=(nanoseconds // 1000000000) % 60,\n\t\tminute=(nanoseconds // 1000000000000) % 60,\n\t\thour=nanoseconds // 1000000000000000\n\t)\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\ttz = pytz.utc\n\treturn tz.localize(datetime.datetime(1, 1, 1, 0, 0, 0, 0, tzinfo=tz) + datetime.timedelta(nanoseconds=nanoseconds)).time()\n\n\n", "def hydrate_time(nanoseconds, tz=None):\n\treturn datetime.time(nanoseconds / 3600, (nanoseconds % 3600) / 60, nanoseconds % 60, 0, tz)\n\n", "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\n\tif tz is None:\n\t\ttz = timezone.get_current_timezone()\n\n\treturn datetime.time(\n\t\thour=nanoseconds // 3600000000000,\n\t\tminute=(nanoseconds // 60000000000) % 60,\n\t\tsecond=(nanoseconds // 1000000000) % 60,\n\t\tmicrosecond=(nanoseconds // 1000) % 1000,\n\t\ttzinfo=tz\n\t)\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tmicroseconds = nanoseconds // 1000\n\tif tz:\n\t\tdt = datetime.datetime.fromtimestamp(microseconds / 1e6, tz)\n\telse:\n\t\tdt = datetime.datetime.fromtimestamp(microseconds / 1e6)\n\treturn dt.strftime('%H:%M:%S')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn nanoseconds\n\tif nanoseconds > 0:\n\t\tnanoseconds = int(nanoseconds)\n\t\treturn datetime.time(\n\t\t\thour=nanoseconds//NANOSECONDS_PER_HOUR,\n\t\t\tminute=(nanoseconds%NANOSECONDS_PER_HOUR)//NANOSECONDS_PER_MINUTE,\n\t\t\tsecond=(nanoseconds%NANOSECONDS_PER_MINUTE)//NANOSECONDS_PER_SECOND,\n\t\t\tmicrosecond=(nanoseconds%NANOSECONDS_PER_SECOND)//NANOSECONDS_PER_MICROSECOND,\n\t\t\ttzinfo=tz\n\t\t)\n\telse:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\treturn hydrate_datetime(nanoseconds, tz).time()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tseconds = nanoseconds / 1000000000\n\tif tz:\n\t\treturn datetime.datetime.fromtimestamp(seconds, tz).time().strftime(settings.TIME_FORMAT)\n\telse:\n\t\treturn datetime.datetime.fromtimestamp(seconds).time().strftime(settings.TIME_FORMAT)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\treturn datetime.time(microsecond=(nanoseconds / 1000) % 1000000,\n\t\t\t\t\t\t second=(nanoseconds / 1000000000) % 60,\n\t\t\t\t\t\t minute=(nanoseconds / 60000000000) % 60,\n\t\t\t\t\t\t hour=(nanoseconds / 3600000000000) % 24,\n\t\t\t\t\t\t tzinfo=tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tsec, nano = divmod(nanoseconds, 1000000000)\n\thour, sec = divmod(sec, 3600)\n\tmin, sec = divmod(sec, 60)\n\tusec, nano = divmod(nano, 1000)\n\treturn time(hour, min, sec, usec, nano, tz)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\ttz = timezone.utc\n\treturn time(tzinfo=tz).replace(\n\t\tmicrosecond=(nanoseconds // 1000) % 1000000,\n\t\tsecond=(nanoseconds // 1000000000) % 60,\n\t\tminute=(nanoseconds // 1000000000000) % 60,\n\t\thour=nanoseconds // 1000000000000000\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\tif tz is None:\n\t\ttz = pytz.utc\n\treturn tz.localize(datetime.datetime(1, 1, 1, 0, 0, 0, 0, tzinfo=tz) + datetime.timedelta(nanoseconds=nanoseconds)).time()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\treturn datetime.time(nanoseconds / 3600, (nanoseconds % 3600) / 60, nanoseconds % 60, 0, tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hydrate_time(nanoseconds, tz=None):\n\tif nanoseconds is None:\n\t\treturn None\n\n\tif tz is None:\n\t\ttz = timezone.get_current_timezone()\n\n\treturn datetime.time(\n\t\thour=nanoseconds // 3600000000000,\n\t\tminute=(nanoseconds // 60000000000) % 60,\n\t\tsecond=(nanoseconds // 1000000000) % 60,\n\t\tmicrosecond=(nanoseconds // 1000) % 1000,\n\t\ttzinfo=tz\n\t)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_timedelta", "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:", "ques_id": "62e60f3bd76274f8a4026e10", "code": ["def dehydrate_timedelta(value):\n\tif value.days:\n\t\treturn Structure(years=value.days / 365, months=value.days / 30, days=value.days % 30, hours=value.seconds / 3600, minutes=value.seconds / 60, seconds=value.seconds % 60)\n\telse:\n\t\treturn Structure(hours=value.seconds / 3600, minutes=value.seconds / 60, seconds=value.seconds % 60)\n\n", "def dehydrate_timedelta(value):\n\tif isinstance(value, timedelta):\n\t\treturn Structure(seconds=value.seconds, microseconds=value.microseconds, days=value.days)\n\treturn value\n\n", "def dehydrate_timedelta(value):\n\treturn value.days * 24 * 60 * 60 + value.seconds\n\n\n", "def dehydrate_timedelta(value):\n\treturn Structure(**value._asdict())\n\n", "def dehydrate_timedelta(value):\n\tif isinstance(value, timedelta):\n\t\treturn Structure(hours=value.seconds/3600, minutes=(value.seconds/60) % 60, seconds=value.seconds % 60)\n\telse:\n\t\treturn value\n\n", "def dehydrate_timedelta(value):\n\treturn ''\n\n", "def dehydrate_timedelta(value):\n\treturn timedelta(seconds=value)\n\n", "def dehydrate_timedelta(value):\n\tif isinstance(value, datetime.timedelta):\n\t\treturn datetime.timedelta(seconds=value.total_seconds())\n\treturn value\n\n\n", "def dehydrate_timedelta(value):\n\ttry:\n\t\tif value is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn datetime.timedelta(seconds=value)\n\texcept TypeError:\n\t\treturn None\n\n", "def dehydrate_timedelta(value):\n\treturn {\n\t\t'value': value.total_seconds(),\n\t\t'unit': 'second',\n\t}\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_timedelta(value):\n\tif value.days:\n\t\treturn Structure(years=value.days / 365, months=value.days / 30, days=value.days % 30, hours=value.seconds / 3600, minutes=value.seconds / 60, seconds=value.seconds % 60)\n\telse:\n\t\treturn Structure(hours=value.seconds / 3600, minutes=value.seconds / 60, seconds=value.seconds % 60)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\tif isinstance(value, timedelta):\n\t\treturn Structure(seconds=value.seconds, microseconds=value.microseconds, days=value.days)\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn value.days * 24 * 60 * 60 + value.seconds\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn Structure(**value._asdict())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\tif isinstance(value, timedelta):\n\t\treturn Structure(hours=value.seconds/3600, minutes=(value.seconds/60) % 60, seconds=value.seconds % 60)\n\telse:\n\t\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn ''\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn timedelta(seconds=value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\tif isinstance(value, datetime.timedelta):\n\t\treturn datetime.timedelta(seconds=value.total_seconds())\n\treturn value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\ttry:\n\t\tif value is None:\n\t\t\treturn None\n\t\telse:\n\t\t\treturn datetime.timedelta(seconds=value)\n\texcept TypeError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_timedelta(value):\n\treturn {\n\t\t'value': value.total_seconds(),\n\t\t'unit': 'second',\n\t}\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/temporal.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_time", "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:", "ques_id": "62e60f37d76274f8a4026dfd", "code": ["def dehydrate_time(value):\n\treturn value.get_time()\n\n\n", "def dehydrate_time(value):\n\treturn ticks_to_structure(value.ticks)\n\n", "def dehydrate_time(value):\n\treturn Structure(value.ticks)\n\n", "def dehydrate_time(value):\n\treturn Structure(value.ticks)\n\n\n", "def dehydrate_time(value):\n\treturn Time(value).structure\n\n", "def dehydrate_time(value):\n\treturn Structure(time=value)\n\n", "def dehydrate_time(value):\n\tif value.ticks:\n\t\treturn Structure(ticks=value.ticks)\n\n\n", "def dehydrate_time(value):\n\treturn value.ticks\n\n\n", "def dehydrate_time(value):\n\treturn value.ticks\n\n", "def dehydrate_time(value):\n\treturn value.ticks\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_time(value):\n\treturn value.get_time()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn ticks_to_structure(value.ticks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn Structure(value.ticks)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn Structure(value.ticks)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn Time(value).structure\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn Structure(time=value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\tif value.ticks:\n\t\treturn Structure(ticks=value.ticks)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.ticks\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.ticks\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_time(value):\n\treturn value.ticks\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/hydration/v1/spatial.py", "project": "neo4j/neo4j-python-driver", "name": "dehydrate_point", "docstring": "Dehydrator for Point data.\n\n:param value:\n:type value: Point\n:return:", "ques_id": "62e60f33d76274f8a4026de9", "code": ["def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn Point(value[0], value[1], value[2])\n\telse:\n\t\traise ValueError('Invalid point value.')\n\n", "def dehydrate_point(value):\n\tif len(value) == 4:\n\t\treturn [value[0], value[1], value[2]]\n\telse:\n\t\treturn [value[0], value[1], value[2], value[3]]\n", "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(x=value[0], y=value[1])\n\telse:\n\t\treturn Point(x=value[0], y=value[1], z=value[2])\n\n\n", "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn Point3D(value[0], value[1], value[2])\n\telse:\n\t\treturn value\n\n", "def dehydrate_point(value):\n\tif not value:\n\t\treturn None\n\tif len(value) == 2:\n\t\treturn Point(value, srid=4326)\n\tif len(value) == 4:\n\t\treturn LineString(value, srid=4326)\n\tif len(value) == 6:\n\t\treturn Polygon(value, srid=4326)\n\treturn value\n\n", "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point2D(*value)\n\telif len(value) == 3:\n\t\treturn Point3D(*value)\n\telif len(value) == 4:\n\t\treturn Point4D(*value)\n\telse:\n\t\traise ValueError(\"Unsupported point length: %s\" % len(value))\n\n", "def dehydrate_point(value):\n\tif value is None:\n\t\treturn None\n\tif len(value) == 2:\n\t\treturn Point(value)\n\treturn PointZ(value)\n\n", "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(value)\n\telif len(value) == 3:\n\t\treturn PointZ(value)\n\telif len(value) == 4:\n\t\treturn PointM(value)\n\telif len(value) == 6:\n\t\treturn PointZM(value)\n\n\n", "def dehydrate_point(value):\n\tif value is None:\n\t\treturn None\n\ttry:\n\t\tif len(value) == 2:\n\t\t\treturn Point2D(*value)\n\t\telif len(value) == 3:\n\t\t\treturn Point3D(*value)\n\t\telse:\n\t\t\traise ValueError(\"Invalid point length: %d\" % len(value))\n\texcept TypeError:\n\t\traise ValueError(\"Invalid point format: %s\" % value)\n\n", "def dehydrate_point(value):\n\tif isinstance(value, Point):\n\t\treturn {\n\t\t\t'x': value.x,\n\t\t\t'y': value.y,\n\t\t\t'z': value.z,\n\t\t\t'w': value.w,\n\t\t}\n\telif isinstance(value, Vector):\n\t\treturn {\n\t\t\t'x': value.x,\n\t\t\t'y': value.y,\n\t\t\t'z': value.z,\n\t\t}\n\telse:\n\t\traise Exception('Invalid type to dehydrate')\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn Point(value[0], value[1], value[2])\n\telse:\n\t\traise ValueError('Invalid point value.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif len(value) == 4:\n\t\treturn [value[0], value[1], value[2]]\n\telse:\n\t\treturn [value[0], value[1], value[2], value[3]]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(x=value[0], y=value[1])\n\telse:\n\t\treturn Point(x=value[0], y=value[1], z=value[2])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(value[0], value[1])\n\telif len(value) == 3:\n\t\treturn Point3D(value[0], value[1], value[2])\n\telse:\n\t\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif not value:\n\t\treturn None\n\tif len(value) == 2:\n\t\treturn Point(value, srid=4326)\n\tif len(value) == 4:\n\t\treturn LineString(value, srid=4326)\n\tif len(value) == 6:\n\t\treturn Polygon(value, srid=4326)\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point2D(*value)\n\telif len(value) == 3:\n\t\treturn Point3D(*value)\n\telif len(value) == 4:\n\t\treturn Point4D(*value)\n\telse:\n\t\traise ValueError(\"Unsupported point length: %s\" % len(value))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif value is None:\n\t\treturn None\n\tif len(value) == 2:\n\t\treturn Point(value)\n\treturn PointZ(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif len(value) == 2:\n\t\treturn Point(value)\n\telif len(value) == 3:\n\t\treturn PointZ(value)\n\telif len(value) == 4:\n\t\treturn PointM(value)\n\telif len(value) == 6:\n\t\treturn PointZM(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif value is None:\n\t\treturn None\n\ttry:\n\t\tif len(value) == 2:\n\t\t\treturn Point2D(*value)\n\t\telif len(value) == 3:\n\t\t\treturn Point3D(*value)\n\t\telse:\n\t\t\traise ValueError(\"Invalid point length: %d\" % len(value))\n\texcept TypeError:\n\t\traise ValueError(\"Invalid point format: %s\" % value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dehydrate_point(value):\n\tif isinstance(value, Point):\n\t\treturn {\n\t\t\t'x': value.x,\n\t\t\t'y': value.y,\n\t\t\t'z': value.z,\n\t\t\t'w': value.w,\n\t\t}\n\telif isinstance(value, Vector):\n\t\treturn {\n\t\t\t'x': value.x,\n\t\t\t'y': value.y,\n\t\t\t'z': value.z,\n\t\t}\n\telse:\n\t\traise Exception('Invalid type to dehydrate')\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "keys", "docstring": "Return the keys of the record.\n\n:return: list of key names", "ques_id": "62e60ed4d76274f8a4026da0", "code": ["def keys(self):\n\treturn self.__keys\n\n", "def keys(self):\n\treturn self.__keys\n\n", "def keys(self):\n\treturn self.__keys\n\n\tdef is_unique(self):\n\t\"\"\"\n\tReturns the __unique property of a class\n\t\"\"\"\n\treturn self.__unique\n\n\tdef is_nullable(self):\n\t\"\"\"\n\tReturns the __nullable property of a class\n\t\"\"\"\n\treturn self.__nullable\n\n\tdef is_primary(self):\n\t\"\"\"\n\tReturns the __primary property of a class\n\t\"\"\"\n\treturn self.__primary\n\n\tdef get_type(self):\n\t\"\"\"\n\tReturns the __type property of a class\n\t\"\"\"\n\treturn self.__type\n\n\tdef set_type(self, type):\n\t\"\"\"\n\tSets the __type property of a class to the specified type\n\t\"\"\"\n\tself.__type = type\n\n\tdef get_length(self):\n\t\"\"\"\n\tReturns the __length property of a class\n\t\"\"\"\n\treturn self.__length\n\n\tdef set_length(self, length):\n\t\"\"\"\n\tSets the __length property of a class to the specified length\n\t\"\"\"\n\tself.__length = length\n\n\tdef get_default(self\n", "def keys(self):\n\t\treturn self.__keys\n\n\tdef keys_of_type(self, type):\n\t\t\"\"\"\n\t\tReturns all keys from a class that have a specified type\n\t\t\"\"\"\n\t\tkeys = []\n\t\tfor key in self.__keys:\n\t\t\tif type == key.type:\n\t\t\t\tkeys.append(key)\n\t\treturn keys\n\n\tdef key_with_name(self, name):\n\t\t\"\"\"\n\t\tReturns a key with a specific name\n\t\t\"\"\"\n\t\tfor key in self.__keys:\n\t\t\tif name == key.name:\n\t\t\t\treturn key\n\t\treturn None\n\n\tdef key_with_value(self, value):\n\t\t\"\"\"\n\t\tReturns the key with a specific value\n\t\t\"\"\"\n\t\tfor key in self.__keys:\n\t\t\tif value == key.value:\n\t\t\t\treturn key\n\t\treturn None\n\n", "def keys(self):\n\treturn self.__keys\n\n", "def keys(self):\n\tif not self.__keys:\n\t\tif self.__parent:\n\t\t\treturn self.__parent.keys()\n\t\treturn []\n\treturn self.__keys\n\t\n", "def keys(self):\n\treturn self.__keys\n\n", "def keys(self):\n\treturn self.__keys\n\n\n", "def keys(self):\n\treturn self.__keys\n\t\n", "def keys(self):\n\treturn self.__keys\n\n\tdef get_keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn self.__keys\n\n\tdef set_keys(self, value):\n\t\"\"\"\n\tSet the __keys property of a class\n\t\"\"\"\n\tself.__keys = value\n\n\tdef del_keys(self):\n\t\"\"\"\n\tDelete the __keys property of a class\n\t\"\"\"\n\tdel self.__keys\n\n\tkeys = property(get_keys, set_keys, del_keys, \"keys's docstring\")\n\n\tdef get_values(self):\n\t\"\"\"\n\tReturns the __values property of a class as a List\n\t\"\"\"\n\treturn self.__values\n\n\tdef set_values(self, value):\n\t\"\"\"\n\tSet the __values property of a class\n\t\"\"\"\n\tself.__values = value\n\n\tdef del_values(self):\n\t\"\"\"\n\tDelete the __values property of a class\n\t\"\"\"\n\tdel self.__values\n\n\tvalues = property(get_values, set_values, del_values, \"values's docstring\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def keys(self):\n\treturn self.__keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\n\tdef is_unique(self):\n\t\"\"\"\n\tReturns the __unique property of a class\n\t\"\"\"\n\treturn self.__unique\n\n\tdef is_nullable(self):\n\t\"\"\"\n\tReturns the __nullable property of a class\n\t\"\"\"\n\treturn self.__nullable\n\n\tdef is_primary(self):\n\t\"\"\"\n\tReturns the __primary property of a class\n\t\"\"\"\n\treturn self.__primary\n\n\tdef get_type(self):\n\t\"\"\"\n\tReturns the __type property of a class\n\t\"\"\"\n\treturn self.__type\n\n\tdef set_type(self, type):\n\t\"\"\"\n\tSets the __type property of a class to the specified type\n\t\"\"\"\n\tself.__type = type\n\n\tdef get_length(self):\n\t\"\"\"\n\tReturns the __length property of a class\n\t\"\"\"\n\treturn self.__length\n\n\tdef set_length(self, length):\n\t\"\"\"\n\tSets the __length property of a class to the specified length\n\t\"\"\"\n\tself.__length = length\n\n\tdef get_default(self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\t\treturn self.__keys\n\n\tdef keys_of_type(self, type):\n\t\t\"\"\"\n\t\tReturns all keys from a class that have a specified type\n\t\t\"\"\"\n\t\tkeys = []\n\t\tfor key in self.__keys:\n\t\t\tif type == key.type:\n\t\t\t\tkeys.append(key)\n\t\treturn keys\n\n\tdef key_with_name(self, name):\n\t\t\"\"\"\n\t\tReturns a key with a specific name\n\t\t\"\"\"\n\t\tfor key in self.__keys:\n\t\t\tif name == key.name:\n\t\t\t\treturn key\n\t\treturn None\n\n\tdef key_with_value(self, value):\n\t\t\"\"\"\n\t\tReturns the key with a specific value\n\t\t\"\"\"\n\t\tfor key in self.__keys:\n\t\t\tif value == key.value:\n\t\t\t\treturn key\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\tif not self.__keys:\n\t\tif self.__parent:\n\t\t\treturn self.__parent.keys()\n\t\treturn []\n\treturn self.__keys\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def keys(self):\n\treturn self.__keys\n\n\tdef get_keys(self):\n\t\"\"\"\n\tReturns the __keys property of a class as a List\n\t\"\"\"\n\treturn self.__keys\n\n\tdef set_keys(self, value):\n\t\"\"\"\n\tSet the __keys property of a class\n\t\"\"\"\n\tself.__keys = value\n\n\tdef del_keys(self):\n\t\"\"\"\n\tDelete the __keys property of a class\n\t\"\"\"\n\tdel self.__keys\n\n\tkeys = property(get_keys, set_keys, del_keys, \"keys's docstring\")\n\n\tdef get_values(self):\n\t\"\"\"\n\tReturns the __values property of a class as a List\n\t\"\"\"\n\treturn self.__values\n\n\tdef set_values(self, value):\n\t\"\"\"\n\tSet the __values property of a class\n\t\"\"\"\n\tself.__values = value\n\n\tdef del_values(self):\n\t\"\"\"\n\tDelete the __values property of a class\n\t\"\"\"\n\tdel self.__values\n\n\tvalues = property(get_values, set_values, del_values, \"values's docstring\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_sync/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60ecfd76274f8a4026d6a", "code": ["def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.protocol_handlers\n\telse:\n\t\treturn cls.get_protocol_handler(protocol_version)\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version == '1':\n\t\treturn [Boltv1Handshake(cls.auth_token, cls.trust), Boltv1(cls.auth_token, cls.trust)]\n\telse:\n\t\treturn [Boltv2Handshake(cls.auth_token, cls.trust), Boltv2(cls.auth_token, cls.trust)]\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls._protocol_handlers\n\telse:\n\t\treturn cls._protocol_handlers[protocol_version]\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version == None:\n\t\treturn cls.protocol_handlers\n\telif isinstance(protocol_version, int):\n\t\treturn cls.protocol_handlers[protocol_version]\n\telse:\n\t\traise ValueError('Unsupported value for protocol_version')\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn protocol_handlers(cls, protocol_version=PROTOCOL_VERSION_3_0)\n\telif protocol_version == PROTOCOL_VERSION_3_0:\n\t\treturn BoltProtocol30\n\telif protocol_version == PROTOCOL_VERSION_4_0:\n\t\treturn BoltProtocol40\n\telse:\n\t\traise ValueError(\"Unsupported protocol version: %r\" % protocol_version)\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\tprotocol_version = cls.protocol_version\n\n\treturn {\n\t\t1: {\n\t\t\t\"init\": cls.initv1,\n\t\t\t\"ack_failure\": cls.ack_failurev1,\n\t\t\t\"run\": cls.runv1,\n\t\t\t\"pull_all\": cls.pull_allv1,\n\t\t\t\"discard_all\": cls.discard_allv1,\n\t\t\t\"reset\": cls.resetv1,\n\t\t\t\"goodbye\": cls.goodbyev1,\n\t\t},\n\t\t2: {\n\t\t\t\"init\": cls.initv2,\n\t\t\t\"ack_failure\": cls.ack_failurev2,\n\t\t\t\"run\": cls.runv2,\n\t\t\t\"pull_all\": cls.pull_allv2,\n\t\t\t\"discard_all\": cls.discard_allv2,\n\t\t\t\"reset\": cls.resetv\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None or protocol_version == 'V1':\n\t\treturn cls.handlers_v1\n\telif protocol_version == 'V2':\n\t\treturn cls.handlers_v2\n\telse:\n\t\traise ValueError(\"Unknown protocol version %s\" % protocol_version)\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tprotocol_version = cls.protocol_version if protocol_version is None else protocol_version\n\tif protocol_version == 0:\n\t\treturn BoltV0Handlers(cls)\n\telif protocol_version == 1:\n\t\treturn BoltV1Handlers(cls)\n\telif protocol_version == 2:\n\t\treturn BoltV2Handlers(cls)\n\telse:\n\t\traise ValueError(\"Unknown protocol version %d\" % protocol_version)\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn {\n\t\t\t\"address_resolver_impl\": cls.address_resolver_impl,\n\t\t\t\"auth_handler_impl\": cls.auth_handler_impl,\n\t\t\t\"chunk_writer_impl\": cls.chunk_writer_impl,\n\t\t\t\"connection_pool_impl\": cls.connection_pool_impl,\n\t\t\t\"logger_impl\": cls.logger_impl,\n\t\t\t\"pack_stream_serializer_impl\": cls.pack_stream_serializer_impl,\n\t\t\t\"result_summary_impl\": cls.result_summary_impl,\n\t\t\t\"routing_table_impl\": cls.routing_table_impl,\n\t\t\t\"session_impl\": cls.session_impl,\n\t\t\t\"statement_runner_impl\": cls.statement_runner_impl,\n\t\t\t\"transaction_impl\": cls.transaction_impl,\n\t\t\t\"user_agent_impl\": cls.user_agent_impl,\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.SERIALIZERS\n\treturn cls.SERIALIZERS.get(protocol_version)\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.protocol_handlers\n\telse:\n\t\treturn cls.get_protocol_handler(protocol_version)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version == '1':\n\t\treturn [Boltv1Handshake(cls.auth_token, cls.trust), Boltv1(cls.auth_token, cls.trust)]\n\telse:\n\t\treturn [Boltv2Handshake(cls.auth_token, cls.trust), Boltv2(cls.auth_token, cls.trust)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls._protocol_handlers\n\telse:\n\t\treturn cls._protocol_handlers[protocol_version]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version == None:\n\t\treturn cls.protocol_handlers\n\telif isinstance(protocol_version, int):\n\t\treturn cls.protocol_handlers[protocol_version]\n\telse:\n\t\traise ValueError('Unsupported value for protocol_version')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn protocol_handlers(cls, protocol_version=PROTOCOL_VERSION_3_0)\n\telif protocol_version == PROTOCOL_VERSION_3_0:\n\t\treturn BoltProtocol30\n\telif protocol_version == PROTOCOL_VERSION_4_0:\n\t\treturn BoltProtocol40\n\telse:\n\t\traise ValueError(\"Unsupported protocol version: %r\" % protocol_version)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\tprotocol_version = cls.protocol_version\n\n\treturn {\n\t\t1: {\n\t\t\t\"init\": cls.initv1,\n\t\t\t\"ack_failure\": cls.ack_failurev1,\n\t\t\t\"run\": cls.runv1,\n\t\t\t\"pull_all\": cls.pull_allv1,\n\t\t\t\"discard_all\": cls.discard_allv1,\n\t\t\t\"reset\": cls.resetv1,\n\t\t\t\"goodbye\": cls.goodbyev1,\n\t\t},\n\t\t2: {\n\t\t\t\"init\": cls.initv2,\n\t\t\t\"ack_failure\": cls.ack_failurev2,\n\t\t\t\"run\": cls.runv2,\n\t\t\t\"pull_all\": cls.pull_allv2,\n\t\t\t\"discard_all\": cls.discard_allv2,\n\t\t\t\"reset\": cls.resetv\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None or protocol_version == 'V1':\n\t\treturn cls.handlers_v1\n\telif protocol_version == 'V2':\n\t\treturn cls.handlers_v2\n\telse:\n\t\traise ValueError(\"Unknown protocol version %s\" % protocol_version)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tprotocol_version = cls.protocol_version if protocol_version is None else protocol_version\n\tif protocol_version == 0:\n\t\treturn BoltV0Handlers(cls)\n\telif protocol_version == 1:\n\t\treturn BoltV1Handlers(cls)\n\telif protocol_version == 2:\n\t\treturn BoltV2Handlers(cls)\n\telse:\n\t\traise ValueError(\"Unknown protocol version %d\" % protocol_version)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn {\n\t\t\t\"address_resolver_impl\": cls.address_resolver_impl,\n\t\t\t\"auth_handler_impl\": cls.auth_handler_impl,\n\t\t\t\"chunk_writer_impl\": cls.chunk_writer_impl,\n\t\t\t\"connection_pool_impl\": cls.connection_pool_impl,\n\t\t\t\"logger_impl\": cls.logger_impl,\n\t\t\t\"pack_stream_serializer_impl\": cls.pack_stream_serializer_impl,\n\t\t\t\"result_summary_impl\": cls.result_summary_impl,\n\t\t\t\"routing_table_impl\": cls.routing_table_impl,\n\t\t\t\"session_impl\": cls.session_impl,\n\t\t\t\"statement_runner_impl\": cls.statement_runner_impl,\n\t\t\t\"transaction_impl\": cls.transaction_impl,\n\t\t\t\"user_agent_impl\": cls.user_agent_impl,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.SERIALIZERS\n\treturn cls.SERIALIZERS.get(protocol_version)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/work/query.py", "project": "neo4j/neo4j-python-driver", "name": "unit_of_work", "docstring": "This function is a decorator for transaction functions that allows extra control over how the transaction is carried out.\n\nFor example, a timeout may be applied::\n\n    from neo4j import unit_of_work\n\n    @unit_of_work(timeout=100)\n    def count_people_tx(tx):\n        result = tx.run(\"MATCH (a:Person) RETURN count(a) AS persons\")\n        record = result.single()\n        return record[\"persons\"]\n\n:param metadata:\n    a dictionary with metadata.\n    Specified metadata will be attached to the executing transaction and visible in the output of ``dbms.listQueries`` and ``dbms.listTransactions`` procedures.\n    It will also get logged to the ``query.log``.\n    This functionality makes it easier to tag transactions and is equivalent to ``dbms.setTXMetaData`` procedure, see https://neo4j.com/docs/operations-manual/current/reference/procedures/ for procedure reference.\n:type metadata: dict\n\n:param timeout:\n    the transaction timeout in seconds.\n    Transactions that execute longer than the configured timeout will be terminated by the database.\n    This functionality allows to limit query/transaction execution time.\n    Specified timeout overrides the default timeout configured in the database using ``dbms.transaction.timeout`` setting.\n    Value should not represent a negative duration.\n    A zero duration will make the transaction execute indefinitely.\n    None will use the default timeout configured in the database.\n:type timeout: float or :const:`None`", "ques_id": "62e60e49d76274f8a4026d25", "code": ["def unit_of_work(metadata=None, timeout=None):\n\treturn unit_of_work_decorator(metadata, timeout)\n\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\tfn.metadata = metadata\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorate(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorate\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.metadata = metadata\n\t\twrapper.timeout = timeout\n\t\treturn wrapper\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\tfn.metadata = metadata\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\tif metadata is not None:\n\t\tkwargs = {'metadata': metadata}\n\tif timeout is not None:\n\t\tkwargs = {'timeout': timeout}\n\tif metadata is not None and timeout is not None:\n\t\tkwargs = {'metadata': metadata, 'timeout': timeout}\n\tif metadata is None and timeout is None:\n\t\tkwargs = {}\n\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith UnitOfWork(**kwargs) as uow:\n\t\t\t\treturn func(*args, **kwargs)\n\t\twrapper.metadata = kwargs.get('metadata')\n\t\twrapper.timeout = kwargs.get('timeout')\n\t\treturn wrapper\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\tdecorator = decorators.unit_of_work(metadata, timeout)\n\tdecorator.metadata = metadata\n\tdecorator.timeout = timeout\n\treturn decorator\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorates function with metadata and timeout attributes.\n\t\t\"\"\"\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n", "def unit_of_work(metadata=None, timeout=None):\n\tdef uow_decorator(func):\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t#print \"decorator\", func, args, kwargs\n\t\t\twith UnitOfWork(metadata, timeout=timeout) as uow:\n\t\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn uow_decorator\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\treturn unit_of_work_decorator(metadata, timeout)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\tfn.metadata = metadata\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorate(func):\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorate\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn func(*args, **kwargs)\n\t\twrapper.metadata = metadata\n\t\twrapper.timeout = timeout\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(fn):\n\t\tfn.metadata = metadata\n\t\tfn.timeout = timeout\n\t\treturn fn\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(f):\n\t\tf.metadata = metadata\n\t\tf.timeout = timeout\n\t\treturn f\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tif metadata is not None:\n\t\tkwargs = {'metadata': metadata}\n\tif timeout is not None:\n\t\tkwargs = {'timeout': timeout}\n\tif metadata is not None and timeout is not None:\n\t\tkwargs = {'metadata': metadata, 'timeout': timeout}\n\tif metadata is None and timeout is None:\n\t\tkwargs = {}\n\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith UnitOfWork(**kwargs) as uow:\n\t\t\t\treturn func(*args, **kwargs)\n\t\twrapper.metadata = kwargs.get('metadata')\n\t\twrapper.timeout = kwargs.get('timeout')\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdecorator = decorators.unit_of_work(metadata, timeout)\n\tdecorator.metadata = metadata\n\tdecorator.timeout = timeout\n\treturn decorator\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tDecorates function with metadata and timeout attributes.\n\t\t\"\"\"\n\t\tfunc.metadata = metadata\n\t\tfunc.timeout = timeout\n\t\treturn func\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def unit_of_work(metadata=None, timeout=None):\n\tdef uow_decorator(func):\n\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t#print \"decorator\", func, args, kwargs\n\t\t\twith UnitOfWork(metadata, timeout=timeout) as uow:\n\t\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn uow_decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "index", "docstring": "Return the index of the given item.\n\n:param key: a key\n:return: index\n:rtype: int", "ques_id": "62e60e05d76274f8a4026cfd", "code": ["def index(self, key):\n\tif type(key) == int:\n\t\treturn key\n\telif type(key) == str:\n\t\treturn ord(key) - ord('a')\n\treturn key\n\n\n", "def index(self, key):\n\treturn self.keys.index(key)\n\t\n", "def index(self, key):\n\treturn self.__keys[key]\n\n", "def index(self, key):\n\treturn int(key)\n\n", "def index(self, key):\n\treturn key\n\n\n\n", "def index(self, key):\n\treturn key[0]\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSets the key in the hash table to the given value.\n\t\t\"\"\"\n\t\thashvalue = self.index(key)\n\n\t\tif self.slots[hashvalue] == None:\n\t\t\tself.slots[hashvalue] = [key, value]\n\t\t\tself.data[hashvalue] = value\n\n\t\telse:\n\t\t\tself.slots[hashvalue].append([key, value])\n\t\t\tself.data[hashvalue].append(value)\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tGets the value of the key and returns it.\n\t\t\"\"\"\n\t\thashvalue = self.index(key)\n\n\t\tif self.slots[hashvalue] == None:\n\t\t\treturn None\n\n\t\tfor i in self.slots[hashvalue]:\n\t\t\tif i[0] == key:\n\t\t\t\treturn i[1]\n\n\tdef __delitem__(self, key):\n\t\n", "def index(self, key):\n\treturn self.__getitem__(key)\n\n\n\t\n", "def index(self, key):\n\treturn int(key)\n\n", "def index(self, key):\n\treturn self.index_dic[key]\n\n", "def index(self, key):\n\treturn self.__indices[key]\n\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturns the string representation of the object.\n\t\"\"\"\n\treturn str(self.__indices)\n\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturns the representation of the object.\n\t\"\"\"\n\treturn repr(self.__indices)\n\n\n\tdef __eq__(self, other):\n\t\"\"\"\n\tReturns the result of equality test between two objects.\n\t\"\"\"\n\treturn self.__indices == other.__indices\n\n\n\tdef __ne__(self, other):\n\t\"\"\"\n\tReturns the result of inequality test between two objects.\n\t\"\"\"\n\treturn self.__indices != other.__indices\n\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturns the number of keys in the object.\n\t\"\"\"\n\treturn len(self.__indices)\n\n\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn self.__indices[key]\n\n\n\tdef __set\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def index(self, key):\n\tif type(key) == int:\n\t\treturn key\n\telif type(key) == str:\n\t\treturn ord(key) - ord('a')\n\treturn key\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.keys.index(key)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.__keys[key]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn int(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn key\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn key[0]\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSets the key in the hash table to the given value.\n\t\t\"\"\"\n\t\thashvalue = self.index(key)\n\n\t\tif self.slots[hashvalue] == None:\n\t\t\tself.slots[hashvalue] = [key, value]\n\t\t\tself.data[hashvalue] = value\n\n\t\telse:\n\t\t\tself.slots[hashvalue].append([key, value])\n\t\t\tself.data[hashvalue].append(value)\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tGets the value of the key and returns it.\n\t\t\"\"\"\n\t\thashvalue = self.index(key)\n\n\t\tif self.slots[hashvalue] == None:\n\t\t\treturn None\n\n\t\tfor i in self.slots[hashvalue]:\n\t\t\tif i[0] == key:\n\t\t\t\treturn i[1]\n\n\tdef __delitem__(self, key):\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.__getitem__(key)\n\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn int(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.index_dic[key]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def index(self, key):\n\treturn self.__indices[key]\n\n\n\tdef __str__(self):\n\t\"\"\"\n\tReturns the string representation of the object.\n\t\"\"\"\n\treturn str(self.__indices)\n\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturns the representation of the object.\n\t\"\"\"\n\treturn repr(self.__indices)\n\n\n\tdef __eq__(self, other):\n\t\"\"\"\n\tReturns the result of equality test between two objects.\n\t\"\"\"\n\treturn self.__indices == other.__indices\n\n\n\tdef __ne__(self, other):\n\t\"\"\"\n\tReturns the result of inequality test between two objects.\n\t\"\"\"\n\treturn self.__indices != other.__indices\n\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturns the number of keys in the object.\n\t\"\"\"\n\treturn len(self.__indices)\n\n\n\tdef __getitem__(self, key):\n\t\"\"\"\n\tReturns the key in the form of int.\n\t\"\"\"\n\treturn self.__indices[key]\n\n\n\tdef __set\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "values", "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list", "ques_id": "62e60da4d76274f8a4026cf1", "code": ["def values(self, *keys):\n\treturn [self.get(key) for key in keys]\n\t\n\t\n", "def values(self, *keys):\n\tif not keys:\n\t\treturn self.data.values()\n\t\t\n\tif isinstance(keys[0], list):\n\t\tkeys = keys[0]\n\t\t\n\tif isinstance(keys[0], dict):\n\t\tkeys = keys[0].keys()\n\t\t\n\tif isinstance(keys[0], str):\n\t\tkeys = [keys]\n\t\t\n\treturn [self.data[key] for key in keys]\n\t\n", "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n\tdef keys(self):\n\t\t\"\"\"\n\t\tReturns a list of the keys in the index.\n\t\t\"\"\"\n\t\treturn self.index\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tPrints the index.\n\t\t\"\"\"\n\t\treturn self.index.__str__()\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of the index.\n\t\t\"\"\"\n\t\treturn self.index.__len__()\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturns the value at the specified key.\n\t\t\"\"\"\n\t\treturn self.index.__getitem__(key)\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSets the value at the specified key.\n\t\t\"\"\"\n\t\tself.index.__setitem__(key, value)\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemoves the value at the specified key.\n\t\t\"\"\"\n\t\tself.index\n", "def values(self, *keys):\n\tif self.index:\n\t\treturn [self[key] for key in keys if key in self.index]\n\telse:\n\t\treturn [self[key] for key in keys]\n\n", "def values(self, *keys):\n\treturn [self[i] for i in self.index if i in keys]\n\n", "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n\tdef items(self, *keys):\n\t\t\"\"\"\n\t\tReturns the key/value pairs filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [(key, self[key]) for key in keys]\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of self.index.\n\t\t\"\"\"\n\t\treturn len(self.index)\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturns an iterator of self.index.\n\t\t\"\"\"\n\t\treturn iter(self.index)\n\n\tdef __reversed__(self):\n\t\t\"\"\"\n\t\tReturns an iterator of self.index in reverse order.\n\t\t\"\"\"\n\t\treturn reversed(self.index)\n\n\tdef __contains__(self, key):\n\t\t\"\"\"\n\t\tReturns True if key is in self.index.\n\t\t\"\"\"\n\t\treturn key in self.index\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturns a string representation of self.\n\t\t\"\"\"\n\t\n", "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n", "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n\n", "def values(self, *keys):\n\treturn [self.index[k] for k in keys]\n\n", "def values(self, *keys):\n\treturn self.filter(keys=keys)\n\n\tdef filter(self, **kwargs):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\tif kwargs.has_key('keys'):\n\t\t\treturn [self[key] for key in kwargs['keys']]\n\t\telse:\n\t\t\treturn [self[key] for key in self.keys()]\n\n\tdef get_by_keys(self, keys):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [self[key] for key in keys]\n\n\tdef get_by_index(self, index):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [self[key] for key in self.index[index]]\n\n\tdef get_by_index_keys(self, index, keys):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def values(self, *keys):\n\treturn [self.get(key) for key in keys]\n\t\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\tif not keys:\n\t\treturn self.data.values()\n\t\t\n\tif isinstance(keys[0], list):\n\t\tkeys = keys[0]\n\t\t\n\tif isinstance(keys[0], dict):\n\t\tkeys = keys[0].keys()\n\t\t\n\tif isinstance(keys[0], str):\n\t\tkeys = [keys]\n\t\t\n\treturn [self.data[key] for key in keys]\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n\tdef keys(self):\n\t\t\"\"\"\n\t\tReturns a list of the keys in the index.\n\t\t\"\"\"\n\t\treturn self.index\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tPrints the index.\n\t\t\"\"\"\n\t\treturn self.index.__str__()\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of the index.\n\t\t\"\"\"\n\t\treturn self.index.__len__()\n\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturns the value at the specified key.\n\t\t\"\"\"\n\t\treturn self.index.__getitem__(key)\n\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSets the value at the specified key.\n\t\t\"\"\"\n\t\tself.index.__setitem__(key, value)\n\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemoves the value at the specified key.\n\t\t\"\"\"\n\t\tself.index\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\tif self.index:\n\t\treturn [self[key] for key in keys if key in self.index]\n\telse:\n\t\treturn [self[key] for key in keys]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\treturn [self[i] for i in self.index if i in keys]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n\tdef items(self, *keys):\n\t\t\"\"\"\n\t\tReturns the key/value pairs filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [(key, self[key]) for key in keys]\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of self.index.\n\t\t\"\"\"\n\t\treturn len(self.index)\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturns an iterator of self.index.\n\t\t\"\"\"\n\t\treturn iter(self.index)\n\n\tdef __reversed__(self):\n\t\t\"\"\"\n\t\tReturns an iterator of self.index in reverse order.\n\t\t\"\"\"\n\t\treturn reversed(self.index)\n\n\tdef __contains__(self, key):\n\t\t\"\"\"\n\t\tReturns True if key is in self.index.\n\t\t\"\"\"\n\t\treturn key in self.index\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturns a string representation of self.\n\t\t\"\"\"\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\treturn [self[key] for key in keys]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\treturn [self.index[k] for k in keys]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def values(self, *keys):\n\treturn self.filter(keys=keys)\n\n\tdef filter(self, **kwargs):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\tif kwargs.has_key('keys'):\n\t\t\treturn [self[key] for key in kwargs['keys']]\n\t\telse:\n\t\t\treturn [self[key] for key in self.keys()]\n\n\tdef get_by_keys(self, keys):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [self[key] for key in keys]\n\n\tdef get_by_index(self, index):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\"\"\"\n\t\treturn [self[key] for key in self.index[index]]\n\n\tdef get_by_index_keys(self, index, keys):\n\t\t\"\"\"\n\t\tReturns the key filtered by self.index in the form of a list.\n\t\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_data.py", "project": "neo4j/neo4j-python-driver", "name": "data", "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified", "ques_id": "62e60b10d76274f8a4026ccd", "code": ["def data(self, *keys):\n\tdata = {}\n\tfor key in keys:\n\t\tdata[key] = self._data[key]\n\treturn data\n\n\tdef export(self, records, **kwargs):\n\t\t\"\"\"\n\t\tExports the records to a file or returns the string.\n\t\t\"\"\"\n\t\tself._keys = kwargs.get('fields', self._keys)\n\t\tself._data = self._export(records, **kwargs)\n\t\treturn self._data\n\n\tdef _export(self, records, **kwargs):\n\t\t\"\"\"\n\t\tExports the records to a file or returns the string.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef _transform(self, record, **kwargs):\n\t\t\"\"\"\n\t\tTransforms a record.\n\t\t\"\"\"\n\t\tdata = {}\n\t\tfor key in self._keys:\n\t\t\tdata[key] = getattr(record, key, None)\n\t\treturn data\n\n\n", "def data(self, *keys):\n\treturn self.get_data(*keys)\n\n\n", "def data(self, *keys):\n\tif len(keys) == 0:\n\t\treturn self.data_\n\telse:\n\t\treturn [self.data_[k] for k in keys]\n\n", "def data(self, *keys):\n\treturn self._data(*keys)\n\n", "def data(self, *keys):\n\tfor key in keys:\n\t\tyield self._data[key]\n\t\t\n", "def data(self, *keys):\n\treturn self._data(*keys)\n\n", "def data(self, *keys):\n\t\t\n\t\tif not self._data:\n\t\t\tself._data = self.record.export(self.exporter)\n\t\t\n\t\tif not keys:\n\t\t\treturn self._data\n\t\t\n\t\treturn [self._data[key] for key in keys]\n\t\t\n\t@property\n\tdef path(self):\n\t\t\"\"\"\n\t\tReturns the path of the file.\n\t\t\"\"\"\n\t\t\n\t\tif not self._path:\n\t\t\tself._path = self.exporter.path(self.data())\n\t\t\n\t\treturn self._path\n\t\t\n\t@property\n\tdef url(self):\n\t\t\"\"\"\n\t\tReturns the url of the file.\n\t\t\"\"\"\n\t\t\n\t\tif not self._url:\n\t\t\tself._url = self.exporter.url(self.data())\n\t\t\n\t\treturn self._url\n\t\t\n\t@property\n\tdef exists(self):\n\t\t\"\"\"\n\t\tReturns True if the file exists.\n\t\t\"\"\"\n\t\t\n\t\tif self._exists is\n", "def data(self, *keys):\n\treturn self.get_data(*keys)\n", "def data(self, *keys):\n\treturn self._data(*keys)\n\t\n", "def data(self, *keys):\n\treturn self.export_record.data(*keys)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def data(self, *keys):\n\tdata = {}\n\tfor key in keys:\n\t\tdata[key] = self._data[key]\n\treturn data\n\n\tdef export(self, records, **kwargs):\n\t\t\"\"\"\n\t\tExports the records to a file or returns the string.\n\t\t\"\"\"\n\t\tself._keys = kwargs.get('fields', self._keys)\n\t\tself._data = self._export(records, **kwargs)\n\t\treturn self._data\n\n\tdef _export(self, records, **kwargs):\n\t\t\"\"\"\n\t\tExports the records to a file or returns the string.\n\t\t\"\"\"\n\t\traise NotImplementedError\n\n\tdef _transform(self, record, **kwargs):\n\t\t\"\"\"\n\t\tTransforms a record.\n\t\t\"\"\"\n\t\tdata = {}\n\t\tfor key in self._keys:\n\t\t\tdata[key] = getattr(record, key, None)\n\t\treturn data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\treturn self.get_data(*keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\tif len(keys) == 0:\n\t\treturn self.data_\n\telse:\n\t\treturn [self.data_[k] for k in keys]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\treturn self._data(*keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\tfor key in keys:\n\t\tyield self._data[key]\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\treturn self._data(*keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\t\t\n\t\tif not self._data:\n\t\t\tself._data = self.record.export(self.exporter)\n\t\t\n\t\tif not keys:\n\t\t\treturn self._data\n\t\t\n\t\treturn [self._data[key] for key in keys]\n\t\t\n\t@property\n\tdef path(self):\n\t\t\"\"\"\n\t\tReturns the path of the file.\n\t\t\"\"\"\n\t\t\n\t\tif not self._path:\n\t\t\tself._path = self.exporter.path(self.data())\n\t\t\n\t\treturn self._path\n\t\t\n\t@property\n\tdef url(self):\n\t\t\"\"\"\n\t\tReturns the url of the file.\n\t\t\"\"\"\n\t\t\n\t\tif not self._url:\n\t\t\tself._url = self.exporter.url(self.data())\n\t\t\n\t\treturn self._url\n\t\t\n\t@property\n\tdef exists(self):\n\t\t\"\"\"\n\t\tReturns True if the file exists.\n\t\t\"\"\"\n\t\t\n\t\tif self._exists is\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\treturn self.get_data(*keys)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\treturn self._data(*keys)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def data(self, *keys):\n\treturn self.export_record.data(*keys)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_codec/packstream/v1/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "pop_u16", "docstring": "Remove the last two bytes of data, returning them as a big-endian\n16-bit unsigned integer.", "ques_id": "62e6087bd76274f8a4026bfa", "code": ["def pop_u16(self):\n\treturn (self.data.pop() << 8) + self.data.pop()\n\n", "def pop_u16(self):\n\treturn self.data.pop() << 8 | self.data.pop()\n\n", "def pop_u16(self):\n\treturn self.pop(2)\n\t\n\t\n", "def pop_u16(self):\n\treturn self.pop(2)\n\n", "def pop_u16(self):\n\treturn self.data.pop() & 0xff + ((self.data.pop() & 0xff) << 8)\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop() & 0xff + ((self.data.pop() & 0xff) << 8) + ((self.data.pop() & 0xff) << 16) + ((self.data.pop() & 0xff) << 24)\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop() & 0xff + ((self.data.pop() & 0xff) << 8) + ((self.data.pop() & 0xff) << 16) + ((self.data.pop() & 0xff) << 24) + ((self.data.pop() & 0xff) << 32) + ((self.data.pop() & 0xff) << 40) + ((self.data.pop() & 0xff) << 48) + ((self.data.pop() & 0xff) << 56\n", "def pop_u16(self):\n\treturn self.pop_n(2)\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\treturn self.pop_n(4)\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last 8 elements in self.data and return\n\t\"\"\"\n\treturn self.pop_n(8)\n\n\tdef pop_n(self, n):\n\t\"\"\"\n\tRemove the last n elements in self.data and return\n\t\"\"\"\n\tresult = self.data[-n:]\n\tself.data = self.data[:-n]\n\treturn result\n\n\tdef __repr__(self):\n\t\treturn '<Packet %s>' % (self.data.hex())\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\n", "def pop_u16(self):\n\treturn self.data.pop() << 8 | self.data.pop()\n\n", "def pop_u16(self):\n\tif not self.data:\n\t\traise IndexError(\"pop from empty list\")\n\treturn self.pop_n_u16(1)[0]\n\n\n", "def pop_u16(self):\n\treturn unpack(self.data.pop(), self.data.pop())\n\n", "def pop_u16(self):\n\tret = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn ret\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\tret = self.data[-4:]\n\tself.data = self.data[:-4]\n\treturn ret\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\tret = self.data[-8:]\n\tself.data = self.data[:-8]\n\treturn ret\n\n\tdef pop_varint(self):\n\t\"\"\"\n\tRemove the last variable-length int from self.data and return\n\t\"\"\"\n\tret = self.data[-1:]\n\tself.data = self.data[:-1]\n\treturn ret\n\n\tdef pop(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\tret = self.data[-1]\n\tself.data = self.data[:-1]\n\treturn ret\n\n\tdef push\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop_u16(self):\n\treturn (self.data.pop() << 8) + self.data.pop()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn self.data.pop() << 8 | self.data.pop()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn self.pop(2)\n\t\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn self.pop(2)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn self.data.pop() & 0xff + ((self.data.pop() & 0xff) << 8)\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop() & 0xff + ((self.data.pop() & 0xff) << 8) + ((self.data.pop() & 0xff) << 16) + ((self.data.pop() & 0xff) << 24)\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\treturn self.data.pop() & 0xff + ((self.data.pop() & 0xff) << 8) + ((self.data.pop() & 0xff) << 16) + ((self.data.pop() & 0xff) << 24) + ((self.data.pop() & 0xff) << 32) + ((self.data.pop() & 0xff) << 40) + ((self.data.pop() & 0xff) << 48) + ((self.data.pop() & 0xff) << 56\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn self.pop_n(2)\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\treturn self.pop_n(4)\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last 8 elements in self.data and return\n\t\"\"\"\n\treturn self.pop_n(8)\n\n\tdef pop_n(self, n):\n\t\"\"\"\n\tRemove the last n elements in self.data and return\n\t\"\"\"\n\tresult = self.data[-n:]\n\tself.data = self.data[:-n]\n\treturn result\n\n\tdef __repr__(self):\n\t\treturn '<Packet %s>' % (self.data.hex())\n\n\tdef __len__(self):\n\t\treturn len(self.data)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn self.data.pop() << 8 | self.data.pop()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\tif not self.data:\n\t\traise IndexError(\"pop from empty list\")\n\treturn self.pop_n_u16(1)[0]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\treturn unpack(self.data.pop(), self.data.pop())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop_u16(self):\n\tret = self.data[-2:]\n\tself.data = self.data[:-2]\n\treturn ret\n\n\tdef pop_u32(self):\n\t\"\"\"\n\tRemove the last four elements in self.data and return\n\t\"\"\"\n\tret = self.data[-4:]\n\tself.data = self.data[:-4]\n\treturn ret\n\n\tdef pop_u64(self):\n\t\"\"\"\n\tRemove the last eight elements in self.data and return\n\t\"\"\"\n\tret = self.data[-8:]\n\tself.data = self.data[:-8]\n\treturn ret\n\n\tdef pop_varint(self):\n\t\"\"\"\n\tRemove the last variable-length int from self.data and return\n\t\"\"\"\n\tret = self.data[-1:]\n\tself.data = self.data[:-1]\n\treturn ret\n\n\tdef pop(self):\n\t\"\"\"\n\tRemove the last element in self.data and return\n\t\"\"\"\n\tret = self.data[-1]\n\tself.data = self.data[:-1]\n\treturn ret\n\n\tdef push\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "discard", "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object", "ques_id": "62e6087ad76274f8a4026bf2", "code": ["def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tmsg = Message(\"DISCARD\", n, qid)\n\treturn self.send_and_receive(msg, dehydration_hooks, hydration_hooks, **handlers)\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self.append(\n\t\tself.create_message(\n\t\t\tDISCARD, n, qid, dehydration_hooks, hydration_hooks, **handlers))\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self._add_message(Message.DISCARD(n, qid),\n\t                         dehydration_hooks=dehydration_hooks,\n\t                         hydration_hooks=hydration_hooks, **handlers)\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tr = self._new_response(n, qid, dehydration_hooks, hydration_hooks, **handlers)\n\tr.discard()\n\treturn r\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tout = self._out\n\tout.append(struct.pack('>B', 0x2f))\n\tout.append(pack_query_id(qid))\n\tout.append(pack_int(n))\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tm = Message(Message.DISCARD,\n\t\t\t\tn=n,\n\t\t\t\tqid=qid,\n\t\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\t\thydration_hooks=hydration_hooks,\n\t\t\t\t**handlers)\n\tself.enqueue(m)\n\treturn m\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tmsg = packstream.PackStream.pack(\n\t\t{\n\t\t\t\"op\": \"DISCARD\",\n\t\t\t\"qid\": qid,\n\t\t\t\"n\": n,\n\t\t}\n\t)\n\treturn self._send_message(\n\t\tmsg,\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers\n\t)\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tm = self.message(Message.DISCARD, qid, n)\n\treturn self.response(m, dehydration_hooks, hydration_hooks, **handlers)\n\n    def pull_all(self, qid=-1, dehydration_hooks=None, hydration_hooks=None,\n                 **handlers):\n\t\"\"\"\n\tAppends a PULL_ALL message to the output queue.\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tdef _discard_hook(value, dehydration_hooks):\n\t\treturn None\n\n\tresponse = self._send_message(\n\t\tmessage_type=message.DISCARD,\n\t\tmessage_body=message.MessageBody(\n\t\t\tn=n,\n\t\t\tqid=qid\n\t\t),\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers\n\t)\n\tresponse.on_next_hooks.append(\n\t\tpartial(_discard_hook, dehydration_hooks=dehydration_hooks)\n\t)\n\treturn response\n\n\n", "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tif qid < 0:\n\t\tqid = self._last_query\n\tself.push(message.Discard(qid),\n                  dehydration_hooks=dehydration_hooks,\n                  hydration_hooks=hydration_hooks,\n                  **handlers)\n\treturn Response(self, **handlers)\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tmsg = Message(\"DISCARD\", n, qid)\n\treturn self.send_and_receive(msg, dehydration_hooks, hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self.append(\n\t\tself.create_message(\n\t\t\tDISCARD, n, qid, dehydration_hooks, hydration_hooks, **handlers))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\treturn self._add_message(Message.DISCARD(n, qid),\n\t                         dehydration_hooks=dehydration_hooks,\n\t                         hydration_hooks=hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tr = self._new_response(n, qid, dehydration_hooks, hydration_hooks, **handlers)\n\tr.discard()\n\treturn r\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tout = self._out\n\tout.append(struct.pack('>B', 0x2f))\n\tout.append(pack_query_id(qid))\n\tout.append(pack_int(n))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tm = Message(Message.DISCARD,\n\t\t\t\tn=n,\n\t\t\t\tqid=qid,\n\t\t\t\tdehydration_hooks=dehydration_hooks,\n\t\t\t\thydration_hooks=hydration_hooks,\n\t\t\t\t**handlers)\n\tself.enqueue(m)\n\treturn m\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tmsg = packstream.PackStream.pack(\n\t\t{\n\t\t\t\"op\": \"DISCARD\",\n\t\t\t\"qid\": qid,\n\t\t\t\"n\": n,\n\t\t}\n\t)\n\treturn self._send_message(\n\t\tmsg,\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tm = self.message(Message.DISCARD, qid, n)\n\treturn self.response(m, dehydration_hooks, hydration_hooks, **handlers)\n\n    def pull_all(self, qid=-1, dehydration_hooks=None, hydration_hooks=None,\n                 **handlers):\n\t\"\"\"\n\tAppends a PULL_ALL message to the output queue.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tdef _discard_hook(value, dehydration_hooks):\n\t\treturn None\n\n\tresponse = self._send_message(\n\t\tmessage_type=message.DISCARD,\n\t\tmessage_body=message.MessageBody(\n\t\t\tn=n,\n\t\t\tqid=qid\n\t\t),\n\t\tdehydration_hooks=dehydration_hooks,\n\t\thydration_hooks=hydration_hooks,\n\t\t**handlers\n\t)\n\tresponse.on_next_hooks.append(\n\t\tpartial(_discard_hook, dehydration_hooks=dehydration_hooks)\n\t)\n\treturn response\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n\tif qid < 0:\n\t\tqid = self._last_query\n\tself.push(message.Discard(qid),\n                  dehydration_hooks=dehydration_hooks,\n                  hydration_hooks=hydration_hooks,\n                  **handlers)\n\treturn Response(self, **handlers)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_async/io/_bolt3.py", "project": "neo4j/neo4j-python-driver", "name": "begin", "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object", "ques_id": "62e60879d76274f8a4026bec", "code": ["def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._request(BEGIN, mode, bookmarks, metadata, timeout,\n\t                     db=db, imp_user=imp_user,\n\t                     dehydration_hooks=dehydration_hooks,\n\t                     hydration_hooks=hydration_hooks, **handlers)\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tself._handler_check(handlers)\n\trecord = _Record()\n\tresponse = Response(record, self._connection, self._log,\n\t                    self._encryption_handshake_complete,\n\t                    self._handshake_complete,\n\t                    self._metadata_received,\n\t                    self._release,\n\t                    self._loop,\n\t                    self._run_in_executor,\n\t                    self._dehydration_hooks,\n\t                    self._hydration_hooks,\n\t                    **handlers)\n\tself._begin(record, mode, bookmarks, metadata, timeout, db, imp_user)\n\treturn response\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._enqueue(BEGIN, mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t\tdehydration_hooks, hydration_hooks, **handlers)\n\n\tdef commit(self, **handlers):\n\t\"\"\"\n\tAppends a COMMIT message to the output queue.\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t# TODO: fix the docstring.\n\t# TODO: If not already connected, we should connect here.\n\t# TODO: If already connected, we should check that we are in a closed state.\n\t# TODO: We should check if metadata is a dict.\n\t# TODO: We should check if mode is one of \"READ\" or \"WRITE\"\n\t# TODO: We should check if timeout is a number.\n\t# TODO: We should check if db is a string.\n\t# TODO: We should check if imp_user is a string.\n\t# TODO: We should check if dehydration_hooks is a dict.\n\t# TODO: We should check if hydration_hooks is a dict.\n\n\t# TODO: We should raise an error if we are already running a transaction.\n\t# TODO: We should raise an error if we are already running a session.\n\t# TODO: We should raise an error if we are already running a request.\n\t# TODO: We should raise an error if we are not connected.\n\n\tif bookmarks is None:\n\t\tbookmarks = []\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif self._tx_open:\n\t\traise ProtocolError(\"Cannot begin a transaction while another \"\n\t\t\t\t\t\t\t\"transaction is open.\")\n\tif not isinstance(mode, str):\n\t\traise TypeError(\"mode must be a string\")\n\tif not mode in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\"mode must be either READ or WRITE\")\n\tif bookmarks is not None:\n\t\tif not isinstance(bookmarks, collections.abc.Iterable):\n\t\t\traise TypeError(\"bookmarks must be an iterable\")\n\t\tfor bookmark in bookmarks:\n\t\t\tif not isinstance(bookmark, str):\n\t\t\t\traise TypeError(\"bookmark values must be strings\")\n\tif metadata is not None:\n\t\tif not isinstance(metadata, dict):\n\t\t\traise TypeError(\"metadata must be a dict\")\n\t\tfor key, value in metadata.items():\n\t\t\tif not isinstance(key, str):\n\t\t\t\traise TypeError(\"metadata keys must be strings\")\n\t\t\tif not isinstance(value,\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tmessage = {}\n\tif mode is not None:\n\t\tmessage[\"mode\"] = mode\n\tif bookmarks is not None:\n\t\tmessage[\"bookmarks\"] = list(bookmarks)\n\tif metadata is not None:\n\t\tmessage[\"txMetadata\"] = metadata\n\tif timeout is not None:\n\t\tmessage[\"txTimeout\"] = timeout\n\tif db is not None:\n\t\tmessage[\"db\"] = db\n\tif imp_user is not None:\n\t\tmessage[\"imp_user\"] = imp_user\n\tself._append(BEGIN, message, dehydration_hooks=dehydration_hooks,\n\t             hydration_hooks=hydration_hooks, **handlers)\n\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tself.add_message(Message(BEGIN, {\n\t\t\"mode\": mode,\n\t\t\"bookmarks\": bookmarks,\n\t\t\"metadata\": metadata,\n\t\t\"timeout\": timeout,\n\t\t\"db\": db,\n\t\t\"imp_user\": imp_user,\n\t\t\"dehydration_hooks\": dehydration_hooks,\n\t\t\"hydration_hooks\": hydration_hooks,\n\t}, handlers))\n\treturn self.response(**handlers)\n\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif db is not None and not self._driver._protocol_version.has_db_begin():\n\t\traise BoltHandshakeError(\n\t\t\t\"Driver protocol version does not support explicit database selection\")\n\tif imp_user is not None and not self._driver._protocol_version.has_imp_user():\n\t\traise BoltHandshakeError(\n\t\t\t\"Driver protocol version does not support the imp_user parameter\")\n\tif dehydration_hooks is not None and hydration_hooks is not None:\n\t\traise ValueError(\"Dehydration and hydration hooks cannot be set at the same time\")\n\n\tif dehydration_hooks is not None:\n\t\tif not self._driver._protocol_version.has_dehydration_hooks():\n\t\t\traise BoltHandshakeError(\n\t\t\t\t\"Driver protocol version does not support dehydration hooks\")\n\t\thandlers['dehydration_hooks'] = dehydration_hooks\n\n\tif hydration_hooks is not None:\n\t\tif not self._driver._protocol_version.has_hydration_hooks():\n\t\t\traise\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif not self.connected:\n\t\traise ServiceUnavailable(\"Cannot begin: Not connected to server\")\n\tif self.transaction:\n\t\traise ProtocolError(\"Cannot begin: Transaction already open\")\n\tif not mode:\n\t\tmode = \"WRITE\"\n\tif mode not in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\"Invalid transaction mode: %r\" % mode)\n\tif metadata is not None and not isinstance(metadata, dict):\n\t\traise TypeError(\"metadata must be dict or None\")\n\tif not bookmarks:\n\t\tbookmarks = []\n\tif not isinstance(bookmarks, collections.Iterable):\n\t\traise TypeError(\"bookmarks must be iterable or None\")\n\tif timeout is not None and not isinstance(timeout, numbers.Real):\n\t\traise TypeError(\"timeout must be number or None\")\n\tif db is not None and not isinstance(db, str):\n\t\traise TypeError(\"db must be str or None\")\n\tif imp_user is not None and not isinstance(imp_user, str):\n\t\traise TypeError(\"\n", "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif self._mode is not None:\n\t\traise BoltException(\"Cannot BEGIN a transaction while another \"\n\t\t\t\t\t\t\t\"transaction is in progress\")\n\tif self._tx_response is not None:\n\t\traise BoltException(\"Cannot BEGIN a transaction while a previous \"\n\t\t\t\t\t\t\t\"transaction is still in progress\")\n\tself._mode = mode\n\tself._db = db\n\tself._imp_user = imp_user\n\tself._dehydration_hooks = dehydration_hooks\n\tself._hydration_hooks = hydration_hooks\n\tfor k, v in handlers.items():\n\t\tself._handlers[k] = v\n\tif metadata is None:\n\t\tmetadata = {}\n\tif bookmarks is None:\n\t\tbookmarks = []\n\tif timeout is None:\n\t\ttimeout = 0\n\t# TODO: should we have a default mode?\n\tself._queue.append(Message(BEGIN, (mode, bookmarks, metadata, timeout, db, imp_user)))\n\tself._tx_response = Response(self\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._request(BEGIN, mode, bookmarks, metadata, timeout,\n\t                     db=db, imp_user=imp_user,\n\t                     dehydration_hooks=dehydration_hooks,\n\t                     hydration_hooks=hydration_hooks, **handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tself._handler_check(handlers)\n\trecord = _Record()\n\tresponse = Response(record, self._connection, self._log,\n\t                    self._encryption_handshake_complete,\n\t                    self._handshake_complete,\n\t                    self._metadata_received,\n\t                    self._release,\n\t                    self._loop,\n\t                    self._run_in_executor,\n\t                    self._dehydration_hooks,\n\t                    self._hydration_hooks,\n\t                    **handlers)\n\tself._begin(record, mode, bookmarks, metadata, timeout, db, imp_user)\n\treturn response\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\treturn self._enqueue(BEGIN, mode, bookmarks, metadata, timeout, db, imp_user,\n\t\t\tdehydration_hooks, hydration_hooks, **handlers)\n\n\tdef commit(self, **handlers):\n\t\"\"\"\n\tAppends a COMMIT message to the output queue.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\t# TODO: fix the docstring.\n\t# TODO: If not already connected, we should connect here.\n\t# TODO: If already connected, we should check that we are in a closed state.\n\t# TODO: We should check if metadata is a dict.\n\t# TODO: We should check if mode is one of \"READ\" or \"WRITE\"\n\t# TODO: We should check if timeout is a number.\n\t# TODO: We should check if db is a string.\n\t# TODO: We should check if imp_user is a string.\n\t# TODO: We should check if dehydration_hooks is a dict.\n\t# TODO: We should check if hydration_hooks is a dict.\n\n\t# TODO: We should raise an error if we are already running a transaction.\n\t# TODO: We should raise an error if we are already running a session.\n\t# TODO: We should raise an error if we are already running a request.\n\t# TODO: We should raise an error if we are not connected.\n\n\tif bookmarks is None:\n\t\tbookmarks = []\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif self._tx_open:\n\t\traise ProtocolError(\"Cannot begin a transaction while another \"\n\t\t\t\t\t\t\t\"transaction is open.\")\n\tif not isinstance(mode, str):\n\t\traise TypeError(\"mode must be a string\")\n\tif not mode in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\"mode must be either READ or WRITE\")\n\tif bookmarks is not None:\n\t\tif not isinstance(bookmarks, collections.abc.Iterable):\n\t\t\traise TypeError(\"bookmarks must be an iterable\")\n\t\tfor bookmark in bookmarks:\n\t\t\tif not isinstance(bookmark, str):\n\t\t\t\traise TypeError(\"bookmark values must be strings\")\n\tif metadata is not None:\n\t\tif not isinstance(metadata, dict):\n\t\t\traise TypeError(\"metadata must be a dict\")\n\t\tfor key, value in metadata.items():\n\t\t\tif not isinstance(key, str):\n\t\t\t\traise TypeError(\"metadata keys must be strings\")\n\t\t\tif not isinstance(value,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tmessage = {}\n\tif mode is not None:\n\t\tmessage[\"mode\"] = mode\n\tif bookmarks is not None:\n\t\tmessage[\"bookmarks\"] = list(bookmarks)\n\tif metadata is not None:\n\t\tmessage[\"txMetadata\"] = metadata\n\tif timeout is not None:\n\t\tmessage[\"txTimeout\"] = timeout\n\tif db is not None:\n\t\tmessage[\"db\"] = db\n\tif imp_user is not None:\n\t\tmessage[\"imp_user\"] = imp_user\n\tself._append(BEGIN, message, dehydration_hooks=dehydration_hooks,\n\t             hydration_hooks=hydration_hooks, **handlers)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tself.add_message(Message(BEGIN, {\n\t\t\"mode\": mode,\n\t\t\"bookmarks\": bookmarks,\n\t\t\"metadata\": metadata,\n\t\t\"timeout\": timeout,\n\t\t\"db\": db,\n\t\t\"imp_user\": imp_user,\n\t\t\"dehydration_hooks\": dehydration_hooks,\n\t\t\"hydration_hooks\": hydration_hooks,\n\t}, handlers))\n\treturn self.response(**handlers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif db is not None and not self._driver._protocol_version.has_db_begin():\n\t\traise BoltHandshakeError(\n\t\t\t\"Driver protocol version does not support explicit database selection\")\n\tif imp_user is not None and not self._driver._protocol_version.has_imp_user():\n\t\traise BoltHandshakeError(\n\t\t\t\"Driver protocol version does not support the imp_user parameter\")\n\tif dehydration_hooks is not None and hydration_hooks is not None:\n\t\traise ValueError(\"Dehydration and hydration hooks cannot be set at the same time\")\n\n\tif dehydration_hooks is not None:\n\t\tif not self._driver._protocol_version.has_dehydration_hooks():\n\t\t\traise BoltHandshakeError(\n\t\t\t\t\"Driver protocol version does not support dehydration hooks\")\n\t\thandlers['dehydration_hooks'] = dehydration_hooks\n\n\tif hydration_hooks is not None:\n\t\tif not self._driver._protocol_version.has_hydration_hooks():\n\t\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif not self.connected:\n\t\traise ServiceUnavailable(\"Cannot begin: Not connected to server\")\n\tif self.transaction:\n\t\traise ProtocolError(\"Cannot begin: Transaction already open\")\n\tif not mode:\n\t\tmode = \"WRITE\"\n\tif mode not in (\"READ\", \"WRITE\"):\n\t\traise ValueError(\"Invalid transaction mode: %r\" % mode)\n\tif metadata is not None and not isinstance(metadata, dict):\n\t\traise TypeError(\"metadata must be dict or None\")\n\tif not bookmarks:\n\t\tbookmarks = []\n\tif not isinstance(bookmarks, collections.Iterable):\n\t\traise TypeError(\"bookmarks must be iterable or None\")\n\tif timeout is not None and not isinstance(timeout, numbers.Real):\n\t\traise TypeError(\"timeout must be number or None\")\n\tif db is not None and not isinstance(db, str):\n\t\traise TypeError(\"db must be str or None\")\n\tif imp_user is not None and not isinstance(imp_user, str):\n\t\traise TypeError(\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n\tif self._mode is not None:\n\t\traise BoltException(\"Cannot BEGIN a transaction while another \"\n\t\t\t\t\t\t\t\"transaction is in progress\")\n\tif self._tx_response is not None:\n\t\traise BoltException(\"Cannot BEGIN a transaction while a previous \"\n\t\t\t\t\t\t\t\"transaction is still in progress\")\n\tself._mode = mode\n\tself._db = db\n\tself._imp_user = imp_user\n\tself._dehydration_hooks = dehydration_hooks\n\tself._hydration_hooks = hydration_hooks\n\tfor k, v in handlers.items():\n\t\tself._handlers[k] = v\n\tif metadata is None:\n\t\tmetadata = {}\n\tif bookmarks is None:\n\t\tbookmarks = []\n\tif timeout is None:\n\t\ttimeout = 0\n\t# TODO: should we have a default mode?\n\tself._queue.append(Message(BEGIN, (mode, bookmarks, metadata, timeout, db, imp_user)))\n\tself._tx_response = Response(self\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/_arithmetic.py", "project": "neo4j/neo4j-python-driver", "name": "round_half_to_even", "docstring": ">>> round_half_to_even(3)\n    3\n    >>> round_half_to_even(3.2)\n    3\n    >>> round_half_to_even(3.5)\n    4\n    >>> round_half_to_even(3.7)\n    4\n    >>> round_half_to_even(4)\n    4\n    >>> round_half_to_even(4.2)\n    4\n    >>> round_half_to_even(4.5)\n    4\n    >>> round_half_to_even(4.7)\n    5\n\n:param n:\n:return:", "ques_id": "62e60723d76274f8a4026b75", "code": ["def round_half_to_even(n):\n\tif math.isnan(n):\n\t\treturn n\n\tif n > 0:\n\t\treturn math.floor(n + 0.5)\n\tif n < 0:\n\t\treturn math.ceil(n - 0.5)\n\treturn 0\n\n", "def round_half_to_even(n):\n\treturn round(n*2)/2\n\n", "def round_half_to_even(n):\n\treturn round(n * 2) / 2\n\n", "def round_half_to_even(n):\n\treturn round(n, 0)\n\n", "def round_half_to_even(n):\n\tif n - int(n) < 0.5:\n\t\treturn int(n)\n\treturn int(n+1)\n\n", "def round_half_to_even(n):\n\treturn round(n * 2) / 2\n\n", "def round_half_to_even(n):\n\tn = int(2 * n) / 2.0\n\treturn n\n\n", "def round_half_to_even(n):\n\trounded = round(n * 2) / 2\n\treturn rounded\n\n", "def round_half_to_even(n):\n\tn = round(n)\n\tif n % 2 == 0:\n\t\treturn int(n)\n\treturn int(n + 1)\n\n\n", "def round_half_to_even(n):\n\treturn round(n*2)/2\n"], "level": "self_contained", "generate_results": [{"generate_code": "def round_half_to_even(n):\n\tif math.isnan(n):\n\t\treturn n\n\tif n > 0:\n\t\treturn math.floor(n + 0.5)\n\tif n < 0:\n\t\treturn math.ceil(n - 0.5)\n\treturn 0\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n*2)/2\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n * 2) / 2\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n, 0)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\tif n - int(n) < 0.5:\n\t\treturn int(n)\n\treturn int(n+1)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n * 2) / 2\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\tn = int(2 * n) / 2.0\n\treturn n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\trounded = round(n * 2) / 2\n\treturn rounded\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def round_half_to_even(n):\n\tn = round(n)\n\tif n % 2 == 0:\n\t\treturn int(n)\n\treturn int(n + 1)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def round_half_to_even(n):\n\treturn round(n*2)/2\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/_spatial/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "point_type", "docstring": "Dynamically create a Point subclass.", "ques_id": "62e60707d76274f8a4026b69", "code": ["def point_type(name, fields, srid_map):\n\tclass Point(object):\n\t\t__slots__ = ['_id'] + list(fields)\n\n\t\t_types = {'geom': GEOSGeometry}\n\t\t_name = name\n\t\t_fields = fields\n\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tif len(args) > len(self._fields):\n\t\t\t\traise TypeError('Expected {} arguments'.format(len(self._fields)))\n\n\t\t\t# Set all of the positional arguments\n\t\t\tfor name, value in zip(self._fields, args):\n\t\t\t\tsetattr(self, name, value)\n\n\t\t\t# Set the remaining keyword arguments (if any)\n\t\t\tfor name in self._fields[len(args):]:\n\t\t\t\tsetattr(self, name, kwargs.pop(name))\n\n\t\t\t# Check for any remaining unknown arguments\n\t\t\tif kwargs:\n\t\t\t\traise TypeError('Invalid argument(s): {}'.format(','.join(kwargs)))\n\n\t\tdef __repr__(self):\n\n", "def point_type(name, fields, srid_map):\n\tdef __init__(self, *args, **kwargs):\n\t\tfor n, arg in enumerate(args):\n\t\t\tsetattr(self, fields[n], arg)\n\n\t\tfor field, value in kwargs.items():\n\t\t\tsetattr(self, field, value)\n\n\tdef __getitem__(self, key):\n\t\treturn getattr(self, key)\n\n\tdef __iter__(self):\n\t\tfor field in fields:\n\t\t\tyield getattr(self, field)\n\n\tdef __repr__(self):\n\t\treturn '%s(%s)' % (name, ', '.join('%s=%r' % (field, getattr(self, field)) for field in fields))\n\n\tdef __str__(self):\n\t\treturn ', '.join('%s=%r' % (field, getattr(self, field)) for field in fields)\n\n\tdef __eq__(self, other):\n\t\treturn isinstance(other, self.__class__) and all(getattr(self, field) == getattr(other, field)\n", "def point_type(name, fields, srid_map):\n\tsrid_map = srid_map\n\tfields = fields\n\tclass Point(object):\n\t\tdef __init__(self, **kwargs):\n\t\t\tself.name = name\n\t\t\tself.srid_map = srid_map\n\t\t\tfor field in self.fields:\n\t\t\t\tsetattr(self, field, kwargs.get(field, None))\n\t\t\t\tif field == 'srid':\n\t\t\t\t\tsetattr(self, field, kwargs.get(field, 4326))\n\t\tdef __repr__(self):\n\t\t\treturn '<%s: %s>' % (self.name, ', '.join(['%s=%s' % (k, getattr(self, k)) for k in self.fields]))\n\tPoint.fields = fields\n\tPoint.srid_map = srid_map\n\treturn Point\n\n", "def point_type(name, fields, srid_map):\n\tfield_list = []\n\tfor field in fields:\n\t\tfield_list.append(field.name)\n\t\tif field.name in srid_map:\n\t\t\tsrid = srid_map[field.name]\n\t\telse:\n\t\t\tsrid = srid_map['default']\n\t\tfield.srid = srid\n\tpoint_class = type(name, (Point,), dict(fields=field_list))\n\treturn point_class\n\n\n", "def point_type(name, fields, srid_map):\n\tclass Point(object):\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tif len(kwargs) > 0:\n\t\t\t\tfor key in kwargs:\n\t\t\t\t\tsetattr(self, key, kwargs[key])\n\t\t\telif len(args) > 0:\n\t\t\t\tfor i in range(len(fields)):\n\t\t\t\t\tsetattr(self, fields[i], args[i])\n\n\t\tdef __repr__(self):\n\t\t\treturn '<%s: %s>' % (self.__class__.__name__, self.__str__())\n\n\t\tdef __str__(self):\n\t\t\treturn '%s(%s)' % (self.__class__.__name__, ', '.join(['%s=%s' % (f, getattr(self, f)) for f in fields]))\n\n\t\tdef get_point(self):\n\t\t\t\"\"\"\n\t\t\tReturn the point in the appropriate format\n\t\t\t\"\"\"\n\t\t\treturn self.\n", "def point_type(name, fields, srid_map):\n\tattrs = {\n\t\t'__slots__': tuple(fields),\n\t\t'__init__': _make_init(fields),\n\t\t'__repr__': _make_repr(name, fields),\n\t\t'__eq__': _make_eq(fields),\n\t\t'__iter__': _make_iter(fields),\n\t\t'__getitem__': _make_getitem(fields),\n\t\t'__len__': _make_len(fields),\n\t\t'__contains__': _make_contains(fields),\n\t\t'__getattr__': _make_getattr(fields),\n\t\t'__setattr__': _make_setattr(fields),\n\t\t'__delattr__': _make_delattr(fields),\n\t\t'__add__': _make_add(fields),\n\t\t'__radd__': _make_add(fields),\n\t\t'__iadd__': _make_iadd(fields),\n\t\t'__sub__': _make_sub(fields),\n\t\t'__rsub__': _make_\n", "def point_type(name, fields, srid_map):\n\treturn type(name, (Point,), {'srid_map': srid_map, 'fields': fields})\n\n\n", "def point_type(name, fields, srid_map):\n\tfields = [('srid', 'i2', srid_map[name])] + fields\n\n\tclass Point(Structure):\n\t\t_fields_ = fields\n\n\t\tdef __str__(self):\n\t\t\treturn '%s%s' % (self.__class__.__name__, tuple(self))\n\n\t\tdef __repr__(self):\n\t\t\treturn '%s(%s)' % (self.__class__.__name__, repr(tuple(self)))\n\n\t\tdef __getattr__(self, name):\n\t\t\tif name == 'x':\n\t\t\t\treturn self.x\n\t\t\telif name == 'y':\n\t\t\t\treturn self.y\n\t\t\telse:\n\t\t\t\traise AttributeError(name)\n\n\t\tdef __setattr__(self, name, value):\n\t\t\tif name == 'x':\n\t\t\t\tself.x = value\n\t\t\telif name == 'y':\n\t\t\t\tself.y = value\n\t\t\telse:\n\t\t\t\traise Attribute\n", "def point_type(name, fields, srid_map):\n\t\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\t\n\t\tdef __init__(self, *args):\n\t\t\tfor name, val in zip(self.__slots__, args):\n\t\t\t\tsetattr(self, name, val)\n\t\t\t\t\n\t\t@property\n\t\tdef wkt(self):\n\t\t\treturn \"POINT({} {})\".format(*self)\n\t\t\t\n\t\tdef __repr__(self):\n\t\t\treturn \"{}({})\".format(name, \", \".join(map(repr, self)))\n\t\t\t\n\t\tdef __iter__(self):\n\t\t\treturn (getattr(self, field) for field in self.__slots__)\n\t\t\t\n\t\tdef __getitem__(self, index):\n\t\t\treturn getattr(self, self.__slots__[index])\n\t\t\t\n\t\tdef __len__(self):\n\t\t\treturn len(self.__slots__)\n\t\t\t\n\t@property\n\tdef srid\n", "def point_type(name, fields, srid_map):\n\tfields_string = \",\".join(fields)\n\treturn type(name, (Point,), {\"__init__\": lambda self, x, y, srid: Point.__init__(self, x, y, srid_map[srid])})\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def point_type(name, fields, srid_map):\n\tclass Point(object):\n\t\t__slots__ = ['_id'] + list(fields)\n\n\t\t_types = {'geom': GEOSGeometry}\n\t\t_name = name\n\t\t_fields = fields\n\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tif len(args) > len(self._fields):\n\t\t\t\traise TypeError('Expected {} arguments'.format(len(self._fields)))\n\n\t\t\t# Set all of the positional arguments\n\t\t\tfor name, value in zip(self._fields, args):\n\t\t\t\tsetattr(self, name, value)\n\n\t\t\t# Set the remaining keyword arguments (if any)\n\t\t\tfor name in self._fields[len(args):]:\n\t\t\t\tsetattr(self, name, kwargs.pop(name))\n\n\t\t\t# Check for any remaining unknown arguments\n\t\t\tif kwargs:\n\t\t\t\traise TypeError('Invalid argument(s): {}'.format(','.join(kwargs)))\n\n\t\tdef __repr__(self):\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tdef __init__(self, *args, **kwargs):\n\t\tfor n, arg in enumerate(args):\n\t\t\tsetattr(self, fields[n], arg)\n\n\t\tfor field, value in kwargs.items():\n\t\t\tsetattr(self, field, value)\n\n\tdef __getitem__(self, key):\n\t\treturn getattr(self, key)\n\n\tdef __iter__(self):\n\t\tfor field in fields:\n\t\t\tyield getattr(self, field)\n\n\tdef __repr__(self):\n\t\treturn '%s(%s)' % (name, ', '.join('%s=%r' % (field, getattr(self, field)) for field in fields))\n\n\tdef __str__(self):\n\t\treturn ', '.join('%s=%r' % (field, getattr(self, field)) for field in fields)\n\n\tdef __eq__(self, other):\n\t\treturn isinstance(other, self.__class__) and all(getattr(self, field) == getattr(other, field)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tsrid_map = srid_map\n\tfields = fields\n\tclass Point(object):\n\t\tdef __init__(self, **kwargs):\n\t\t\tself.name = name\n\t\t\tself.srid_map = srid_map\n\t\t\tfor field in self.fields:\n\t\t\t\tsetattr(self, field, kwargs.get(field, None))\n\t\t\t\tif field == 'srid':\n\t\t\t\t\tsetattr(self, field, kwargs.get(field, 4326))\n\t\tdef __repr__(self):\n\t\t\treturn '<%s: %s>' % (self.name, ', '.join(['%s=%s' % (k, getattr(self, k)) for k in self.fields]))\n\tPoint.fields = fields\n\tPoint.srid_map = srid_map\n\treturn Point\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tfield_list = []\n\tfor field in fields:\n\t\tfield_list.append(field.name)\n\t\tif field.name in srid_map:\n\t\t\tsrid = srid_map[field.name]\n\t\telse:\n\t\t\tsrid = srid_map['default']\n\t\tfield.srid = srid\n\tpoint_class = type(name, (Point,), dict(fields=field_list))\n\treturn point_class\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tclass Point(object):\n\t\tdef __init__(self, *args, **kwargs):\n\t\t\tif len(kwargs) > 0:\n\t\t\t\tfor key in kwargs:\n\t\t\t\t\tsetattr(self, key, kwargs[key])\n\t\t\telif len(args) > 0:\n\t\t\t\tfor i in range(len(fields)):\n\t\t\t\t\tsetattr(self, fields[i], args[i])\n\n\t\tdef __repr__(self):\n\t\t\treturn '<%s: %s>' % (self.__class__.__name__, self.__str__())\n\n\t\tdef __str__(self):\n\t\t\treturn '%s(%s)' % (self.__class__.__name__, ', '.join(['%s=%s' % (f, getattr(self, f)) for f in fields]))\n\n\t\tdef get_point(self):\n\t\t\t\"\"\"\n\t\t\tReturn the point in the appropriate format\n\t\t\t\"\"\"\n\t\t\treturn self.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tattrs = {\n\t\t'__slots__': tuple(fields),\n\t\t'__init__': _make_init(fields),\n\t\t'__repr__': _make_repr(name, fields),\n\t\t'__eq__': _make_eq(fields),\n\t\t'__iter__': _make_iter(fields),\n\t\t'__getitem__': _make_getitem(fields),\n\t\t'__len__': _make_len(fields),\n\t\t'__contains__': _make_contains(fields),\n\t\t'__getattr__': _make_getattr(fields),\n\t\t'__setattr__': _make_setattr(fields),\n\t\t'__delattr__': _make_delattr(fields),\n\t\t'__add__': _make_add(fields),\n\t\t'__radd__': _make_add(fields),\n\t\t'__iadd__': _make_iadd(fields),\n\t\t'__sub__': _make_sub(fields),\n\t\t'__rsub__': _make_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\treturn type(name, (Point,), {'srid_map': srid_map, 'fields': fields})\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def point_type(name, fields, srid_map):\n\tfields = [('srid', 'i2', srid_map[name])] + fields\n\n\tclass Point(Structure):\n\t\t_fields_ = fields\n\n\t\tdef __str__(self):\n\t\t\treturn '%s%s' % (self.__class__.__name__, tuple(self))\n\n\t\tdef __repr__(self):\n\t\t\treturn '%s(%s)' % (self.__class__.__name__, repr(tuple(self)))\n\n\t\tdef __getattr__(self, name):\n\t\t\tif name == 'x':\n\t\t\t\treturn self.x\n\t\t\telif name == 'y':\n\t\t\t\treturn self.y\n\t\t\telse:\n\t\t\t\traise AttributeError(name)\n\n\t\tdef __setattr__(self, name, value):\n\t\t\tif name == 'x':\n\t\t\t\tself.x = value\n\t\t\telif name == 'y':\n\t\t\t\tself.y = value\n\t\t\telse:\n\t\t\t\traise Attribute\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\t\n\tclass Point(object):\n\t\t__slots__ = fields\n\t\t\n\t\tdef __init__(self, *args):\n\t\t\tfor name, val in zip(self.__slots__, args):\n\t\t\t\tsetattr(self, name, val)\n\t\t\t\t\n\t\t@property\n\t\tdef wkt(self):\n\t\t\treturn \"POINT({} {})\".format(*self)\n\t\t\t\n\t\tdef __repr__(self):\n\t\t\treturn \"{}({})\".format(name, \", \".join(map(repr, self)))\n\t\t\t\n\t\tdef __iter__(self):\n\t\t\treturn (getattr(self, field) for field in self.__slots__)\n\t\t\t\n\t\tdef __getitem__(self, index):\n\t\t\treturn getattr(self, self.__slots__[index])\n\t\t\t\n\t\tdef __len__(self):\n\t\t\treturn len(self.__slots__)\n\t\t\t\n\t@property\n\tdef srid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def point_type(name, fields, srid_map):\n\tfields_string = \",\".join(fields)\n\treturn type(name, (Point,), {\"__init__\": lambda self, x, y, srid: Point.__init__(self, x, y, srid_map[srid])})\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_meta.py", "project": "neo4j/neo4j-python-driver", "name": "deprecated", "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass", "ques_id": "62e5dc9ed76274f8a4026b5b", "code": ["def deprecated(message):\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef new_f(*args, **kwargs):\n\t\t\twarnings.simplefilter('always', DeprecationWarning)\n\t\t\twarnings.warn(\"Call to deprecated function {}. {}\".format(f.__name__, message), category=DeprecationWarning, stacklevel=2)\n\t\t\twarnings.simplefilter('default', DeprecationWarning)\n\t\t\treturn f(*args, **kwargs)\n\t\treturn new_f\n\treturn decorator\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\treturn _deprecated(func, message)\n\treturn decorator\n\n", "def deprecated(message):\n\tdef _deprecated(func):\n\t\t\"\"\"\n\t\tWrap a function and print a deprecation warning.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef _wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tPrint a deprecation warning, then call the wrapped function.\n\t\t\t\"\"\"\n\t\t\t_logger.warning('Call to deprecated function {name}: {message}'.format(\n\t\t\t\tname=func.__name__, message=message))\n\t\t\treturn func(*args, **kwargs)\n\t\treturn _wrapper\n\treturn _deprecated\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tReturn a wrapped function that will log a deprecation warning when\n\t\tcalled.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tlogger.warning(\"Call to deprecated function %s. %s\", func.__name__, message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treturn wrapper\n\n\treturn decorator\n\n\n", "def deprecated(message):\n\tdef deprecated_decorator(func):\n\t\t@wraps(func)\n\t\tdef deprecated_decorated(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_decorated\n\treturn deprecated_decorator\n\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "def deprecated(message):\n\tdef deprecated_decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tlogging.warning(message + \": \" + func.__name__)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn deprecated_decorator\n\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "def deprecated(message):\n\tdef deprecated_decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\n\treturn deprecated_decorator\n\n\n", "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(\"{0}\".format(message), DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deprecated(message):\n\tdef decorator(f):\n\t\t@functools.wraps(f)\n\t\tdef new_f(*args, **kwargs):\n\t\t\twarnings.simplefilter('always', DeprecationWarning)\n\t\t\twarnings.warn(\"Call to deprecated function {}. {}\".format(f.__name__, message), category=DeprecationWarning, stacklevel=2)\n\t\t\twarnings.simplefilter('default', DeprecationWarning)\n\t\t\treturn f(*args, **kwargs)\n\t\treturn new_f\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\treturn _deprecated(func, message)\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef _deprecated(func):\n\t\t\"\"\"\n\t\tWrap a function and print a deprecation warning.\n\t\t\"\"\"\n\t\t@functools.wraps(func)\n\t\tdef _wrapper(*args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tPrint a deprecation warning, then call the wrapped function.\n\t\t\t\"\"\"\n\t\t\t_logger.warning('Call to deprecated function {name}: {message}'.format(\n\t\t\t\tname=func.__name__, message=message))\n\t\t\treturn func(*args, **kwargs)\n\t\treturn _wrapper\n\treturn _deprecated\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t\"\"\"\n\t\tReturn a wrapped function that will log a deprecation warning when\n\t\tcalled.\n\t\t\"\"\"\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tlogger.warning(\"Call to deprecated function %s. %s\", func.__name__, message)\n\t\t\treturn func(*args, **kwargs)\n\n\t\treturn wrapper\n\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef deprecated_decorator(func):\n\t\t@wraps(func)\n\t\tdef deprecated_decorated(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_decorated\n\treturn deprecated_decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef deprecated_decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\tlogging.warning(message + \": \" + func.__name__)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\treturn deprecated_decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(message, category=DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deprecated(message):\n\tdef deprecated_decorator(func):\n\t\tdef deprecated_func(*args, **kwargs):\n\t\t\twarnings.warn(message, DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn deprecated_func\n\n\treturn deprecated_decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def deprecated(message):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twarnings.warn(\"{0}\".format(message), DeprecationWarning, stacklevel=2)\n\t\t\treturn func(*args, **kwargs)\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/r.py", "project": "pre-commit/pre-commit", "name": "_inline_r_setup", "docstring": "Some behaviour of R cannot be configured via env variables, but can\nonly be configured via R options once R has started. These are set here.", "ques_id": "62e4fc3c85ea98643089041e", "code": ["def _inline_r_setup(code: str) -> str:\n\treturn f\"\"\"\n\toptions(error=quote({_R_ERROR_HANDLER}))\n\toptions(warn=quote({_R_WARNING_HANDLER}))\n\toptions(showWarnCalls=FALSE)\n\toptions(showErrorCalls=FALSE)\n\toptions(show.error.locations=FALSE)\n\toptions(show.error.messages=FALSE)\n\toptions(show.signif.stars=FALSE)\n\toptions(scipen=999)\n\toptions(OutDec=',')\n\t\"\"\" + code\n\n", "def _inline_r_setup(code: str) -> str:\n\tfrom . import _python_version\n\t# R_HOME is set in the environment, but R needs to know where python is.\n\t# R does a \"sys.executable\" if it exists, so we do the same.\n\t# R does not look for pip, so we set that too.\n\t# All this is not necessary for python 3.4, but is required for 3.5+.\n\tif _python_version < (3, 4):\n\t\t# We do this because python's os.path.join does not work that well with\n\t\t# paths with spaces in them.\n\t\t# We also need to replace backslashes with forward slashes, as R uses\n\t\t# forward slashes.\n\t\tpython_path = os.path.normpath(sys.executable).replace('\\\\', '/')\n\t\tpip_path = os.path.normpath(sys.executable.replace('python.exe', 'pip')).replace('\\\\', '/')\n\t\tcode += \"\\noptions(python = '%s', pip = '%s')\" % (python_path, pip_path)\n\t\n", "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tcode\n\t\t+ \"\"\"\n", "def _inline_r_setup(code: str) -> str:\n\treturn f\"\"\"\n\t\toptions(stringsAsFactors = FALSE)\n\t\toptions(warn = 1)\n\t\toptions(showWarnCalls = TRUE)\n\t\toptions(showErrorCalls = TRUE)\n\t\toptions(show.error.messages = TRUE)\n\t\toptions(error = quote(dump.frames(to.file = TRUE, n = 10)))\n\t\t{code}\n\t\"\"\"\n\n", "def _inline_r_setup(code: str) -> str:\n\t# Suppress certain messages\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/message\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/warning\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/stop\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/cat\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/print\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/warning.condition\n\t#    See http://r.789695.n4.nabble.com/warning-suppressed-messages-td4718123.html\n\tcode += \"\"\"\n", "def _inline_r_setup(code: str) -> str:\n\treturn \"\"\"\n", "def _inline_r_setup(code: str) -> str:\n\treturn '\\n'.join([\n\t\t'options(bitmapType=\"cairo\")',\n\t\t'options(repos=\"https://cloud.r-project.org/\")',\n\t\tcode\n\t])\n\n", "def _inline_r_setup(code: str) -> str:\n\treturn code + \"\\n\" + \"\"\"\n\toptions(warn = -1)\n\toptions(showWarnCalls = FALSE)\n\toptions(showErrorCalls = FALSE)\n\toptions(show.error.messages = FALSE)\n\toptions(show.signif.stars = FALSE)\n\toptions(error = quote(stop(\"Cancelled\")))\n\t\"\"\"\n\n\n\n", "def _inline_r_setup(code: str) -> str:\n\treturn _inline_r(code)\n\n\n", "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tcode\n\t\t+ \"\"\"\n\t\toptions(width=10000)\n\t\toptions(error=function() {\n\t\t\tdev.off()\n\t\t\tstop(paste(\"R error:\", geterrmessage()), call. = FALSE)\n\t\t})\n\t\t\"\"\"\n\t)\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn f\"\"\"\n\toptions(error=quote({_R_ERROR_HANDLER}))\n\toptions(warn=quote({_R_WARNING_HANDLER}))\n\toptions(showWarnCalls=FALSE)\n\toptions(showErrorCalls=FALSE)\n\toptions(show.error.locations=FALSE)\n\toptions(show.error.messages=FALSE)\n\toptions(show.signif.stars=FALSE)\n\toptions(scipen=999)\n\toptions(OutDec=',')\n\t\"\"\" + code\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\tfrom . import _python_version\n\t# R_HOME is set in the environment, but R needs to know where python is.\n\t# R does a \"sys.executable\" if it exists, so we do the same.\n\t# R does not look for pip, so we set that too.\n\t# All this is not necessary for python 3.4, but is required for 3.5+.\n\tif _python_version < (3, 4):\n\t\t# We do this because python's os.path.join does not work that well with\n\t\t# paths with spaces in them.\n\t\t# We also need to replace backslashes with forward slashes, as R uses\n\t\t# forward slashes.\n\t\tpython_path = os.path.normpath(sys.executable).replace('\\\\', '/')\n\t\tpip_path = os.path.normpath(sys.executable.replace('python.exe', 'pip')).replace('\\\\', '/')\n\t\tcode += \"\\noptions(python = '%s', pip = '%s')\" % (python_path, pip_path)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tcode\n\t\t+ \"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn f\"\"\"\n\t\toptions(stringsAsFactors = FALSE)\n\t\toptions(warn = 1)\n\t\toptions(showWarnCalls = TRUE)\n\t\toptions(showErrorCalls = TRUE)\n\t\toptions(show.error.messages = TRUE)\n\t\toptions(error = quote(dump.frames(to.file = TRUE, n = 10)))\n\t\t{code}\n\t\"\"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\t# Suppress certain messages\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/message\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/warning\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/stop\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/cat\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/print\n\t#    See https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/warning.condition\n\t#    See http://r.789695.n4.nabble.com/warning-suppressed-messages-td4718123.html\n\tcode += \"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn \"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn '\\n'.join([\n\t\t'options(bitmapType=\"cairo\")',\n\t\t'options(repos=\"https://cloud.r-project.org/\")',\n\t\tcode\n\t])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn code + \"\\n\" + \"\"\"\n\toptions(warn = -1)\n\toptions(showWarnCalls = FALSE)\n\toptions(showErrorCalls = FALSE)\n\toptions(show.error.messages = FALSE)\n\toptions(show.signif.stars = FALSE)\n\toptions(error = quote(stop(\"Cancelled\")))\n\t\"\"\"\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn _inline_r(code)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _inline_r_setup(code: str) -> str:\n\treturn (\n\t\tcode\n\t\t+ \"\"\"\n\t\toptions(width=10000)\n\t\toptions(error=function() {\n\t\t\tdev.off()\n\t\t\tstop(paste(\"R error:\", geterrmessage()), call. = FALSE)\n\t\t})\n\t\t\"\"\"\n\t)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/xargs.py", "project": "pre-commit/pre-commit", "name": "xargs", "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently", "ques_id": "62e4fbda85ea986430890405", "code": ["def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tassert target_concurrency >= 1\n\n\tif len(varargs) == 0:\n\t\treturn\n\n\tif color:\n\t\tcmd = _colorize(cmd)\n\n\tif _max_length is None:\n\t\treturn _xargs_no_max_length(cmd, varargs, target_concurrency, **kwargs)\n\telse:\n\t\treturn _xargs_with_max_length(cmd, varargs, target_concurrency, _max_length, **kwargs)\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t...\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\timport subprocess\n\tfrom io import StringIO\n\tfrom itertools import islice\n\tfrom multiprocessing import Pool\n\tfrom typing import Iterator, Tuple, Union\n\n\tdef _exec(cmds: Sequence[str]) -> Iterator[Tuple[int, str, str]]:\n\t\tfor c in cmds:\n\t\t\tp = subprocess.Popen(c, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n\t\t\tstdout, stderr = p.communicate()\n\t\t\tyield (p.returncode, stdout.decode(), stderr.decode())\n\n\tdef _iter_cmd(varargs: Sequence[str]) -> Iterator[str]:\n\t\tfor cmd in varargs:\n\t\t\tyield ' '.join(cmd)\n\n\tdef _iter_stdout(cmds: Sequence[str], _max_length: int) -> Iterator[Tuple[int, str, str]]:\n\t\tcmd = ' '.join(cmd)\n\t\tfor c in cmds:\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\n\t# check the max length of the command\n\tif _max_length is not None and sum(map(len, cmd + varargs)) >= _max_length:\n\t\traise XargsError(f\"Command line too long: {_max_length}\")\n\t\n\t# create a process pool\n\twith multiprocessing.Pool(target_concurrency) as pool:\n\t\t# map the command to the arguments\n\t\tfor result in pool.imap_unordered(partial(_run_cmd, cmd, color, **kwargs), varargs):\n\t\t\t# print the output\n\t\t\tfor line in result.output:\n\t\t\t\tprint(line, end=\"\")\n\t\t\t\n\t\t\t# yield the result\n\t\t\tyield result\n\t\t\t\n\t\t\t# check if the command failed\n\t\t\tif result.returncode != 0:\n\t\t\t\traise XargsError(f\"Command failed with exit code: {result.returncode}\")\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\timport math\n\timport subprocess\n\tfrom typing import Iterable, List, Tuple\n\n\t# If we can't determine the max length of the command line,\n\t# just use a huge number so we don't break.\n\tmax_length = _max_length or 2 ** 15\n\n\t# Round target_concurrency down to the nearest power of two.\n\t# This is how xargs does it.\n\ttarget_concurrency = math.floor(math.log(target_concurrency) / math.log(2))\n\ttarget_concurrency = 2 ** target_concurrency\n\n\tdef chunks(it: Iterable[str], chunk_size: int) -> Iterable[List[str]]:\n\t\t\"\"\"\n\t\tSplit the iterable into chunks.\n\t\t\"\"\"\n\t\tchunk = []\n\t\tfor item in it:\n\t\t\tchunk.append(item)\n\t\t\tif len(chunk) >= chunk_size:\n\t\t\t\tyield chunk\n\t\t\t\tchunk = []\n\t\tif chunk:\n\t\t\tyield chunk\n\n\tdef get_commands(items:\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif not varargs:\n\t\treturn\n\tif '-L' in cmd:\n\t\tlogger.warning(\"-L is not supported since it's not implemented\")\n\t\tcmd = [i for i in cmd if i != '-L']\n\n\t# xargs -I\n\tif '-I' in cmd:\n\t\tlogger.warning(\"-I is not supported since it's not implemented\")\n\t\tcmd = [i for i in cmd if i != '-I']\n\n\tif '-P' in cmd:\n\t\ttarget_concurrency = int(cmd[cmd.index('-P') + 1])\n\t\tcmd = [i for i in cmd if i != '-P']\n\n\tif cmd[-1] == '--':\n\t\tcmd = cmd[:-1]\n\n\tcmd_length = sum(len(i) + 1 for i in cmd) + 1\n\t# the +1 is for the space between command and varargs\n\tif cmd_length + max(len(i) for i in varargs) > _max_length:\n\t\tlogger.warning(\n\t\t\t\"\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t# TODO: Implement Xargs in Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tpass\n\t\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif len(varargs) > 0:\n\t\tsubprocess.run(cmd + varargs)\n\t\tvarargs.clear()\n\tfor line in sys.stdin:\n\t\tline = line.rstrip()\n\t\tif len(varargs) + len(line) + 1 > _max_length:\n\t\t\tsubprocess.run(cmd + varargs)\n\t\t\tvarargs.clear()\n\t\tvarargs.append(line)\n\tif len(varargs) > 0:\n\t\tsubprocess.run(cmd + varargs)\n", "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tdef _execute(args: Sequence[str], **kwargs: Any) -> int:\n\t\t\"\"\"\n\t\tExecute the command\n\t\t\"\"\"\n\t\t# Override the color option to not color the command\n\t\tkwargs[\"color\"] = False\n\t\treturn cli.execute(cmd + args, **kwargs)\n\n\treturn _xargs(\n\t\tvarargs,\n\t\t_execute,\n\t\tcolor=color,\n\t\ttarget_concurrency=target_concurrency,\n\t\t_max_length=_max_length,\n\t\t**kwargs\n\t)\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tassert target_concurrency >= 1\n\n\tif len(varargs) == 0:\n\t\treturn\n\n\tif color:\n\t\tcmd = _colorize(cmd)\n\n\tif _max_length is None:\n\t\treturn _xargs_no_max_length(cmd, varargs, target_concurrency, **kwargs)\n\telse:\n\t\treturn _xargs_with_max_length(cmd, varargs, target_concurrency, _max_length, **kwargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t...\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\timport subprocess\n\tfrom io import StringIO\n\tfrom itertools import islice\n\tfrom multiprocessing import Pool\n\tfrom typing import Iterator, Tuple, Union\n\n\tdef _exec(cmds: Sequence[str]) -> Iterator[Tuple[int, str, str]]:\n\t\tfor c in cmds:\n\t\t\tp = subprocess.Popen(c, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n\t\t\tstdout, stderr = p.communicate()\n\t\t\tyield (p.returncode, stdout.decode(), stderr.decode())\n\n\tdef _iter_cmd(varargs: Sequence[str]) -> Iterator[str]:\n\t\tfor cmd in varargs:\n\t\t\tyield ' '.join(cmd)\n\n\tdef _iter_stdout(cmds: Sequence[str], _max_length: int) -> Iterator[Tuple[int, str, str]]:\n\t\tcmd = ' '.join(cmd)\n\t\tfor c in cmds:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t\n\t# check the max length of the command\n\tif _max_length is not None and sum(map(len, cmd + varargs)) >= _max_length:\n\t\traise XargsError(f\"Command line too long: {_max_length}\")\n\t\n\t# create a process pool\n\twith multiprocessing.Pool(target_concurrency) as pool:\n\t\t# map the command to the arguments\n\t\tfor result in pool.imap_unordered(partial(_run_cmd, cmd, color, **kwargs), varargs):\n\t\t\t# print the output\n\t\t\tfor line in result.output:\n\t\t\t\tprint(line, end=\"\")\n\t\t\t\n\t\t\t# yield the result\n\t\t\tyield result\n\t\t\t\n\t\t\t# check if the command failed\n\t\t\tif result.returncode != 0:\n\t\t\t\traise XargsError(f\"Command failed with exit code: {result.returncode}\")\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\timport math\n\timport subprocess\n\tfrom typing import Iterable, List, Tuple\n\n\t# If we can't determine the max length of the command line,\n\t# just use a huge number so we don't break.\n\tmax_length = _max_length or 2 ** 15\n\n\t# Round target_concurrency down to the nearest power of two.\n\t# This is how xargs does it.\n\ttarget_concurrency = math.floor(math.log(target_concurrency) / math.log(2))\n\ttarget_concurrency = 2 ** target_concurrency\n\n\tdef chunks(it: Iterable[str], chunk_size: int) -> Iterable[List[str]]:\n\t\t\"\"\"\n\t\tSplit the iterable into chunks.\n\t\t\"\"\"\n\t\tchunk = []\n\t\tfor item in it:\n\t\t\tchunk.append(item)\n\t\t\tif len(chunk) >= chunk_size:\n\t\t\t\tyield chunk\n\t\t\t\tchunk = []\n\t\tif chunk:\n\t\t\tyield chunk\n\n\tdef get_commands(items:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif not varargs:\n\t\treturn\n\tif '-L' in cmd:\n\t\tlogger.warning(\"-L is not supported since it's not implemented\")\n\t\tcmd = [i for i in cmd if i != '-L']\n\n\t# xargs -I\n\tif '-I' in cmd:\n\t\tlogger.warning(\"-I is not supported since it's not implemented\")\n\t\tcmd = [i for i in cmd if i != '-I']\n\n\tif '-P' in cmd:\n\t\ttarget_concurrency = int(cmd[cmd.index('-P') + 1])\n\t\tcmd = [i for i in cmd if i != '-P']\n\n\tif cmd[-1] == '--':\n\t\tcmd = cmd[:-1]\n\n\tcmd_length = sum(len(i) + 1 for i in cmd) + 1\n\t# the +1 is for the space between command and varargs\n\tif cmd_length + max(len(i) for i in varargs) > _max_length:\n\t\tlogger.warning(\n\t\t\t\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\t# TODO: Implement Xargs in Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO: Implement Xargs for iOS\n\t# TODO: Implement Xargs for Linux\n\t# TODO: Implement Xargs for Windows\n\t# TODO: Implement Xargs for Mac\n\t# TODO: Implement Xargs for Android\n\t# TODO\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tif len(varargs) > 0:\n\t\tsubprocess.run(cmd + varargs)\n\t\tvarargs.clear()\n\tfor line in sys.stdin:\n\t\tline = line.rstrip()\n\t\tif len(varargs) + len(line) + 1 > _max_length:\n\t\t\tsubprocess.run(cmd + varargs)\n\t\t\tvarargs.clear()\n\t\tvarargs.append(line)\n\tif len(varargs) > 0:\n\t\tsubprocess.run(cmd + varargs)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs:\n\tdef _execute(args: Sequence[str], **kwargs: Any) -> int:\n\t\t\"\"\"\n\t\tExecute the command\n\t\t\"\"\"\n\t\t# Override the color option to not color the command\n\t\tkwargs[\"color\"] = False\n\t\treturn cli.execute(cmd + args, **kwargs)\n\n\treturn _xargs(\n\t\tvarargs,\n\t\t_execute,\n\t\tcolor=color,\n\t\ttarget_concurrency=target_concurrency,\n\t\t_max_length=_max_length,\n\t\t**kwargs\n\t)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pre_commit/languages/helpers.py", "project": "pre-commit/pre-commit", "name": "_shuffled", "docstring": "Deterministically shuffle", "ques_id": "62e4fbda85ea986430890403", "code": ["def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn random.Random(FIXED_RANDOM_SEED).sample(seq, len(seq))\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn list(seq)\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn list(seq)\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tshuffled = list(seq)\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(shuffled)\n\treturn shuffled\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tres = list(seq)\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(res)\n\treturn res\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tr = random.Random(FIXED_RANDOM_SEED)\n\treturn r.sample(seq, len(seq))\n\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trand = random.Random(FIXED_RANDOM_SEED)\n\treturn rand.sample(seq, len(seq))\n\n", "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseq_shuffled = list(seq)\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq_shuffled)\n\treturn seq_shuffled\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\treturn random.Random(FIXED_RANDOM_SEED).sample(seq, len(seq))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn random.sample(seq, len(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn list(seq)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\treturn list(seq)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tshuffled = list(seq)\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(shuffled)\n\treturn shuffled\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tres = list(seq)\n\trandom.Random(FIXED_RANDOM_SEED).shuffle(res)\n\treturn res\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq)\n\treturn seq\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tr = random.Random(FIXED_RANDOM_SEED)\n\treturn r.sample(seq, len(seq))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\trand = random.Random(FIXED_RANDOM_SEED)\n\treturn rand.sample(seq, len(seq))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _shuffled(seq: Sequence[str]) -> list[str]:\n\tseq_shuffled = list(seq)\n\trandom.seed(FIXED_RANDOM_SEED)\n\trandom.shuffle(seq_shuffled)\n\treturn seq_shuffled\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/util.py", "project": "pre-commit/pre-commit", "name": "parse_version", "docstring": "poor man's version comparison", "ques_id": "62e4fb6585ea98643089032b", "code": ["def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple([int(i) for i in s.split(\".\")])\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(e) for e in s.split(\".\"))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(part) for part in s.split('.'))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(i) for i in s.split(\".\"))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(x) for x in s.split('.'))\n\n\n", "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(c) for c in s.split(\".\"))\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple([int(i) for i in s.split(\".\")])\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(e) for e in s.split(\".\"))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(part) for part in s.split('.'))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(i) for i in s.split('.'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split(\".\")))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(i) for i in s.split(\".\"))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(map(int, s.split('.')))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(x) for x in s.split('.'))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parse_version(s: str) -> tuple[int, ...]:\n\treturn tuple(int(c) for c in s.split(\".\"))\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pre_commit/parse_shebang.py", "project": "pre-commit/pre-commit", "name": "normalize_cmd", "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine", "ques_id": "62e4fb4d85ea9864308902e7", "code": ["def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(which(arg) or arg for arg in cmd)\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\ttry:\n\t\tcmd = tuple(os.path.abspath(cmd[0])) + tuple(cmd[1:])\n\texcept FileNotFoundError:\n\t\tpass\n\treturn cmd\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(\n\t\tos.path.normpath(os.path.abspath(exe))\n\t\tfor exe in cmd\n\t)\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(full_path(exe) for exe in cmd)\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(which_exe(exe) for exe in cmd)\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif len(cmd) == 0:\n\t\treturn ()\n\texe = shutil.which(cmd[0])\n\tif exe is None:\n\t\treturn cmd\n\telse:\n\t\treturn (exe,) + cmd[1:]\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tname = cmd[0]\n\tpath = find_executable(name)\n\tif path:\n\t\treturn (path,) + cmd[1:]\n\telse:\n\t\treturn cmd\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not cmd:\n\t\treturn ()\n\tassert isinstance(cmd[0], str)\n\tassert None not in cmd\n\texe_path = which(cmd[0])\n\tif exe_path is not None:\n\t\treturn (exe_path, *cmd[1:])\n\treturn cmd\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\texe = cmd[0]\n\tif exe.endswith(\".py\"):\n\t\texe = \"python3\"\n\texe = shutil.which(exe)\n\tif exe is None:\n\t\traise ValueError(\"{} is not an executable\".format(cmd[0]))\n\treturn (exe,) + cmd[1:]\n\n\n", "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif len(cmd) > 1:\n\t\treturn (which(cmd[0]) or cmd[0],) + cmd[1:]\n\treturn which(cmd[0]) or cmd[0]\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(which(arg) or arg for arg in cmd)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\ttry:\n\t\tcmd = tuple(os.path.abspath(cmd[0])) + tuple(cmd[1:])\n\texcept FileNotFoundError:\n\t\tpass\n\treturn cmd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(\n\t\tos.path.normpath(os.path.abspath(exe))\n\t\tfor exe in cmd\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(full_path(exe) for exe in cmd)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\treturn tuple(which_exe(exe) for exe in cmd)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif len(cmd) == 0:\n\t\treturn ()\n\texe = shutil.which(cmd[0])\n\tif exe is None:\n\t\treturn cmd\n\telse:\n\t\treturn (exe,) + cmd[1:]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tname = cmd[0]\n\tpath = find_executable(name)\n\tif path:\n\t\treturn (path,) + cmd[1:]\n\telse:\n\t\treturn cmd\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif not cmd:\n\t\treturn ()\n\tassert isinstance(cmd[0], str)\n\tassert None not in cmd\n\texe_path = which(cmd[0])\n\tif exe_path is not None:\n\t\treturn (exe_path, *cmd[1:])\n\treturn cmd\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\texe = cmd[0]\n\tif exe.endswith(\".py\"):\n\t\texe = \"python3\"\n\texe = shutil.which(exe)\n\tif exe is None:\n\t\traise ValueError(\"{} is not an executable\".format(cmd[0]))\n\treturn (exe,) + cmd[1:]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n\tif len(cmd) > 1:\n\t\treturn (which(cmd[0]) or cmd[0],) + cmd[1:]\n\treturn which(cmd[0]) or cmd[0]\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cached", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nresults in a cache.", "ques_id": "62b8d27a48ba5a41d1c3f4c6", "code": ["def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\tif not isinstance(k, tuple):\n\t\t\t\tk = (k,)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\tif k in cache:\n\t\t\t\tret = cache[k]\n\t\t\telse:\n\t\t\t\tret = func(*args, **kwargs)\n\t\t\t\tcache[k] = ret\n\t\t\tif lock is not None:\n\t\t\t\tlock.release()\n\t\t\treturn ret\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(function):\n\t\t@wraps(function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\tvalue = cache.get(key(function.__name__, *args, **kwargs))\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = function(*args, **kwargs)\n\t\t\t\t\tcache[key(function.__name__, *args, **kwargs)] = value\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not lock is None:\n\t\t\t\twith lock:\n\t\t\t\t\tcached_value = cache.get(key(*args, **kwargs))\n\t\t\t\t\tif cached_value is None:\n\t\t\t\t\t\tcached_value = func(*args, **kwargs)\n\t\t\t\t\t\tcache.set(key(*args, **kwargs), cached_value)\n\t\t\telse:\n\t\t\t\tcached_value = cache.get(key(*args, **kwargs))\n\t\t\t\tif cached_value is None:\n\t\t\t\t\tcached_value = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key(*args, **kwargs), cached_value)\n\t\t\treturn cached_value\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(function):\n\t\t@functools.wraps(function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcachekey = key(function.__name__, args, kwargs)\n\t\t\tif cachekey not in cache:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tif cachekey not in cache:\n\t\t\t\t\t\t\tcache[cachekey] = function(*args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tcache[cachekey] = function(*args, **kwargs)\n\t\t\treturn cache[cachekey]\n\t\treturn wrapper\n\treturn decorator\n\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tcached_value = cache.get(key(*args, **kwargs))\n\t\t\t\tif cached_value is None:\n\t\t\t\t\tcached_value = func(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = cached_value\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn cached_value\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\tlock = lock or threading.RLock()\n\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\t_key = key(*args, **kwargs)\n\t\t\t\tif _key in cache:\n\t\t\t\t\treturn cache[_key]\n\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[_key] = result\n\n\t\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorator\n\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey_value = key(*args, **kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tvalue = cache.get(key_value)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key_value, value)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "def cached(cache, key=hashkey, lock=None):\n\tif cache is None:\n\t\treturn lambda f: f\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\th = key(args, kwargs)\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[h]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[h] = result\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "def cached(cache, key=hashkey, lock=None):\n\tif lock is None:\n\t\tlock = threading.Lock()\n\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# key is not used here\n\t\t\twith lock:\n\t\t\t\tif key(*args, **kwargs) in cache:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\telse:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\tif not isinstance(k, tuple):\n\t\t\t\tk = (k,)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\tif k in cache:\n\t\t\t\tret = cache[k]\n\t\t\telse:\n\t\t\t\tret = func(*args, **kwargs)\n\t\t\t\tcache[k] = ret\n\t\t\tif lock is not None:\n\t\t\t\tlock.release()\n\t\t\treturn ret\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(function):\n\t\t@wraps(function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\tvalue = cache.get(key(function.__name__, *args, **kwargs))\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = function(*args, **kwargs)\n\t\t\t\t\tcache[key(function.__name__, *args, **kwargs)] = value\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not lock is None:\n\t\t\t\twith lock:\n\t\t\t\t\tcached_value = cache.get(key(*args, **kwargs))\n\t\t\t\t\tif cached_value is None:\n\t\t\t\t\t\tcached_value = func(*args, **kwargs)\n\t\t\t\t\t\tcache.set(key(*args, **kwargs), cached_value)\n\t\t\telse:\n\t\t\t\tcached_value = cache.get(key(*args, **kwargs))\n\t\t\t\tif cached_value is None:\n\t\t\t\t\tcached_value = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key(*args, **kwargs), cached_value)\n\t\t\treturn cached_value\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(function):\n\t\t@functools.wraps(function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tcachekey = key(function.__name__, args, kwargs)\n\t\t\tif cachekey not in cache:\n\t\t\t\tif lock:\n\t\t\t\t\twith lock:\n\t\t\t\t\t\tif cachekey not in cache:\n\t\t\t\t\t\t\tcache[cachekey] = function(*args, **kwargs)\n\t\t\t\telse:\n\t\t\t\t\tcache[cachekey] = function(*args, **kwargs)\n\t\t\treturn cache[cachekey]\n\t\treturn wrapper\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tcached_value = cache.get(key(*args, **kwargs))\n\t\t\t\tif cached_value is None:\n\t\t\t\t\tcached_value = func(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = cached_value\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn cached_value\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tlock = lock or threading.RLock()\n\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\t_key = key(*args, **kwargs)\n\t\t\t\tif _key in cache:\n\t\t\t\t\treturn cache[_key]\n\n\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\tcache[_key] = result\n\n\t\t\t\treturn result\n\n\t\treturn wrapper\n\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tkey_value = key(*args, **kwargs)\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tvalue = cache.get(key_value)\n\t\t\t\tif value is None:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache.set(key_value, value)\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn value\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tif cache is None:\n\t\treturn lambda f: f\n\tdef decorator(f):\n\t\t@wraps(f)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\th = key(args, kwargs)\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[h]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = f(*args, **kwargs)\n\t\t\t\t\tcache[h] = result\n\t\t\t\t\treturn result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\ttry:\n\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\texcept KeyError:\n\t\t\t\tpass\n\t\t\tresult = func(*args, **kwargs)\n\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cached(cache, key=hashkey, lock=None):\n\tif lock is None:\n\t\tlock = threading.Lock()\n\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t# key is not used here\n\t\t\twith lock:\n\t\t\t\tif key(*args, **kwargs) in cache:\n\t\t\t\t\treturn cache[key(*args, **kwargs)]\n\t\t\t\telse:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache[key(*args, **kwargs)] = result\n\t\t\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "ttl_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.", "ques_id": "62b8d24048ba5a41d1c3f49f", "code": ["def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args, kwds]\n\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn cache.info()\n\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\n\treturn decorating_function\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = ttl_lru_cache(maxsize, ttl, timer, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function)(*args, **kwds)\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args][kwds]\n\t\twrapper.__doc__ = user_function.__doc__\n\t\twrapper.__name__ = user_function.__name__\n\t\twrapper.__dict__ = user_function.__dict__\n\t\twrapper.__wrapped__ = user_function\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\treturn functools.lru_cache(maxsize=maxsize, typed=typed)(ttl_cache_wrapper(ttl, timer))\n\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _TTLCache(maxsize, ttl, timer, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn cache[args][kwargs]\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn cache.info()\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize the global _TTL_CACHE_COUNTER and _TTL_CACHE_ZERO variables.\n\t# If they don't exist yet, initialize them.\n\tglobal _TTL_CACHE_COUNTER\n\tglobal _TTL_CACHE_ZERO\n\tif not hasattr(_TTL_CACHE_COUNTER, 'value'):\n\t\t_TTL_CACHE_COUNTER = _TTL_CACHE_COUNTER()\n\tif not hasattr(_TTL_CACHE_ZERO, 'value'):\n\t\t_TTL_CACHE_ZERO = _TTL_CACHE_ZERO()\n\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\t\tcache_get = cache.cache_info().__get__\n\t\t_len = len\n\t\tlock = RLock()           # because linkedlist updates aren't threadsafe\n\t\troot = []                # root of the circular doubly linked list\n\t\troot[\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\n\t\tdef wrapper(*args):\n\t\t\tnow = timer()\n\t\t\ttry:\n\t\t\t\tresult, last_update = cache_get(args)\n\t\t\t\tif (now - last_update) > ttl:\n\t\t\t\t\traise AttributeError\n\t\t\t\treturn result\n\t\t\texcept (KeyError, AttributeError):\n\t\t\t\tresult = user_function(*args)\n\t\t\t\tcache_set(args, (result, now))\n\t\t\t\treturn result\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache_clear = cache.cache_clear\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__name__ = user_function.__name__\n\t\twrapper.__doc__ = user_function.__doc__\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function,\n\t\t\t\t\t\t\tmaxsize=maxsize,\n\t\t\t\t\t\t\ttyped=typed,\n\t\t\t\t\t\t\t_CacheInfo=_CacheInfo,\n\t\t\t\t\t\t\t_ttl_make_key=_ttl_make_key,\n\t\t\t\t\t\t\t_ttl_expire=_ttl_expire,\n\t\t\t\t\t\t\t_ttl_mutex=threading.Lock(),\n\t\t\t\t\t\t\t_ttl_cache={},\n\t\t\t\t\t\t\t_ttl_cache_get=dict.get,\n\t\t\t\t\t\t\t_ttl_cache_len=len,\n\t\t\t\t\t\t\t_ttl_cache_popitem=dict.popitem,\n\t\t\t\t\t\t\t_ttl_cache_contains=dict.__contains__,\n\t\t\t\t\t\t\t_ttl_cache_update=dict.update,\n\t\t\t\t\t\t\t_ttl_cache_setitem=\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize variable access\n\t_CacheInfo = _hint_CacheInfo\n\t_HINT_TUPLE = _hint_tuple_template\n\tlock = _thread.RLock()\n\troot = []\n\troot[:] = [root, root, None, None]\n\tnonlocal_root = [root]\n\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\n\tmake_key = _make_key\n\tcache_get = _hint_dict_get\n\t_maxsize = maxsize\n\t_ttl = ttl\n\t_clock = timer\n\tdef decorating_function(user_function):\n\t\tcache = nonlocal_root[0]\n\t\thits, misses, currsize, ttl_cache = 0, 0, 0, []\n\t\tif _maxsize == 0:\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tnonlocal misses\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tmisses += 1\n\t\t\t\treturn result\n\t\telif\n", "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _TTLCache(maxsize, ttl, timer, typed)\n\t\treturn _lru_cache_wrapper(user_function, cache)\n\treturn decorating_function\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args, kwds]\n\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn cache.info()\n\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = ttl_lru_cache(maxsize, ttl, timer, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function)(*args, **kwds)\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = TTLCache(maxsize, ttl, timer, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args][kwds]\n\t\twrapper.__doc__ = user_function.__doc__\n\t\twrapper.__name__ = user_function.__name__\n\t\twrapper.__dict__ = user_function.__dict__\n\t\twrapper.__wrapped__ = user_function\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\treturn functools.lru_cache(maxsize=maxsize, typed=typed)(ttl_cache_wrapper(ttl, timer))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _TTLCache(maxsize, ttl, timer, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\treturn cache[args][kwargs]\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn cache.info()\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize the global _TTL_CACHE_COUNTER and _TTL_CACHE_ZERO variables.\n\t# If they don't exist yet, initialize them.\n\tglobal _TTL_CACHE_COUNTER\n\tglobal _TTL_CACHE_ZERO\n\tif not hasattr(_TTL_CACHE_COUNTER, 'value'):\n\t\t_TTL_CACHE_COUNTER = _TTL_CACHE_COUNTER()\n\tif not hasattr(_TTL_CACHE_ZERO, 'value'):\n\t\t_TTL_CACHE_ZERO = _TTL_CACHE_ZERO()\n\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\t\tcache_get = cache.cache_info().__get__\n\t\t_len = len\n\t\tlock = RLock()           # because linkedlist updates aren't threadsafe\n\t\troot = []                # root of the circular doubly linked list\n\t\troot[\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache(maxsize, typed=typed)\n\n\t\tdef wrapper(*args):\n\t\t\tnow = timer()\n\t\t\ttry:\n\t\t\t\tresult, last_update = cache_get(args)\n\t\t\t\tif (now - last_update) > ttl:\n\t\t\t\t\traise AttributeError\n\t\t\t\treturn result\n\t\t\texcept (KeyError, AttributeError):\n\t\t\t\tresult = user_function(*args)\n\t\t\t\tcache_set(args, (result, now))\n\t\t\t\treturn result\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.cache_clear = cache.cache_clear\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__name__ = user_function.__name__\n\t\twrapper.__doc__ = user_function.__doc__\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function,\n\t\t\t\t\t\t\tmaxsize=maxsize,\n\t\t\t\t\t\t\ttyped=typed,\n\t\t\t\t\t\t\t_CacheInfo=_CacheInfo,\n\t\t\t\t\t\t\t_ttl_make_key=_ttl_make_key,\n\t\t\t\t\t\t\t_ttl_expire=_ttl_expire,\n\t\t\t\t\t\t\t_ttl_mutex=threading.Lock(),\n\t\t\t\t\t\t\t_ttl_cache={},\n\t\t\t\t\t\t\t_ttl_cache_get=dict.get,\n\t\t\t\t\t\t\t_ttl_cache_len=len,\n\t\t\t\t\t\t\t_ttl_cache_popitem=dict.popitem,\n\t\t\t\t\t\t\t_ttl_cache_contains=dict.__contains__,\n\t\t\t\t\t\t\t_ttl_cache_update=dict.update,\n\t\t\t\t\t\t\t_ttl_cache_setitem=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\t# Localize variable access\n\t_CacheInfo = _hint_CacheInfo\n\t_HINT_TUPLE = _hint_tuple_template\n\tlock = _thread.RLock()\n\troot = []\n\troot[:] = [root, root, None, None]\n\tnonlocal_root = [root]\n\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\n\tmake_key = _make_key\n\tcache_get = _hint_dict_get\n\t_maxsize = maxsize\n\t_ttl = ttl\n\t_clock = timer\n\tdef decorating_function(user_function):\n\t\tcache = nonlocal_root[0]\n\t\thits, misses, currsize, ttl_cache = 0, 0, 0, []\n\t\tif _maxsize == 0:\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tnonlocal misses\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tmisses += 1\n\t\t\t\treturn result\n\t\telif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _TTLCache(maxsize, ttl, timer, typed)\n\t\treturn _lru_cache_wrapper(user_function, cache)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "mru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.", "ques_id": "62b8d23b48ba5a41d1c3f49a", "code": ["def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\n\t\tcache = OrderedDict()\n\t\tstats = [0, 0]                  # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get           # bound method to lookup key or return None\n\t\t_len = len                      # localize the global len() function\n\t\tlock = RLock()                  # because linkedlist updates aren't threadsafe\n\t\troot = []                       # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]      # initialize by pointing to self\n\t\tnonlocal_root = [root]                  # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3    # names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# no caching, just do a statistics update after a successful call\n\t\t\t\tresult\n", "def mru_cache(maxsize=128, typed=False):\n\treturn _lru_cache_wrapper(maxsize, typed, False)\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(user_function, maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache[args]\n\t\t\tif result is not _HAS_ARG:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\twrapper.clear = clear\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__doc__ = getattr(user_function, '__doc__', None)\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\tlock = RLock()\n\tHITS, MISSES, MAXSIZE, CURRSIZE, FULL, REMOVED, INITIALIZED = range(7)\n\n\tdef decorating_function(user_function):\n\t\tstats = [0, 0, maxsize, 0, False, False, False]\n\t\troot = []\t\t\t\t\t\t\t\t\t\t\t\t\t# root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]\t\t\t\t\t\t\t# initialize by pointing to self\n\t\tnonlocal_root = [root]\t\t\t\t\t\t\t\t\t\t# make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\t\t\t\t\t\t# names for the link fields\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# size limited caching that tracks accesses by recency\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(user_function, maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache[args]\n\t\t\tif result is not None:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\treturn _update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "def mru_cache(maxsize=128, typed=False):\n\treturn functools.partial(lru_cache, maxsize=maxsize, typed=typed)\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = MRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args, kwds]\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\t\t\t# order: least recent to most recent\n\t\tstats = [0, 0]\t\t\t\t\t# make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1\t\t\t\t# names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get\t\t\t# bound method to lookup a key or return None\n\t\t_len = len\t\t\t\t\t\t# localize the global len() function\n\t\tlock = RLock()\t\t\t\t\t# because linkedlist updates aren't threadsafe\n\t\troot = []\t\t\t\t\t\t# root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]\t\t# initialize by pointing to self\n\t\tnonlocal_root = [root]\t\t\t# make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\t# names for the link fields\n\t\tif maxsize == 0\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\t\t\t# order: least recent to most recent\n\t\tcache_get = cache.get\t\t\t# bound method to lookup key or return None\n\t\t_len = cache.__len__\t\t\t# localize the global len() function\n\t\tlock = RLock()\t\t\t\t\t# because linkedlist updates aren't threadsafe\n\t\troot = []\t\t\t\t\t\t# root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]\t# initialize by pointing to self\n\t\tnonlocal_root = [root]\t\t\t# make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\t# names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# no caching, just do a statistics update after a successful call\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\treturn result\n\n\t\n", "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\treturn _CacheInfo(user_function, maxsize, typed)\n\treturn decorating_function\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\n\t\tcache = OrderedDict()\n\t\tstats = [0, 0]                  # make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1             # names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get           # bound method to lookup key or return None\n\t\t_len = len                      # localize the global len() function\n\t\tlock = RLock()                  # because linkedlist updates aren't threadsafe\n\t\troot = []                       # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]      # initialize by pointing to self\n\t\tnonlocal_root = [root]                  # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3    # names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# no caching, just do a statistics update after a successful call\n\t\t\t\tresult\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\treturn _lru_cache_wrapper(maxsize, typed, False)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(user_function, maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache[args]\n\t\t\tif result is not _HAS_ARG:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\twrapper.clear = clear\n\t\twrapper.cache_info = cache.cache_info\n\t\twrapper.__wrapped__ = user_function\n\t\twrapper.__doc__ = getattr(user_function, '__doc__', None)\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tlock = RLock()\n\tHITS, MISSES, MAXSIZE, CURRSIZE, FULL, REMOVED, INITIALIZED = range(7)\n\n\tdef decorating_function(user_function):\n\t\tstats = [0, 0, maxsize, 0, False, False, False]\n\t\troot = []\t\t\t\t\t\t\t\t\t\t\t\t\t# root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]\t\t\t\t\t\t\t# initialize by pointing to self\n\t\tnonlocal_root = [root]\t\t\t\t\t\t\t\t\t\t# make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\t\t\t\t\t\t# names for the link fields\n\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# size limited caching that tracks accesses by recency\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tif typed:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = _CacheInfo(user_function, maxsize, typed)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tresult = cache[args]\n\t\t\tif result is not None:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\treturn _update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\treturn functools.partial(lru_cache, maxsize=maxsize, typed=typed)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = MRUCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache[args, kwds]\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\t\t\t# order: least recent to most recent\n\t\tstats = [0, 0]\t\t\t\t\t# make statistics updateable non-locally\n\t\tHITS, MISSES = 0, 1\t\t\t\t# names for the stats fields\n\t\tmake_key = _make_key\n\t\tcache_get = cache.get\t\t\t# bound method to lookup a key or return None\n\t\t_len = len\t\t\t\t\t\t# localize the global len() function\n\t\tlock = RLock()\t\t\t\t\t# because linkedlist updates aren't threadsafe\n\t\troot = []\t\t\t\t\t\t# root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]\t\t# initialize by pointing to self\n\t\tnonlocal_root = [root]\t\t\t# make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\t# names for the link fields\n\t\tif maxsize == 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()\t\t\t# order: least recent to most recent\n\t\tcache_get = cache.get\t\t\t# bound method to lookup key or return None\n\t\t_len = cache.__len__\t\t\t# localize the global len() function\n\t\tlock = RLock()\t\t\t\t\t# because linkedlist updates aren't threadsafe\n\t\troot = []\t\t\t\t\t\t# root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]\t# initialize by pointing to self\n\t\tnonlocal_root = [root]\t\t\t# make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3\t# names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# no caching, just do a statistics update after a successful call\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\treturn result\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def mru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\treturn _CacheInfo(user_function, maxsize, typed)\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lru_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.", "ques_id": "62b8d23948ba5a41d1c3f498", "code": ["def lru_cache(maxsize=128, typed=False):\n\tlock = RLock()  # create a lock object\n\tcache = {}\n\tcache_get = cache.get  # bound method to lookup a key or return None\n\tcache_len = cache.__len__  # get cache size\n\n\tdef decorating_function(user_function):\n\t\t# the decorator\n\t\tnonlocal lock\n\t\tnonlocal cache\n\t\tnonlocal cache_get\n\t\tnonlocal cache_len\n\n\t\tsentinel = object()  # unique object used to signal cache misses\n\t\tmake_key = _make_key  # build a key from the function arguments\n\t\troot = []  # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]  # initialize by pointing to self\n\t\tnonlocal_root = [root]  # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3  # names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# simple caching without ordering or size\n", "def lru_cache(maxsize=128, typed=False):\n\t\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache_object(maxsize, typed)\n\t\t\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\tresult = cache[key] = user_function(*args, **kwds)\n\t\t\treturn result\n\t\t\n\t\tdef cache_info():\n\t\t\treturn cache.info()\n\t\t\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\t\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\t\n\treturn decorating_function\n\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tif args in cache:\n\t\t\t\tif cache[args] == _NONE:\n\t\t\t\t\traise KeyError\n\t\t\t\telse:\n\t\t\t\t\treturn cache[args]\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\tdef cache_info():\n\t\t\treturn cache.info()\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\twrapper.cache_clear = cache_clear\n\t\twrapper.cache_info = cache_info\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def lru_cache(maxsize=128, typed=False):\n\tif isinstance(maxsize, int):\n\t\tdef decorating_function(user_function):\n\t\t\tcache = LRUCache(maxsize, typed)\n\t\t\t@wraps(user_function)\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tkey = args\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\twrapper.hits += 1\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\twrapper.misses += 1\n\t\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\t\twrapper.hits = wrapper.misses = 0\n\t\t\treturn wrapper\n\t\treturn decorating_function\n\telif maxsize == 0:\n\t\tdef decorating_function(user_function):\n\t\t\t@wraps(user_function)\n\t\t\tdef\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()  # order: least recent to most recent\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# All positional and keyword arguments to the wrapped\n\t\t\t# function must be hashable for LRU cache to work.\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for v in kwds.values())\n\n\t\t\ttry:\n\t\t\t\tresult = cache.pop(key)\n\t\t\t\twrapper.hits += 1\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\twrapper.misses += 1\n\t\t\t\tif len(cache) >= maxsize:\n\t\n", "def lru_cache(maxsize=128, typed=False):\n\tlock = RLock()  # because linkedlist updates aren't threadsafe\n\troot = []  # root of the circular double linked list\n\troot[:] = [root, root, None, None]  # initialize by pointing to self\n\tif maxsize == 0:\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# No caching -- just a statistics update after a successful call\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\twrapper.hits += 1\n\t\t\treturn result\n\telif maxsize is None:\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# Simple caching without ordering or size limit\n\t\t\tcache = wrapper.cache  # local cache for this invocation\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\t\twrapper.hits += 1\n\t\t\t\treturn result\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] =\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache_object(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function)(*args, **kwds)\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "def lru_cache(maxsize=128, typed=False):\n\t# Users should only access the lru_cache through its public API:\n\t#       cache_info, cache_clear, and f.__wrapped__\n\t# The internals of the lru_cache are encapsulated for thread\n\t# safety and to allow the implementation to change (including a\n\t# possible C version).\n\treturn _lru_cache_wrapper(maxsize, typed, _CacheInfo)\n\n\n", "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function, maxsize=maxsize, typed=typed):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(kwds.items())\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in kwds.items())\n\t\t\tresult = cache[key]\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def lru_cache(maxsize=128, typed=False):\n\treturn functools.lru_cache(maxsize, typed)\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tlock = RLock()  # create a lock object\n\tcache = {}\n\tcache_get = cache.get  # bound method to lookup a key or return None\n\tcache_len = cache.__len__  # get cache size\n\n\tdef decorating_function(user_function):\n\t\t# the decorator\n\t\tnonlocal lock\n\t\tnonlocal cache\n\t\tnonlocal cache_get\n\t\tnonlocal cache_len\n\n\t\tsentinel = object()  # unique object used to signal cache misses\n\t\tmake_key = _make_key  # build a key from the function arguments\n\t\troot = []  # root of the circular doubly linked list\n\t\troot[:] = [root, root, None, None]  # initialize by pointing to self\n\t\tnonlocal_root = [root]  # make updateable non-locally\n\t\tPREV, NEXT, KEY, RESULT = 0, 1, 2, 3  # names for the link fields\n\n\t\tif maxsize == 0:\n\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\t# simple caching without ordering or size\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache_object(maxsize, typed)\n\t\t\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\tresult = cache[key] = user_function(*args, **kwds)\n\t\t\treturn result\n\t\t\n\t\tdef cache_info():\n\t\t\treturn cache.info()\n\t\t\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\t\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\t\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tif args in cache:\n\t\t\t\tif cache[args] == _NONE:\n\t\t\t\t\traise KeyError\n\t\t\t\telse:\n\t\t\t\t\treturn cache[args]\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[args] = result\n\t\t\treturn result\n\t\tdef clear():\n\t\t\tcache.clear()\n\t\tdef cache_info():\n\t\t\treturn cache.info()\n\t\tdef cache_clear():\n\t\t\tcache.clear()\n\t\twrapper.cache_clear = cache_clear\n\t\twrapper.cache_info = cache_info\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tif isinstance(maxsize, int):\n\t\tdef decorating_function(user_function):\n\t\t\tcache = LRUCache(maxsize, typed)\n\t\t\t@wraps(user_function)\n\t\t\tdef wrapper(*args, **kwds):\n\t\t\t\tkey = args\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\t\ttry:\n\t\t\t\t\tresult = cache[key]\n\t\t\t\t\twrapper.hits += 1\n\t\t\t\texcept KeyError:\n\t\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\t\twrapper.misses += 1\n\t\t\t\t\tcache[key] = result\n\t\t\t\treturn result\n\t\t\twrapper.hits = wrapper.misses = 0\n\t\t\treturn wrapper\n\t\treturn decorating_function\n\telif maxsize == 0:\n\t\tdef decorating_function(user_function):\n\t\t\t@wraps(user_function)\n\t\t\tdef\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = OrderedDict()  # order: least recent to most recent\n\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# All positional and keyword arguments to the wrapped\n\t\t\t# function must be hashable for LRU cache to work.\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for v in kwds.values())\n\n\t\t\ttry:\n\t\t\t\tresult = cache.pop(key)\n\t\t\t\twrapper.hits += 1\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\twrapper.misses += 1\n\t\t\t\tif len(cache) >= maxsize:\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tlock = RLock()  # because linkedlist updates aren't threadsafe\n\troot = []  # root of the circular double linked list\n\troot[:] = [root, root, None, None]  # initialize by pointing to self\n\tif maxsize == 0:\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# No caching -- just a statistics update after a successful call\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\twrapper.hits += 1\n\t\t\treturn result\n\telif maxsize is None:\n\t\tdef wrapper(*args, **kwds):\n\t\t\t# Simple caching without ordering or size limit\n\t\t\tcache = wrapper.cache  # local cache for this invocation\n\t\t\tkey = make_key(args, kwds, typed)\n\t\t\ttry:\n\t\t\t\tresult = cache[key]\n\t\t\t\twrapper.hits += 1\n\t\t\t\treturn result\n\t\t\texcept KeyError:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lru_cache_object(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function)(*args, **kwds)\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\t# Users should only access the lru_cache through its public API:\n\t#       cache_info, cache_clear, and f.__wrapped__\n\t# The internals of the lru_cache are encapsulated for thread\n\t# safety and to allow the implementation to change (including a\n\t# possible C version).\n\treturn _lru_cache_wrapper(maxsize, typed, _CacheInfo)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function, maxsize=maxsize, typed=typed):\n\t\tcache = LRUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(kwds.items())\n\t\t\tif typed:\n\t\t\t\tkey += tuple(type(v) for v in args)\n\t\t\t\tif kwds:\n\t\t\t\t\tkey += tuple(type(v) for k, v in kwds.items())\n\t\t\tresult = cache[key]\n\t\t\tif result is None:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lru_cache(maxsize=128, typed=False):\n\treturn functools.lru_cache(maxsize, typed)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/func.py", "project": "pexip/os-python-cachetools", "name": "lfu_cache", "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.", "ques_id": "62b8d23748ba5a41d1c3f496", "code": ["def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cached(maxsize, typed, user_function)\n\t\treturn update_wrapper(cache, user_function)\n\treturn decorating_function\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed, _CacheInfo)\n\n", "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LfuCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is not NO_VALUE:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn _lru_cache_wrapper(maxsize, typed, False, None)\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed, True)\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed)\n\n\n", "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cache_object(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function)(*args, **kwds)\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed)\n\n", "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LFUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is _NONE:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn cache.cache_info()\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n\n", "def lfu_cache(maxsize=128, typed=False):\n\treturn lru_cache(maxsize=maxsize, typed=typed,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcache_class=_lfu_cache_factory)\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cached(maxsize, typed, user_function)\n\t\treturn update_wrapper(cache, user_function)\n\treturn decorating_function\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed, _CacheInfo)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LfuCache(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is not NO_VALUE:\n\t\t\t\treturn result\n\t\t\tresult = user_function(*args, **kwds)\n\t\t\tcache[key] = result\n\t\t\treturn result\n\t\treturn update_wrapper(wrapper, user_function)\n\treturn decorating_function\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn _lru_cache_wrapper(maxsize, typed, False, None)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed, True)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = lfu_cache_object(maxsize, typed)\n\t\t@wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\treturn cache(user_function)(*args, **kwds)\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn _lfu_cache(maxsize, typed)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\tdef decorating_function(user_function):\n\t\tcache = LFUCache(maxsize, typed)\n\t\t@functools.wraps(user_function)\n\t\tdef wrapper(*args, **kwds):\n\t\t\tkey = args\n\t\t\tif kwds:\n\t\t\t\tkey += tuple(sorted(kwds.items()))\n\t\t\tresult = cache[key]\n\t\t\tif result is _NONE:\n\t\t\t\tresult = user_function(*args, **kwds)\n\t\t\t\tcache[key] = result\n\t\t\treturn result\n\t\tdef cache_info():\n\t\t\t\"\"\"Report cache statistics\"\"\"\n\t\t\treturn cache.cache_info()\n\t\tdef cache_clear():\n\t\t\t\"\"\"Clear the cache and cache statistics\"\"\"\n\t\t\tcache.clear()\n\t\twrapper.cache_info = cache_info\n\t\twrapper.cache_clear = cache_clear\n\t\treturn wrapper\n\treturn decorating_function\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def lfu_cache(maxsize=128, typed=False):\n\treturn lru_cache(maxsize=maxsize, typed=typed,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcache_class=_lfu_cache_factory)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/fifo.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair first inserted.", "ques_id": "62b8d22f48ba5a41d1c3f488", "code": ["def popitem(self):\n\treturn self._bucket.popitem()\n\t\n", "def popitem(self):\n\tif self.first_node is None:\n\t\traise KeyError(\"dictionary is empty\")\n\tkey = self.first_node.key\n\tvalue = self.first_node.value\n\tself.first_node = self.first_node.next_node\n\treturn key, value\n\n", "def popitem(self):\n\tdef setdefault(self, key, default_value):\n\t\t\"\"\"\n\t\tIf key is in the dictionary, return its value. If not, insert key with a value of default_value and return default_value.\n\t\t\"\"\"\n\t\tdef update(self, iterable):\n\t\t\t\"\"\"\n\t\t\tUpdate the dictionary with the key/value pairs from other, overwriting existing keys.\n\t\t\t\"\"\"\n\t\t\tdef values(self):\n\t\t\t\t\"\"\"\n\t\t\t\tReturn a list of all the values in the dictionary.\n\t\t\t\t\"\"\"\n\t\t\t\tdef items(self):\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tReturn a list of all the key/value pairs in the dictionary.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tdef clear(self):\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\t\tRemove all items from the dictionary.\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\t\tdef copy(self):\n\t\t\t\t\t\t\t\"\"\"\n\t\t\t\t\t\t\tReturn a shallow copy of the dictionary.\n\t\n", "def popitem(self):\n\treturn self.popitem(last=False)\n\n\tdef popitem(self, last=True):\n\t\"\"\"\n\tRemove the value corresponding to the last inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tif last:\n\t\tkey = reversed(self).next()\n\telse:\n\t\tkey = iter(self).next()\n\tvalue = self.pop(key)\n\treturn key, value\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn String representation of the dictionary.\n\t\"\"\"\n\tif not self:\n\t\treturn '%s()' % (self.__class__.__name__,)\n\treturn '%s(%r)' % (self.__class__.__name__, self.items())\n\n\tdef __reduce__(self):\n\t\"\"\"\n\tCallable method for pickle to reduce the dictionary.\n\t\"\"\"\n\treturn self.__class__, (dict(self),)\n\n\tdef copy(self):\n\t\"\"\"\n\tReturn a shallow copy of the dictionary.\n", "def popitem(self):\n\tif self._head:\n\t\treturn self.pop(self._head.key)\n\telse:\n\t\traise KeyError('popitem(): dictionary is empty')\n", "def popitem(self):\n\treturn self.pop_first()\n\n\tdef clear(self):\n\t\"\"\"\n\tRemove all the items from the dictionary.\n\t\"\"\"\n\tself.first = None\n\tself.last = None\n\tself.length = 0\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturns True if the key exists in the dictionary and False if not.\n\t\"\"\"\n\treturn self.has_key(key)\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturns the length of the dictionary.\n\t\"\"\"\n\treturn self.length\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an iterable object for the dictionary.\n\t\"\"\"\n\treturn self.keys()\n\n\tdef __reversed__(self):\n\t\"\"\"\n\tReturns a reversed iterable object for the dictionary.\n\t\"\"\"\n\treturn self.keys(reverse = True)\n\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDeletes the value corresponding to the key.\n\t\"\"\"\n\tself.node_of_key(key).delete()\n\n\tdef __getitem__(self, key):\n\t\n", "def popitem(self):\n\treturn self._odict.popitem()\n\n", "def popitem(self):\n\treturn self._dict.popitem()\n\t\n", "def popitem(self):\n\ttry:\n\t\tkey=next(iter(self.keys()))\n\t\tval=self.pop(key)\n\t\treturn (key,val)\n\texcept StopIteration:\n\t\traise KeyError(\"dictionary is empty\")\n\t\t\n", "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn (key, value)\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\t\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\treturn self._bucket.popitem()\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.first_node is None:\n\t\traise KeyError(\"dictionary is empty\")\n\tkey = self.first_node.key\n\tvalue = self.first_node.value\n\tself.first_node = self.first_node.next_node\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tdef setdefault(self, key, default_value):\n\t\t\"\"\"\n\t\tIf key is in the dictionary, return its value. If not, insert key with a value of default_value and return default_value.\n\t\t\"\"\"\n\t\tdef update(self, iterable):\n\t\t\t\"\"\"\n\t\t\tUpdate the dictionary with the key/value pairs from other, overwriting existing keys.\n\t\t\t\"\"\"\n\t\t\tdef values(self):\n\t\t\t\t\"\"\"\n\t\t\t\tReturn a list of all the values in the dictionary.\n\t\t\t\t\"\"\"\n\t\t\t\tdef items(self):\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tReturn a list of all the key/value pairs in the dictionary.\n\t\t\t\t\t\"\"\"\n\t\t\t\t\tdef clear(self):\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\t\tRemove all items from the dictionary.\n\t\t\t\t\t\t\"\"\"\n\t\t\t\t\t\tdef copy(self):\n\t\t\t\t\t\t\t\"\"\"\n\t\t\t\t\t\t\tReturn a shallow copy of the dictionary.\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self.popitem(last=False)\n\n\tdef popitem(self, last=True):\n\t\"\"\"\n\tRemove the value corresponding to the last inserted key and returns the key and value in tuple format.\n\t\"\"\"\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tif last:\n\t\tkey = reversed(self).next()\n\telse:\n\t\tkey = iter(self).next()\n\tvalue = self.pop(key)\n\treturn key, value\n\n\tdef __repr__(self):\n\t\"\"\"\n\tReturn String representation of the dictionary.\n\t\"\"\"\n\tif not self:\n\t\treturn '%s()' % (self.__class__.__name__,)\n\treturn '%s(%r)' % (self.__class__.__name__, self.items())\n\n\tdef __reduce__(self):\n\t\"\"\"\n\tCallable method for pickle to reduce the dictionary.\n\t\"\"\"\n\treturn self.__class__, (dict(self),)\n\n\tdef copy(self):\n\t\"\"\"\n\tReturn a shallow copy of the dictionary.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self._head:\n\t\treturn self.pop(self._head.key)\n\telse:\n\t\traise KeyError('popitem(): dictionary is empty')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self.pop_first()\n\n\tdef clear(self):\n\t\"\"\"\n\tRemove all the items from the dictionary.\n\t\"\"\"\n\tself.first = None\n\tself.last = None\n\tself.length = 0\n\n\tdef __contains__(self, key):\n\t\"\"\"\n\tReturns True if the key exists in the dictionary and False if not.\n\t\"\"\"\n\treturn self.has_key(key)\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturns the length of the dictionary.\n\t\"\"\"\n\treturn self.length\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an iterable object for the dictionary.\n\t\"\"\"\n\treturn self.keys()\n\n\tdef __reversed__(self):\n\t\"\"\"\n\tReturns a reversed iterable object for the dictionary.\n\t\"\"\"\n\treturn self.keys(reverse = True)\n\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDeletes the value corresponding to the key.\n\t\"\"\"\n\tself.node_of_key(key).delete()\n\n\tdef __getitem__(self, key):\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self._odict.popitem()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self._dict.popitem()\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey=next(iter(self.keys()))\n\t\tval=self.pop(key)\n\t\treturn (key,val)\n\texcept StopIteration:\n\t\traise KeyError(\"dictionary is empty\")\n\t\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn (key, value)\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\t\t\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "setdefault", "docstring": "D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D", "ques_id": "62b8d22a48ba5a41d1c3f47e", "code": ["def setdefault(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self.__getitem__(key)\n\texcept KeyError:\n\t\tself.__setitem__(key, default)\n\t\treturn default\n\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDeletes the key-value pair corresponding to key.\n\t\"\"\"\n\t\ttry:\n\t\t\tself.__dict__.__delitem__(key)\n\t\texcept KeyError:\n\t\t\tself.__dict__.__delitem__(self.__keytransform__(key))\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an iterator over the keys in the dictionary.\n\t\"\"\"\n\t\treturn (self.__keytransform__(key) for key in self.__dict__.__iter__())\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturns the number of key-value pairs in the dictionary.\n\t\"\"\"\n\t\treturn self.__dict__.__len__()\n\n\tdef __keytransform__(self, key):\n\t\"\"\"\n\tTransforms the key using the key_transform function.\n\t\"\"\"\n\t\treturn\n", "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n\n", "def setdefault(self, key, default=None):\n\treturn super().setdefault(key, default)\n\n", "def setdefault(self, key, default=None):\n\tif key in self.__dict__.keys():\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tSimilar to setdefault.\n\t\t\"\"\"\n\t\treturn self.setdefault(key, default)\n\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tUpdate the dictionary with the key-value pairs from other, overwriting existing keys. Return None.\n\t\t\"\"\"\n\t\tif len(args) > 1:\n\t\t\traise TypeError(\"update expected at most 1 arguments, got %d\" % len(args))\n\t\telif args:\n\t\t\tother = args[0]\n\t\t\tif isinstance(other, dict):\n\t\t\t\tfor key in other:\n\t\t\t\t\tself[key] = other[key]\n\t\t\telif hasattr(other, \"keys\"):\n\t\t\t\tfor key in other.keys():\n\t\t\t\t\tself\n", "def setdefault(self, key, default=None):\n\tif key not in self:\n\t\tself[key] = default\n\treturn self[key]\n\n\n", "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n\n", "def setdefault(self, key, default=None):\n\tif self._haskey(key):\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\n", "def setdefault(self, key, default=None):\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\t\t\n", "def setdefault(self, key, default=None):\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\t\n"], "level": "self_contained", "generate_results": [{"generate_code": "def setdefault(self, key, default=None):\n\tif key in self:\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self.__getitem__(key)\n\texcept KeyError:\n\t\tself.__setitem__(key, default)\n\t\treturn default\n\n\tdef __delitem__(self, key):\n\t\"\"\"\n\tDeletes the key-value pair corresponding to key.\n\t\"\"\"\n\t\ttry:\n\t\t\tself.__dict__.__delitem__(key)\n\t\texcept KeyError:\n\t\t\tself.__dict__.__delitem__(self.__keytransform__(key))\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an iterator over the keys in the dictionary.\n\t\"\"\"\n\t\treturn (self.__keytransform__(key) for key in self.__dict__.__iter__())\n\n\tdef __len__(self):\n\t\"\"\"\n\tReturns the number of key-value pairs in the dictionary.\n\t\"\"\"\n\t\treturn self.__dict__.__len__()\n\n\tdef __keytransform__(self, key):\n\t\"\"\"\n\tTransforms the key using the key_transform function.\n\t\"\"\"\n\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\treturn super().setdefault(key, default)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\tif key in self.__dict__.keys():\n\t\treturn self.__dict__[key]\n\telse:\n\t\tself.__dict__[key] = default\n\t\treturn default\n\n\tdef get(self, key, default=None):\n\t\t\"\"\"\n\t\tSimilar to setdefault.\n\t\t\"\"\"\n\t\treturn self.setdefault(key, default)\n\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tUpdate the dictionary with the key-value pairs from other, overwriting existing keys. Return None.\n\t\t\"\"\"\n\t\tif len(args) > 1:\n\t\t\traise TypeError(\"update expected at most 1 arguments, got %d\" % len(args))\n\t\telif args:\n\t\t\tother = args[0]\n\t\t\tif isinstance(other, dict):\n\t\t\t\tfor key in other:\n\t\t\t\t\tself[key] = other[key]\n\t\t\telif hasattr(other, \"keys\"):\n\t\t\t\tfor key in other.keys():\n\t\t\t\t\tself\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\tif key not in self:\n\t\tself[key] = default\n\treturn self[key]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\tself[key] = default\n\t\treturn default\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\tif self._haskey(key):\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def setdefault(self, key, default=None):\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\t\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def setdefault(self, key, default=None):\n\tif key in self.keys():\n\t\treturn self[key]\n\telse:\n\t\tself[key] = default\n\t\treturn default\n\t\n", "is_pass": true, "return_code": 0}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "get", "docstring": "D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.", "ques_id": "62b8d22948ba5a41d1c3f47c", "code": ["def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\n", "def get(self, key, default=None):\n\treturn self.data.get(key, default)\n\n\n\tdef items(self):\n\t\"\"\"\n\tReturns a list of the class's (key, value) pairs, as 2-tuples\n\t\"\"\"\n\treturn self.data.items()\n\n\n\tdef keys(self):\n\t\"\"\"\n\tReturns a list of the class's keys\n\t\"\"\"\n\treturn self.data.keys()\n\n\n\tdef values(self):\n\t\"\"\"\n\tReturns a list of the class's values\n\t\"\"\"\n\treturn self.data.values()\n\n\n\tdef __repr__(self):\n\t\"\"\"\n\tString representation of the class\n\t\"\"\"\n\treturn repr(self.data)\n\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an iterable object of the class's keys\n\t\"\"\"\n\treturn iter(self.data)\n\n\n\tdef __setattr__(self, name, value):\n\t\"\"\"\n\tTakes in a name and value, and if name is a key in the class, sets the value of that key to the value. Otherwise, it sets the attribute of the class.\n\t\"\"\"\n\tif name\n", "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n", "def get(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n\tdef pop(self, key, default=None):\n\t\t\"\"\"\n\t\tIf a key exists in the class, the value corresponding to the key is returned and the key is removed from the class. Otherwise, default is returned.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\t\tdel self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\t\treturn value\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tReturns a random key and value pair from the class. The pair is removed from the class.\n\t\t\"\"\"\n\t\tkey = random.choice(self.keys())\n\t\treturn (key, self.pop(key))\n\n\n", "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\n", "def get(self, key, default=None):\n\tif self.has_key(key):\n\t\treturn self[key]\n\telse:\n\t\treturn default\n\n\n", "def get(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n\tdef update(self, other=None, **kwargs):\n\t\t\"\"\"\n\t\tUpdates the class with the key/value pairs from other, overwriting existing keys.\n\t\t\"\"\"\n\t\tif other is None:\n\t\t\tpass\n\t\telif isinstance(other, dict):\n\t\t\tfor key in other:\n\t\t\t\tself[key] = other[key]\n\t\telse:\n\t\t\tfor key, value in other:\n\t\t\t\tself[key] = value\n\t\tfor key, value in kwargs.items():\n\t\t\tself[key] = value\n\n\tdef pop(self, key, default=None):\n\t\t\"\"\"\n\t\tIf key is in the class, remove it and return its value, else return default.\n\t\tIf default is not given and key is not in the class, a KeyError is raised.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\texcept KeyError:\n\t\t\tif\n", "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n", "def get(self, key, default=None):\n\treturn self.__dict__.get(key,default)\n\n\n\n", "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\treturn self.data.get(key, default)\n\n\n\tdef items(self):\n\t\"\"\"\n\tReturns a list of the class's (key, value) pairs, as 2-tuples\n\t\"\"\"\n\treturn self.data.items()\n\n\n\tdef keys(self):\n\t\"\"\"\n\tReturns a list of the class's keys\n\t\"\"\"\n\treturn self.data.keys()\n\n\n\tdef values(self):\n\t\"\"\"\n\tReturns a list of the class's values\n\t\"\"\"\n\treturn self.data.values()\n\n\n\tdef __repr__(self):\n\t\"\"\"\n\tString representation of the class\n\t\"\"\"\n\treturn repr(self.data)\n\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturns an iterable object of the class's keys\n\t\"\"\"\n\treturn iter(self.data)\n\n\n\tdef __setattr__(self, name, value):\n\t\"\"\"\n\tTakes in a name and value, and if name is a key in the class, sets the value of that key to the value. Otherwise, it sets the attribute of the class.\n\t\"\"\"\n\tif name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n\tdef pop(self, key, default=None):\n\t\t\"\"\"\n\t\tIf a key exists in the class, the value corresponding to the key is returned and the key is removed from the class. Otherwise, default is returned.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\t\tdel self[key]\n\t\texcept KeyError:\n\t\t\treturn default\n\t\treturn value\n\n\tdef popitem(self):\n\t\t\"\"\"\n\t\tReturns a random key and value pair from the class. The pair is removed from the class.\n\t\t\"\"\"\n\t\tkey = random.choice(self.keys())\n\t\treturn (key, self.pop(key))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\tif self.has_key(key):\n\t\treturn self[key]\n\telse:\n\t\treturn default\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\ttry:\n\t\treturn self[key]\n\texcept KeyError:\n\t\treturn default\n\n\tdef update(self, other=None, **kwargs):\n\t\t\"\"\"\n\t\tUpdates the class with the key/value pairs from other, overwriting existing keys.\n\t\t\"\"\"\n\t\tif other is None:\n\t\t\tpass\n\t\telif isinstance(other, dict):\n\t\t\tfor key in other:\n\t\t\t\tself[key] = other[key]\n\t\telse:\n\t\t\tfor key, value in other:\n\t\t\t\tself[key] = value\n\t\tfor key, value in kwargs.items():\n\t\t\tself[key] = value\n\n\tdef pop(self, key, default=None):\n\t\t\"\"\"\n\t\tIf key is in the class, remove it and return its value, else return default.\n\t\tIf default is not given and key is not in the class, a KeyError is raised.\n\t\t\"\"\"\n\t\ttry:\n\t\t\tvalue = self[key]\n\t\texcept KeyError:\n\t\t\tif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\treturn self.__dict__.get(key,default)\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get(self, key, default=None):\n\treturn self.__dict__.get(key, default)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/decorators.py", "project": "pexip/os-python-cachetools", "name": "cachedmethod", "docstring": "Decorator to wrap a class or instance method with a memoizing\ncallable that saves results in a cache.", "ques_id": "62b8d22548ba5a41d1c3f472", "code": ["def cachedmethod(cache, key=hashkey, lock=None):\n\tcache = cache or {}\n\tif lock is None:\n\t\tlock = threading.Lock()\n\tdef decorator(func):\n\t\tdef func_wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\tk = key(*args, **kwargs)\n\t\t\t\tif k not in cache:\n\t\t\t\t\tcache[k] = func(*args, **kwargs)\n\t\t\treturn cache[k]\n\t\treturn func_wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\treturn lambda f: CachedMethod(cache, f, key, lock=lock)\n\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\thashed = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\treturn cache[hashed]\n\t\t\texcept KeyError:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[hashed]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache[hashed] = value\n\t\t\t\t\treturn value\n\t\t\t\tfinally:\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tcache_key = key(*args, **kwargs)\n\t\t\t\tif cache_key in cache:\n\t\t\t\t\tresult = cache[cache_key]\n\t\t\t\telse:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache[cache_key] = result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\tr = cache.get(key(args, kwargs))\n\t\t\tif r is not None:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn r\n\t\t\tr = f(*args, **kwargs)\n\t\t\tcache[key(args, kwargs)] = r\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\treturn r\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\t\n\tdef decorator(method):\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\t# Get the arguments\n\t\t\targs = list(args)\n\t\t\t\n\t\t\t# Get the key\n\t\t\tckey = key(*args, **kwargs)\n\t\t\t\n\t\t\t# Get the result from the cache\n\t\t\tresult = cache.get(ckey)\n\t\t\t\n\t\t\t# If the result is None\n\t\t\tif result is None:\n\t\t\t\t\n\t\t\t\t# Get the lock\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\t\n\t\t\t\t# Get the result from the cache\n\t\t\t\tresult = cache.get(ckey)\n\t\t\t\t\n\t\t\t\t# If the result is None\n\t\t\t\tif result is None:\n\t\t\t\t\t\n\t\t\t\t\t# Get the result\n\t\t\t\t\tresult = method(*args, **kwargs)\n\t\t\t\t\t\n\t\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tname = func.__name__\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache.get(key)\n\t\t\t\tif result is not None:\n\t\t\t\t\treturn result\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(method):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not cache:\n\t\t\t\treturn method(*args, **kwargs)\n\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tif not cache.has_key(key(*args, **kwargs)):\n\t\t\t\t\tcache[key(*args, **kwargs)] = method(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn cache[key(*args, **kwargs)]\n\t\treturn wrapper\n\treturn decorator\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef _cachedmethod(func):\n\t\t\"\"\"\n\t\tDecorator function to call a result in the cache.\n\t\t\"\"\"\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef __cachedmethod(self, *args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tDecorated function to call a result in the cache.\n\t\t\t\"\"\"\n\t\t\t# Get the key\n\t\t\tckey = key(self, *args, **kwargs)\n\t\t\t# Try getting the result from the cache\n\t\t\twith lock:\n\t\t\t\tresult = cache.get(ckey)\n\t\t\tif result is None:\n\t\t\t\t# The result was not in the cache, so get it\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\t# Add the result to the cache\n\t\t\t\twith lock:\n\t\t\t\t\tcache[ckey] = result\n\t\t\treturn result\n\t\treturn __cachedmethod\n\treturn _cachedmethod\n\n\n", "def cachedmethod(cache, key=hashkey, lock=None):\n\tlock = lock if lock else threading.Lock()\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tif k in cache:\n\t\t\t\t\treturn cache[k]\n\t\t\tv = func(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tcache[k] = v\n\t\t\treturn v\n\t\treturn wrapper\n\treturn decorator\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tcache = cache or {}\n\tif lock is None:\n\t\tlock = threading.Lock()\n\tdef decorator(func):\n\t\tdef func_wrapper(*args, **kwargs):\n\t\t\twith lock:\n\t\t\t\tk = key(*args, **kwargs)\n\t\t\t\tif k not in cache:\n\t\t\t\t\tcache[k] = func(*args, **kwargs)\n\t\t\treturn cache[k]\n\t\treturn func_wrapper\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\treturn lambda f: CachedMethod(cache, f, key, lock=lock)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\thashed = key(*args, **kwargs)\n\t\t\ttry:\n\t\t\t\treturn cache[hashed]\n\t\t\texcept KeyError:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.acquire()\n\t\t\t\ttry:\n\t\t\t\t\treturn cache[hashed]\n\t\t\t\texcept KeyError:\n\t\t\t\t\tvalue = func(*args, **kwargs)\n\t\t\t\t\tcache[hashed] = value\n\t\t\t\t\treturn value\n\t\t\t\tfinally:\n\t\t\t\t\tif lock:\n\t\t\t\t\t\tlock.release()\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tcache_key = key(*args, **kwargs)\n\t\t\t\tif cache_key in cache:\n\t\t\t\t\tresult = cache[cache_key]\n\t\t\t\telse:\n\t\t\t\t\tresult = func(*args, **kwargs)\n\t\t\t\t\tcache[cache_key] = result\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(f):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\tr = cache.get(key(args, kwargs))\n\t\t\tif r is not None:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\t\treturn r\n\t\t\tr = f(*args, **kwargs)\n\t\t\tcache[key(args, kwargs)] = r\n\t\t\tif lock:\n\t\t\t\tlock.release()\n\t\t\treturn r\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\t\n\tdef decorator(method):\n\t\t\n\t\tdef wrapper(*args, **kwargs):\n\t\t\t\n\t\t\t# Get the arguments\n\t\t\targs = list(args)\n\t\t\t\n\t\t\t# Get the key\n\t\t\tckey = key(*args, **kwargs)\n\t\t\t\n\t\t\t# Get the result from the cache\n\t\t\tresult = cache.get(ckey)\n\t\t\t\n\t\t\t# If the result is None\n\t\t\tif result is None:\n\t\t\t\t\n\t\t\t\t# Get the lock\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.acquire()\n\t\t\t\t\n\t\t\t\t# Get the result from the cache\n\t\t\t\tresult = cache.get(ckey)\n\t\t\t\t\n\t\t\t\t# If the result is None\n\t\t\t\tif result is None:\n\t\t\t\t\t\n\t\t\t\t\t# Get the result\n\t\t\t\t\tresult = method(*args, **kwargs)\n\t\t\t\t\t\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(self, *args, **kwargs):\n\t\t\tname = func.__name__\n\t\t\tif lock is not None:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tresult = cache.get(key)\n\t\t\t\tif result is not None:\n\t\t\t\t\treturn result\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\tcache[key] = result\n\t\t\tfinally:\n\t\t\t\tif lock is not None:\n\t\t\t\t\tlock.release()\n\t\t\treturn result\n\t\treturn wrapper\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef decorator(method):\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tif not cache:\n\t\t\t\treturn method(*args, **kwargs)\n\n\t\t\tif lock:\n\t\t\t\tlock.acquire()\n\t\t\ttry:\n\t\t\t\tif not cache.has_key(key(*args, **kwargs)):\n\t\t\t\t\tcache[key(*args, **kwargs)] = method(*args, **kwargs)\n\t\t\tfinally:\n\t\t\t\tif lock:\n\t\t\t\t\tlock.release()\n\t\t\treturn cache[key(*args, **kwargs)]\n\t\treturn wrapper\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tdef _cachedmethod(func):\n\t\t\"\"\"\n\t\tDecorator function to call a result in the cache.\n\t\t\"\"\"\n\t\tif lock is None:\n\t\t\tlock = threading.Lock()\n\t\tdef __cachedmethod(self, *args, **kwargs):\n\t\t\t\"\"\"\n\t\t\tDecorated function to call a result in the cache.\n\t\t\t\"\"\"\n\t\t\t# Get the key\n\t\t\tckey = key(self, *args, **kwargs)\n\t\t\t# Try getting the result from the cache\n\t\t\twith lock:\n\t\t\t\tresult = cache.get(ckey)\n\t\t\tif result is None:\n\t\t\t\t# The result was not in the cache, so get it\n\t\t\t\tresult = func(self, *args, **kwargs)\n\t\t\t\t# Add the result to the cache\n\t\t\t\twith lock:\n\t\t\t\t\tcache[ckey] = result\n\t\t\treturn result\n\t\treturn __cachedmethod\n\treturn _cachedmethod\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def cachedmethod(cache, key=hashkey, lock=None):\n\tlock = lock if lock else threading.Lock()\n\tdef decorator(func):\n\t\t@functools.wraps(func)\n\t\tdef wrapper(*args, **kwargs):\n\t\t\tk = key(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tif k in cache:\n\t\t\t\t\treturn cache[k]\n\t\t\tv = func(*args, **kwargs)\n\t\t\twith lock:\n\t\t\t\tcache[k] = v\n\t\t\treturn v\n\t\treturn wrapper\n\treturn decorator\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "extostr", "docstring": "Format an exception.\n:param e: Any exception instance.\n:type e: Exception\n:param max_level: Maximum call stack level (default 30)\n:type max_level: int\n:param max_path_level: Maximum path level (default 5)\n:type max_path_level: int\n:return The exception readable string\n:rtype str", "ques_id": "62b8c517e0d34b282c18122e", "code": ["def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level <= 0:\n\t\treturn \"...\"\n\texc_str = '%s: %s\\n' % (e.__class__.__name__, str(e))\n\tif isinstance(e, EnvironmentError):\n\t\texc_str += 'Errno: ' + str(e.errno or '') + '\\n'\n\t\texc_str += 'Filename: ' + str(e.filename or '') + '\\n'\n\t\texc_str += 'Strerror: ' + str(e.strerror or '') + '\\n'\n\texc_str += ''.join(traceback.format_tb(e.__traceback__))\n\texc_str += \"\".join(traceback.format_exception_only(type(e), e))\n\tif isinstance(e, EnvironmentError) and e.__context__ is not None:\n\t\texc_str += \"During handling of the above exception, another exception \" \\\n\t\t\t\"occurred:\\n\\n\"\n\t\texc_str += cls.extostr(e.\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level == 0:\n\t\treturn \"...\"\n\te_str = str(e)\n\tif max_path_level > 0:\n\t\te_str = e_str.replace(r\"\\\\\", \"/\")\n\t\te_str = re.sub(r\"(?<=\\w)/\", \"\", e_str, max_path_level)\n\tif e.__cause__:\n\t\te_cause_str = cls.extostr(e.__cause__, max_level-1, max_path_level)\n\t\te_str += \"; caused by \" + e_cause_str\n\tif e.__context__:\n\t\te_context_str = cls.extostr(e.__context__, max_level-1, max_path_level)\n\t\te_str += \"; while \" + e_context_str\n\treturn e_str\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tlines = []\n\tlevel = 0\n\tpath_level = 0\n\twhile e:\n\t\tlines.append(cls.lines_from_exception(e))\n\t\tif max_level is not None and level >= max_level:\n\t\t\tlines.append(\"(Maximum exception level reached)\")\n\t\t\tbreak\n\t\te = e.__cause__\n\t\tlevel += 1\n\twhile e.__context__:\n\t\tlines.append(cls.lines_from_exception(e.__context__))\n\t\tif max_path_level is not None and path_level >= max_path_level:\n\t\t\tlines.append(\"(Maximum exception context level reached)\")\n\t\t\tbreak\n\t\te = e.__context__\n\t\tpath_level += 1\n\n\tlines.reverse()\n\treturn ''.join(lines)\n\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif isinstance(e, basestring):\n\t\treturn e\n\tif e is None:\n\t\treturn \"\"\n\tif not isinstance(e, BaseException):\n\t\te = Exception(str(e))\n\n\tif max_level <= 0:\n\t\treturn \"<Maximum recursion level reached>\"\n\n\tif isinstance(e, KeyboardInterrupt):\n\t\treturn \"KeyboardInterrupt\"\n\n\tif isinstance(e, (SystemExit, MemoryError)):\n\t\treturn str(e)\n\n\tif isinstance(e, (ImportError, EnvironmentError)):\n\t\treturn str(e)\n\n\tif isinstance(e, (SyntaxError, IndentationError)):\n\t\treturn str(e)\n\n\tif isinstance(e, unicode):\n\t\treturn e.encode('utf8')\n\n\tif isinstance(e, str):\n\t\treturn e\n\n\tif isinstance(e, Exception):\n\t\tif hasattr(e, 'message') and e.message:\n\t\t\treturn e.message\n\t\tif hasattr(e, 'msg') and e.msg\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tlevel = 0\n\tpath_level = 0\n\tmsg = ''\n\twhile e:\n\t\tif level > max_level:\n\t\t\tmsg += '\\nMaximum exception level reached'\n\t\t\tbreak\n\t\tif path_level > max_level:\n\t\t\tmsg += '\\nMaximum path level reached'\n\t\t\tbreak\n\t\tif msg:\n\t\t\tmsg += '\\n\\n'\n\t\tmsg += '%s: %s' % (cls.__name__, e)\n\t\tif hasattr(e, '__context__') and e.__context__ is not None:\n\t\t\te = e.__context__\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif hasattr(e, '__cause__') and e.__cause__ is not None:\n\t\t\te = e.__cause__\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif hasattr(e, '__traceback__') and e.__traceback__ is not None:\n\t\t\te = e.__traceback__\n\t\t\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level < 0:\n\t\treturn '<Maximum recursion level reached>'\n\ttb = e.__traceback__\n\tif tb is None:\n\t\treturn '<no traceback>'\n\treturn ''.join(extostrlist(cls, e, max_level, max_path_level))\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level == 0:\n\t\treturn \"...\"\n\tif isinstance(e, (str, unicode)):\n\t\treturn e\n\tif isinstance(e, (list, tuple)):\n\t\treturn \"\\n\".join(map(lambda x: extostr(x, max_level-1, max_path_level), e))\n\tif isinstance(e, dict):\n\t\treturn \"\\n\".join(map(lambda x: \"%s: %s\" % (x, extostr(e[x], max_level-1, max_path_level)), e.keys()))\n\tif isinstance(e, Exception):\n\t\treturn str(e)\n\treturn repr(e)\n\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level == 0:\n\t\treturn '...'\n\n\te_type, e_value, e_tb = e.exc_info()\n\tf = e_tb.tb_frame\n\tf_locals = f.f_locals\n\tf_globals = f.f_globals\n\n\tresult = []\n\n\t# traceback\n\twhile e_tb:\n\t\tresult.append('  File \"%s\", line %d, in %s' % (e_tb.tb_frame.f_code.co_filename, e_tb.tb_lineno, e_tb.tb_frame.f_code.co_name))\n\t\te_tb = e_tb.tb_next\n\n\t# exception\n\tresult.append('%s: %s' % (e_type.__name__, str(e_value)))\n\n\t# locals\n\tif len(f_locals):\n\t\tresult.append('  locals:')\n\t\tfor k, v in f_locals.iteritems():\n\t\t\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tl = []\n\tl.append(str(e))\n\tif hasattr(e, '__traceback__'):\n\t\tt = e.__traceback__\n\t\twhile t is not None and max_level > 0:\n\t\t\tmax_level -= 1\n\t\t\tl.append('  File \"%s\", line %d, in %s' % (\n\t\t\t\t\tt.tb_frame.f_code.co_filename,\n\t\t\t\t\tt.tb_lineno,\n\t\t\t\t\tt.tb_frame.f_code.co_name))\n\t\t\tif hasattr(t.tb_frame, 'f_locals'):\n\t\t\t\tl.append('    Locals: %s' % (', '.join(\n\t\t\t\t\t\t\t\t\t\t'%s=%s' % (k, v)\n\t\t\t\t\t\t\t\t\t\tfor k, v in t.tb_frame.f_locals.items())))\n\t\t\telse:\n\t\t\t\tl.append('\n", "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level < 0:\n\t\treturn '<Maximum recursion level reached>'\n\tmax_level -= 1\n\tmax_path_level -= 1\n\tformatted = []\n\tformatted.append('%s: %s' % (e.__class__.__name__, str(e)))\n\tif max_path_level >= 0 and len(e.args) > 0:\n\t\tformatted.append('  Arguments:')\n\t\tfor arg in e.args:\n\t\t\tif isinstance(arg, Exception):\n\t\t\t\tformatted.append('    %s' % extostr(cls, arg, max_level, max_path_level))\n\t\t\telse:\n\t\t\t\tformatted.append('    %s' % arg)\n\t#if len(e.cause) > 0:\n\t#\tformatted.append('  Caused by:')\n\t#\tfor cause in e.cause:\n\t#\t\tformatted.append('    %s' % extostr(cls, cause, max_level, max_path_level))\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level <= 0:\n\t\treturn \"...\"\n\texc_str = '%s: %s\\n' % (e.__class__.__name__, str(e))\n\tif isinstance(e, EnvironmentError):\n\t\texc_str += 'Errno: ' + str(e.errno or '') + '\\n'\n\t\texc_str += 'Filename: ' + str(e.filename or '') + '\\n'\n\t\texc_str += 'Strerror: ' + str(e.strerror or '') + '\\n'\n\texc_str += ''.join(traceback.format_tb(e.__traceback__))\n\texc_str += \"\".join(traceback.format_exception_only(type(e), e))\n\tif isinstance(e, EnvironmentError) and e.__context__ is not None:\n\t\texc_str += \"During handling of the above exception, another exception \" \\\n\t\t\t\"occurred:\\n\\n\"\n\t\texc_str += cls.extostr(e.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level == 0:\n\t\treturn \"...\"\n\te_str = str(e)\n\tif max_path_level > 0:\n\t\te_str = e_str.replace(r\"\\\\\", \"/\")\n\t\te_str = re.sub(r\"(?<=\\w)/\", \"\", e_str, max_path_level)\n\tif e.__cause__:\n\t\te_cause_str = cls.extostr(e.__cause__, max_level-1, max_path_level)\n\t\te_str += \"; caused by \" + e_cause_str\n\tif e.__context__:\n\t\te_context_str = cls.extostr(e.__context__, max_level-1, max_path_level)\n\t\te_str += \"; while \" + e_context_str\n\treturn e_str\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tlines = []\n\tlevel = 0\n\tpath_level = 0\n\twhile e:\n\t\tlines.append(cls.lines_from_exception(e))\n\t\tif max_level is not None and level >= max_level:\n\t\t\tlines.append(\"(Maximum exception level reached)\")\n\t\t\tbreak\n\t\te = e.__cause__\n\t\tlevel += 1\n\twhile e.__context__:\n\t\tlines.append(cls.lines_from_exception(e.__context__))\n\t\tif max_path_level is not None and path_level >= max_path_level:\n\t\t\tlines.append(\"(Maximum exception context level reached)\")\n\t\t\tbreak\n\t\te = e.__context__\n\t\tpath_level += 1\n\n\tlines.reverse()\n\treturn ''.join(lines)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif isinstance(e, basestring):\n\t\treturn e\n\tif e is None:\n\t\treturn \"\"\n\tif not isinstance(e, BaseException):\n\t\te = Exception(str(e))\n\n\tif max_level <= 0:\n\t\treturn \"<Maximum recursion level reached>\"\n\n\tif isinstance(e, KeyboardInterrupt):\n\t\treturn \"KeyboardInterrupt\"\n\n\tif isinstance(e, (SystemExit, MemoryError)):\n\t\treturn str(e)\n\n\tif isinstance(e, (ImportError, EnvironmentError)):\n\t\treturn str(e)\n\n\tif isinstance(e, (SyntaxError, IndentationError)):\n\t\treturn str(e)\n\n\tif isinstance(e, unicode):\n\t\treturn e.encode('utf8')\n\n\tif isinstance(e, str):\n\t\treturn e\n\n\tif isinstance(e, Exception):\n\t\tif hasattr(e, 'message') and e.message:\n\t\t\treturn e.message\n\t\tif hasattr(e, 'msg') and e.msg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tlevel = 0\n\tpath_level = 0\n\tmsg = ''\n\twhile e:\n\t\tif level > max_level:\n\t\t\tmsg += '\\nMaximum exception level reached'\n\t\t\tbreak\n\t\tif path_level > max_level:\n\t\t\tmsg += '\\nMaximum path level reached'\n\t\t\tbreak\n\t\tif msg:\n\t\t\tmsg += '\\n\\n'\n\t\tmsg += '%s: %s' % (cls.__name__, e)\n\t\tif hasattr(e, '__context__') and e.__context__ is not None:\n\t\t\te = e.__context__\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif hasattr(e, '__cause__') and e.__cause__ is not None:\n\t\t\te = e.__cause__\n\t\t\tlevel += 1\n\t\t\tcontinue\n\t\tif hasattr(e, '__traceback__') and e.__traceback__ is not None:\n\t\t\te = e.__traceback__\n\t\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level < 0:\n\t\treturn '<Maximum recursion level reached>'\n\ttb = e.__traceback__\n\tif tb is None:\n\t\treturn '<no traceback>'\n\treturn ''.join(extostrlist(cls, e, max_level, max_path_level))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level == 0:\n\t\treturn \"...\"\n\tif isinstance(e, (str, unicode)):\n\t\treturn e\n\tif isinstance(e, (list, tuple)):\n\t\treturn \"\\n\".join(map(lambda x: extostr(x, max_level-1, max_path_level), e))\n\tif isinstance(e, dict):\n\t\treturn \"\\n\".join(map(lambda x: \"%s: %s\" % (x, extostr(e[x], max_level-1, max_path_level)), e.keys()))\n\tif isinstance(e, Exception):\n\t\treturn str(e)\n\treturn repr(e)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level == 0:\n\t\treturn '...'\n\n\te_type, e_value, e_tb = e.exc_info()\n\tf = e_tb.tb_frame\n\tf_locals = f.f_locals\n\tf_globals = f.f_globals\n\n\tresult = []\n\n\t# traceback\n\twhile e_tb:\n\t\tresult.append('  File \"%s\", line %d, in %s' % (e_tb.tb_frame.f_code.co_filename, e_tb.tb_lineno, e_tb.tb_frame.f_code.co_name))\n\t\te_tb = e_tb.tb_next\n\n\t# exception\n\tresult.append('%s: %s' % (e_type.__name__, str(e_value)))\n\n\t# locals\n\tif len(f_locals):\n\t\tresult.append('  locals:')\n\t\tfor k, v in f_locals.iteritems():\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tl = []\n\tl.append(str(e))\n\tif hasattr(e, '__traceback__'):\n\t\tt = e.__traceback__\n\t\twhile t is not None and max_level > 0:\n\t\t\tmax_level -= 1\n\t\t\tl.append('  File \"%s\", line %d, in %s' % (\n\t\t\t\t\tt.tb_frame.f_code.co_filename,\n\t\t\t\t\tt.tb_lineno,\n\t\t\t\t\tt.tb_frame.f_code.co_name))\n\t\t\tif hasattr(t.tb_frame, 'f_locals'):\n\t\t\t\tl.append('    Locals: %s' % (', '.join(\n\t\t\t\t\t\t\t\t\t\t'%s=%s' % (k, v)\n\t\t\t\t\t\t\t\t\t\tfor k, v in t.tb_frame.f_locals.items())))\n\t\t\telse:\n\t\t\t\tl.append('\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extostr(cls, e, max_level=30, max_path_level=5):\n\tif max_level < 0:\n\t\treturn '<Maximum recursion level reached>'\n\tmax_level -= 1\n\tmax_path_level -= 1\n\tformatted = []\n\tformatted.append('%s: %s' % (e.__class__.__name__, str(e)))\n\tif max_path_level >= 0 and len(e.args) > 0:\n\t\tformatted.append('  Arguments:')\n\t\tfor arg in e.args:\n\t\t\tif isinstance(arg, Exception):\n\t\t\t\tformatted.append('    %s' % extostr(cls, arg, max_level, max_path_level))\n\t\t\telse:\n\t\t\t\tformatted.append('    %s' % arg)\n\t#if len(e.cause) > 0:\n\t#\tformatted.append('  Caused by:')\n\t#\tfor cause in e.cause:\n\t#\t\tformatted.append('    %s' % extostr(cls, cause, max_level, max_path_level))\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "append_text_to_file", "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int", "ques_id": "62b8bbbfe0d34b282c181210", "code": ["def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tflags = 'w' if overwrite else 'a+'\n\n\twith codecs.open(file_name, flags, encoding) as outfile:\n\t\toutfile.write(text_buffer.getvalue())\n\n\treturn True\n\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\t# Don't append to the file, overwrite it\n\t\twith codecs.open(file_name, \"w\", encoding) as f:\n\t\t\tfor line in text_buffer:\n\t\t\t\tf.write(line)\n\telse:\n\t\t# Append to the file\n\t\twith codecs.open(file_name, \"a\", encoding) as f:\n\t\t\tfor line in text_buffer:\n\t\t\t\tf.write(line)\n\t\t\t\t\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\tfile_handle = codecs.open(file_name, mode, encoding)\n\tfile_handle.write(text_buffer)\n\tfile_handle.close()\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif os.path.exists(file_name) and overwrite is False:\n\t\treturn False\n\t\n\tif not os.path.exists(os.path.dirname(file_name)):\n\t\tos.makedirs(os.path.dirname(file_name))\n\t\n\ttext_file = open(file_name, 'w+', encoding=encoding)\n\ttext_file.write(text_buffer)\n\ttext_file.close()\n\t\n\treturn True\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\topen_mode = \"w\"\n\telse:\n\t\topen_mode = \"a\" # Append if the file exists\n\t\t\n\twith open(file_name, open_mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n\t\n\t\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t#TODO: check for errors\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t# Open the file in write mode.\n\tif overwrite:\n\t\tf = codecs.open(file_name, 'w', encoding)\n\telse:\n\t\tf = codecs.open(file_name, 'a', encoding)\n\tf.write(text_buffer)\n\tf.close()\n\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\t\t\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\t\n\t\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\ttry:\n\t\tfile_mode = \"w\" if overwrite else \"a\"\n\t\twith codecs.open(file_name, file_mode, encoding) as f:\n\t\t\tf.write(text_buffer)\n\t\t\tf.write(\"\\n\")\n\texcept:\n\t\tpass\n\n", "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif (overwrite):\n\t\tfile_mode = 'w'\n\telse:\n\t\tfile_mode = 'a'\n\n\tfile_object = codecs.open(file_name, file_mode, encoding)\n\tfile_object.write(text_buffer)\n\tfile_object.close()\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tflags = 'w' if overwrite else 'a+'\n\n\twith codecs.open(file_name, flags, encoding) as outfile:\n\t\toutfile.write(text_buffer.getvalue())\n\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\t# Don't append to the file, overwrite it\n\t\twith codecs.open(file_name, \"w\", encoding) as f:\n\t\t\tfor line in text_buffer:\n\t\t\t\tf.write(line)\n\telse:\n\t\t# Append to the file\n\t\twith codecs.open(file_name, \"a\", encoding) as f:\n\t\t\tfor line in text_buffer:\n\t\t\t\tf.write(line)\n\t\t\t\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\tfile_handle = codecs.open(file_name, mode, encoding)\n\tfile_handle.write(text_buffer)\n\tfile_handle.close()\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif os.path.exists(file_name) and overwrite is False:\n\t\treturn False\n\t\n\tif not os.path.exists(os.path.dirname(file_name)):\n\t\tos.makedirs(os.path.dirname(file_name))\n\t\n\ttext_file = open(file_name, 'w+', encoding=encoding)\n\ttext_file.write(text_buffer)\n\ttext_file.close()\n\t\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\topen_mode = \"w\"\n\telse:\n\t\topen_mode = \"a\" # Append if the file exists\n\t\t\n\twith open(file_name, open_mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n\t\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t#TODO: check for errors\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\twith open(file_name, mode, encoding=encoding) as f:\n\t\tf.write(text_buffer)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\t# Open the file in write mode.\n\tif overwrite:\n\t\tf = codecs.open(file_name, 'w', encoding)\n\telse:\n\t\tf = codecs.open(file_name, 'a', encoding)\n\tf.write(text_buffer)\n\tf.close()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif overwrite:\n\t\tmode = 'w'\n\telse:\n\t\tmode = 'a'\n\t\t\n\twith open(file_name, mode, encoding=encoding) as file:\n\t\tfile.write(text_buffer)\n\t\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\ttry:\n\t\tfile_mode = \"w\" if overwrite else \"a\"\n\t\twith codecs.open(file_name, file_mode, encoding) as f:\n\t\t\tf.write(text_buffer)\n\t\t\tf.write(\"\\n\")\n\texcept:\n\t\tpass\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n\tif (overwrite):\n\t\tfile_mode = 'w'\n\telse:\n\t\tfile_mode = 'a'\n\n\tfile_object = codecs.open(file_name, file_mode, encoding)\n\tfile_object.write(text_buffer)\n\tfile_object.close()\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "file_to_textbuffer", "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str", "ques_id": "62b8bbbfe0d34b282c18120f", "code": ["def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = gtk.TextBuffer()\n\ttry:\n\t\tfile = open(file_name, 'r')\n\t\ttext = unicode(file.read(), encoding)\n\t\tfile.close()\n\t\ttextbuffer.set_text(text)\n\texcept:\n\t\tpass\n\treturn textbuffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\tfile_obj = open(file_name, 'r')\n\t\tcontent = file_obj.read()\n\t\ttextbuffer = gtk.TextBuffer()\n\t\ttextbuffer.set_text(content)\n\t\tfile_obj.close()\n\t\treturn textbuffer\n\texcept:\n\t\treturn False\n\n", "def file_to_textbuffer(file_name, encoding):\n\tf_content = None\n\ttry:\n\t\tf_content = open(file_name, \"r\").readlines()\n\texcept:\n\t\tprint(\"Error opening file \" + file_name)\n\n\t# Create a new text buffer\n\ttextbuffer = Gtk.TextBuffer()\n\n\t# Insert text, iterating over the lines of the file\n\ttext = \"\"\n\tfor line in f_content:\n\t\ttext = text + line\n\n\t# Decode to unicode\n\ttext = clean_string(text)\n\ttext = text.decode(encoding).encode(\"utf-8\")\n\n\ttextbuffer.set_text(text)\n\n\treturn textbuffer\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttext_buffer = Gtk.TextBuffer()\n\ttry:\n\t\twith open(file_name, encoding=encoding) as f:\n\t\t\ttext_buffer.set_text(f.read())\n\t\t\tf.close()\n\texcept:\n\t\twith open(file_name, encoding='utf8') as f:\n\t\t\ttext_buffer.set_text(f.read())\n\t\t\tf.close()\n\treturn text_buffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\tfile_descriptor = open(file_name, \"r\")\n\t\tfile_content = file_descriptor.read()\n\t\tfile_descriptor.close()\n\t\treturn file_content\n\texcept:\n\t\treturn \"\"\n\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttext_buffer = gtk.TextBuffer()\n\t\n\ttry:\n\t\twith open(file_name, \"r\") as f:\n\t\t\ttext_buffer.set_text(f.read())\n\t\n\texcept IOError as e:\n\t\tprint e\n\t\treturn None\n\t\n\treturn text_buffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = Gtk.TextBuffer()\n\ttry:\n\t\twith open(file_name, encoding=encoding) as file_handler:\n\t\t\ttextbuffer.set_text(file_handler.read())\n\texcept OSError:\n\t\tpass\n\treturn textbuffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\t# Read the file content\n\twith open(file_name, 'r', encoding=encoding) as file:\n\t\tfile_content = file.read()\n\t# Create a text buffer\n\tbuffer = Gtk.TextBuffer()\n\t# Load the file content\n\tbuffer.set_text(file_content)\n\t# Return the buffer\n\treturn buffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = Gtk.TextBuffer()\n\ttry:\n\t\twith open(file_name, 'r') as f:\n\t\t\ttext = f.read()\n\texcept Exception as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\ttry:\n\t\ttext = text.decode(encoding)\n\texcept Exception as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\ttextbuffer.set_text(text)\n\treturn textbuffer\n\n", "def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = gtk.TextBuffer()\n\n\ttry:\n\t\tf = open(file_name, 'r')\n\texcept Exception, e:\n\t\tprint \"Error loading file %s: %s\" % (file_name, e)\n\t\treturn textbuffer\n\n\ttext = f.read()\n\n\tif encoding != None:\n\t\ttry:\n\t\t\ttext = unicode(text, encoding)\n\t\texcept Exception, e:\n\t\t\tprint \"Error decoding file %s: %s\" % (file_name, e)\n\t\t\treturn textbuffer\n\n\ttextbuffer.set_text(text)\n\tf.close()\n\n\treturn textbuffer\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = gtk.TextBuffer()\n\ttry:\n\t\tfile = open(file_name, 'r')\n\t\ttext = unicode(file.read(), encoding)\n\t\tfile.close()\n\t\ttextbuffer.set_text(text)\n\texcept:\n\t\tpass\n\treturn textbuffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\tfile_obj = open(file_name, 'r')\n\t\tcontent = file_obj.read()\n\t\ttextbuffer = gtk.TextBuffer()\n\t\ttextbuffer.set_text(content)\n\t\tfile_obj.close()\n\t\treturn textbuffer\n\texcept:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\tf_content = None\n\ttry:\n\t\tf_content = open(file_name, \"r\").readlines()\n\texcept:\n\t\tprint(\"Error opening file \" + file_name)\n\n\t# Create a new text buffer\n\ttextbuffer = Gtk.TextBuffer()\n\n\t# Insert text, iterating over the lines of the file\n\ttext = \"\"\n\tfor line in f_content:\n\t\ttext = text + line\n\n\t# Decode to unicode\n\ttext = clean_string(text)\n\ttext = text.decode(encoding).encode(\"utf-8\")\n\n\ttextbuffer.set_text(text)\n\n\treturn textbuffer\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttext_buffer = Gtk.TextBuffer()\n\ttry:\n\t\twith open(file_name, encoding=encoding) as f:\n\t\t\ttext_buffer.set_text(f.read())\n\t\t\tf.close()\n\texcept:\n\t\twith open(file_name, encoding='utf8') as f:\n\t\t\ttext_buffer.set_text(f.read())\n\t\t\tf.close()\n\treturn text_buffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttry:\n\t\tfile_descriptor = open(file_name, \"r\")\n\t\tfile_content = file_descriptor.read()\n\t\tfile_descriptor.close()\n\t\treturn file_content\n\texcept:\n\t\treturn \"\"\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttext_buffer = gtk.TextBuffer()\n\t\n\ttry:\n\t\twith open(file_name, \"r\") as f:\n\t\t\ttext_buffer.set_text(f.read())\n\t\n\texcept IOError as e:\n\t\tprint e\n\t\treturn None\n\t\n\treturn text_buffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = Gtk.TextBuffer()\n\ttry:\n\t\twith open(file_name, encoding=encoding) as file_handler:\n\t\t\ttextbuffer.set_text(file_handler.read())\n\texcept OSError:\n\t\tpass\n\treturn textbuffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\t# Read the file content\n\twith open(file_name, 'r', encoding=encoding) as file:\n\t\tfile_content = file.read()\n\t# Create a text buffer\n\tbuffer = Gtk.TextBuffer()\n\t# Load the file content\n\tbuffer.set_text(file_content)\n\t# Return the buffer\n\treturn buffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = Gtk.TextBuffer()\n\ttry:\n\t\twith open(file_name, 'r') as f:\n\t\t\ttext = f.read()\n\texcept Exception as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\ttry:\n\t\ttext = text.decode(encoding)\n\texcept Exception as e:\n\t\tprint(\"Error: %s\" % e)\n\t\treturn None\n\ttextbuffer.set_text(text)\n\treturn textbuffer\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def file_to_textbuffer(file_name, encoding):\n\ttextbuffer = gtk.TextBuffer()\n\n\ttry:\n\t\tf = open(file_name, 'r')\n\texcept Exception, e:\n\t\tprint \"Error loading file %s: %s\" % (file_name, e)\n\t\treturn textbuffer\n\n\ttext = f.read()\n\n\tif encoding != None:\n\t\ttry:\n\t\t\ttext = unicode(text, encoding)\n\t\texcept Exception, e:\n\t\t\tprint \"Error decoding file %s: %s\" % (file_name, e)\n\t\t\treturn textbuffer\n\n\ttextbuffer.set_text(text)\n\tf.close()\n\n\treturn textbuffer\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "pysolbase/FileUtility.py", "project": "champax/pysolbase", "name": "is_file_exist", "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool", "ques_id": "62b8bbbce0d34b282c18120d", "code": ["def is_file_exist(file_name):\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "def is_file_exist(file_name):\n\ttry:\n\t\topen(file_name)\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n\n", "def is_file_exist(file_name):\n\tif not os.path.isfile(file_name):\n\t\tprint(\"{} is not a file.\".format(file_name))\n\t\texit(1)\n\telse:\n\t\treturn True\n\n\n", "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n", "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_file_exist(file_name):\n\tif os.path.isfile(file_name):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\ttry:\n\t\topen(file_name)\n\t\treturn True\n\texcept IOError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\tif not os.path.isfile(file_name):\n\t\tprint(\"{} is not a file.\".format(file_name))\n\t\texit(1)\n\telse:\n\t\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_file_exist(file_name):\n\treturn os.path.isfile(file_name)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "pysolbase/SolBase.py", "project": "champax/pysolbase", "name": "_reset_logging", "docstring": "Reset", "ques_id": "62b8b99de0d34b282c1811f8", "code": ["def _reset_logging(cls):\n\t# Remove all handlers\n\tcls._logger.handlers = []\n\t\n\t# Add the default handler\n\tcls._logger.addHandler(cls._defaultHandler)\n\t\n\t# Reset the logging level\n\tcls._logger.setLevel(logging.DEBUG)\n\t\n\t# Reset the log format\n\tcls._formatter = logging.Formatter(cls._format, cls._datefmt)\n\t\n\t# Reset the handler\n\tcls._defaultHandler.setFormatter(cls._formatter)\n\t\n\t# Reset the logger\n\tcls._logger.propagate = cls._propagate\n\t\n\t# Reset the extra data\n\tcls._extra_data = {}\n\t\n\t# Reset the filter\n\tcls._logger.addFilter(cls._filter)\n\t\n\t# Reset the handlers\n\tcls._handlers = {}\n\t\n\t# Reset the filters\n\tcls._filters = {}\n\t\n\t# Reset the log level\n\tcls._level = logging.DEBUG\n\t\n\t# Reset the log format\n", "def _reset_logging(cls):\n\t# Remove all handlers\n\tfor handler in cls.logger.handlers[:]:\n\t\tcls.logger.removeHandler(handler)\n\n\t# Reset the logger\n\tcls.logger = logging.getLogger(cls.__name__)\n\n\n", "def _reset_logging(cls):\n\tcls._logger = None\n\tcls._logger_initialized = False\n\t\n\t# remove all handlers\n\tfor h in logging.root.handlers:\n\t\tlogging.root.removeHandler(h)\n\t\n\t# remove filters\n\tlogging.root.filters = []\n\t\n\t# remove formatters\n\tlogging.root.handlers = []\n\t\n\t# remove loggers\n\tfor l in logging.root.manager.loggerDict.keys():\n\t\tlogging.root.manager.loggerDict.pop(l)\n\t\n\t# reset the logger\n\tlogging.root = logging.root.manager.root\n\t\n\t# restore level\n\tlogging.root.setLevel(cls._logger_level)\n\t\n\t# restore handlers\n\tfor h in cls._logger_handlers:\n\t\tlogging.root.addHandler(h)\n\t\n\t# reset the logger\n\tcls.__init_logger()\n\n\n", "def _reset_logging(cls):\n\t# Remove handlers\n\tlog = logging.getLogger()\n\tfor hdlr in log.handlers[:]:\n\t\tlog.removeHandler(hdlr)\n\t# Reset formatters\n\tfor fmt in logging.root._handlers.values():\n\t\tfmt.setFormatter(logging.Formatter(\n\t\t\tfmt = \"%(asctime)s %(levelname)s: %(message)s\",\n\t\t\tdatefmt = \"%Y-%m-%d %H:%M:%S\"\n\t\t))\n\t# Reset logger level\n\tlogging.root.setLevel(logging.INFO)\n\t# Reset default handler\n\tlogging.root.addHandler(logging.StreamHandler())\n\n", "def _reset_logging(cls):\n\tif cls.__logging_configured:\n\t\tlogging.shutdown()\n\t\tcls.__logging_configured = False\n\n", "def _reset_logging(cls):\n\tcls.log = logging.getLogger(__name__)\n\tif len(cls.log.handlers) == 0:\n\t\tstdout_handler = logging.StreamHandler(sys.stdout)\n\t\tstdout_handler.setFormatter(logging.Formatter(\n\t\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n\t\t))\n\t\tcls.log.addHandler(stdout_handler)\n\t\tcls.log.setLevel(logging.DEBUG)\n\tcls.log.info('Logging system started')\n\n\n", "def _reset_logging(cls):\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\tlogging.basicConfig(level=logging.INFO)\n\n\n", "def _reset_logging(cls):\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\t\thandler.close()\n\n\tcls.logger = logging.getLogger()\n\tcls.logger.setLevel(logging.DEBUG)\n\thandler = logging.StreamHandler()\n\thandler.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tcls.logger.addHandler(handler)\n\n", "def _reset_logging(cls):\n\t# Remove all handlers\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\n\t# Reset the log level of the root logger\n\tlogging.root.setLevel(logging.DEBUG)\n\n\n", "def _reset_logging(cls):\n\tif hasattr(cls, \"__logger__\"):\n\t\tdel cls.__logger__\n\t\t\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _reset_logging(cls):\n\t# Remove all handlers\n\tcls._logger.handlers = []\n\t\n\t# Add the default handler\n\tcls._logger.addHandler(cls._defaultHandler)\n\t\n\t# Reset the logging level\n\tcls._logger.setLevel(logging.DEBUG)\n\t\n\t# Reset the log format\n\tcls._formatter = logging.Formatter(cls._format, cls._datefmt)\n\t\n\t# Reset the handler\n\tcls._defaultHandler.setFormatter(cls._formatter)\n\t\n\t# Reset the logger\n\tcls._logger.propagate = cls._propagate\n\t\n\t# Reset the extra data\n\tcls._extra_data = {}\n\t\n\t# Reset the filter\n\tcls._logger.addFilter(cls._filter)\n\t\n\t# Reset the handlers\n\tcls._handlers = {}\n\t\n\t# Reset the filters\n\tcls._filters = {}\n\t\n\t# Reset the log level\n\tcls._level = logging.DEBUG\n\t\n\t# Reset the log format\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\t# Remove all handlers\n\tfor handler in cls.logger.handlers[:]:\n\t\tcls.logger.removeHandler(handler)\n\n\t# Reset the logger\n\tcls.logger = logging.getLogger(cls.__name__)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tcls._logger = None\n\tcls._logger_initialized = False\n\t\n\t# remove all handlers\n\tfor h in logging.root.handlers:\n\t\tlogging.root.removeHandler(h)\n\t\n\t# remove filters\n\tlogging.root.filters = []\n\t\n\t# remove formatters\n\tlogging.root.handlers = []\n\t\n\t# remove loggers\n\tfor l in logging.root.manager.loggerDict.keys():\n\t\tlogging.root.manager.loggerDict.pop(l)\n\t\n\t# reset the logger\n\tlogging.root = logging.root.manager.root\n\t\n\t# restore level\n\tlogging.root.setLevel(cls._logger_level)\n\t\n\t# restore handlers\n\tfor h in cls._logger_handlers:\n\t\tlogging.root.addHandler(h)\n\t\n\t# reset the logger\n\tcls.__init_logger()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\t# Remove handlers\n\tlog = logging.getLogger()\n\tfor hdlr in log.handlers[:]:\n\t\tlog.removeHandler(hdlr)\n\t# Reset formatters\n\tfor fmt in logging.root._handlers.values():\n\t\tfmt.setFormatter(logging.Formatter(\n\t\t\tfmt = \"%(asctime)s %(levelname)s: %(message)s\",\n\t\t\tdatefmt = \"%Y-%m-%d %H:%M:%S\"\n\t\t))\n\t# Reset logger level\n\tlogging.root.setLevel(logging.INFO)\n\t# Reset default handler\n\tlogging.root.addHandler(logging.StreamHandler())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tif cls.__logging_configured:\n\t\tlogging.shutdown()\n\t\tcls.__logging_configured = False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tcls.log = logging.getLogger(__name__)\n\tif len(cls.log.handlers) == 0:\n\t\tstdout_handler = logging.StreamHandler(sys.stdout)\n\t\tstdout_handler.setFormatter(logging.Formatter(\n\t\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n\t\t))\n\t\tcls.log.addHandler(stdout_handler)\n\t\tcls.log.setLevel(logging.DEBUG)\n\tcls.log.info('Logging system started')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\tlogging.basicConfig(level=logging.INFO)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\t\thandler.close()\n\n\tcls.logger = logging.getLogger()\n\tcls.logger.setLevel(logging.DEBUG)\n\thandler = logging.StreamHandler()\n\thandler.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\thandler.setFormatter(formatter)\n\tcls.logger.addHandler(handler)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\t# Remove all handlers\n\tfor handler in logging.root.handlers[:]:\n\t\tlogging.root.removeHandler(handler)\n\n\t# Reset the log level of the root logger\n\tlogging.root.setLevel(logging.DEBUG)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _reset_logging(cls):\n\tif hasattr(cls, \"__logger__\"):\n\t\tdel cls.__logger__\n\t\t\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/tests/test_declarations.py", "project": "pexip/os-zope", "name": "_getTargetClass", "docstring": "Define this to return the implementation in use,\nwithout the 'Py' or 'Fallback' suffix.", "ques_id": "62b8b59feb7e40a82d2d1291", "code": ["def _getTargetClass(self):\n\traise NotImplementedError\n\n", "def _getTargetClass(self):\n\traise NotImplementedError\n\n", "def _getTargetClass(self):\n\ttry:\n\t\tfrom _json import scanstring as py_scanstring\n\texcept ImportError:\n\t\tpy_scanstring = None\n\n\tif py_scanstring:\n\t\treturn py_scanstring\n\treturn FallbackScanner\n\n", "def _getTargetClass(self):\n\traise NotImplementedError\n\n", "def _getTargetClass(self):\n\traise NotImplementedError()\n\n\n", "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n", "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n", "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n", "def _getTargetClass(self):\n\traise NotImplementedError('Must override')\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\ttry:\n\t\tfrom _json import scanstring as py_scanstring\n\texcept ImportError:\n\t\tpy_scanstring = None\n\n\tif py_scanstring:\n\t\treturn py_scanstring\n\treturn FallbackScanner\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _getTargetClass(self):\n\traise NotImplementedError('Must override')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/ro.py", "project": "pexip/os-zope", "name": "_legacy_mergeOrderings", "docstring": "Merge multiple orderings so that within-ordering order is preserved\n\nOrderings are constrained in such a way that if an object appears\nin two or more orderings, then the suffix that begins with the\nobject must be in both orderings.\n\nFor example:\n\n>>> _mergeOrderings([\n... ['x', 'y', 'z'],\n... ['q', 'z'],\n... [1, 3, 5],\n... ['z']\n... ])\n['x', 'y', 'q', 1, 3, 5, 'z']", "ques_id": "62b8b590eb7e40a82d2d1275", "code": ["def _legacy_mergeOrderings(orderings):\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n\n\n", "def _legacy_mergeOrderings(orderings):\n\treturn list(set(sum(orderings, [])))\n\n\n", "def _legacy_mergeOrderings(orderings):\n\tif not orderings:\n\t\treturn []\n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\torderings = list(orderings) # make a copy\n\tfor i in xrange(1, len(orderings)):\n\t\torderings[0] = _mergeOrderings(orderings[0], orderings[i])\n\treturn orderings[0]\n\n", "def _legacy_mergeOrderings(orderings):\n\tfinal_ordering = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in final_ordering:\n\t\t\t\tfinal_ordering.append(element)\n\treturn final_ordering\n\n\n", "def _legacy_mergeOrderings(orderings):\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult.extend([x for x in ordering if x not in result])\n\treturn result\n\n", "def _legacy_mergeOrderings(orderings):\n\treturn _legacy_mergeOrderings(orderings)\n\n", "def _legacy_mergeOrderings(orderings):\n\treturn reduce(lambda x, y: x + [e for e in y if e not in x], orderings, [])\n\n", "def _legacy_mergeOrderings(orderings):\n\tseen = set()\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor key in ordering:\n\t\t\tif key not in seen:\n\t\t\t\tseen.add(key)\n\t\t\t\tresult.append(key)\n\treturn result\n\n\n", "def _legacy_mergeOrderings(orderings):\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor value in ordering:\n\t\t\tif value not in result:\n\t\t\t\tresult.append(value)\n\treturn result\n\n", "def _legacy_mergeOrderings(orderings):\n\t# We can't use set() because it doesn't preserve the order\n\t# http://stackoverflow.com/questions/7961363/removing-duplicates-in-lists\n\tseen = set()\n\tseen_add = seen.add\n\treturn [x for x in orderings if not (x in seen or seen_add(x))]\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _legacy_mergeOrderings(orderings):\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor item in ordering:\n\t\t\tif item not in result:\n\t\t\t\tresult.append(item)\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\treturn list(set(sum(orderings, [])))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tif not orderings:\n\t\treturn []\n\tif len(orderings) == 1:\n\t\treturn orderings[0]\n\torderings = list(orderings) # make a copy\n\tfor i in xrange(1, len(orderings)):\n\t\torderings[0] = _mergeOrderings(orderings[0], orderings[i])\n\treturn orderings[0]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tfinal_ordering = []\n\tfor ordering in orderings:\n\t\tfor element in ordering:\n\t\t\tif element not in final_ordering:\n\t\t\t\tfinal_ordering.append(element)\n\treturn final_ordering\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tresult = []\n\tfor ordering in orderings:\n\t\tresult.extend([x for x in ordering if x not in result])\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\treturn _legacy_mergeOrderings(orderings)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\treturn reduce(lambda x, y: x + [e for e in y if e not in x], orderings, [])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tseen = set()\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor key in ordering:\n\t\t\tif key not in seen:\n\t\t\t\tseen.add(key)\n\t\t\t\tresult.append(key)\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\tresult = []\n\tfor ordering in orderings:\n\t\tfor value in ordering:\n\t\t\tif value not in result:\n\t\t\t\tresult.append(value)\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _legacy_mergeOrderings(orderings):\n\t# We can't use set() because it doesn't preserve the order\n\t# http://stackoverflow.com/questions/7961363/removing-duplicates-in-lists\n\tseen = set()\n\tseen_add = seen.add\n\treturn [x for x in orderings if not (x in seen or seen_add(x))]\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "directlyProvidedBy", "docstring": "Return the interfaces directly provided by the given object\n\nThe value returned is an `~zope.interface.interfaces.IDeclaration`.", "ques_id": "62b8b58deb7e40a82d2d1269", "code": ["def directlyProvidedBy(object):\n\tif not isinstance(object, Component):\n\t\traise TypeError(\"Can only deal with zope components\")\n\tif hasattr(object, \"provides\"):\n\t\treturn object.provides\n\tif hasattr(object, \"interface\"):\n\t\treturn object.interface\n\treturn None\n\n", "def directlyProvidedBy(object):\n\treturn _providedBy(object)\n\n", "def directlyProvidedBy(object):\n\treturn _objectImplements(object, _directlyProvidedBy)\n\n\n", "def directlyProvidedBy(object):\n\treturn _adapter_hooks.directlyProvidedBy(object)\n\n", "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n", "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n", "def directlyProvidedBy(object):\n\treturn directlyProvides(object, asDeclaration=True)\n\n", "def directlyProvidedBy(object):\n\treturn implementedBy(object)\n\n", "def directlyProvidedBy(object):\n\treturn getUtility(IObjectProvides, context=object)()\n\n", "def directlyProvidedBy(object):\n\treturn directlyProvidedBy_(_context, object)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def directlyProvidedBy(object):\n\tif not isinstance(object, Component):\n\t\traise TypeError(\"Can only deal with zope components\")\n\tif hasattr(object, \"provides\"):\n\t\treturn object.provides\n\tif hasattr(object, \"interface\"):\n\t\treturn object.interface\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn _providedBy(object)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn _objectImplements(object, _directlyProvidedBy)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn _adapter_hooks.directlyProvidedBy(object)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn directlyProvidedBy(object)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn directlyProvides(object, asDeclaration=True)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn implementedBy(object)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn getUtility(IObjectProvides, context=object)()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def directlyProvidedBy(object):\n\treturn directlyProvidedBy_(_context, object)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "minimalBases", "docstring": "Reduce a list of base classes to its ordered minimum equivalent", "ques_id": "62b8b559eb7e40a82d2d11f8", "code": ["def minimalBases(classes):\n\tminBases = []\n\tfor cl in classes:\n\t\tif cl.bases == []:\n\t\t\tminBases.append(cl)\n\treturn minBases\n\n", "def minimalBases(classes):\n\tallClasses = set(classes)\n\tfor c in classes:\n\t\tfor sup in c.get_superclasses():\n\t\t\tif sup in allClasses:\n\t\t\t\tallClasses.remove(sup)\n\treturn allClasses\n\n", "def minimalBases(classes):\n\tbases = []\n\tfor c in classes:\n\t\tif c not in classes:\n\t\t\tbases.append(c)\n\treturn bases\n\n\n", "def minimalBases(classes):\n\tminimalBases = list(classes)\n\tfor c in classes:\n\t\tfor sc in c.__subclasses__():\n\t\t\tif sc in minimalBases:\n\t\t\t\tminimalBases.remove(sc)\n\treturn minimalBases\n\n", "def minimalBases(classes):\n\tfor c in classes:\n\t\tfor c2 in classes:\n\t\t\tif c!=c2 and c in c2.subclasses():\n\t\t\t\tclasses.remove(c)\n\t\t\t\tbreak # c is no more in classes, skip the other classes\n\treturn classes\n\n", "def minimalBases(classes):\n\tminimal = []\n\tfor c in classes:\n\t\tif c.subclasses == []:\n\t\t\tminimal.append(c)\n\treturn minimal\n\n", "def minimalBases(classes):\n\tminimal = list(classes)\n\tfor key, value in classes.items():\n\t\tfor child in value:\n\t\t\tif child in minimal:\n\t\t\t\tminimal.remove(child)\n\treturn minimal\n\n", "def minimalBases(classes):\n\tbases = []\n\tfor cl in classes:\n\t\tif cl.subclasses is not None:\n\t\t\tfor subcl in cl.subclasses:\n\t\t\t\tsubcl.subclassOf.remove(cl)\n\t\telse:\n\t\t\tbases.append(cl)\n\treturn bases\n\n", "def minimalBases(classes):\n\tminimal = []\n\tfor cl in classes:\n\t\tfor cl2 in classes:\n\t\t\tif cl != cl2:\n\t\t\t\tif cl.issubclass(cl2) or cl2.issubclass(cl):\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tminimal.append(cl)\n\treturn minimal\n\n", "def minimalBases(classes):\n\tminimal = []\n\tfor c in classes:\n\t\tfor c2 in classes:\n\t\t\tif c == c2: continue\n\t\t\tif c2.isSubclassOf(c): break\n\t\telse:\n\t\t\tminimal.append(c)\n\treturn minimal\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def minimalBases(classes):\n\tminBases = []\n\tfor cl in classes:\n\t\tif cl.bases == []:\n\t\t\tminBases.append(cl)\n\treturn minBases\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tallClasses = set(classes)\n\tfor c in classes:\n\t\tfor sup in c.get_superclasses():\n\t\t\tif sup in allClasses:\n\t\t\t\tallClasses.remove(sup)\n\treturn allClasses\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tbases = []\n\tfor c in classes:\n\t\tif c not in classes:\n\t\t\tbases.append(c)\n\treturn bases\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tminimalBases = list(classes)\n\tfor c in classes:\n\t\tfor sc in c.__subclasses__():\n\t\t\tif sc in minimalBases:\n\t\t\t\tminimalBases.remove(sc)\n\treturn minimalBases\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tfor c in classes:\n\t\tfor c2 in classes:\n\t\t\tif c!=c2 and c in c2.subclasses():\n\t\t\t\tclasses.remove(c)\n\t\t\t\tbreak # c is no more in classes, skip the other classes\n\treturn classes\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tminimal = []\n\tfor c in classes:\n\t\tif c.subclasses == []:\n\t\t\tminimal.append(c)\n\treturn minimal\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tminimal = list(classes)\n\tfor key, value in classes.items():\n\t\tfor child in value:\n\t\t\tif child in minimal:\n\t\t\t\tminimal.remove(child)\n\treturn minimal\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tbases = []\n\tfor cl in classes:\n\t\tif cl.subclasses is not None:\n\t\t\tfor subcl in cl.subclasses:\n\t\t\t\tsubcl.subclassOf.remove(cl)\n\t\telse:\n\t\t\tbases.append(cl)\n\treturn bases\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tminimal = []\n\tfor cl in classes:\n\t\tfor cl2 in classes:\n\t\t\tif cl != cl2:\n\t\t\t\tif cl.issubclass(cl2) or cl2.issubclass(cl):\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tminimal.append(cl)\n\treturn minimal\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def minimalBases(classes):\n\tminimal = []\n\tfor c in classes:\n\t\tfor c2 in classes:\n\t\t\tif c == c2: continue\n\t\t\tif c2.isSubclassOf(c): break\n\t\telse:\n\t\t\tminimal.append(c)\n\treturn minimal\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "namesAndDescriptions", "docstring": "Return attribute names and descriptions defined by interface.", "ques_id": "62b8b4b9eb7e40a82d2d1134", "code": ["def namesAndDescriptions(self, all=False):\n\tret = []\n\tfor item in self.items(all):\n\t\tret.append((item.name, item.description))\n\treturn ret\n\n", "def namesAndDescriptions(self, all=False):\n\tname, desc = [], []\n\tfor attr in self.attributes(all):\n\t\tname.append(attr.name())\n\t\tdesc.append(attr.description())\n\treturn name, desc\n\n\tdef funcs(self, all=False):\n\t\t\"\"\"\n\t\tReturns the functions of the current class.\n\t\t\"\"\"\n\t\treturn self.__funcs(all)\n\n\tdef __funcs(self, all=False):\n\t\t\"\"\"\n\t\tReturns the functions of the current class.\n\t\t\"\"\"\n\t\tif not all:\n\t\t\treturn self.__functions\n\t\telse:\n\t\t\tfuncs = self.__functions\n\t\t\tif self.__parent:\n\t\t\t\tfuncs.extend(self.__parent.__funcs(all))\n\t\t\treturn funcs\n\n\tdef functions(self, all=False):\n\t\t\"\"\"\n\t\tReturns the functions of the current class.\n\t\t\"\"\"\n\t\treturn self.__functions\n\n\tdef function(self, name, all=False):\n\t\t\"\"\"\n\t\tReturns\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn self.name, self.description\n\telse:\n\t\treturn self.name, self.description\n\n\n", "def namesAndDescriptions(self, all=False):\n\treturn self._getNameDesc(self.classAttr, all)\n\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn self.__names\n\telse:\n\t\treturn self.__names[self.__current]\n\n\n\tdef setAttribute(self, index):\n\t\t\"\"\"\n\t\tSet the class attribute to the given index.\n\t\t\"\"\"\n\t\tif index == self.__current:\n\t\t\treturn\n\t\tself.__current = index\n\t\tself.notify()\n\n\n\tdef getAttribute(self):\n\t\t\"\"\"\n\t\tReturn the current class attribute.\n\t\t\"\"\"\n\t\treturn self.__names[self.__current]\n\n\n\tdef getIndex(self):\n\t\t\"\"\"\n\t\tReturn the index of the current class attribute.\n\t\t\"\"\"\n\t\treturn self.__current\n\n\n\tdef __len__(self):\n\t\treturn len(self.__names)\n\n\n\tdef __repr__(self):\n\t\treturn \"AttrDescriptor %s\" % self.__names\n\n\n\tdef __getitem__(self, index):\n\t\treturn self.__names[index]\n\n\n\tdef __setitem__(self, index\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn self._namesAndDescriptions\n\telse:\n\t\treturn self._namesAndDescriptions[self._name]\n\n\n", "def namesAndDescriptions(self, all=False):\n\treturn self.namesAndDescriptionsOf(self.__class__, all)\n\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn [(name, getattr(self, name).__doc__) for name in self.__dict__]\n\telse:\n\t\treturn [(name, getattr(self, name).__doc__) for name in self.__dict__ if not name.startswith('_')]\n\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\tfor name in dir(self):\n\t\t\tobj = getattr(self, name)\n\t\t\tif type(obj) is types.MethodType:\n\t\t\t\tprint(\"%s: %s\" % (name, obj.__doc__))\n\telse:\n\t\tobj = getattr(self, self.__class__.__name__)\n\t\tprint(\"%s: %s\" % (self.__class__.__name__, obj.__doc__))\n\n\n", "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn [(name, getattr(self, name).description) for name in self.names()]\n\telse:\n\t\treturn [(name, getattr(self, name).description) for name in self.names() if getattr(self, name).isSet()]\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def namesAndDescriptions(self, all=False):\n\tret = []\n\tfor item in self.items(all):\n\t\tret.append((item.name, item.description))\n\treturn ret\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tname, desc = [], []\n\tfor attr in self.attributes(all):\n\t\tname.append(attr.name())\n\t\tdesc.append(attr.description())\n\treturn name, desc\n\n\tdef funcs(self, all=False):\n\t\t\"\"\"\n\t\tReturns the functions of the current class.\n\t\t\"\"\"\n\t\treturn self.__funcs(all)\n\n\tdef __funcs(self, all=False):\n\t\t\"\"\"\n\t\tReturns the functions of the current class.\n\t\t\"\"\"\n\t\tif not all:\n\t\t\treturn self.__functions\n\t\telse:\n\t\t\tfuncs = self.__functions\n\t\t\tif self.__parent:\n\t\t\t\tfuncs.extend(self.__parent.__funcs(all))\n\t\t\treturn funcs\n\n\tdef functions(self, all=False):\n\t\t\"\"\"\n\t\tReturns the functions of the current class.\n\t\t\"\"\"\n\t\treturn self.__functions\n\n\tdef function(self, name, all=False):\n\t\t\"\"\"\n\t\tReturns\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn self.name, self.description\n\telse:\n\t\treturn self.name, self.description\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\treturn self._getNameDesc(self.classAttr, all)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn self.__names\n\telse:\n\t\treturn self.__names[self.__current]\n\n\n\tdef setAttribute(self, index):\n\t\t\"\"\"\n\t\tSet the class attribute to the given index.\n\t\t\"\"\"\n\t\tif index == self.__current:\n\t\t\treturn\n\t\tself.__current = index\n\t\tself.notify()\n\n\n\tdef getAttribute(self):\n\t\t\"\"\"\n\t\tReturn the current class attribute.\n\t\t\"\"\"\n\t\treturn self.__names[self.__current]\n\n\n\tdef getIndex(self):\n\t\t\"\"\"\n\t\tReturn the index of the current class attribute.\n\t\t\"\"\"\n\t\treturn self.__current\n\n\n\tdef __len__(self):\n\t\treturn len(self.__names)\n\n\n\tdef __repr__(self):\n\t\treturn \"AttrDescriptor %s\" % self.__names\n\n\n\tdef __getitem__(self, index):\n\t\treturn self.__names[index]\n\n\n\tdef __setitem__(self, index\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn self._namesAndDescriptions\n\telse:\n\t\treturn self._namesAndDescriptions[self._name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\treturn self.namesAndDescriptionsOf(self.__class__, all)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn [(name, getattr(self, name).__doc__) for name in self.__dict__]\n\telse:\n\t\treturn [(name, getattr(self, name).__doc__) for name in self.__dict__ if not name.startswith('_')]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\tfor name in dir(self):\n\t\t\tobj = getattr(self, name)\n\t\t\tif type(obj) is types.MethodType:\n\t\t\t\tprint(\"%s: %s\" % (name, obj.__doc__))\n\telse:\n\t\tobj = getattr(self, self.__class__.__name__)\n\t\tprint(\"%s: %s\" % (self.__class__.__name__, obj.__doc__))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def namesAndDescriptions(self, all=False):\n\tif all:\n\t\treturn [(name, getattr(self, name).description) for name in self.names()]\n\telse:\n\t\treturn [(name, getattr(self, name).description) for name in self.names() if getattr(self, name).isSet()]\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/interface.py", "project": "pexip/os-zope", "name": "names", "docstring": "Return the attribute names defined by the interface.", "ques_id": "62b8b416eb7e40a82d2d1129", "code": ["def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\tnames = self.__dict__.keys()\n\t\tfor i in names:\n\t\t\tif i.startswith('__'):\n\t\t\t\tnames.remove(i)\n\t\treturn names\n\n\tdef __repr__(self):\n\t\treturn self.__class__.__name__ + '(' + repr(self.__dict__) + ')'\n\n\tdef __str__(self):\n\t\treturn self.__class__.__name__ + '(' + str(self.__dict__) + ')'\n\n\tdef __getattr__(self, name):\n\t\treturn self.__dict__[name]\n\n\tdef __setattr__(self, name, value):\n\t\treturn self.__dict__.__setitem__(name, value)\n\n\tdef __delattr__(self, name):\n\t\treturn self.__dict__.__delitem__(name)\n\n\tdef __getitem__(self, name):\n\t\treturn self.__dict__[name]\n\n\n", "def names(self, all=False):\n\treturn inspect.getmembers(current, lambda a:not(inspect.isroutine(a)))\n\n", "def names(self, all=False):\n\tif all:\n\t    return list(self.__dict__.keys()) + self.__class__.__bases__[0].names()\n\telse:\n\t    return list(self.__dict__.keys())\n\t\n    def __str__(self):\n\t\"\"\"\n\tReturn the string representation of current object.\n\t\"\"\"\n\treturn str(self.__dict__)\n\n    def __iter__(self):\n\t\"\"\"\n\tReturn the iterator of current object.\n\t\"\"\"\n\treturn self.__dict__.__iter__()\n\n    def __repr__(self):\n\t\"\"\"\n\tReturn the string representation of current object.\n\t\"\"\"\n\treturn str(self.__dict__)\n\n    def __call__(self, *args, **kwargs):\n\t\"\"\"\n\tReturn the string representation of current object.\n\t\"\"\"\n\treturn str(self.__dict__)\n\n    def __setitem__(self, key, value):\n\t\"\"\"\n\tSet value to the specified attribute.\n\t\"\"\"\n\tself.__dict__[key] = value\n\n    def __getitem__(self\n", "def names(self, all=False):\n\tif all:\n\t\treturn self.__attrs__\n\telse:\n\t\treturn [ k for k in self.__attrs__ if not k.startswith('_') ]\n\n\tdef __repr__(self):\n\t\treturn '<%s: %s>' % (\n\t\t\tself.__class__.__name__,\n\t\t\t', '.join( '%s=%s' % (k, getattr(self, k)) for k in self.__attrs__ )\n\t\t\t)\n\n\tdef __str__(self):\n\t\treturn '%s' % self.__repr__()\n\n\tdef __getattr__(self, k):\n\t\tif k in self.__attrs__:\n\t\t\treturn getattr(self.__db__, k)\n\t\telse:\n\t\t\traise AttributeError('%s has no attribute %s' % (self.__class__.__name__, k))\n\n\tdef __setattr__(self, k, v):\n\t\tif k in self.__attrs__:\n\t\t\t\n", "def names(self, all=False):\n\tif all:\n\t    return self._all_names\n\treturn self._names\n\n\n", "def names(self, all=False):\n\tif all:\n\t\treturn self.attrs.keys()\n\treturn self.attrs.keys()\n\t\n\t\n", "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn self.__names__\n\n\t@classmethod\n\tdef get(cls, name, default=None):\n\t\treturn getattr(cls, name, default)\n\n\t@classmethod\n\tdef set(cls, name, value):\n\t\treturn setattr(cls, name, value)\n\n\t@classmethod\n\tdef has(cls, name):\n\t\treturn hasattr(cls, name)\n\n\tdef __repr__(self):\n\t\treturn '<{} {}>'.format(\n\t\t\tself.__class__.__name__,\n\t\t\t' '.join('{}={}'.format(k, v) for k, v in self.__dict__.items())\n\t\t)\n\n\tdef __str__(self):\n\t\treturn '<{} {}>'.format(\n\t\t\tself.__class__.__name__,\n\t\t\t' '.join('{}={}'.format(k, v) for k, v in self.__dict\n", "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn [name for name in self.__dict__.keys() if name[0] != '_']\n\n\n\tdef __getattr__(self, name):\n\t\t\"\"\"\n\t\tGet the attribute value of current class by name.\n\t\t\"\"\"\n\t\tif name[0] != '_':\n\t\t\treturn self.__getattribute__(name)\n\t\telse:\n\t\t\traise AttributeError('Attribute: %s not found!' % name)\n\n\n\tdef __setattr__(self, name, value):\n\t\t\"\"\"\n\t\tSet the attribute value of current class by name.\n\t\t\"\"\"\n\t\tif name[0] != '_':\n\t\t\tself.__dict__[name] = value\n\t\telse:\n\t\t\traise AttributeError('Attribute: %s is private!' % name)\n\n\n\tdef __delattr__(self, name):\n\t\t\"\"\"\n\t\tDelete the attribute of current class by name.\n\t\t\"\"\"\n\t\tif name[0]\n", "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys() + self.__class__.__dict__.keys()\n\treturn self.__dict__.keys()\n\n", "def names(self, all=False):\n\tnames = []\n\tif all:\n\t\tfor cls in self.__mro__:\n\t\t\tfor k,v in cls.__dict__.items():\n\t\t\t\tif k not in names:\n\t\t\t\t\tnames.append(k)\n\telse:\n\t\tfor k,v in self.__dict__.items():\n\t\t\tif k not in names:\n\t\t\t\tnames.append(k)\n\treturn names\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\tnames = self.__dict__.keys()\n\t\tfor i in names:\n\t\t\tif i.startswith('__'):\n\t\t\t\tnames.remove(i)\n\t\treturn names\n\n\tdef __repr__(self):\n\t\treturn self.__class__.__name__ + '(' + repr(self.__dict__) + ')'\n\n\tdef __str__(self):\n\t\treturn self.__class__.__name__ + '(' + str(self.__dict__) + ')'\n\n\tdef __getattr__(self, name):\n\t\treturn self.__dict__[name]\n\n\tdef __setattr__(self, name, value):\n\t\treturn self.__dict__.__setitem__(name, value)\n\n\tdef __delattr__(self, name):\n\t\treturn self.__dict__.__delitem__(name)\n\n\tdef __getitem__(self, name):\n\t\treturn self.__dict__[name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\treturn inspect.getmembers(current, lambda a:not(inspect.isroutine(a)))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t    return list(self.__dict__.keys()) + self.__class__.__bases__[0].names()\n\telse:\n\t    return list(self.__dict__.keys())\n\t\n    def __str__(self):\n\t\"\"\"\n\tReturn the string representation of current object.\n\t\"\"\"\n\treturn str(self.__dict__)\n\n    def __iter__(self):\n\t\"\"\"\n\tReturn the iterator of current object.\n\t\"\"\"\n\treturn self.__dict__.__iter__()\n\n    def __repr__(self):\n\t\"\"\"\n\tReturn the string representation of current object.\n\t\"\"\"\n\treturn str(self.__dict__)\n\n    def __call__(self, *args, **kwargs):\n\t\"\"\"\n\tReturn the string representation of current object.\n\t\"\"\"\n\treturn str(self.__dict__)\n\n    def __setitem__(self, key, value):\n\t\"\"\"\n\tSet value to the specified attribute.\n\t\"\"\"\n\tself.__dict__[key] = value\n\n    def __getitem__(self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.__attrs__\n\telse:\n\t\treturn [ k for k in self.__attrs__ if not k.startswith('_') ]\n\n\tdef __repr__(self):\n\t\treturn '<%s: %s>' % (\n\t\t\tself.__class__.__name__,\n\t\t\t', '.join( '%s=%s' % (k, getattr(self, k)) for k in self.__attrs__ )\n\t\t\t)\n\n\tdef __str__(self):\n\t\treturn '%s' % self.__repr__()\n\n\tdef __getattr__(self, k):\n\t\tif k in self.__attrs__:\n\t\t\treturn getattr(self.__db__, k)\n\t\telse:\n\t\t\traise AttributeError('%s has no attribute %s' % (self.__class__.__name__, k))\n\n\tdef __setattr__(self, k, v):\n\t\tif k in self.__attrs__:\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t    return self._all_names\n\treturn self._names\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.attrs.keys()\n\treturn self.attrs.keys()\n\t\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn self.__names__\n\n\t@classmethod\n\tdef get(cls, name, default=None):\n\t\treturn getattr(cls, name, default)\n\n\t@classmethod\n\tdef set(cls, name, value):\n\t\treturn setattr(cls, name, value)\n\n\t@classmethod\n\tdef has(cls, name):\n\t\treturn hasattr(cls, name)\n\n\tdef __repr__(self):\n\t\treturn '<{} {}>'.format(\n\t\t\tself.__class__.__name__,\n\t\t\t' '.join('{}={}'.format(k, v) for k, v in self.__dict__.items())\n\t\t)\n\n\tdef __str__(self):\n\t\treturn '<{} {}>'.format(\n\t\t\tself.__class__.__name__,\n\t\t\t' '.join('{}={}'.format(k, v) for k, v in self.__dict\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys()\n\telse:\n\t\treturn [name for name in self.__dict__.keys() if name[0] != '_']\n\n\n\tdef __getattr__(self, name):\n\t\t\"\"\"\n\t\tGet the attribute value of current class by name.\n\t\t\"\"\"\n\t\tif name[0] != '_':\n\t\t\treturn self.__getattribute__(name)\n\t\telse:\n\t\t\traise AttributeError('Attribute: %s not found!' % name)\n\n\n\tdef __setattr__(self, name, value):\n\t\t\"\"\"\n\t\tSet the attribute value of current class by name.\n\t\t\"\"\"\n\t\tif name[0] != '_':\n\t\t\tself.__dict__[name] = value\n\t\telse:\n\t\t\traise AttributeError('Attribute: %s is private!' % name)\n\n\n\tdef __delattr__(self, name):\n\t\t\"\"\"\n\t\tDelete the attribute of current class by name.\n\t\t\"\"\"\n\t\tif name[0]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tif all:\n\t\treturn self.__dict__.keys() + self.__class__.__dict__.keys()\n\treturn self.__dict__.keys()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def names(self, all=False):\n\tnames = []\n\tif all:\n\t\tfor cls in self.__mro__:\n\t\t\tfor k,v in cls.__dict__.items():\n\t\t\t\tif k not in names:\n\t\t\t\t\tnames.append(k)\n\telse:\n\t\tfor k,v in self.__dict__.items():\n\t\t\tif k not in names:\n\t\t\t\tnames.append(k)\n\treturn names\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/declarations.py", "project": "pexip/os-zope", "name": "_normalizeargs", "docstring": "Normalize declaration arguments\n\nNormalization arguments might contain Declarions, tuples, or single\ninterfaces.\n\nAnything but individial interfaces or implements specs will be expanded.", "ques_id": "62b8b3d6eb7e40a82d2d111c", "code": ["def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, Declaration):\n\t\tyield sequence\n\telif isinstance(sequence, tuple):\n\t\tfor elem in sequence:\n\t\t\tyield from _normalizeargs(elem, output=output)\n\telse:\n\t\tyield sequence\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif item is not None:\n\t\t\tif isinstance(item, tuple):\n\t\t\t\tif item[1] is not None:\n\t\t\t\t\t_normalizeargs(item[1], output)\n\t\t\telif isinstance(item, Declaration):\n\t\t\t\t_normalizeargs(item.args, output)\n\t\t\telif isinstance(item, Interface):\n\t\t\t\toutput.append(item)\n\treturn output\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence.interfaces\n\telif isinstance(sequence, tuple):\n\t\treturn _normalizeargs(sequence[1:], output or []) + [sequence[0]]\n\telif isinstance(sequence, list):\n\t\toutput = output or []\n\t\tfor v in sequence:\n\t\t\toutput += _normalizeargs(v)\n\t\treturn output\n\telse:\n\t\treturn [sequence]\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\n\tif isinstance(sequence, _normalizeargs):\n\t\tfor s in sequence:\n\t\t\t_normalizeargs(s, output)\n\telif isinstance(sequence, tuple):\n\t\tfor s in sequence:\n\t\t\t_normalizeargs(s, output)\n\telse:\n\t\toutput.append(sequence)\n\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif isinstance(item, basestring):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, Declaration):\n\t\t\tfor iface in item:\n\t\t\t\toutput.append(iface)\n\t\telse:\n\t\t\t_normalizeargs(item, output)\n\treturn output\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif isinstance(item, Interface):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, Declaration):\n\t\t\t_normalizeargs(item.interfaces, output)\n\t\telse:\n\t\t\ttry:\n\t\t\t\titerator = iter(item)\n\t\t\texcept TypeError:\n\t\t\t\traise TypeError(\"interface declarations must be made of interfaces, not %r\" % (item,))\n\t\t\t_normalizeargs(iterator, output)\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Declaration):\n\t\t\t_normalizeargs(arg.args, output)\n\t\telif isinstance(arg, tuple):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(arg)\n\treturn output\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Declaration):\n\t\t\t_normalizeargs(arg, output)\n\t\telif isinstance(arg, tuple):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(arg)\n\treturn output\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tif isinstance(sequence, Declaration):\n\t\tsequence = sequence.extends\n\tif isinstance(sequence, (tuple, list, set)):\n\t\tfor item in sequence:\n\t\t\t_normalizeargs(item, output)\n\telse:\n\t\toutput.append(sequence)\n\treturn output\n\n\n", "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, tuple):\n\t\tif len(sequence) == 1:\n\t\t\treturn _normalizeargs(sequence[0], output)\n\t\telse:\n\t\t\tfor element in sequence:\n\t\t\t\t_normalizeargs(element, output)\n\telif isinstance(sequence, Declaration):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Interface):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Implements):\n\t\toutput.append(sequence.interface)\n\telse:\n\t\traise TypeError(\"Cannot normalize argument: %s\" % (repr(sequence)))\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, Declaration):\n\t\tyield sequence\n\telif isinstance(sequence, tuple):\n\t\tfor elem in sequence:\n\t\t\tyield from _normalizeargs(elem, output=output)\n\telse:\n\t\tyield sequence\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif item is not None:\n\t\t\tif isinstance(item, tuple):\n\t\t\t\tif item[1] is not None:\n\t\t\t\t\t_normalizeargs(item[1], output)\n\t\t\telif isinstance(item, Declaration):\n\t\t\t\t_normalizeargs(item.args, output)\n\t\t\telif isinstance(item, Interface):\n\t\t\t\toutput.append(item)\n\treturn output\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, Declaration):\n\t\treturn sequence.interfaces\n\telif isinstance(sequence, tuple):\n\t\treturn _normalizeargs(sequence[1:], output or []) + [sequence[0]]\n\telif isinstance(sequence, list):\n\t\toutput = output or []\n\t\tfor v in sequence:\n\t\t\toutput += _normalizeargs(v)\n\t\treturn output\n\telse:\n\t\treturn [sequence]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\n\tif isinstance(sequence, _normalizeargs):\n\t\tfor s in sequence:\n\t\t\t_normalizeargs(s, output)\n\telif isinstance(sequence, tuple):\n\t\tfor s in sequence:\n\t\t\t_normalizeargs(s, output)\n\telse:\n\t\toutput.append(sequence)\n\n\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif isinstance(item, basestring):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, Declaration):\n\t\t\tfor iface in item:\n\t\t\t\toutput.append(iface)\n\t\telse:\n\t\t\t_normalizeargs(item, output)\n\treturn output\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor item in sequence:\n\t\tif isinstance(item, Interface):\n\t\t\toutput.append(item)\n\t\telif isinstance(item, Declaration):\n\t\t\t_normalizeargs(item.interfaces, output)\n\t\telse:\n\t\t\ttry:\n\t\t\t\titerator = iter(item)\n\t\t\texcept TypeError:\n\t\t\t\traise TypeError(\"interface declarations must be made of interfaces, not %r\" % (item,))\n\t\t\t_normalizeargs(iterator, output)\n\treturn output\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Declaration):\n\t\t\t_normalizeargs(arg.args, output)\n\t\telif isinstance(arg, tuple):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(arg)\n\treturn output\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tfor arg in sequence:\n\t\tif isinstance(arg, Declaration):\n\t\t\t_normalizeargs(arg, output)\n\t\telif isinstance(arg, tuple):\n\t\t\t_normalizeargs(arg, output)\n\t\telse:\n\t\t\toutput.append(arg)\n\treturn output\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif output is None:\n\t\toutput = []\n\tif isinstance(sequence, Declaration):\n\t\tsequence = sequence.extends\n\tif isinstance(sequence, (tuple, list, set)):\n\t\tfor item in sequence:\n\t\t\t_normalizeargs(item, output)\n\telse:\n\t\toutput.append(sequence)\n\treturn output\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _normalizeargs(sequence, output=None):\n\tif isinstance(sequence, tuple):\n\t\tif len(sequence) == 1:\n\t\t\treturn _normalizeargs(sequence[0], output)\n\t\telse:\n\t\t\tfor element in sequence:\n\t\t\t\t_normalizeargs(element, output)\n\telif isinstance(sequence, Declaration):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Interface):\n\t\toutput.append(sequence)\n\telif isinstance(sequence, Implements):\n\t\toutput.append(sequence.interface)\n\telse:\n\t\traise TypeError(\"Cannot normalize argument: %s\" % (repr(sequence)))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_available", "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.", "ques_id": "62b8b3d5eb7e40a82d2d1110", "code": ["def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError:\n\t\treturn None\n\telse:\n\t\treturn c_optimizations\n\n", "def _c_optimizations_available():\n\t# Check for C optimizations\n\ttry:\n\t\t# Use the C optimizations\n\t\tfrom _optimizations import Optimizations\n\texcept ImportError:\n\t\t# No C optimizations available\n\t\treturn False\n\treturn Optimizations\n\n\n", "def _c_optimizations_available():\n\ttry:\n\t\tfrom . import _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n\treturn _c_optimizations\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn None\n\n", "def _c_optimizations_available():\n\ttry:\n\t\tfrom _c_optimizations import c_optimizations\n\texcept ImportError:\n\t\tc_optimizations = False\n\treturn c_optimizations\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport pymc3.theanof import floatX\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn floatX\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n", "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\texcept ImportError:\n\t\treturn None\n\telse:\n\t\treturn c_optimizations\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\t# Check for C optimizations\n\ttry:\n\t\t# Use the C optimizations\n\t\tfrom _optimizations import Optimizations\n\texcept ImportError:\n\t\t# No C optimizations available\n\t\treturn False\n\treturn Optimizations\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\tfrom . import _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n\treturn _c_optimizations\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport _optimizations\n\t\treturn _optimizations\n\texcept ImportError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\tfrom _c_optimizations import c_optimizations\n\texcept ImportError:\n\t\tc_optimizations = False\n\treturn c_optimizations\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport pymc3.theanof import floatX\n\texcept ImportError:\n\t\treturn False\n\telse:\n\t\treturn floatX\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport c_optimizations\n\t\treturn c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_available():\n\ttry:\n\t\timport _c_optimizations\n\t\treturn _c_optimizations\n\texcept ImportError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_should_attempt_c_optimizations", "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110f", "code": ["def _should_attempt_c_optimizations():\n\treturn True\n\n", "def _should_attempt_c_optimizations():\n\treturn True\n\t#return os.getenv('USE_C_OPT_VAR') is not None\n\n", "def _should_attempt_c_optimizations():\n\treturn bool(os.environ.get('AIMA_C_OPTIMIZATIONS', False))\n\n\n", "def _should_attempt_c_optimizations():\n\tif os.environ.get('PYPY_USING_CODE_GEN_OPTIMIZATIONS', '0') != '0':\n\t\treturn True\n\tfor w_type in objectmodel.type_cache.values():\n\t\tif isinstance(w_type, W_TypeObject):\n\t\t\tif w_type.uses_object_getdict:\n\t\t\t\treturn False\n\t\t\telif w_type.uses_object_getclass:\n\t\t\t\treturn False\n\t\t\telif w_type.uses_object_getclass_caching:\n\t\t\t\treturn False\n\treturn True\n\n", "def _should_attempt_c_optimizations():\n\tif \"DISABLE_C_OPTIMIZATIONS\" in os.environ:\n\t\treturn False\n\n\ttry:\n\t\tfrom . import _c_optimizations as _c_optimizations\n\t\treturn True\n\texcept:\n\t\treturn False\n\n\n", "def _should_attempt_c_optimizations():\n\tif os.environ.get(\"PYTHON_SKIP_C_OPTIMIZATIONS\", \"\").lower() in (\"1\", \"yes\", \"true\"):\n\t\treturn False\n\treturn True\n\n", "def _should_attempt_c_optimizations():\n\treturn _use_c_optimizations\n\n", "def _should_attempt_c_optimizations():\n\treturn _c_optimizations_available and _c_optimizations_enabled\n\n\n\n", "def _should_attempt_c_optimizations():\n\treturn True\n\n", "def _should_attempt_c_optimizations():\n\treturn True if _check_c_optimizations() else False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _should_attempt_c_optimizations():\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn True\n\t#return os.getenv('USE_C_OPT_VAR') is not None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn bool(os.environ.get('AIMA_C_OPTIMIZATIONS', False))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\tif os.environ.get('PYPY_USING_CODE_GEN_OPTIMIZATIONS', '0') != '0':\n\t\treturn True\n\tfor w_type in objectmodel.type_cache.values():\n\t\tif isinstance(w_type, W_TypeObject):\n\t\t\tif w_type.uses_object_getdict:\n\t\t\t\treturn False\n\t\t\telif w_type.uses_object_getclass:\n\t\t\t\treturn False\n\t\t\telif w_type.uses_object_getclass_caching:\n\t\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\tif \"DISABLE_C_OPTIMIZATIONS\" in os.environ:\n\t\treturn False\n\n\ttry:\n\t\tfrom . import _c_optimizations as _c_optimizations\n\t\treturn True\n\texcept:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\tif os.environ.get(\"PYTHON_SKIP_C_OPTIMIZATIONS\", \"\").lower() in (\"1\", \"yes\", \"true\"):\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn _use_c_optimizations\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn _c_optimizations_available and _c_optimizations_enabled\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _should_attempt_c_optimizations():\n\treturn True if _check_c_optimizations() else False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_ignored", "docstring": "The opposite of `_c_optimizations_required`.", "ques_id": "62b8b3d4eb7e40a82d2d110e", "code": ["def _c_optimizations_ignored():\n\treturn os.environ.get('PURE_PYTHON', '0') != '0'\n\n\n", "def _c_optimizations_ignored():\n\treturn os.environ.get(\"PURE_PYTHON\") not in (None, \"0\")\n\n", "def _c_optimizations_ignored():\n\treturn os.getenv(\"PURE_PYTHON\", \"0\") != \"0\"\n\n", "def _c_optimizations_ignored():\n\tif PURE_PYTHON:\n\t\tprint(\n\t\t\t\"\\n\"\n\t\t\t+ \" !!! \"\n\t\t\t+ \"Warning: C-optimizations are ignored\"\n\t\t\t+ \" !!!\"\n\t\t\t+ \"\\n\"\n\t\t)\n\t\treturn True\n\treturn False\n\n\n", "def _c_optimizations_ignored():\n\tfrom os import environ\n\treturn environ.get('PURE_PYTHON') is not None and not environ.get('PURE_PYTHON') == '0'\n\n", "def _c_optimizations_ignored():\n\ttry:\n\t\tif 'PURE_PYTHON' in os.environ:\n\t\t\tif not os.environ['PURE_PYTHON'] in ['0', '', 'false', 'False', None]:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n", "def _c_optimizations_ignored():\n\treturn os.environ.get(\"PURE_PYTHON\") is not None and int(os.environ.get(\"PURE_PYTHON\")) != 0\n\n", "def _c_optimizations_ignored():\n\timport os\n\treturn os.environ.get('PURE_PYTHON') not in (None, 0, '0', 'False', 'false')\n\n", "def _c_optimizations_ignored():\n\treturn os.environ.get('PURE_PYTHON', 0) != 0\n\n", "def _c_optimizations_ignored():\n\treturn os.getenv('PURE_PYTHON') is not None and os.getenv('PURE_PYTHON') != \"0\"\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_ignored():\n\treturn os.environ.get('PURE_PYTHON', '0') != '0'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\treturn os.environ.get(\"PURE_PYTHON\") not in (None, \"0\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\treturn os.getenv(\"PURE_PYTHON\", \"0\") != \"0\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\tif PURE_PYTHON:\n\t\tprint(\n\t\t\t\"\\n\"\n\t\t\t+ \" !!! \"\n\t\t\t+ \"Warning: C-optimizations are ignored\"\n\t\t\t+ \" !!!\"\n\t\t\t+ \"\\n\"\n\t\t)\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_ignored():\n\tfrom os import environ\n\treturn environ.get('PURE_PYTHON') is not None and not environ.get('PURE_PYTHON') == '0'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\ttry:\n\t\tif 'PURE_PYTHON' in os.environ:\n\t\t\tif not os.environ['PURE_PYTHON'] in ['0', '', 'false', 'False', None]:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\treturn os.environ.get(\"PURE_PYTHON\") is not None and int(os.environ.get(\"PURE_PYTHON\")) != 0\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\timport os\n\treturn os.environ.get('PURE_PYTHON') not in (None, 0, '0', 'False', 'false')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\treturn os.environ.get('PURE_PYTHON', 0) != 0\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_ignored():\n\treturn os.getenv('PURE_PYTHON') is not None and os.getenv('PURE_PYTHON') != \"0\"\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/zope/interface/_compat.py", "project": "pexip/os-zope", "name": "_c_optimizations_required", "docstring": "Return a true value if the C optimizations are required.\n\nThis uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.", "ques_id": "62b8b3d4eb7e40a82d2d110d", "code": ["def _c_optimizations_required():\n\treturn not _c_optimizations_available()\n\n", "def _c_optimizations_required():\n\treturn py_platform == \"darwin\"\n\n\n", "def _c_optimizations_required():\n\treturn _c_optimizations_supported() and not os.environ.get('DISABLE_C_OPTIMIZATIONS')\n\n", "def _c_optimizations_required():\n\treturn True\n\n", "def _c_optimizations_required():\n\treturn not (\n\t\tgetattr(sys, 'frozen', None) or\n\t\tgetattr(sys, 'importers', None) or\n\t\tgetattr(sys, '__egginsert', None))\n\n\n", "def _c_optimizations_required():\n\treturn '--optimize' in sys.argv\n\n\n", "def _c_optimizations_required():\n\t\n\treturn _c_optimizations_available()\n\n\n", "def _c_optimizations_required():\n\treturn _c_optimizations_required_value\n\n\n", "def _c_optimizations_required():\n\treturn os.environ.get(\"TEST_C_OPTIMIZATIONS\", \"1\") == \"1\"\n\n", "def _c_optimizations_required():\n\treturn _c_optimizations_available() and not _c_optimizations_disabled()\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _c_optimizations_required():\n\treturn not _c_optimizations_available()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn py_platform == \"darwin\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn _c_optimizations_supported() and not os.environ.get('DISABLE_C_OPTIMIZATIONS')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn not (\n\t\tgetattr(sys, 'frozen', None) or\n\t\tgetattr(sys, 'importers', None) or\n\t\tgetattr(sys, '__egginsert', None))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn '--optimize' in sys.argv\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _c_optimizations_required():\n\t\n\treturn _c_optimizations_available()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn _c_optimizations_required_value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn os.environ.get(\"TEST_C_OPTIMIZATIONS\", \"1\") == \"1\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _c_optimizations_required():\n\treturn _c_optimizations_available() and not _c_optimizations_disabled()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "reset", "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).", "ques_id": "62b87b989a0c4fa8b80b35ee", "code": ["def reset(self):\n\tself.__dict__['context'] = {}\n\tself.__dict__['bins'] = self.__dict__['make_bins']()\n\t\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturns the bin *key*.\n\t\t\"\"\"\n\t\treturn self.bins[key]\n\t\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSets the bin *key* to *value*.\n\t\t\"\"\"\n\t\tself.bins[key] = value\n\t\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemoves the bin *key*.\n\t\t\"\"\"\n\t\tdel self.bins[key]\n\t\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of the bins.\n\t\t\"\"\"\n\t\treturn len(self.bins)\n\t\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturns the iterator of the bins.\n\t\t\"\"\"\n\t\treturn iter(self.bins)\n\t\n\tdef __cont\n", "def reset(self):\n\tself.current_context = {}\n\tfor bin in self.bins:\n\t\tbin.reset()\n\tif self.initial_value is not None:\n\t\tfor bin in self.bins:\n\t\t\tbin.value = self.initial_value\n\telse:\n\t\tself.make_bins()\n\n\tself.total_count = 0\n\tself.total_value = 0.\n\tself.total_value_squared = 0.\n\tself.total_value_squared_uncertainty = 0.\n\tself.total_value_uncertainty = 0.\n\n\tself.reset_flag = True\n\treturn\n\n\n", "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\n", "def reset(self):\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\n\tdef get_bins(self):\n\t\"\"\"\n\tReturns the bins of the class as a dict.\n\t\"\"\"\n\treturn self.bins\n\n\tdef add_bin(self, name, bin):\n\t\"\"\"\n\tAdds a bin to the class.\n\n\t:param name: name of the bin\n\t:param bin: the bin to add\n\t\"\"\"\n\tself.bins[name] = bin\n\n\tdef make_bins(self):\n\t\"\"\"\n\tReturns a dict of bins with keys the names of the bins and values the bins.\n\t\"\"\"\n\treturn {}\n\n\tdef get_count(self, name, context=None):\n\t\"\"\"\n\tGets the count of a bin given its name and the context.\n\n\t:param name: name of the bin\n\t:param context: context for which to get the count of the bin\n\t:return: count of the bin\n\t\"\"\"\n\tif context is None:\n\t\tcontext = self.current_context\n\treturn self.bins[name].get_count\n", "def reset(self):\n\tself.context = {}\n\tself.reinitialize_bins()\n\n", "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\t\n", "def reset(self):\n\tself.__context.clear()\n\tfor bin in self.__bins.values():\n\t\tbin.reset()\n\t\tif self.__initial_value is not None:\n\t\t\tbin.set(self.__initial_value)\n\t\telse:\n\t\t\tbin.make_bins()\n\treturn self\n\n\n", "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\n\n", "def reset(self):\n\tself.context = {}\n\tself.__init__(self.bins, self.initial_value)\n\t\n", "def reset(self):\n\tself.__context = {}\n\tself.__bins = {}\n\tself.__init_bins()\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def reset(self):\n\tself.__dict__['context'] = {}\n\tself.__dict__['bins'] = self.__dict__['make_bins']()\n\t\n\tdef __getitem__(self, key):\n\t\t\"\"\"\n\t\tReturns the bin *key*.\n\t\t\"\"\"\n\t\treturn self.bins[key]\n\t\n\tdef __setitem__(self, key, value):\n\t\t\"\"\"\n\t\tSets the bin *key* to *value*.\n\t\t\"\"\"\n\t\tself.bins[key] = value\n\t\n\tdef __delitem__(self, key):\n\t\t\"\"\"\n\t\tRemoves the bin *key*.\n\t\t\"\"\"\n\t\tdel self.bins[key]\n\t\n\tdef __len__(self):\n\t\t\"\"\"\n\t\tReturns the length of the bins.\n\t\t\"\"\"\n\t\treturn len(self.bins)\n\t\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\tReturns the iterator of the bins.\n\t\t\"\"\"\n\t\treturn iter(self.bins)\n\t\n\tdef __cont\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.current_context = {}\n\tfor bin in self.bins:\n\t\tbin.reset()\n\tif self.initial_value is not None:\n\t\tfor bin in self.bins:\n\t\t\tbin.value = self.initial_value\n\telse:\n\t\tself.make_bins()\n\n\tself.total_count = 0\n\tself.total_value = 0.\n\tself.total_value_squared = 0.\n\tself.total_value_squared_uncertainty = 0.\n\tself.total_value_uncertainty = 0.\n\n\tself.reset_flag = True\n\treturn\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.current_context = {}\n\tself.bins = self.make_bins()\n\n\tdef get_bins(self):\n\t\"\"\"\n\tReturns the bins of the class as a dict.\n\t\"\"\"\n\treturn self.bins\n\n\tdef add_bin(self, name, bin):\n\t\"\"\"\n\tAdds a bin to the class.\n\n\t:param name: name of the bin\n\t:param bin: the bin to add\n\t\"\"\"\n\tself.bins[name] = bin\n\n\tdef make_bins(self):\n\t\"\"\"\n\tReturns a dict of bins with keys the names of the bins and values the bins.\n\t\"\"\"\n\treturn {}\n\n\tdef get_count(self, name, context=None):\n\t\"\"\"\n\tGets the count of a bin given its name and the context.\n\n\t:param name: name of the bin\n\t:param context: context for which to get the count of the bin\n\t:return: count of the bin\n\t\"\"\"\n\tif context is None:\n\t\tcontext = self.current_context\n\treturn self.bins[name].get_count\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.reinitialize_bins()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.__context.clear()\n\tfor bin in self.__bins.values():\n\t\tbin.reset()\n\t\tif self.__initial_value is not None:\n\t\t\tbin.set(self.__initial_value)\n\t\telse:\n\t\t\tbin.make_bins()\n\treturn self\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.make_bins()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.context = {}\n\tself.__init__(self.bins, self.initial_value)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def reset(self):\n\tself.__context = {}\n\tself.__bins = {}\n\tself.__init_bins()\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "to_csv", "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.", "ques_id": "62b87b859a0c4fa8b80b35d7", "code": ["def to_csv(self, separator=\",\", header=None):\n\treturn _to_csv(self.points, separator, header)\n\n", "def to_csv(self, separator=\",\", header=None):\n\tif header is None:\n\t\theader = \"\"\n\telse:\n\t\theader = header + \"\\n\"\n\treturn header + \"\\n\".join(separator.join(str(x) for x in p) +\n\t\tseparator + separator.join(str(v) for v in self[p])\n\t\t\tfor p in self)\n\n", "def to_csv(self, separator=\",\", header=None):\n\treturn self.to_table(header=header).to_csv(separator=separator)\n\n\tdef to_table(self, header=None):\n\t\t\"\"\"Convert graph's points to a table.\n\n", "def to_csv(self, separator=\",\", header=None):\n\t# TODO: the order of points in the output is not guaranteed.\n\tif header is not None:\n\t\tyield header + \"\\n\"\n\n\tfor pt in self.points:\n\t\tpoint_str = separator.join(map(str, pt))\n\t\tfor v in self.value(pt):\n\t\t\tyield point_str + separator + str(v) + \"\\n\"\n\n", "def to_csv(self, separator=\",\", header=None):\n\treturn to_csv(self, separator, header)\n\n", "def to_csv(self, separator=\",\", header=None):\n\t# TODO: check if it is possible to remove to_csv.\n\t# TODO: if to_csv is removed, check if it can be replaced by\n\t# a common function in Lena.\n\t# TODO: if it is possible, do it.\n\t# TODO: if it is not possible, write the reason.\n\twarnings.warn(\"to_csv is deprecated in Lena 0.5\",\n\t\tDeprecationWarning)\n\tif header is not None:\n\t\tyield header + \"\\n\"\n\tfor coordinate, value in self.items():\n\t\tcoordinate_str = separator.join(map(str, coordinate))\n\t\tvalue_str = separator.join(map(str, value))\n\t\tyield \"{0}{1}{2}\\n\".format(\n\t\t\tcoordinate_str, separator, value_str)\n\n", "def to_csv(self, separator=\",\", header=None):\n\treturn self._to_csv(separator, header)\n\n", "def to_csv(self, separator=\",\", header=None):\n\timport warnings\n\twarnings.warn(\"to_csv is deprecated since Lena 0.5, \"\\\n\t              \"it is not used\", DeprecationWarning)\n\treturn self._to_csv_impl(header, separator)\n\n\n", "def to_csv(self, separator=\",\", header=None):\n\tfrom lena.core import to_csv\n\treturn to_csv(self, separator, header)\n\n", "def to_csv(self, separator=\",\", header=None):\n\tdeprecation(\"0.5\", \"to_csv is not used. Iterables are converted to tables.\")\n\treturn self._to_csv(separator, header)\n\n\t# def _to_csv(self, separator=\",\", header=None):\n\t# \t\"\"\"Internal method to convert graph's points to CSV.\"\"\"\n\t# \t# points = self.points\n\t# \t# if not points:\n\t# \t# \treturn \"\"\n\t# \t# # this is to check that points are consistent\n\t# \t# point = next(iter(points))\n\t# \t# # this is to discover the number of values at each point\n\t# \t# value = next(iter(points[point]))\n\t# \t# # this is to check that values are consistent\n\t# \t# value_len = len(value)\n\t# \t# # this is to check that all points have the same number of values\n\t# \t# if not all(\n\t# \t# \tlen(next(iter(v))) == value_len\n\t# \t# \t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def to_csv(self, separator=\",\", header=None):\n\treturn _to_csv(self.points, separator, header)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\tif header is None:\n\t\theader = \"\"\n\telse:\n\t\theader = header + \"\\n\"\n\treturn header + \"\\n\".join(separator.join(str(x) for x in p) +\n\t\tseparator + separator.join(str(v) for v in self[p])\n\t\t\tfor p in self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\treturn self.to_table(header=header).to_csv(separator=separator)\n\n\tdef to_table(self, header=None):\n\t\t\"\"\"Convert graph's points to a table.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t# TODO: the order of points in the output is not guaranteed.\n\tif header is not None:\n\t\tyield header + \"\\n\"\n\n\tfor pt in self.points:\n\t\tpoint_str = separator.join(map(str, pt))\n\t\tfor v in self.value(pt):\n\t\t\tyield point_str + separator + str(v) + \"\\n\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\treturn to_csv(self, separator, header)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\t# TODO: check if it is possible to remove to_csv.\n\t# TODO: if to_csv is removed, check if it can be replaced by\n\t# a common function in Lena.\n\t# TODO: if it is possible, do it.\n\t# TODO: if it is not possible, write the reason.\n\twarnings.warn(\"to_csv is deprecated in Lena 0.5\",\n\t\tDeprecationWarning)\n\tif header is not None:\n\t\tyield header + \"\\n\"\n\tfor coordinate, value in self.items():\n\t\tcoordinate_str = separator.join(map(str, coordinate))\n\t\tvalue_str = separator.join(map(str, value))\n\t\tyield \"{0}{1}{2}\\n\".format(\n\t\t\tcoordinate_str, separator, value_str)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\treturn self._to_csv(separator, header)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\timport warnings\n\twarnings.warn(\"to_csv is deprecated since Lena 0.5, \"\\\n\t              \"it is not used\", DeprecationWarning)\n\treturn self._to_csv_impl(header, separator)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\tfrom lena.core import to_csv\n\treturn to_csv(self, separator, header)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def to_csv(self, separator=\",\", header=None):\n\tdeprecation(\"0.5\", \"to_csv is not used. Iterables are converted to tables.\")\n\treturn self._to_csv(separator, header)\n\n\t# def _to_csv(self, separator=\",\", header=None):\n\t# \t\"\"\"Internal method to convert graph's points to CSV.\"\"\"\n\t# \t# points = self.points\n\t# \t# if not points:\n\t# \t# \treturn \"\"\n\t# \t# # this is to check that points are consistent\n\t# \t# point = next(iter(points))\n\t# \t# # this is to discover the number of values at each point\n\t# \t# value = next(iter(points[point]))\n\t# \t# # this is to check that values are consistent\n\t# \t# value_len = len(value)\n\t# \t# # this is to check that all points have the same number of values\n\t# \t# if not all(\n\t# \t# \tlen(next(iter(v))) == value_len\n\t# \t# \t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_get_err_indices", "docstring": "Get error indices corresponding to a coordinate.", "ques_id": "62b87b839a0c4fa8b80b35cb", "code": ["def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor i in range(len(self.coord_names)):\n\t\tif self.coord_names[i] == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n\n", "def _get_err_indices(self, coord_name):\n\treturn np.where(self.err_coord_names==coord_name)[0]\n\n\tdef _get_coord_index(self, coord_name):\n\t\t\"\"\"\n\t\tFind the index of coord_name in the coordinates.\n\t\t\"\"\"\n\t\treturn np.where(self.coord_names==coord_name)[0][0]\n\n\tdef _get_coord_value(self, coord_name):\n\t\t\"\"\"\n\t\tFind the value of coord_name in the coordinates.\n\t\t\"\"\"\n\t\treturn self.coords[self._get_coord_index(coord_name)]\n\n\tdef _get_coord_errors(self, coord_name):\n\t\t\"\"\"\n\t\tFind the errors of coord_name in the coordinates.\n\t\t\"\"\"\n\t\terr_indices = self._get_err_indices(coord_name)\n\t\treturn self.coords_err[err_indices]\n\n\tdef _get_coord_units(self, coord_name):\n\t\t\"\"\"\n\t\tFind the units of coord_name in the coordinates.\n\t\t\"\"\"\n\t\treturn self\n", "def _get_err_indices(self, coord_name):\n\treturn [i for i, err in enumerate(self.errors) if err.coord_name == coord_name]\n\n", "def _get_err_indices(self, coord_name):\n\terror_indices = []\n\tfor err_ind, err_name in enumerate(self._error_names):\n\t\tif err_name[0:len(coord_name)] == coord_name:\n\t\t\terror_indices.append(err_ind)\n\treturn error_indices\n\n", "def _get_err_indices(self, coord_name):\n\tindices = list()\n\tfor i in range(len(self.error_names)):\n\t\tif self.error_names[i] == coord_name:\n\t\t\tindices.append(i)\n\treturn indices\n\n\n", "def _get_err_indices(self, coord_name):\n\treturn [i for i, c in enumerate(self.coords) if c.name == coord_name]\n\n", "def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor index, (name, _) in enumerate(self.coord_names):\n\t\tif name == coord_name:\n\t\t\terr_indices.append(index)\n\treturn err_indices\n\n\t#---------------------------------------------\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n", "def _get_err_indices(self, coord_name):\n\t# Get all error indices\n\terr_indices = [i for i, entry in enumerate(self.errors) if entry.coord_name == coord_name]\n\n\treturn err_indices\n\n\n", "def _get_err_indices(self, coord_name):\n\treturn [i for i in range(len(self.error_names)) if self.error_names[i] == coord_name]\n\n\n", "def _get_err_indices(self, coord_name):\n\t# TODO: add support for multi-dimensional coordinates\n\terr_indices = [i for i, s in enumerate(self.err_names) if s == coord_name]\n\treturn err_indices\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor i in range(len(self.coord_names)):\n\t\tif self.coord_names[i] == coord_name:\n\t\t\terr_indices.append(i)\n\treturn err_indices\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\treturn np.where(self.err_coord_names==coord_name)[0]\n\n\tdef _get_coord_index(self, coord_name):\n\t\t\"\"\"\n\t\tFind the index of coord_name in the coordinates.\n\t\t\"\"\"\n\t\treturn np.where(self.coord_names==coord_name)[0][0]\n\n\tdef _get_coord_value(self, coord_name):\n\t\t\"\"\"\n\t\tFind the value of coord_name in the coordinates.\n\t\t\"\"\"\n\t\treturn self.coords[self._get_coord_index(coord_name)]\n\n\tdef _get_coord_errors(self, coord_name):\n\t\t\"\"\"\n\t\tFind the errors of coord_name in the coordinates.\n\t\t\"\"\"\n\t\terr_indices = self._get_err_indices(coord_name)\n\t\treturn self.coords_err[err_indices]\n\n\tdef _get_coord_units(self, coord_name):\n\t\t\"\"\"\n\t\tFind the units of coord_name in the coordinates.\n\t\t\"\"\"\n\t\treturn self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\treturn [i for i, err in enumerate(self.errors) if err.coord_name == coord_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\terror_indices = []\n\tfor err_ind, err_name in enumerate(self._error_names):\n\t\tif err_name[0:len(coord_name)] == coord_name:\n\t\t\terror_indices.append(err_ind)\n\treturn error_indices\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\tindices = list()\n\tfor i in range(len(self.error_names)):\n\t\tif self.error_names[i] == coord_name:\n\t\t\tindices.append(i)\n\treturn indices\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\treturn [i for i, c in enumerate(self.coords) if c.name == coord_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\terr_indices = []\n\tfor index, (name, _) in enumerate(self.coord_names):\n\t\tif name == coord_name:\n\t\t\terr_indices.append(index)\n\treturn err_indices\n\n\t#---------------------------------------------\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t# Get all error indices\n\terr_indices = [i for i, entry in enumerate(self.errors) if entry.coord_name == coord_name]\n\n\treturn err_indices\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\treturn [i for i in range(len(self.error_names)) if self.error_names[i] == coord_name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_err_indices(self, coord_name):\n\t# TODO: add support for multi-dimensional coordinates\n\terr_indices = [i for i, s in enumerate(self.err_names) if s == coord_name]\n\treturn err_indices\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "_update_context", "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.", "ques_id": "62b87b7e9a0c4fa8b80b35bc", "code": ["def _update_context(self, context):\n\tif self.error is not None:\n\t\tcontext.error = {}\n\t\tfor i in self.error:\n\t\t\tif i < len(self):\n\t\t\t\tcontext.error[self.fields[i]] = {\"index\": i}\n\t\t\telse:\n\t\t\t\tcontext.error[self.fields[i - len(self)]] = {\"index\": i}\n\tfor i, name in enumerate(self.fields):\n\t\tif name == \"x\":\n\t\t\tcontext.x = {\"index\": i}\n\t\telif name == \"y\":\n\t\t\tcontext.y = {\"index\": i}\n\t\telif name == \"z\":\n\t\t\tcontext.z = {\"index\": i}\n\t\telif i < len(self):\n\t\t\tcontext.value.setdefault(name, {})[\"index\"] = i\n\t\telse:\n\t\t\tcontext.value.setdefault(name, {})[\"index\"] = i - len(self)\n\n", "def _update_context(self, context):\n\tcontext[\"value\"] = self._to_dict()\n\tfor key, value in context[\"value\"].items():\n\t\tif key in self._errors:\n\t\t\tcontext[\"value\"][key].update(self._errors[key]._to_dict())\n\t\t\tcontext[\"error\"] = self._errors[key]._to_dict_for_error(context[\"error\"])\n", "def _update_context(self, context):\n\tif not context.value:\n\t\tcontext.value = {}\n\tif not context.error:\n\t\tcontext.error = {}\n\tcontext.value[\"x\"] = self.t\n\tcontext.value[\"y\"] = self.E\n\tcontext.error[\"x\"] = {\"index\": 1}\n\tcontext.error[\"y\"] = {\"index\": 2}\n\n", "def _update_context(self, context):\n\tfields = self.fields\n\terrors = self.errors\n\tfor k,v in self.context.items():\n\t\tcontext[k] = v\n\tif fields:\n\t\tcontext[\"value\"] = {}\n\t\tfor f in fields:\n\t\t\tcontext[\"value\"][f] = {\"index\": fields.index(f)}\n\tif errors:\n\t\tcontext[\"error\"] = {}\n\t\tfor e in errors:\n\t\t\tcontext[\"error\"][e] = {\"index\": errors.index(e)}\n\n\n", "def _update_context(self, context):\n\tif self.error is not None:\n\t\tcontext[\"error\"] = {}\n\t\tfor name in self.error.keys():\n\t\t\tcontext[\"error\"][name] = {\n\t\t\t\t\"index\": self.error[name][\"index\"]\n\t\t\t}\n\t\t\tif self.error[name][\"label\"] is not None:\n\t\t\t\tcontext[\"error\"][name][\"label\"] = \\\n\t\t\t\t\tself.error[name][\"label\"]\n\t\t\t\n\t\t\tcontext[\"error\"][name][\"value\"] = self.error[name][\"value\"]\n\n\t\t\tif self.error[name][\"scale\"] is not None:\n\t\t\t\tcontext[\"error\"][name][\"scale\"] = \\\n\t\t\t\t\tself.error[name][\"scale\"]\n\n\t\t\tif self.error[name][\"unit\"] is not None:\n\t\t\t\tcontext[\"error\"][name][\"unit\"] = \\\n\t\t\t\t\tself.error[name][\"unit\"]\n\t\t\t\n\t\t\tif self.error[name\n", "def _update_context(self, context):\n\t#self.__logger.debug(\"update context\")\n\t#self.__logger.debug(context)\n\t#self.__logger.debug(context.value)\n\t#self.__logger.debug(context.error)\n\tif context.value is None:\n\t\tcontext.value = {}\n\tif context.error is None:\n\t\tcontext.error = {}\n\tif self.__x is not None:\n\t\tcontext.value[\"x\"] = self.__x\n\tif self.__y is not None:\n\t\tcontext.value[\"y\"] = self.__y\n\tif self.__z is not None:\n\t\tcontext.value[\"z\"] = self.__z\n\tif self.__x_low is not None:\n\t\tcontext.error[\"x_low\"] = {\"index\": self.__x_low}\n\tif self.__x_high is not None:\n\t\tcontext.error[\"x_high\"] = {\"index\": self.__x_high}\n\tif self.__y_low is not None:\n\t\tcontext.error[\"y_\n", "def _update_context(self, context):\n\tcontext[\"value\"] = {}\n\tif self.E is not None:\n\t\tcontext[\"value\"][\"E\"] = self.E\n\tif self.t is not None:\n\t\tcontext[\"value\"][\"t\"] = self.t\n\tcontext[\"error\"] = {}\n\tfor name in self.errors:\n\t\terror = self.errors[name]\n\t\tif error is not None:\n\t\t\tcontext[\"error\"][name] = {\"index\": error}\n\n", "def _update_context(self, context):\n\t# TODO: Use __getstate__ and __setstate__\n\t# instead of manually copying data.\n\t# TODO: Use *context.graph* for storing\n\t# graph-specific data.\n\t# TODO: Check for existing fields.\n\t# TODO: Use *context.graph.error* instead of *context.error*.\n\t# TODO: Use *context.graph.error.x_low* instead of *context.error.x_low*.\n\t# TODO: Use *context.graph.error.x_high* instead of *context.error.x_high*.\n\t# TODO: Use *context.graph.error.y_low* instead of *context.error.y_low*.\n\t# TODO: Use *context.graph.error.y_high* instead of *context.error.y_high*.\n\t# TODO: Use *context.graph.error.z_low* instead of *context.error.z_low*.\n\t# TODO: Use *context.graph.error.z_high* instead of *context.error.z_high*.\n\t# TODO\n", "def _update_context(self, context):\n\tif \"error\" not in context:\n\t\tcontext[\"error\"] = {}\n\tfor key in self.error:\n\t\ti = self._field_index[key]\n\t\tif i < len(self.fields) - 1:\n\t\t\tname = self.fields[i]\n\t\telse:\n\t\t\tname = self.fields[-1]\n\t\tif name not in context[\"error\"]:\n\t\t\tcontext[\"error\"][name] = {}\n\t\tcontext[\"error\"][name][\"index\"] = i\n\t\t#context[\"error\"][name][\"value\"] = self.error[key]\n\t\t#?\n\n\tcontext[\"fields\"] = self.fields\n\tcontext[\"value\"] = self.value\n\n", "def _update_context(self, context):\n\tfor key, value in self.items():\n\t\tif key == \"error\":\n\t\t\tcontext.setdefault(\"error\", {})\n\t\t\tfor error_key, error_value in value.items():\n\t\t\t\tcontext.error.setdefault(error_key, {})\n\t\t\t\tcontext.error[error_key][\"index\"] = error_value\n\t\telse:\n\t\t\tcontext.setdefault(key, {})\n\t\t\tcontext[key][\"index\"] = value\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _update_context(self, context):\n\tif self.error is not None:\n\t\tcontext.error = {}\n\t\tfor i in self.error:\n\t\t\tif i < len(self):\n\t\t\t\tcontext.error[self.fields[i]] = {\"index\": i}\n\t\t\telse:\n\t\t\t\tcontext.error[self.fields[i - len(self)]] = {\"index\": i}\n\tfor i, name in enumerate(self.fields):\n\t\tif name == \"x\":\n\t\t\tcontext.x = {\"index\": i}\n\t\telif name == \"y\":\n\t\t\tcontext.y = {\"index\": i}\n\t\telif name == \"z\":\n\t\t\tcontext.z = {\"index\": i}\n\t\telif i < len(self):\n\t\t\tcontext.value.setdefault(name, {})[\"index\"] = i\n\t\telse:\n\t\t\tcontext.value.setdefault(name, {})[\"index\"] = i - len(self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tcontext[\"value\"] = self._to_dict()\n\tfor key, value in context[\"value\"].items():\n\t\tif key in self._errors:\n\t\t\tcontext[\"value\"][key].update(self._errors[key]._to_dict())\n\t\t\tcontext[\"error\"] = self._errors[key]._to_dict_for_error(context[\"error\"])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tif not context.value:\n\t\tcontext.value = {}\n\tif not context.error:\n\t\tcontext.error = {}\n\tcontext.value[\"x\"] = self.t\n\tcontext.value[\"y\"] = self.E\n\tcontext.error[\"x\"] = {\"index\": 1}\n\tcontext.error[\"y\"] = {\"index\": 2}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tfields = self.fields\n\terrors = self.errors\n\tfor k,v in self.context.items():\n\t\tcontext[k] = v\n\tif fields:\n\t\tcontext[\"value\"] = {}\n\t\tfor f in fields:\n\t\t\tcontext[\"value\"][f] = {\"index\": fields.index(f)}\n\tif errors:\n\t\tcontext[\"error\"] = {}\n\t\tfor e in errors:\n\t\t\tcontext[\"error\"][e] = {\"index\": errors.index(e)}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tif self.error is not None:\n\t\tcontext[\"error\"] = {}\n\t\tfor name in self.error.keys():\n\t\t\tcontext[\"error\"][name] = {\n\t\t\t\t\"index\": self.error[name][\"index\"]\n\t\t\t}\n\t\t\tif self.error[name][\"label\"] is not None:\n\t\t\t\tcontext[\"error\"][name][\"label\"] = \\\n\t\t\t\t\tself.error[name][\"label\"]\n\t\t\t\n\t\t\tcontext[\"error\"][name][\"value\"] = self.error[name][\"value\"]\n\n\t\t\tif self.error[name][\"scale\"] is not None:\n\t\t\t\tcontext[\"error\"][name][\"scale\"] = \\\n\t\t\t\t\tself.error[name][\"scale\"]\n\n\t\t\tif self.error[name][\"unit\"] is not None:\n\t\t\t\tcontext[\"error\"][name][\"unit\"] = \\\n\t\t\t\t\tself.error[name][\"unit\"]\n\t\t\t\n\t\t\tif self.error[name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t#self.__logger.debug(\"update context\")\n\t#self.__logger.debug(context)\n\t#self.__logger.debug(context.value)\n\t#self.__logger.debug(context.error)\n\tif context.value is None:\n\t\tcontext.value = {}\n\tif context.error is None:\n\t\tcontext.error = {}\n\tif self.__x is not None:\n\t\tcontext.value[\"x\"] = self.__x\n\tif self.__y is not None:\n\t\tcontext.value[\"y\"] = self.__y\n\tif self.__z is not None:\n\t\tcontext.value[\"z\"] = self.__z\n\tif self.__x_low is not None:\n\t\tcontext.error[\"x_low\"] = {\"index\": self.__x_low}\n\tif self.__x_high is not None:\n\t\tcontext.error[\"x_high\"] = {\"index\": self.__x_high}\n\tif self.__y_low is not None:\n\t\tcontext.error[\"y_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tcontext[\"value\"] = {}\n\tif self.E is not None:\n\t\tcontext[\"value\"][\"E\"] = self.E\n\tif self.t is not None:\n\t\tcontext[\"value\"][\"t\"] = self.t\n\tcontext[\"error\"] = {}\n\tfor name in self.errors:\n\t\terror = self.errors[name]\n\t\tif error is not None:\n\t\t\tcontext[\"error\"][name] = {\"index\": error}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\t# TODO: Use __getstate__ and __setstate__\n\t# instead of manually copying data.\n\t# TODO: Use *context.graph* for storing\n\t# graph-specific data.\n\t# TODO: Check for existing fields.\n\t# TODO: Use *context.graph.error* instead of *context.error*.\n\t# TODO: Use *context.graph.error.x_low* instead of *context.error.x_low*.\n\t# TODO: Use *context.graph.error.x_high* instead of *context.error.x_high*.\n\t# TODO: Use *context.graph.error.y_low* instead of *context.error.y_low*.\n\t# TODO: Use *context.graph.error.y_high* instead of *context.error.y_high*.\n\t# TODO: Use *context.graph.error.z_low* instead of *context.error.z_low*.\n\t# TODO: Use *context.graph.error.z_high* instead of *context.error.z_high*.\n\t# TODO\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tif \"error\" not in context:\n\t\tcontext[\"error\"] = {}\n\tfor key in self.error:\n\t\ti = self._field_index[key]\n\t\tif i < len(self.fields) - 1:\n\t\t\tname = self.fields[i]\n\t\telse:\n\t\t\tname = self.fields[-1]\n\t\tif name not in context[\"error\"]:\n\t\t\tcontext[\"error\"][name] = {}\n\t\tcontext[\"error\"][name][\"index\"] = i\n\t\t#context[\"error\"][name][\"value\"] = self.error[key]\n\t\t#?\n\n\tcontext[\"fields\"] = self.fields\n\tcontext[\"value\"] = self.value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _update_context(self, context):\n\tfor key, value in self.items():\n\t\tif key == \"error\":\n\t\t\tcontext.setdefault(\"error\", {})\n\t\t\tfor error_key, error_value in value.items():\n\t\t\t\tcontext.error.setdefault(error_key, {})\n\t\t\t\tcontext.error[error_key][\"index\"] = error_value\n\t\telse:\n\t\t\tcontext.setdefault(key, {})\n\t\t\tcontext[key][\"index\"] = value\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "integral", "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.", "ques_id": "62b87b4f9a0c4fa8b80b3580", "code": ["def integral(bins, edges):\n\treturn np.sum(bins) * np.diff(edges)[0]\n\n", "def integral(bins, edges):\n\tarea = 0\n\tfor i in range(len(bins)-1):\n\t\tarea += (bins[i] + bins[i+1]) * (edges[i+1] - edges[i]) / 2\n\treturn area\n\n", "def integral(bins, edges):\n\tedges = np.diff(edges)\n\treturn np.sum(edges * bins)\n\n", "def integral(bins, edges):\n\ttotal_area = 0\n\tfor i in range(len(bins)):\n\t\ttotal_area += bins[i]*(edges[i+1]-edges[i])\n\treturn total_area\n\n", "def integral(bins, edges):\n\t#print bins, edges\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n\n", "def integral(bins, edges):\n\treturn sum([(bins[i] * (edges[i+1] - edges[i])) for i in range(len(bins))])\n\n", "def integral(bins, edges):\n\t# Integrate the bins from the binned data\n\tarea = np.trapz(bins, edges[:-1])\n\treturn area\n\n", "def integral(bins, edges):\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n\n", "def integral(bins, edges):\n\t\n\tintegral = 0\n\t\n\tfor i in range(len(bins) - 1):\n\t\tintegral += (bins[i] + bins[i+1])/2 * (edges[i+1] - edges[i])\n\t\t\n\treturn integral\n\t\n", "def integral(bins, edges):\n\tintegral = 0.\n\tfor i in range(len(bins)):\n\t\tintegral += bins[i] * (edges[i+1]-edges[i])\n\treturn integral\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def integral(bins, edges):\n\treturn np.sum(bins) * np.diff(edges)[0]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tarea = 0\n\tfor i in range(len(bins)-1):\n\t\tarea += (bins[i] + bins[i+1]) * (edges[i+1] - edges[i]) / 2\n\treturn area\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tedges = np.diff(edges)\n\treturn np.sum(edges * bins)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\ttotal_area = 0\n\tfor i in range(len(bins)):\n\t\ttotal_area += bins[i]*(edges[i+1]-edges[i])\n\treturn total_area\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t#print bins, edges\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\treturn sum([(bins[i] * (edges[i+1] - edges[i])) for i in range(len(bins))])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t# Integrate the bins from the binned data\n\tarea = np.trapz(bins, edges[:-1])\n\treturn area\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tarea = 0\n\tfor i in range(len(bins)):\n\t\tarea += bins[i] * (edges[i+1] - edges[i])\n\treturn area\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\t\n\tintegral = 0\n\t\n\tfor i in range(len(bins) - 1):\n\t\tintegral += (bins[i] + bins[i+1])/2 * (edges[i+1] - edges[i])\n\t\t\n\treturn integral\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def integral(bins, edges):\n\tintegral = 0.\n\tfor i in range(len(bins)):\n\t\tintegral += bins[i] * (edges[i+1]-edges[i])\n\treturn integral\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_seq", "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.", "ques_id": "62b87b199a0c4fa8b80b354e", "code": ["def is_fill_request_seq(seq):\n\tif is_bool(seq):\n\t\treturn True\n\n\tif isinstance(seq, str):\n\t\ttry:\n\t\t\tFillRequestSeq(seq)\n\t\t\treturn True\n\t\texcept:\n\t\t\tpass\n\n\treturn False\n\n\n", "def is_fill_request_seq(seq):\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "def is_fill_request_seq(seq):\n\treturn seq.__class__ is bool\n\n", "def is_fill_request_seq(seq):\n\treturn isinstance(seq, FillRequestSeq) or isinstance(seq, bool)\n\n", "def is_fill_request_seq(seq):\n\treturn type(seq) == list and all(is_fill_request(fr) for fr in seq) and len(seq) == 2\n\n\n", "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequestSeq):\n\t\treturn True\n\tif not isinstance(seq, list):\n\t\treturn False\n\tif len(seq) < 4:\n\t\treturn False\n\tif not isinstance(seq[0], str):\n\t\treturn False\n\tif not isinstance(seq[1], int):\n\t\treturn False\n\tif not isinstance(seq[2], str):\n\t\treturn False\n\tif not isinstance(seq[3], int):\n\t\treturn False\n\tfor i in seq[4:]:\n\t\tif not isinstance(i, FillRequest):\n\t\t\treturn False\n\treturn True\n\n", "def is_fill_request_seq(seq):\n\tif isinstance(seq, bool):\n\t\treturn True\n\treturn False\n\n\n", "def is_fill_request_seq(seq):\n\treturn isinstance(seq, FillRequestSeq)\n\n\n", "def is_fill_request_seq(seq):\n\ttry:\n\t\tfill_request_seq = FillRequestSeq(seq)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n\n\n", "def is_fill_request_seq(seq):\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def is_fill_request_seq(seq):\n\tif is_bool(seq):\n\t\treturn True\n\n\tif isinstance(seq, str):\n\t\ttry:\n\t\t\tFillRequestSeq(seq)\n\t\t\treturn True\n\t\texcept:\n\t\t\tpass\n\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\ttry:\n\t\tFillRequestSeq(seq)\n\t\treturn True\n\texcept:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn seq.__class__ is bool\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn isinstance(seq, FillRequestSeq) or isinstance(seq, bool)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn type(seq) == list and all(is_fill_request(fr) for fr in seq) and len(seq) == 2\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\tif isinstance(seq, FillRequestSeq):\n\t\treturn True\n\tif not isinstance(seq, list):\n\t\treturn False\n\tif len(seq) < 4:\n\t\treturn False\n\tif not isinstance(seq[0], str):\n\t\treturn False\n\tif not isinstance(seq[1], int):\n\t\treturn False\n\tif not isinstance(seq[2], str):\n\t\treturn False\n\tif not isinstance(seq[3], int):\n\t\treturn False\n\tfor i in seq[4:]:\n\t\tif not isinstance(i, FillRequest):\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\tif isinstance(seq, bool):\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn isinstance(seq, FillRequestSeq)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\ttry:\n\t\tfill_request_seq = FillRequestSeq(seq)\n\t\treturn True\n\texcept Exception:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_seq(seq):\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_request_el", "docstring": "Object contains executable methods 'fill' and 'request'.", "ques_id": "62b87b099a0c4fa8b80b3538", "code": ["def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "def is_fill_request_el(obj):\n\treturn isinstance(obj, FillRequestEL)\n\n", "def is_fill_request_el(obj):\n\tif not isinstance(obj, (list, tuple)):\n\t\treturn False\n\tif len(obj) < 2:\n\t\treturn False\n\tif not isinstance(obj[0], basestring):\n\t\treturn False\n\tif not isinstance(obj[1], dict):\n\t\treturn False\n\treturn True\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\tif hasattr(obj, \"fill\") and hasattr(obj, \"request\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_fill_request_el(obj):\n\treturn (hasattr(obj, \"fill\") and hasattr(obj, \"request\"))\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"request\")\n\n\n", "def is_fill_request_el(obj):\n\treturn is_el(obj) and hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "def is_fill_request_el(obj):\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn isinstance(obj, FillRequestEL)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n\tif not isinstance(obj, (list, tuple)):\n\t\treturn False\n\tif len(obj) < 2:\n\t\treturn False\n\tif not isinstance(obj[0], basestring):\n\t\treturn False\n\tif not isinstance(obj[1], dict):\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\tif hasattr(obj, \"fill\") and hasattr(obj, \"request\"):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn (hasattr(obj, \"fill\") and hasattr(obj, \"request\"))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, \"fill\") and hasattr(obj, \"request\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\treturn is_el(obj) and hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_request_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'request')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_request_el(obj):\n\ttry:\n\t\tobj.fill\n\t\tobj.request\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_run_el", "docstring": "Object contains executable method 'run'.", "ques_id": "62b87af99a0c4fa8b80b3524", "code": ["def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\t# get the obj class\n\tclass_obj = obj.__class__\n\t# get the class name\n\tclass_name = class_obj.__name__\n\t# check whether the class name is in the base class list\n\tif class_name in base_class_list:\n\t\treturn False\n\t# check whether the class is in the base class list\n\tif class_obj in base_class_list:\n\t\treturn False\n\t# check whether the class is in the base class list\n\tif is_subclass_of(class_obj, base_class_list):\n\t\treturn False\n\t# check whether the class has the run method\n\tif hasattr(class_obj, 'run'):\n\t\treturn True\n\t# return False\n\treturn False\n\n\n", "def is_run_el(obj):\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n", "def is_run_el(obj):\n\tif is_el(obj):\n\t\treturn hasattr(obj, 'run')\n\treturn False\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, \"run\")\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\t\n", "def is_run_el(obj):\n\treturn is_el(obj) and hasattr(obj, 'run')\n\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n", "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\t# get the obj class\n\tclass_obj = obj.__class__\n\t# get the class name\n\tclass_name = class_obj.__name__\n\t# check whether the class name is in the base class list\n\tif class_name in base_class_list:\n\t\treturn False\n\t# check whether the class is in the base class list\n\tif class_obj in base_class_list:\n\t\treturn False\n\t# check whether the class is in the base class list\n\tif is_subclass_of(class_obj, base_class_list):\n\t\treturn False\n\t# check whether the class has the run method\n\tif hasattr(class_obj, 'run'):\n\t\treturn True\n\t# return False\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n\ttry:\n\t\tobj.run()\n\t\treturn True\n\texcept AttributeError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n\tif is_el(obj):\n\t\treturn hasattr(obj, 'run')\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, \"run\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn is_el(obj) and hasattr(obj, 'run')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_run_el(obj):\n\treturn hasattr(obj, 'run')\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/check_sequence_type.py", "project": "ynikitenko/lena", "name": "is_fill_compute_el", "docstring": "Object contains executable methods 'fill' and 'compute'.", "ques_id": "62b87af69a0c4fa8b80b351a", "code": ["def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "def is_fill_compute_el(obj):\n\treturn is_has_methods(obj, ['fill','compute'])\n\n", "def is_fill_compute_el(obj):\n\treturn (hasattr(obj,'fill') and hasattr(obj,'compute'))\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "def is_fill_compute_el(obj):\n\tif not hasattr(obj, 'fill'):\n\t\treturn False\n\tif not hasattr(obj, 'compute'):\n\t\treturn False\n\treturn True\n\n", "def is_fill_compute_el(obj):\n\tif not (isinstance(obj, types.TypeType) or isinstance(obj, types.ClassType)):\n\t\traise ValueError('obj must be a class')\n\treturn (hasattr(obj, 'fill') and hasattr(obj, 'compute'))\n\n", "def is_fill_compute_el(obj):\n\treturn (hasattr(obj, 'fill') and hasattr(obj, 'compute') and \n\t\t\tcallable(obj.fill) and callable(obj.compute))\n\n", "def is_fill_compute_el(obj):\n\treturn (hasattr(obj, 'fill') and hasattr(obj, 'compute'))\n\n", "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "def is_fill_compute_el(obj):\n\tif hasattr(obj, 'fill') and hasattr(obj, 'compute'):\n\t\treturn True\n\telse:\n\t\treturn False\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn is_has_methods(obj, ['fill','compute'])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn (hasattr(obj,'fill') and hasattr(obj,'compute'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\tif not hasattr(obj, 'fill'):\n\t\treturn False\n\tif not hasattr(obj, 'compute'):\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\tif not (isinstance(obj, types.TypeType) or isinstance(obj, types.ClassType)):\n\t\traise ValueError('obj must be a class')\n\treturn (hasattr(obj, 'fill') and hasattr(obj, 'compute'))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn (hasattr(obj, 'fill') and hasattr(obj, 'compute') and \n\t\t\tcallable(obj.fill) and callable(obj.compute))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn (hasattr(obj, 'fill') and hasattr(obj, 'compute'))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\treturn hasattr(obj, 'fill') and hasattr(obj, 'compute')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_fill_compute_el(obj):\n\tif hasattr(obj, 'fill') and hasattr(obj, 'compute'):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/context/functions.py", "project": "ynikitenko/lena", "name": "difference", "docstring": "Return a dictionary with items from *d1* not contained in *d2*.\n\n*level* sets the maximum depth of recursion. For infinite recursion,\nset that to -1. For level 1,\nif a key is present both in *d1* and *d2* but has different values,\nit is included into the difference.\nSee :func:`intersection` for more details.\n\n*d1* and *d2* remain unchanged. However, *d1* or some of its\nsubdictionaries may be returned directly.\nMake a deep copy of the result when appropriate.\n\n.. versionadded:: 0.5\n   add keyword argument *level*.", "ques_id": "62b87af19a0c4fa8b80b34f7", "code": ["def difference(d1, d2, level=-1):\n\tdiff = {}\n\tif level == 0:\n\t\tfor k, v in d1.items():\n\t\t\tif k not in d2:\n\t\t\t\tdiff[k] = v\n\telse:\n\t\tfor k, v1 in d1.items():\n\t\t\tif k not in d2:\n\t\t\t\tdiff[k] = v1\n\t\t\telif isinstance(v1, dict):\n\t\t\t\tdiff[k] = difference(v1, d2[k], level - 1)\n\treturn diff\n", "def difference(d1, d2, level=-1):\n\tif level == 0:\n\t\treturn d1\n\telse:\n\t\td = {}\n\t\tfor k, v in d1.items():\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif k in d2 and isinstance(d2[k], dict):\n\t\t\t\t\tv = difference(v, d2[k], level - 1)\n\t\t\t\t\tif v:\n\t\t\t\t\t\td[k] = v\n\t\t\t\telse:\n\t\t\t\t\td[k] = v\n\t\t\telse:\n\t\t\t\tif k not in d2:\n\t\t\t\t\td[k] = v\n\t\treturn d\n\n\n", "def difference(d1, d2, level=-1):\n\tret = {}\n\tif level == 0:\n\t\tif d1 != d2:\n\t\t\tret = d1\n\t\treturn ret\n\tfor k, v in d1.items():\n\t\tif k not in d2:\n\t\t\tret[k] = v\n\t\telif isinstance(v, dict):\n\t\t\tdiff = difference(d1[k], d2[k], level - 1)\n\t\t\tif diff:\n\t\t\t\tret[k] = diff\n\t\telif v != d2[k]:\n\t\t\tret[k] = v\n\treturn ret\n\n\n", "def difference(d1, d2, level=-1):\n\titems = list(d1.items())\n\tfor i, (key, value) in enumerate(items):\n\t\tif level >= 0:\n\t\t\tif key not in d2:\n\t\t\t\tcontinue\n\t\t\tif isinstance(value, dict):\n\t\t\t\tif isinstance(d2[key], dict):\n\t\t\t\t\titems[i] = (key, difference(value, d2[key], level=level-1))\n\t\t\t\t\tif not items[i][1]:\n\t\t\t\t\t\titems[i] = (key, value)\n\t\t\t\telse:\n\t\t\t\t\titems[i] = (key, value)\n\t\t\telse:\n\t\t\t\tif value == d2[key]:\n\t\t\t\t\titems[i] = (key, value)\n\t\t\t\telse:\n\t\t\t\t\titems[i] = (key, value)\n\t\telse:\n\t\t\tif not isinstance(value, dict):\n\t\t\t\tif key in d2 and value == d2[key]:\n\n", "def difference(d1, d2, level=-1):\n\treturn difference_update(copy(d1), d2, level)\n\n\n", "def difference(d1, d2, level=-1):\n\tif level == 0:\n\t\treturn d1\n\t\t\n\tif not isinstance(d1, dict) or not isinstance(d2, dict):\n\t\treturn d1\n\t\t\n\tdiff = {}\n\tfor k, v1 in d1.items():\n\t\tif k not in d2:\n\t\t\tdiff[k] = v1\n\t\telse:\n\t\t\tv2 = d2[k]\n\t\t\tdiff[k] = difference(v1, v2, level - 1)\n\treturn diff\n\t\n", "def difference(d1, d2, level=-1):\n\tif level == 0:\n\t\treturn d1\n\n\tif isinstance(d1, dict) and isinstance(d2, dict):\n\t\td1, d2 = d1.copy(), d2.copy()\n\t\tfor key in d1.keys():\n\t\t\tif key in d2:\n\t\t\t\td1[key] = difference(d1[key], d2.pop(key), level=level-1)\n\t\treturn d1\n\n\tif isinstance(d1, list) and isinstance(d2, list):\n\t\td1, d2 = d1.copy(), d2.copy()\n\t\tfor item in d1.copy():\n\t\t\tif item in d2:\n\t\t\t\td1.remove(item)\n\t\t\t\td2.remove(item)\n\t\treturn d1\n\n\treturn d1\n\n\n", "def difference(d1, d2, level=-1):\n\td = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\td[k] = d1[k]\n\t\telif level > 0 and isinstance(d1[k], dict) and isinstance(d2[k], dict):\n\t\t\td[k] = difference(d1[k], d2[k], level=level - 1)\n\treturn d\n\n\n", "def difference(d1, d2, level=-1):\n\td = {}\n\tif level == -1:\n\t\tfor key in d1:\n\t\t\tif key not in d2:\n\t\t\t\td[key] = d1[key]\n\t\treturn d\n\telse:\n\t\tfor key in d1:\n\t\t\tif key not in d2:\n\t\t\t\td[key] = d1[key]\n\t\t\telse:\n\t\t\t\tif level == 0:\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\td[key] = difference(d1[key], d2[key], level-1)\n\t\treturn d\n\t\t\n", "def difference(d1, d2, level=-1):\n\td = {}\n\tfor key1 in d1:\n\t\tif d1[key1] != d2.get(key1):\n\t\t\td[key1] = d1[key1]\n\t\telif type(d1[key1]) == dict:\n\t\t\tdifference = compareDicts(d1[key1], d2[key1])\n\t\t\tif len(difference) > 0:\n\t\t\t\td[key1] = difference\n\treturn d\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def difference(d1, d2, level=-1):\n\tdiff = {}\n\tif level == 0:\n\t\tfor k, v in d1.items():\n\t\t\tif k not in d2:\n\t\t\t\tdiff[k] = v\n\telse:\n\t\tfor k, v1 in d1.items():\n\t\t\tif k not in d2:\n\t\t\t\tdiff[k] = v1\n\t\t\telif isinstance(v1, dict):\n\t\t\t\tdiff[k] = difference(v1, d2[k], level - 1)\n\treturn diff\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\tif level == 0:\n\t\treturn d1\n\telse:\n\t\td = {}\n\t\tfor k, v in d1.items():\n\t\t\tif isinstance(v, dict):\n\t\t\t\tif k in d2 and isinstance(d2[k], dict):\n\t\t\t\t\tv = difference(v, d2[k], level - 1)\n\t\t\t\t\tif v:\n\t\t\t\t\t\td[k] = v\n\t\t\t\telse:\n\t\t\t\t\td[k] = v\n\t\t\telse:\n\t\t\t\tif k not in d2:\n\t\t\t\t\td[k] = v\n\t\treturn d\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\tret = {}\n\tif level == 0:\n\t\tif d1 != d2:\n\t\t\tret = d1\n\t\treturn ret\n\tfor k, v in d1.items():\n\t\tif k not in d2:\n\t\t\tret[k] = v\n\t\telif isinstance(v, dict):\n\t\t\tdiff = difference(d1[k], d2[k], level - 1)\n\t\t\tif diff:\n\t\t\t\tret[k] = diff\n\t\telif v != d2[k]:\n\t\t\tret[k] = v\n\treturn ret\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\titems = list(d1.items())\n\tfor i, (key, value) in enumerate(items):\n\t\tif level >= 0:\n\t\t\tif key not in d2:\n\t\t\t\tcontinue\n\t\t\tif isinstance(value, dict):\n\t\t\t\tif isinstance(d2[key], dict):\n\t\t\t\t\titems[i] = (key, difference(value, d2[key], level=level-1))\n\t\t\t\t\tif not items[i][1]:\n\t\t\t\t\t\titems[i] = (key, value)\n\t\t\t\telse:\n\t\t\t\t\titems[i] = (key, value)\n\t\t\telse:\n\t\t\t\tif value == d2[key]:\n\t\t\t\t\titems[i] = (key, value)\n\t\t\t\telse:\n\t\t\t\t\titems[i] = (key, value)\n\t\telse:\n\t\t\tif not isinstance(value, dict):\n\t\t\t\tif key in d2 and value == d2[key]:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\treturn difference_update(copy(d1), d2, level)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\tif level == 0:\n\t\treturn d1\n\t\t\n\tif not isinstance(d1, dict) or not isinstance(d2, dict):\n\t\treturn d1\n\t\t\n\tdiff = {}\n\tfor k, v1 in d1.items():\n\t\tif k not in d2:\n\t\t\tdiff[k] = v1\n\t\telse:\n\t\t\tv2 = d2[k]\n\t\t\tdiff[k] = difference(v1, v2, level - 1)\n\treturn diff\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\tif level == 0:\n\t\treturn d1\n\n\tif isinstance(d1, dict) and isinstance(d2, dict):\n\t\td1, d2 = d1.copy(), d2.copy()\n\t\tfor key in d1.keys():\n\t\t\tif key in d2:\n\t\t\t\td1[key] = difference(d1[key], d2.pop(key), level=level-1)\n\t\treturn d1\n\n\tif isinstance(d1, list) and isinstance(d2, list):\n\t\td1, d2 = d1.copy(), d2.copy()\n\t\tfor item in d1.copy():\n\t\t\tif item in d2:\n\t\t\t\td1.remove(item)\n\t\t\t\td2.remove(item)\n\t\treturn d1\n\n\treturn d1\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\td = {}\n\tfor k in d1:\n\t\tif k not in d2:\n\t\t\td[k] = d1[k]\n\t\telif level > 0 and isinstance(d1[k], dict) and isinstance(d2[k], dict):\n\t\t\td[k] = difference(d1[k], d2[k], level=level - 1)\n\treturn d\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\td = {}\n\tif level == -1:\n\t\tfor key in d1:\n\t\t\tif key not in d2:\n\t\t\t\td[key] = d1[key]\n\t\treturn d\n\telse:\n\t\tfor key in d1:\n\t\t\tif key not in d2:\n\t\t\t\td[key] = d1[key]\n\t\t\telse:\n\t\t\t\tif level == 0:\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\td[key] = difference(d1[key], d2[key], level-1)\n\t\treturn d\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def difference(d1, d2, level=-1):\n\td = {}\n\tfor key1 in d1:\n\t\tif d1[key1] != d2.get(key1):\n\t\t\td[key1] = d1[key1]\n\t\telif type(d1[key1]) == dict:\n\t\t\tdifference = compareDicts(d1[key1], d2[key1])\n\t\t\tif len(difference) > 0:\n\t\t\t\td[key1] = difference\n\treturn d\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "fill", "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.", "ques_id": "62b87af09a0c4fa8b80b34f1", "code": ["def fill(self, coord, weight=1):\n\tfor i, v in enumerate(coord):\n\t\tif v < self._min[i] or v > self._max[i]:\n\t\t\treturn\n\tself._fill(coord, weight)\n\n\tdef _fill(self, coord, weight):\n\t\traise NotImplementedError()\n\n\tdef get_bin_contents(self, bin_index):\n\t\t\"\"\"\n\t\tReturn the contents of the bin at *bin_index*.\n\t\t\"\"\"\n\t\treturn self._get_bin_contents(bin_index)\n\n\tdef _get_bin_contents(self, bin_index):\n\t\traise NotImplementedError()\n\n\tdef get_all_bin_contents(self):\n\t\t\"\"\"\n\t\tReturn a list of all bin contents.\n\t\t\"\"\"\n\t\treturn [self.get_bin_contents(bin_index) for bin_index in self.iter_bin_indices()]\n\n\tdef get_bin_error(self, bin_index):\n\t\t\"\"\"\n\t\tReturn the error of the bin at *bin_index*.\n\t\t\"\"\"\n\n", "def fill(self, coord, weight=1):\n\t# NB: it is important to use the local variable 'self' here,\n\t# since we want to access the instance variable, not the\n\t# variable of the same name defined in the for-loop below.\n\tfor i, x in enumerate(coord):\n\t\tif not (self.bounds[i][0] <= x < self.bounds[i][1]):\n\t\t\treturn\n\t# Use the local variable 'self' here, since we want to\n\t# access the variable of the same name defined in the\n\t# for-loop above.\n\tself.n += weight\n\tfor i, x in enumerate(coord):\n\t\tif self.binnings[i].__class__ is Binning:\n\t\t\tbin = self.binnings[i].bin(x)\n\t\t\tif bin is not None:\n\t\t\t\tself.bins[i][bin] += weight\n\t\telse:\n\t\t\tj = int((x - self.bounds[i][0]) / self.binnings[i])\n\t\t\tself.bins[i][j]\n", "def fill(self, coord, weight=1):\n\tif self.nn_dim:\n\t\tif len(coord) == self.ndim:\n\t\t\tcoord = coord + (0,)\n\t\telif len(coord) != self.ndim+1:\n\t\t\traise ValueError(\"coordinates must be of length %s\" % (self.ndim+1,))\n\telse:\n\t\tif len(coord) != self.ndim:\n\t\t\traise ValueError(\"coordinates must be of length %s\" % (self.ndim,))\n\t\n\tif self.ndim == 1:\n\t\tself._fill1d(coord, weight)\n\telif self.ndim == 2:\n\t\tself._fill2d(coord, weight)\n\telif self.ndim == 3:\n\t\tself._fill3d(coord, weight)\n\telse:\n\t\tself._fillnd(coord, weight)\n\n\n", "def fill(self, coord, weight=1):\n\t_hist_fill(self, coord, weight)\n\n", "def fill(self, coord, weight=1):\n\tself.Fill(coord, weight)\n\n", "def fill(self, coord, weight=1):\n\tif self.is_empty:\n\t\traise RuntimeError(\"Histogram is empty.\")\n\tif len(coord) != len(self.axes):\n\t\traise ValueError(\"coord has wrong dimension.\")\n\tfor i, ax in enumerate(self.axes):\n\t\tif coord[i] < ax.lower_edge or coord[i] >= ax.upper_edge:\n\t\t\treturn\n\tindex = self._coord_to_index(coord)\n\tself._data[index] += weight\n\n\tdef fill_n(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram for an array of coordinates *coords*.\n\tIf *weights* is not None, it must be a 1-d array of the same length as *coords*.\n\t\"\"\"\n\tif self.is_empty:\n\t\traise RuntimeError(\"Histogram is empty.\")\n\tif len(coords.shape) != 2:\n\t\traise ValueError(\"coords must be a 2-d array.\")\n\tif coords.shape[1] != len(self.axes):\n\t\traise ValueError(\"co\n", "def fill(self, coord, weight=1):\n\tpass\n\t\n", "def fill(self, coord, weight=1):\n\tif len(coord) != self.dimensions():\n\t\traise TypeError(\"Wrong number of arguments\")\n\tif self.__dimensions == 1:\n\t\tself.__fill_1d(coord[0], weight)\n\telif self.__dimensions == 2:\n\t\tself.__fill_2d(coord[0], coord[1], weight)\n\telse:\n\t\traise NotImplementedError(\"Filling of 3D histograms not implemented\")\n\n\tdef __fill_1d(self, x, weight=1):\n\t\tif not self.__cdf_valid:\n\t\t\tself.__generate_cdf()\n\t\tif x < self.__bins[0] or x > self.__bins[-1]:\n\t\t\treturn\n\t\ti = bisect.bisect_right(self.__bins, x)\n\t\tself.__data[i-1] += weight\n\n\tdef __fill_2d(self, x, y, weight=1):\n\t\tif not self.__cdf_valid:\n\t\t\tself.\n", "def fill(self, coord, weight=1):\n\tself.h.Fill(coord[0], coord[1], weight)\n\n", "def fill(self, coord, weight=1):\n\t...\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fill(self, coord, weight=1):\n\tfor i, v in enumerate(coord):\n\t\tif v < self._min[i] or v > self._max[i]:\n\t\t\treturn\n\tself._fill(coord, weight)\n\n\tdef _fill(self, coord, weight):\n\t\traise NotImplementedError()\n\n\tdef get_bin_contents(self, bin_index):\n\t\t\"\"\"\n\t\tReturn the contents of the bin at *bin_index*.\n\t\t\"\"\"\n\t\treturn self._get_bin_contents(bin_index)\n\n\tdef _get_bin_contents(self, bin_index):\n\t\traise NotImplementedError()\n\n\tdef get_all_bin_contents(self):\n\t\t\"\"\"\n\t\tReturn a list of all bin contents.\n\t\t\"\"\"\n\t\treturn [self.get_bin_contents(bin_index) for bin_index in self.iter_bin_indices()]\n\n\tdef get_bin_error(self, bin_index):\n\t\t\"\"\"\n\t\tReturn the error of the bin at *bin_index*.\n\t\t\"\"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t# NB: it is important to use the local variable 'self' here,\n\t# since we want to access the instance variable, not the\n\t# variable of the same name defined in the for-loop below.\n\tfor i, x in enumerate(coord):\n\t\tif not (self.bounds[i][0] <= x < self.bounds[i][1]):\n\t\t\treturn\n\t# Use the local variable 'self' here, since we want to\n\t# access the variable of the same name defined in the\n\t# for-loop above.\n\tself.n += weight\n\tfor i, x in enumerate(coord):\n\t\tif self.binnings[i].__class__ is Binning:\n\t\t\tbin = self.binnings[i].bin(x)\n\t\t\tif bin is not None:\n\t\t\t\tself.bins[i][bin] += weight\n\t\telse:\n\t\t\tj = int((x - self.bounds[i][0]) / self.binnings[i])\n\t\t\tself.bins[i][j]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tif self.nn_dim:\n\t\tif len(coord) == self.ndim:\n\t\t\tcoord = coord + (0,)\n\t\telif len(coord) != self.ndim+1:\n\t\t\traise ValueError(\"coordinates must be of length %s\" % (self.ndim+1,))\n\telse:\n\t\tif len(coord) != self.ndim:\n\t\t\traise ValueError(\"coordinates must be of length %s\" % (self.ndim,))\n\t\n\tif self.ndim == 1:\n\t\tself._fill1d(coord, weight)\n\telif self.ndim == 2:\n\t\tself._fill2d(coord, weight)\n\telif self.ndim == 3:\n\t\tself._fill3d(coord, weight)\n\telse:\n\t\tself._fillnd(coord, weight)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t_hist_fill(self, coord, weight)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tself.Fill(coord, weight)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tif self.is_empty:\n\t\traise RuntimeError(\"Histogram is empty.\")\n\tif len(coord) != len(self.axes):\n\t\traise ValueError(\"coord has wrong dimension.\")\n\tfor i, ax in enumerate(self.axes):\n\t\tif coord[i] < ax.lower_edge or coord[i] >= ax.upper_edge:\n\t\t\treturn\n\tindex = self._coord_to_index(coord)\n\tself._data[index] += weight\n\n\tdef fill_n(self, coords, weights=None):\n\t\"\"\"\n\tFill histogram for an array of coordinates *coords*.\n\tIf *weights* is not None, it must be a 1-d array of the same length as *coords*.\n\t\"\"\"\n\tif self.is_empty:\n\t\traise RuntimeError(\"Histogram is empty.\")\n\tif len(coords.shape) != 2:\n\t\traise ValueError(\"coords must be a 2-d array.\")\n\tif coords.shape[1] != len(self.axes):\n\t\traise ValueError(\"co\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tif len(coord) != self.dimensions():\n\t\traise TypeError(\"Wrong number of arguments\")\n\tif self.__dimensions == 1:\n\t\tself.__fill_1d(coord[0], weight)\n\telif self.__dimensions == 2:\n\t\tself.__fill_2d(coord[0], coord[1], weight)\n\telse:\n\t\traise NotImplementedError(\"Filling of 3D histograms not implemented\")\n\n\tdef __fill_1d(self, x, weight=1):\n\t\tif not self.__cdf_valid:\n\t\t\tself.__generate_cdf()\n\t\tif x < self.__bins[0] or x > self.__bins[-1]:\n\t\t\treturn\n\t\ti = bisect.bisect_right(self.__bins, x)\n\t\tself.__data[i-1] += weight\n\n\tdef __fill_2d(self, x, y, weight=1):\n\t\tif not self.__cdf_valid:\n\t\t\tself.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\tself.h.Fill(coord[0], coord[1], weight)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fill(self, coord, weight=1):\n\t...\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_validate_labels", "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]", "ques_id": "62b86aa3b4d922cb0e688d36", "code": ["def _validate_labels(labels):\n\tif labels is None:\n\t\treturn\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\n\n", "def _validate_labels(labels):\n\tfor k, v in labels.items():\n\t\tvalidate_key(k)\n\t\tvalidate_value(v)\n\n", "def _validate_labels(labels):\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n\n", "def _validate_labels(labels):\n\tif not isinstance(labels, dict) or not labels:\n\t\treturn False\n\tfor key, value in labels.items():\n\t\tif not _validate_key(key):\n\t\t\treturn False\n\t\tif not _validate_value(value):\n\t\t\treturn False\n\treturn True\n\n", "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise ValueError(\"labels must be a dict\")\n\tfor k, v in labels.items():\n\t\tvalidate_key(k)\n\t\tvalidate_value(v)\n\n\n", "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise MetricTypeError(\"Labels must be a dict\")\n\n\tfor key in labels:\n\t\tvalidate_key(key)\n\t\tvalidate_value(labels[key])\n\n", "def _validate_labels(labels):\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "def _validate_labels(labels):\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "def _validate_labels(labels):\n\treturn dict(map(lambda (k, v): (validate_key(k), validate_value(v)), labels.items()))\n\n", "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise ValueError('Labels must be a dictionary, not {}'.format(type(labels)))\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _validate_labels(labels):\n\tif labels is None:\n\t\treturn\n\tfor key, value in labels.items():\n\t\t_validate_key(key)\n\t\t_validate_value(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tfor k, v in labels.items():\n\t\tvalidate_key(k)\n\t\tvalidate_value(v)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tif not isinstance(labels, dict) or not labels:\n\t\treturn False\n\tfor key, value in labels.items():\n\t\tif not _validate_key(key):\n\t\t\treturn False\n\t\tif not _validate_value(value):\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise ValueError(\"labels must be a dict\")\n\tfor k, v in labels.items():\n\t\tvalidate_key(k)\n\t\tvalidate_value(v)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise MetricTypeError(\"Labels must be a dict\")\n\n\tfor key in labels:\n\t\tvalidate_key(key)\n\t\tvalidate_value(labels[key])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\treturn dict(map(lambda (k, v): (validate_key(k), validate_value(v)), labels.items()))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _validate_labels(labels):\n\tif not isinstance(labels, dict):\n\t\traise ValueError('Labels must be a dictionary, not {}'.format(type(labels)))\n\tfor key, value in labels.items():\n\t\tvalidate_key(key)\n\t\tvalidate_value(value)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "_get_resource_name_regex", "docstring": "Build or return the regular expressions that are used to validate\nthe name of the Krake resources.\n\nReturns:\n    (re.Pattern): the compiled regular expressions, to validate\n    the resource name.", "ques_id": "62b86a9eb4d922cb0e688d25", "code": ["def _get_resource_name_regex():\n\treturn re.compile(r\"^[a-zA-Z][-a-zA-Z0-9]*$\")\n\n\n", "def _get_resource_name_regex():\n\treturn re.compile(\"^[a-zA-Z_][a-zA-Z0-9_]*$\")\n\n\n", "def _get_resource_name_regex():\n\treturn _get_resource_regex(r\"([a-zA-Z0-9_\\-]+)\")\n\n\n", "def _get_resource_name_regex():\n\treturn re.compile(r'^[a-z0-9\\-_\\.]{1,255}$')\n\n\n", "def _get_resource_name_regex():\n\treturn {\n\t\t\"application\": r\"^[a-z][a-z0-9\\-]{0,127}$\",\n\t\t\"deployment\": r\"^[a-z][a-z0-9\\-]{0,127}$\"\n\t}\n\n\n", "def _get_resource_name_regex():\n\treturn re.compile(\"^[a-zA-Z0-9_]+$\")\n\n\n", "def _get_resource_name_regex():\n\treturn r'^[a-z]([-a-z0-9]*[a-z0-9])?$'\n\n\n", "def _get_resource_name_regex():\n\treturn re.compile(r\"^[a-zA-Z][\\w-]*$\")\n\n\n", "def _get_resource_name_regex():\n\treturn _RESOURCE_NAME_REGEX\n\n\n", "def _get_resource_name_regex():\n\treturn {\n\t\t\"namespace\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"job_name\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"task_name\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"task_status\": r\"^(pending|running|terminated)$\",\n\t\t\"task_type\": r\"^(start|stop|restart|rebuild|reconfigure|update)$\",\n\t\t\"task_result\": r\"^(success|failure|timeout)$\",\n\t\t\"pod_name\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"service_name\": r\"^[a-z0-9]([-a\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _get_resource_name_regex():\n\treturn re.compile(r\"^[a-zA-Z][-a-zA-Z0-9]*$\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn re.compile(\"^[a-zA-Z_][a-zA-Z0-9_]*$\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn _get_resource_regex(r\"([a-zA-Z0-9_\\-]+)\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn re.compile(r'^[a-z0-9\\-_\\.]{1,255}$')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn {\n\t\t\"application\": r\"^[a-z][a-z0-9\\-]{0,127}$\",\n\t\t\"deployment\": r\"^[a-z][a-z0-9\\-]{0,127}$\"\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn re.compile(\"^[a-zA-Z0-9_]+$\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn r'^[a-z]([-a-z0-9]*[a-z0-9])?$'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn re.compile(r\"^[a-zA-Z][\\w-]*$\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn _RESOURCE_NAME_REGEX\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_resource_name_regex():\n\treturn {\n\t\t\"namespace\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"job_name\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"task_name\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"task_status\": r\"^(pending|running|terminated)$\",\n\t\t\"task_type\": r\"^(start|stop|restart|rebuild|reconfigure|update)$\",\n\t\t\"task_result\": r\"^(success|failure|timeout)$\",\n\t\t\"pod_name\": r\"^[a-z0-9]([-a-z0-9]*[a-z0-9])?$\",\n\t\t\"service_name\": r\"^[a-z0-9]([-a\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_value", "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf8", "code": ["def validate_value(value):\n\tif not re.match(r'^[A-Z]{3}$', value):\n\t\traise ValueError('Invalid value')\n\n\treturn value\n\n\n", "def validate_value(value):\n\tif value is None:\n\t\treturn None\n\n\tif value == \"\":\n\t\treturn None\n\n\tif value in (\"None\", \"none\", \"NONE\"):\n\t\treturn None\n\n\ttry:\n\t\tvalue = int(value)\n\n\texcept:\n\t\tvalue = None\n\n\treturn value\n\n\n", "def validate_value(value):\n\tif value in ['null', 'NULL']:\n\t\treturn True\n\n\t# find the corresponding regular expression for this value\n\tfor regex in regexes:\n\t\tif re.match(regex, value):\n\t\t\treturn True\n\n\treturn False\n\n", "def validate_value(value):\n\tif value is None:\n\t\treturn False\n\n\tif not isinstance(value, str):\n\t\treturn False\n\n\treturn True\n\n\n", "def validate_value(value):\n\tif not re.match(patterns[value.type], value.value):\n\t\tprint 'Error: value \"%s\" is not valid for type %s' % (value.value, value.type)\n\t\treturn False\n\treturn True\n\n\n", "def validate_value(value):\n\treturn re.match(VALID_VALUES[type(value)], str(value)) is not None\n\n\n", "def validate_value(value):\n\tif value is None:\n\t\treturn False\n\treturn re.match(r'^[a-zA-Z0-9_-]{6,20}$', value) is not None\n\n\n", "def validate_value(value):\n\treturn VALIDATION_REGEXES[type(value)](value)\n\n\n", "def validate_value(value):\n\tif value:\n\t\tif not re.match(config['regexp'], value):\n\t\t\treturn False\n\treturn True\n\n", "def validate_value(value):\n\tif not value:\n\t\treturn False\n\tif not VALID_VALUE_REGEX.match(value):\n\t\traise ValueError(\"Value '%s' is not valid. Value must match regular expression '%s'\" % (value, VALID_VALUE_REGEX.pattern))\n\treturn True\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_value(value):\n\tif not re.match(r'^[A-Z]{3}$', value):\n\t\traise ValueError('Invalid value')\n\n\treturn value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif value is None:\n\t\treturn None\n\n\tif value == \"\":\n\t\treturn None\n\n\tif value in (\"None\", \"none\", \"NONE\"):\n\t\treturn None\n\n\ttry:\n\t\tvalue = int(value)\n\n\texcept:\n\t\tvalue = None\n\n\treturn value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif value in ['null', 'NULL']:\n\t\treturn True\n\n\t# find the corresponding regular expression for this value\n\tfor regex in regexes:\n\t\tif re.match(regex, value):\n\t\t\treturn True\n\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif value is None:\n\t\treturn False\n\n\tif not isinstance(value, str):\n\t\treturn False\n\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not re.match(patterns[value.type], value.value):\n\t\tprint 'Error: value \"%s\" is not valid for type %s' % (value.value, value.type)\n\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\treturn re.match(VALID_VALUES[type(value)], str(value)) is not None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif value is None:\n\t\treturn False\n\treturn re.match(r'^[a-zA-Z0-9_-]{6,20}$', value) is not None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\treturn VALIDATION_REGEXES[type(value)](value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif value:\n\t\tif not re.match(config['regexp'], value):\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_value(value):\n\tif not value:\n\t\treturn False\n\tif not VALID_VALUE_REGEX.match(value):\n\t\traise ValueError(\"Value '%s' is not valid. Value must match regular expression '%s'\" % (value, VALID_VALUE_REGEX.pattern))\n\treturn True\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/data/core.py", "project": "rak-n-rok/Krake", "name": "validate_key", "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.", "ques_id": "62b86a4fb4d922cb0e688cf7", "code": ["def validate_key(key):\n\tkey_regex = {\n\t\t\"END\": re.compile(\"^end$\"),\n\t\t\"BEGIN\": re.compile(\"^begin$\"),\n\t\t\"TEXT\": re.compile(\"^text$\"),\n\t\t\"SIZE\": re.compile(\"^size$\"),\n\t\t\"FONT\": re.compile(\"^font$\"),\n\t\t\"COLOR\": re.compile(\"^color$\"),\n\t\t\"BGCOLOR\": re.compile(\"^bgcolor$\"),\n\t\t\"STYLE\": re.compile(\"^style$\"),\n\t\t\"ALIGN\": re.compile(\"^align$\"),\n\t\t\"SPACING\": re.compile(\"^spacing$\"),\n\t\t\"BORDER\": re.compile(\"^border$\"),\n\t\t\"PADDING\": re.compile(\"^padding$\"),\n\t\t\"MARGIN\": re.compile(\"^margin$\"),\n\t}\n\tif key in key_regex.keys():\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def validate_key(key):\n\tif not key:\n\t\treturn False\n\treturn bool(re.match(KEY_REGEX, key))\n\n\n", "def validate_key(key):\n\tif key not in key_regexes.keys():\n\t\treturn False\n\telse:\n\t\tregex = re.compile(key_regexes[key])\n\t\tif regex.match(key):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "def validate_key(key):\n\tvalid = True\n\tfor r in key_regexps:\n\t\tif r[0].match(key) is not None:\n\t\t\treturn True\n\tprint \"Given key %s does not match any of the regular expressions.\" % key\n\tprint \"The following key formats are supported:\"\n\tfor r in key_regexps:\n\t\tprint r[1]\n\treturn False\n\n", "def validate_key(key):\n\tif key in _validators:\n\t\treturn _validators[key].match(key)\n\telse:\n\t\treturn True\n\n\n", "def validate_key(key):\n\tif key == 'request_id':\n\t\treturn validate_request_id(key)\n\telif key == 'name':\n\t\treturn validate_name(key)\n\telif key == 'email':\n\t\treturn validate_email(key)\n\telif key == 'latitude':\n\t\treturn validate_latitude(key)\n\telif key == 'longitude':\n\t\treturn validate_longitude(key)\n\telif key == 'start_time':\n\t\treturn validate_start_time(key)\n\telif key == 'end_time':\n\t\treturn validate_end_time(key)\n\telif key == 'request_type':\n\t\treturn validate_request_type(key)\n\telif key == 'description':\n\t\treturn validate_description(key)\n\telif key == 'max_distance':\n\t\treturn validate_max_distance(key)\n\telif key == 'max_cost':\n\t\treturn validate_max_cost(key)\n\telif key == 'min_rating':\n\t\treturn validate_min_rating(key\n", "def validate_key(key):\n\tif not _re_key.match(key):\n\t\traise ValueError(\"Invalid key: %s\" % key)\n\treturn key\n\n\n", "def validate_key(key):\n\tkey_regex = {\n\t\t'name': r'[a-zA-Z0-9_\\-\\.]+',\n\t\t'ip': r'[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+'\n\t}\n\tif key not in key_regex:\n\t\traise ValueError('Invalid key: {}'.format(key))\n\treturn re.compile(key_regex[key])\n\n\n", "def validate_key(key):\n\tif key in REGEXPS:\n\t\treturn REGEXPS[key].match(key) is not None\n\telse:\n\t\treturn True\n\n\n", "def validate_key(key):\n\tif key == 'name':\n\t\treturn True\n\telif key == 'version':\n\t\treturn True\n\telif key == 'compat':\n\t\treturn True\n\telif key == 'distro':\n\t\treturn True\n\telif key == 'url':\n\t\treturn True\n\telif key == 'summary':\n\t\treturn True\n\telif key == 'description':\n\t\treturn True\n\telif key == 'depends':\n\t\treturn True\n\telif key == 'conflicts':\n\t\treturn True\n\telif key == 'suggests':\n\t\treturn True\n\telif key == 'provides':\n\t\treturn True\n\telif key == 'replaces':\n\t\treturn True\n\telif key == 'files':\n\t\treturn True\n\telif key == 'license':\n\t\treturn True\n\telif key == 'maintainer':\n\t\treturn True\n\telif key == 'builddepends':\n\t\treturn True\n\telif key == 'flavor':\n\t\treturn True\n\telif key == 'arch':\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_key(key):\n\tkey_regex = {\n\t\t\"END\": re.compile(\"^end$\"),\n\t\t\"BEGIN\": re.compile(\"^begin$\"),\n\t\t\"TEXT\": re.compile(\"^text$\"),\n\t\t\"SIZE\": re.compile(\"^size$\"),\n\t\t\"FONT\": re.compile(\"^font$\"),\n\t\t\"COLOR\": re.compile(\"^color$\"),\n\t\t\"BGCOLOR\": re.compile(\"^bgcolor$\"),\n\t\t\"STYLE\": re.compile(\"^style$\"),\n\t\t\"ALIGN\": re.compile(\"^align$\"),\n\t\t\"SPACING\": re.compile(\"^spacing$\"),\n\t\t\"BORDER\": re.compile(\"^border$\"),\n\t\t\"PADDING\": re.compile(\"^padding$\"),\n\t\t\"MARGIN\": re.compile(\"^margin$\"),\n\t}\n\tif key in key_regex.keys():\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not key:\n\t\treturn False\n\treturn bool(re.match(KEY_REGEX, key))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif key not in key_regexes.keys():\n\t\treturn False\n\telse:\n\t\tregex = re.compile(key_regexes[key])\n\t\tif regex.match(key):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tvalid = True\n\tfor r in key_regexps:\n\t\tif r[0].match(key) is not None:\n\t\t\treturn True\n\tprint \"Given key %s does not match any of the regular expressions.\" % key\n\tprint \"The following key formats are supported:\"\n\tfor r in key_regexps:\n\t\tprint r[1]\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif key in _validators:\n\t\treturn _validators[key].match(key)\n\telse:\n\t\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif key == 'request_id':\n\t\treturn validate_request_id(key)\n\telif key == 'name':\n\t\treturn validate_name(key)\n\telif key == 'email':\n\t\treturn validate_email(key)\n\telif key == 'latitude':\n\t\treturn validate_latitude(key)\n\telif key == 'longitude':\n\t\treturn validate_longitude(key)\n\telif key == 'start_time':\n\t\treturn validate_start_time(key)\n\telif key == 'end_time':\n\t\treturn validate_end_time(key)\n\telif key == 'request_type':\n\t\treturn validate_request_type(key)\n\telif key == 'description':\n\t\treturn validate_description(key)\n\telif key == 'max_distance':\n\t\treturn validate_max_distance(key)\n\telif key == 'max_cost':\n\t\treturn validate_max_cost(key)\n\telif key == 'min_rating':\n\t\treturn validate_min_rating(key\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif not _re_key.match(key):\n\t\traise ValueError(\"Invalid key: %s\" % key)\n\treturn key\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tkey_regex = {\n\t\t'name': r'[a-zA-Z0-9_\\-\\.]+',\n\t\t'ip': r'[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+'\n\t}\n\tif key not in key_regex:\n\t\traise ValueError('Invalid key: {}'.format(key))\n\treturn re.compile(key_regex[key])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif key in REGEXPS:\n\t\treturn REGEXPS[key].match(key) is not None\n\telse:\n\t\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_key(key):\n\tif key == 'name':\n\t\treturn True\n\telif key == 'version':\n\t\treturn True\n\telif key == 'compat':\n\t\treturn True\n\telif key == 'distro':\n\t\treturn True\n\telif key == 'url':\n\t\treturn True\n\telif key == 'summary':\n\t\treturn True\n\telif key == 'description':\n\t\treturn True\n\telif key == 'depends':\n\t\treturn True\n\telif key == 'conflicts':\n\t\treturn True\n\telif key == 'suggests':\n\t\treturn True\n\telif key == 'provides':\n\t\treturn True\n\telif key == 'replaces':\n\t\treturn True\n\telif key == 'files':\n\t\treturn True\n\telif key == 'license':\n\t\treturn True\n\telif key == 'maintainer':\n\t\treturn True\n\telif key == 'builddepends':\n\t\treturn True\n\telif key == 'flavor':\n\t\treturn True\n\telif key == 'arch':\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema_dict", "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.", "ques_id": "62b86a01b4d922cb0e688ccc", "code": ["def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) is list:\n\t\t\tif type(value[0]) is dict:\n\t\t\t\tnew_dict[key] = []\n\t\t\t\tfor dict_item in value:\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(dict_item))\n\t\t\telse:\n\t\t\t\tnew_dict[key] = value\n\t\telse:\n\t\t\tnew_dict[key] = value\n\treturn new_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tif first_level:\n\t\tdefault_observer_schema_dict = {\n\t\t\t\"name\" : \"\",\n\t\t\t\"description\" : \"\",\n\t\t\t\"version\" : \"\",\n\t\t\t\"author\" : \"\",\n\t\t\t\"license\" : \"\",\n\t\t\t\"dependencies\" : dict(),\n\t\t\t\"types\" : dict(),\n\t\t\t\"dimensions\" : dict(),\n\t\t\t\"mappings\" : dict()\n\t\t}\n\t\tfor key in manifest_dict:\n\t\t\tif type(manifest_dict[key]) is dict:\n\t\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key])\n\t\t\telif type(manifest_dict[key]) is list:\n\t\t\t\tdefault_observer_schema_dict[key] = []\n\t\t\telse:\n\t\t\t\tdefault_observer_schema_dict[key] = \"\"\n\telse:\n\t\tdefault_observer_schema\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tschema_dict = {}\n\tfor key in manifest_dict.keys():\n\t\tif isinstance(manifest_dict[key], dict):\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key])\n\t\telif isinstance(manifest_dict[key], list):\n\t\t\tschema_dict[key] = generate_default_observer_schema_list(manifest_dict[key], first_level=False)\n\t\telse:\n\t\t\tschema_dict[key] = False\n\treturn schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.iteritems():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_list(value, first_level)\n\t\telse:\n\t\t\tobserver_schema_dict[key] = value\n\treturn observer_schema_dict\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t# print(\"manifest_dict\", manifest_dict)\n\t# print(\"first_level\", first_level)\n\tif first_level:\n\t\tschema_dict = {}\n\telse:\n\t\tschema_dict = {\n\t\t\t\"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\t\t\t\"$id\": \"http://example.com/example.json\",\n\t\t\t\"type\": \"object\",\n\t\t\t\"title\": \"The Root Schema\",\n\t\t\t\"description\": \"The root schema comprises the entire JSON document.\",\n\t\t\t\"default\": {},\n\t\t\t\"examples\": [\n\t\t\t\t{}\n\t\t\t],\n\t\t\t\"required\": [],\n\t\t\t\"additionalProperties\": True,\n\t\t\t\"properties\": {}\n\t\t}\n\t\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tschema_dict[\"properties\"][key] = generate_default_observer_schema_dict(\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tschema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tif key == \"required\":\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tschema_dict[key] = [generate_default_observer_schema_dict(value[0])]\n\t\telif key == \"type\":\n\t\t\tif value == \"object\":\n\t\t\t\tschema_dict[key] = {}\n\t\t\telif value == \"array\":\n\t\t\t\tschema_dict[key] = [{}]\n\t\t\telif value == \"string\":\n\t\t\t\tif first_level:\n\t\t\t\t\tschema_dict[key] = \"\"\n\t\t\t\telse:\n\t\t\t\t\tschema_dict[key] = \".\"\n\t\t\telif value == \"integer\":\n\t\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = []\n\t\telif isinstance(value, str):\n\t\t\tnew_dict[key] = ''\n\t\telif isinstance(value, int):\n\t\t\tnew_dict[key] = 0\n\t\telse:\n\t\t\tnew_dict[key] = value\n\n\tif first_level:\n\t\tnew_dict['observer_schema_version'] = __version__\n\t\tnew_dict['valid'] = True\n\n\treturn new_dict\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = []\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tobserver_schema_dict[key] = None\n\treturn observer_schema_dict\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tif manifest_dict is None:\n\t\treturn None\n\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list) and not first_level:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value[0])\n\t\telse:\n\t\t\tnew_dict[key] = None\n\n\treturn new_dict\n\n\n", "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = [generate_default_observer_schema_dict(value[0])]\n\t\telse:\n\t\t\tnew_dict[key] = None\n\n\treturn new_dict\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif type(value) is dict:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif type(value) is list:\n\t\t\tif type(value[0]) is dict:\n\t\t\t\tnew_dict[key] = []\n\t\t\t\tfor dict_item in value:\n\t\t\t\t\tnew_dict[key].append(generate_default_observer_schema_dict(dict_item))\n\t\t\telse:\n\t\t\t\tnew_dict[key] = value\n\t\telse:\n\t\t\tnew_dict[key] = value\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tif first_level:\n\t\tdefault_observer_schema_dict = {\n\t\t\t\"name\" : \"\",\n\t\t\t\"description\" : \"\",\n\t\t\t\"version\" : \"\",\n\t\t\t\"author\" : \"\",\n\t\t\t\"license\" : \"\",\n\t\t\t\"dependencies\" : dict(),\n\t\t\t\"types\" : dict(),\n\t\t\t\"dimensions\" : dict(),\n\t\t\t\"mappings\" : dict()\n\t\t}\n\t\tfor key in manifest_dict:\n\t\t\tif type(manifest_dict[key]) is dict:\n\t\t\t\tdefault_observer_schema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key])\n\t\t\telif type(manifest_dict[key]) is list:\n\t\t\t\tdefault_observer_schema_dict[key] = []\n\t\t\telse:\n\t\t\t\tdefault_observer_schema_dict[key] = \"\"\n\telse:\n\t\tdefault_observer_schema\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tschema_dict = {}\n\tfor key in manifest_dict.keys():\n\t\tif isinstance(manifest_dict[key], dict):\n\t\t\tschema_dict[key] = generate_default_observer_schema_dict(manifest_dict[key])\n\t\telif isinstance(manifest_dict[key], list):\n\t\t\tschema_dict[key] = generate_default_observer_schema_list(manifest_dict[key], first_level=False)\n\t\telse:\n\t\t\tschema_dict[key] = False\n\treturn schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.iteritems():\n\t\tif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_list(value, first_level)\n\t\telse:\n\t\t\tobserver_schema_dict[key] = value\n\treturn observer_schema_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\t# print(\"manifest_dict\", manifest_dict)\n\t# print(\"first_level\", first_level)\n\tif first_level:\n\t\tschema_dict = {}\n\telse:\n\t\tschema_dict = {\n\t\t\t\"$schema\": \"http://json-schema.org/draft-07/schema#\",\n\t\t\t\"$id\": \"http://example.com/example.json\",\n\t\t\t\"type\": \"object\",\n\t\t\t\"title\": \"The Root Schema\",\n\t\t\t\"description\": \"The root schema comprises the entire JSON document.\",\n\t\t\t\"default\": {},\n\t\t\t\"examples\": [\n\t\t\t\t{}\n\t\t\t],\n\t\t\t\"required\": [],\n\t\t\t\"additionalProperties\": True,\n\t\t\t\"properties\": {}\n\t\t}\n\t\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tschema_dict[\"properties\"][key] = generate_default_observer_schema_dict(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tschema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tif key == \"required\":\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tschema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tschema_dict[key] = [generate_default_observer_schema_dict(value[0])]\n\t\telif key == \"type\":\n\t\t\tif value == \"object\":\n\t\t\t\tschema_dict[key] = {}\n\t\t\telif value == \"array\":\n\t\t\t\tschema_dict[key] = [{}]\n\t\t\telif value == \"string\":\n\t\t\t\tif first_level:\n\t\t\t\t\tschema_dict[key] = \"\"\n\t\t\t\telse:\n\t\t\t\t\tschema_dict[key] = \".\"\n\t\t\telif value == \"integer\":\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = []\n\t\telif isinstance(value, str):\n\t\t\tnew_dict[key] = ''\n\t\telif isinstance(value, int):\n\t\t\tnew_dict[key] = 0\n\t\telse:\n\t\t\tnew_dict[key] = value\n\n\tif first_level:\n\t\tnew_dict['observer_schema_version'] = __version__\n\t\tnew_dict['valid'] = True\n\n\treturn new_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tobserver_schema_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, list):\n\t\t\tobserver_schema_dict[key] = []\n\t\telif isinstance(value, dict):\n\t\t\tobserver_schema_dict[key] = generate_default_observer_schema_dict(value)\n\t\telse:\n\t\t\tobserver_schema_dict[key] = None\n\treturn observer_schema_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tif manifest_dict is None:\n\t\treturn None\n\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list) and not first_level:\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value[0])\n\t\telse:\n\t\t\tnew_dict[key] = None\n\n\treturn new_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n\tnew_dict = {}\n\tfor key, value in manifest_dict.items():\n\t\tif isinstance(value, dict):\n\t\t\tnew_dict[key] = generate_default_observer_schema_dict(value)\n\t\telif isinstance(value, list):\n\t\t\tnew_dict[key] = [generate_default_observer_schema_dict(value[0])]\n\t\telse:\n\t\t\tnew_dict[key] = None\n\n\treturn new_dict\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_list_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869ebb4d922cb0e688cc6", "code": ["def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, field in enumerate(observer_schema):\n\t\tif isinstance(field, dict):\n\t\t\tsub_dict = last_applied_manifest[i]\n\t\t\tsub_schema = field[\"fields\"]\n\t\t\tsub_resp = response[i]\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tsub_dict, sub_schema, sub_resp\n\t\t\t)\n\t\telse:\n\t\t\tif last_applied_manifest[i] is None:\n\t\t\t\tlast_applied_manifest[i] = response[i]\n\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key in observer_schema:\n\t\tif key not in last_applied_manifest:\n\t\t\tif isinstance(key, dict):\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[key] = []\n\t\tif isinstance(key, dict):\n\t\t\tfor field in key:\n\t\t\t\tif field not in last_applied_manifest[key]:\n\t\t\t\t\tif isinstance(key[field], dict):\n\t\t\t\t\t\tlast_applied_manifest[key][field] = {}\n\t\t\t\t\telse:\n\t\t\t\t\t\tlast_applied_manifest[key][field] = []\n\t\t\t\tif isinstance(key[field], dict):\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[key][field],\n\t\t\t\t\t\tkey[field],\n\t\t\t\t\t\tresponse[\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in response.items():\n\t\tif key not in observer_schema:\n\t\t\traise Exception(\n\t\t\t\t\"The key [{0}] is not in the observer_schema\".format(key)\n\t\t\t)\n\n\t\tif key not in last_applied_manifest:\n\t\t\tlast_applied_manifest[key] = copy.deepcopy(value)\n\t\telse:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], observer_schema[key], value\n\t\t\t\t)\n\t\t\telif isinstance(value, list):\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], observer_schema[key], value\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[key] = copy.deepcopy(value\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif field.type == \"list\":\n\t\t\tif not field.name in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field.name] = []\n\t\t\tfor index, element in enumerate(response[field.name]):\n\t\t\t\tif len(last_applied_manifest[field.name]) <= index:\n\t\t\t\t\tlast_applied_manifest[field.name].append(\n\t\t\t\t\t\t{}\n\t\t\t\t\t)\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field.name][index],\n\t\t\t\t\tfield.schema,\n\t\t\t\t\telement\n\t\t\t\t)\n\t\telif field.type == \"dict\":\n\t\t\tif not field.name in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field.name] = {}\n\t\t\tfor key, element\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor idx, _ in enumerate(response):\n\t\tif len(last_applied_manifest) < idx + 1:\n\t\t\tlast_applied_manifest.append({})\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[idx],\n\t\t\tobserver_schema[idx],\n\t\t\tresponse[idx],\n\t\t)\n\n\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif response is None:\n\t\treturn\n\n\tfor i, field in enumerate(observer_schema):\n\t\tif field is None:\n\t\t\tcontinue\n\t\tif field == \"spec\":\n\t\t\tcontinue  # spec is handled in a separate way\n\t\tif field == \"status\":\n\t\t\tif len(response) <= i:\n\t\t\t\tcontinue\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], response[i]\n\t\t\t)\n\t\telif field == \"items\":\n\t\t\tif len(response) <= i:\n\t\t\t\tcontinue\n\t\t\tif last_applied_manifest[i] is None:\n\t\t\t\tlast_applied_manifest[i] = []\n\t\t\tfor j, item in enumerate(response[i]):\n\t\t\t\tif len(last_applied_manifest[i]) <= j:\n\t\t\t\t\tlast_applied_manifest[i].\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in observer_schema.items():\n\t\tif key in response:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tif isinstance(response[key], dict):\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[key],\n\t\t\t\t\t\tvalue,\n\t\t\t\t\t\tresponse[key]\n\t\t\t\t\t)\n\t\t\t\telif isinstance(response[key], list):\n\t\t\t\t\tlast_applied_manifest[key] = []\n\t\t\t\t\tfor i in range(len(response[key])):\n\t\t\t\t\t\tlast_applied_manifest[key].append({})\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[key][i],\n\t\t\t\t\t\t\tvalue,\n\t\t\t\t\t\t\tresponse[key][i]\n\t\t\t\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif not isinstance(last_applied_manifest, list):\n\t\traise Exception(\"last_applied_manifest is not a list\")\n\tif not isinstance(observer_schema, list):\n\t\traise Exception(\"observer_schema is not a list\")\n\tif not isinstance(response, list):\n\t\traise Exception(\"response is not a list\")\n\tif len(last_applied_manifest) != len(observer_schema):\n\t\traise Exception(\"last_applied_manifest and observer_schema don't have the same length\")\n\tif len(last_applied_manifest) != len(response):\n\t\traise Exception(\"last_applied_manifest and response don't have the same length\")\n\tfor i in range(0, len(last_applied_manifest)):\n\t\tif not isinstance(last_applied_manifest[i], dict):\n\t\t\traise Exception(\"last_applied_manifest[{}] is not a dict\".format(i))\n\t\tif not isinstance(observer_schema[i\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif field[\"field\"] in response:\n\t\t\tif field[\"type\"] == \"list\":\n\t\t\t\tfor element in response[field[\"field\"]]:\n\t\t\t\t\tif element not in last_applied_manifest[field[\"field\"]]:\n\t\t\t\t\t\tlast_applied_manifest[field[\"field\"]].append(element)\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field[\"field\"]][-1],\n\t\t\t\t\t\tfield[\"observer_schema\"],\n\t\t\t\t\t\telement,\n\t\t\t\t\t)\n\t\t\telif field[\"type\"] == \"dict\":\n\t\t\t\tif field[\"field\"] not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field[\"field\"]] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied\n", "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor observed_field in observer_schema:\n\t\tif isinstance(observed_field, dict):\n\t\t\tfield_name = list(observed_field.keys())[0]\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserved_field[field_name],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\telif isinstance(observed_field, list):\n\t\t\tfield_name = observed_field[0]\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor i, field in enumerate(observer_schema):\n\t\tif isinstance(field, dict):\n\t\t\tsub_dict = last_applied_manifest[i]\n\t\t\tsub_schema = field[\"fields\"]\n\t\t\tsub_resp = response[i]\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tsub_dict, sub_schema, sub_resp\n\t\t\t)\n\t\telse:\n\t\t\tif last_applied_manifest[i] is None:\n\t\t\t\tlast_applied_manifest[i] = response[i]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key in observer_schema:\n\t\tif key not in last_applied_manifest:\n\t\t\tif isinstance(key, dict):\n\t\t\t\tlast_applied_manifest[key] = {}\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[key] = []\n\t\tif isinstance(key, dict):\n\t\t\tfor field in key:\n\t\t\t\tif field not in last_applied_manifest[key]:\n\t\t\t\t\tif isinstance(key[field], dict):\n\t\t\t\t\t\tlast_applied_manifest[key][field] = {}\n\t\t\t\t\telse:\n\t\t\t\t\t\tlast_applied_manifest[key][field] = []\n\t\t\t\tif isinstance(key[field], dict):\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[key][field],\n\t\t\t\t\t\tkey[field],\n\t\t\t\t\t\tresponse[\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in response.items():\n\t\tif key not in observer_schema:\n\t\t\traise Exception(\n\t\t\t\t\"The key [{0}] is not in the observer_schema\".format(key)\n\t\t\t)\n\n\t\tif key not in last_applied_manifest:\n\t\t\tlast_applied_manifest[key] = copy.deepcopy(value)\n\t\telse:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], observer_schema[key], value\n\t\t\t\t)\n\t\t\telif isinstance(value, list):\n\t\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\t\tlast_applied_manifest[key], observer_schema[key], value\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tlast_applied_manifest[key] = copy.deepcopy(value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif field.type == \"list\":\n\t\t\tif not field.name in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field.name] = []\n\t\t\tfor index, element in enumerate(response[field.name]):\n\t\t\t\tif len(last_applied_manifest[field.name]) <= index:\n\t\t\t\t\tlast_applied_manifest[field.name].append(\n\t\t\t\t\t\t{}\n\t\t\t\t\t)\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied_manifest[field.name][index],\n\t\t\t\t\tfield.schema,\n\t\t\t\t\telement\n\t\t\t\t)\n\t\telif field.type == \"dict\":\n\t\t\tif not field.name in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field.name] = {}\n\t\t\tfor key, element\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor idx, _ in enumerate(response):\n\t\tif len(last_applied_manifest) < idx + 1:\n\t\t\tlast_applied_manifest.append({})\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[idx],\n\t\t\tobserver_schema[idx],\n\t\t\tresponse[idx],\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif response is None:\n\t\treturn\n\n\tfor i, field in enumerate(observer_schema):\n\t\tif field is None:\n\t\t\tcontinue\n\t\tif field == \"spec\":\n\t\t\tcontinue  # spec is handled in a separate way\n\t\tif field == \"status\":\n\t\t\tif len(response) <= i:\n\t\t\t\tcontinue\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[i], observer_schema[i], response[i]\n\t\t\t)\n\t\telif field == \"items\":\n\t\t\tif len(response) <= i:\n\t\t\t\tcontinue\n\t\t\tif last_applied_manifest[i] is None:\n\t\t\t\tlast_applied_manifest[i] = []\n\t\t\tfor j, item in enumerate(response[i]):\n\t\t\t\tif len(last_applied_manifest[i]) <= j:\n\t\t\t\t\tlast_applied_manifest[i].\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key, value in observer_schema.items():\n\t\tif key in response:\n\t\t\tif isinstance(value, dict):\n\t\t\t\tif isinstance(response[key], dict):\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[key],\n\t\t\t\t\t\tvalue,\n\t\t\t\t\t\tresponse[key]\n\t\t\t\t\t)\n\t\t\t\telif isinstance(response[key], list):\n\t\t\t\t\tlast_applied_manifest[key] = []\n\t\t\t\t\tfor i in range(len(response[key])):\n\t\t\t\t\t\tlast_applied_manifest[key].append({})\n\t\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\t\tlast_applied_manifest[key][i],\n\t\t\t\t\t\t\tvalue,\n\t\t\t\t\t\t\tresponse[key][i]\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif not isinstance(last_applied_manifest, list):\n\t\traise Exception(\"last_applied_manifest is not a list\")\n\tif not isinstance(observer_schema, list):\n\t\traise Exception(\"observer_schema is not a list\")\n\tif not isinstance(response, list):\n\t\traise Exception(\"response is not a list\")\n\tif len(last_applied_manifest) != len(observer_schema):\n\t\traise Exception(\"last_applied_manifest and observer_schema don't have the same length\")\n\tif len(last_applied_manifest) != len(response):\n\t\traise Exception(\"last_applied_manifest and response don't have the same length\")\n\tfor i in range(0, len(last_applied_manifest)):\n\t\tif not isinstance(last_applied_manifest[i], dict):\n\t\t\traise Exception(\"last_applied_manifest[{}] is not a dict\".format(i))\n\t\tif not isinstance(observer_schema[i\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema:\n\t\tif field[\"field\"] in response:\n\t\t\tif field[\"type\"] == \"list\":\n\t\t\t\tfor element in response[field[\"field\"]]:\n\t\t\t\t\tif element not in last_applied_manifest[field[\"field\"]]:\n\t\t\t\t\t\tlast_applied_manifest[field[\"field\"]].append(element)\n\t\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\t\tlast_applied_manifest[field[\"field\"]][-1],\n\t\t\t\t\t\tfield[\"observer_schema\"],\n\t\t\t\t\t\telement,\n\t\t\t\t\t)\n\t\t\telif field[\"type\"] == \"dict\":\n\t\t\t\tif field[\"field\"] not in last_applied_manifest:\n\t\t\t\t\tlast_applied_manifest[field[\"field\"]] = {}\n\t\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\t\tlast_applied\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor observed_field in observer_schema:\n\t\tif isinstance(observed_field, dict):\n\t\t\tfield_name = list(observed_field.keys())[0]\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\tobserved_field[field_name],\n\t\t\t\tresponse[field_name],\n\t\t\t)\n\t\telif isinstance(observed_field, list):\n\t\t\tfield_name = observed_field[0]\n\t\t\tif field_name not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[field_name] = []\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field_name],\n\t\t\t\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "update_last_applied_manifest_dict_from_resp", "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present", "ques_id": "62b869eab4d922cb0e688cc5", "code": ["def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema[\"observed_fields\"]:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\tif isinstance(last_applied_manifest[field], dict):\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\t\tlast_applied_manifest[field] = []\n\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema, response[field]\n\t\t\t)\n\t\telif isinstance(last_applied_manifest[field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema, response[field]\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\t# If there is no observed fields, we just return\n\tif not observed_fields:\n\t\treturn\n\tfor observed_field in observed_fields:\n\t\t# We check that the current response contains the observed field\n\t\tif observed_field not in response:\n\t\t\traise KeyError(\n\t\t\t\t\"Unexpected response from Kubernetes: \"\n\t\t\t\t\"observed field '{}' not present in the response. \"\n\t\t\t\t\"Response: {}\".format(observed_field, response)\n\t\t\t)\n\t\t# We check if the observed field is already present in the\n\t\t# last_applied_manifest\n\t\tif observed_field not in last_applied_manifest:\n\t\t\t# If not, we initialize it\n\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema[\"observed\"]:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\tif isinstance(last_applied_manifest[field], dict):\n\t\t\t# Recurse if the field is dict\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema[\"observed\"][field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\t# Recurse if the field is list\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema[\"observed\"][field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor name, field in observer_schema.items():\n\t\tif isinstance(field, dict):\n\t\t\tif field.get(\"observed\"):\n\t\t\t\tif name not in last_applied_manifest[field[\"kind\"]]:\n\t\t\t\t\tlast_applied_manifest[field[\"kind\"]][name] = response[name]\n\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key in observer_schema:\n\t\tif key not in last_applied_manifest:\n\t\t\tlast_applied_manifest[key] = response[key]\n\t\t\tcontinue\n\t\tif isinstance(observer_schema[key], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[key], observer_schema[key], response[key]\n\t\t\t)\n\t\telif isinstance(observer_schema[key], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[key], observer_schema[key], response[key]\n\t\t\t)\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"The field %s is neither a dict nor a list, in the observed schema\"\n\t\t\t\t% key\n\t\t\t)\n\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema['fields']:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\tfor field in observer_schema['dicts']:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[field], observer_schema['dicts'][field], response[field]\n\t\t)\n\tfor field in observer_schema['lists']:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = []\n\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\tlast_applied_manifest[field], observer_schema['lists'][field], response[field]\n\t\t)\n\tfor field in observer_schema['dict_list']:\n\t\tif\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor path in observer_schema[\"observed\"]:\n\t\tcurrent = last_applied_manifest\n\t\tcurrent_response = response\n\t\tfor key in path:\n\t\t\tif key not in current:\n\t\t\t\tcurrent[key] = {}\n\t\t\tcurrent = current[key]\n\t\t\tcurrent_response = current_response[key]\n\t\tcurrent = current_response\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor schema_entry in observer_schema.items():\n\t\tschema_key = schema_entry[0]\n\t\tschema_value = schema_entry[1]\n\t\tif schema_value is None:\n\t\t\tlast_applied_manifest[schema_key] = response[schema_key]\n\t\telse:\n\t\t\tif schema_key not in last_applied_manifest.keys():\n\t\t\t\tlast_applied_manifest[schema_key] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[schema_key],\n\t\t\t\tschema_value,\n\t\t\t\tresponse[schema_key],\n\t\t\t)\n\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor f in observer_schema.get(\"fields\", []):\n\t\tif f[\"field_name\"] in response:\n\t\t\tif f[\"field_name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[f[\"field_name\"]] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[f[\"field_name\"]], f, response[f[\"field_name\"]]\n\t\t\t)\n\t\telse:\n\t\t\traise KeyError(\"Field %s not in %s\" % (f[\"field_name\"], response))\n\n\n", "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif \"type\" in observer_schema and observer_schema[\n        \"type\"\n    ] == \"object\":  # observer_schema is a complex object\n\t\tfor key in observer_schema[\"properties\"]:\n\t\t\tif key not in response:\n\t\t\t\traise KeyError(\n\t\t\t\t    \"Observed field %s is not present in the response\" % key\n\t\t\t\t)\n\t\t\t# recursively explore the object\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t    last_applied_manifest[key],\n\t\t\t    observer_schema[\"properties\"][key],\n\t\t\t    response[key],\n\t\t\t)\n\telse:  # observer_schema is a simple type\n\t\tif observer_schema[\"type\"] == \"array\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t    last_applied_manifest, observer_schema, response\n\t\t\t)\n\t\t\treturn\n\t\tif observer_\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema[\"observed_fields\"]:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\t\tif isinstance(last_applied_manifest[field], dict):\n\t\t\t\tlast_applied_manifest[field] = {}\n\t\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\t\tlast_applied_manifest[field] = []\n\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema, response[field]\n\t\t\t)\n\t\telif isinstance(last_applied_manifest[field], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema, response[field]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tobserved_fields = observer_schema[\"observed_fields\"]\n\t# If there is no observed fields, we just return\n\tif not observed_fields:\n\t\treturn\n\tfor observed_field in observed_fields:\n\t\t# We check that the current response contains the observed field\n\t\tif observed_field not in response:\n\t\t\traise KeyError(\n\t\t\t\t\"Unexpected response from Kubernetes: \"\n\t\t\t\t\"observed field '{}' not present in the response. \"\n\t\t\t\t\"Response: {}\".format(observed_field, response)\n\t\t\t)\n\t\t# We check if the observed field is already present in the\n\t\t# last_applied_manifest\n\t\tif observed_field not in last_applied_manifest:\n\t\t\t# If not, we initialize it\n\t\t\tlast_applied_manifest[observed_field] = response[observed_field]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema[\"observed\"]:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\t\tif isinstance(last_applied_manifest[field], dict):\n\t\t\t# Recurse if the field is dict\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema[\"observed\"][field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\t\telif isinstance(last_applied_manifest[field], list):\n\t\t\t# Recurse if the field is list\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[field], observer_schema[\"observed\"][field],\n\t\t\t\tresponse[field]\n\t\t\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor name, field in observer_schema.items():\n\t\tif isinstance(field, dict):\n\t\t\tif field.get(\"observed\"):\n\t\t\t\tif name not in last_applied_manifest[field[\"kind\"]]:\n\t\t\t\t\tlast_applied_manifest[field[\"kind\"]][name] = response[name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor key in observer_schema:\n\t\tif key not in last_applied_manifest:\n\t\t\tlast_applied_manifest[key] = response[key]\n\t\t\tcontinue\n\t\tif isinstance(observer_schema[key], dict):\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[key], observer_schema[key], response[key]\n\t\t\t)\n\t\telif isinstance(observer_schema[key], list):\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t\tlast_applied_manifest[key], observer_schema[key], response[key]\n\t\t\t)\n\t\telse:\n\t\t\traise KeyError(\n\t\t\t\t\"The field %s is neither a dict nor a list, in the observed schema\"\n\t\t\t\t% key\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor field in observer_schema['fields']:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = response[field]\n\tfor field in observer_schema['dicts']:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = {}\n\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\tlast_applied_manifest[field], observer_schema['dicts'][field], response[field]\n\t\t)\n\tfor field in observer_schema['lists']:\n\t\tif field not in last_applied_manifest:\n\t\t\tlast_applied_manifest[field] = []\n\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\tlast_applied_manifest[field], observer_schema['lists'][field], response[field]\n\t\t)\n\tfor field in observer_schema['dict_list']:\n\t\tif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor path in observer_schema[\"observed\"]:\n\t\tcurrent = last_applied_manifest\n\t\tcurrent_response = response\n\t\tfor key in path:\n\t\t\tif key not in current:\n\t\t\t\tcurrent[key] = {}\n\t\t\tcurrent = current[key]\n\t\t\tcurrent_response = current_response[key]\n\t\tcurrent = current_response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor schema_entry in observer_schema.items():\n\t\tschema_key = schema_entry[0]\n\t\tschema_value = schema_entry[1]\n\t\tif schema_value is None:\n\t\t\tlast_applied_manifest[schema_key] = response[schema_key]\n\t\telse:\n\t\t\tif schema_key not in last_applied_manifest.keys():\n\t\t\t\tlast_applied_manifest[schema_key] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[schema_key],\n\t\t\t\tschema_value,\n\t\t\t\tresponse[schema_key],\n\t\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tfor f in observer_schema.get(\"fields\", []):\n\t\tif f[\"field_name\"] in response:\n\t\t\tif f[\"field_name\"] not in last_applied_manifest:\n\t\t\t\tlast_applied_manifest[f[\"field_name\"]] = {}\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t\tlast_applied_manifest[f[\"field_name\"]], f, response[f[\"field_name\"]]\n\t\t\t)\n\t\telse:\n\t\t\traise KeyError(\"Field %s not in %s\" % (f[\"field_name\"], response))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n\tif \"type\" in observer_schema and observer_schema[\n        \"type\"\n    ] == \"object\":  # observer_schema is a complex object\n\t\tfor key in observer_schema[\"properties\"]:\n\t\t\tif key not in response:\n\t\t\t\traise KeyError(\n\t\t\t\t    \"Observed field %s is not present in the response\" % key\n\t\t\t\t)\n\t\t\t# recursively explore the object\n\t\t\tupdate_last_applied_manifest_dict_from_resp(\n\t\t\t    last_applied_manifest[key],\n\t\t\t    observer_schema[\"properties\"][key],\n\t\t\t    response[key],\n\t\t\t)\n\telse:  # observer_schema is a simple type\n\t\tif observer_schema[\"type\"] == \"array\":\n\t\t\tupdate_last_applied_manifest_list_from_resp(\n\t\t\t    last_applied_manifest, observer_schema, response\n\t\t\t)\n\t\t\treturn\n\t\tif observer_\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "generate_default_observer_schema", "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema", "ques_id": "62b869eab4d922cb0e688cbf", "code": ["def generate_default_observer_schema(app):\n\t# If a custom observer schema has been specified for any of the resources in the app's manifest, we just use that custom observer schema.\n\t# If a custom observer schema hasn't been specified, we generate a default one.\n\tfor kubernetes_resource in app.get('spec', {}).get('manifest', []):\n\t\tkubernetes_resource_schema = kubernetes_resource.get('schema')\n\t\tif kubernetes_resource_schema:\n\t\t\tcontinue\n\n\t\tkubernetes_resource_name = kubernetes_resource.get('name', '')\n\t\tkubernetes_resource_kind = kubernetes_resource.get('kind', '')\n\t\tkubernetes_resource_version = kubernetes_resource.get('version', '')\n\n\t\tdefault_observer_schema = {\n\t\t\t'name': kubernetes_resource_name,\n\t\t\t'kind': kubernetes_resource_kind,\n\t\t\t'version': kubernetes_resource_version\n\t\t}\n\n", "def generate_default_observer_schema(app):\n\tif not app.spec.observer_schema:\n\t\tapp.spec.observer_schema = {}\n\t\tfor m in app.spec.manifest:\n\t\t\tif not m.kind in app.spec.observer_schema:\n\t\t\t\tapp.spec.observer_schema[m.kind] = generate_default_observer_schema_for_kind(m.kind)\n\n", "def generate_default_observer_schema(app):\n\tfor resource in app.manifest.resources:\n\t\tif resource.kind not in DEFAULT_OBSERVER_SCHEMA:\n\t\t\traise exceptions.InvalidResourceKind(resource.kind)\n\n\t\tif resource.observer_schema is not None:\n\t\t\tcontinue\n\n\t\tresource.observer_schema = DEFAULT_OBSERVER_SCHEMA[resource.kind]\n\n\treturn app\n", "def generate_default_observer_schema(app):\n\tobservers = {}\n\n\tfor resource in app.spec.manifest.resources:\n\t\tname = resource.name\n\t\tif name in observers:\n\t\t\tcontinue\n\n\t\tobservers[name] = generate_default_observer_schema_for_resource(resource)\n\n\treturn observers\n\n", "def generate_default_observer_schema(app):\n\tfor kind in app.manifest.kinds:\n\t\tif kind.plural in app.observer_schema:\n\t\t\tcontinue\n\t\tschema = generate_default_observer_schema_for_kind(kind)\n\t\tapp.observer_schema[kind.plural] = schema\n\n", "def generate_default_observer_schema(app):\n\tfor resource_name in app.spec.manifest.keys():\n\t\tif resource_name not in app.spec.observer_schema:\n\t\t\tapp.spec.observer_schema[resource_name] = {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"spec\": {\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\"replicas\": {\n\t\t\t\t\t\t\t\t\"type\": \"number\",\n\t\t\t\t\t\t\t\t\"minimum\": 1,\n\t\t\t\t\t\t\t\t\"default\": 1\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\"readyReplicas\": {\n\t\t\t\t\t\t\t\t\"type\": \"number\",\n\t\t\t\t\t\t\n", "def generate_default_observer_schema(app):\n\tkubernetes_resources = set(app[\"spec\"][\"manifest\"].keys())\n\tif \"customResourceDefinitions\" in app[\"spec\"][\"manifest\"]:\n\t\tfor crd in app[\"spec\"][\"manifest\"][\"customResourceDefinitions\"]:\n\t\t\tkubernetes_resources.add(crd[\"spec\"][\"names\"][\"kind\"].lower())\n\tfor resource_kind in kubernetes_resources:\n\t\tif resource_kind not in app[\"spec\"][\"observers\"]:\n\t\t\tapp[\"spec\"][\"observers\"][resource_kind] = default_observer_schema(resource_kind)\n\treturn app\n\n", "def generate_default_observer_schema(app):\n\t# Generate the default observer schema for each Kubernetes resource.\n\tfor k8s_resource in app.spec.manifest.k8s_resources:\n\t\t# Skip Kubernetes resources for which a custom observer schema is specified.\n\t\tif app.spec.manifest.observer_schema.get(k8s_resource.kind) is not None:\n\t\t\tcontinue\n\n\t\tapp.spec.manifest.observer_schema[k8s_resource.kind] = generate_default_observer_schema_for_k8s_resource(k8s_resource)\n\n", "def generate_default_observer_schema(app):\n\tresources_with_default_observer_schema = {}\n\n\tfor resource in app.spec.manifest:\n\t\tif resource.kind not in resources_with_default_observer_schema:\n\t\t\tresources_with_default_observer_schema[resource.kind] = generate_default_observer_schema_for_resource(resource)\n\n\treturn resources_with_default_observer_schema\n\n", "def generate_default_observer_schema(app):\n\ttry:\n\t\tfor manifest in app.spec.manifest:\n\t\t\tif manifest.schema is None:\n\t\t\t\tmanifest.schema = generate_default_schema(manifest.resource)\n\texcept AttributeError:\n\t\tpass\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def generate_default_observer_schema(app):\n\t# If a custom observer schema has been specified for any of the resources in the app's manifest, we just use that custom observer schema.\n\t# If a custom observer schema hasn't been specified, we generate a default one.\n\tfor kubernetes_resource in app.get('spec', {}).get('manifest', []):\n\t\tkubernetes_resource_schema = kubernetes_resource.get('schema')\n\t\tif kubernetes_resource_schema:\n\t\t\tcontinue\n\n\t\tkubernetes_resource_name = kubernetes_resource.get('name', '')\n\t\tkubernetes_resource_kind = kubernetes_resource.get('kind', '')\n\t\tkubernetes_resource_version = kubernetes_resource.get('version', '')\n\n\t\tdefault_observer_schema = {\n\t\t\t'name': kubernetes_resource_name,\n\t\t\t'kind': kubernetes_resource_kind,\n\t\t\t'version': kubernetes_resource_version\n\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tif not app.spec.observer_schema:\n\t\tapp.spec.observer_schema = {}\n\t\tfor m in app.spec.manifest:\n\t\t\tif not m.kind in app.spec.observer_schema:\n\t\t\t\tapp.spec.observer_schema[m.kind] = generate_default_observer_schema_for_kind(m.kind)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource in app.manifest.resources:\n\t\tif resource.kind not in DEFAULT_OBSERVER_SCHEMA:\n\t\t\traise exceptions.InvalidResourceKind(resource.kind)\n\n\t\tif resource.observer_schema is not None:\n\t\t\tcontinue\n\n\t\tresource.observer_schema = DEFAULT_OBSERVER_SCHEMA[resource.kind]\n\n\treturn app\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tobservers = {}\n\n\tfor resource in app.spec.manifest.resources:\n\t\tname = resource.name\n\t\tif name in observers:\n\t\t\tcontinue\n\n\t\tobservers[name] = generate_default_observer_schema_for_resource(resource)\n\n\treturn observers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor kind in app.manifest.kinds:\n\t\tif kind.plural in app.observer_schema:\n\t\t\tcontinue\n\t\tschema = generate_default_observer_schema_for_kind(kind)\n\t\tapp.observer_schema[kind.plural] = schema\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tfor resource_name in app.spec.manifest.keys():\n\t\tif resource_name not in app.spec.observer_schema:\n\t\t\tapp.spec.observer_schema[resource_name] = {\n\t\t\t\t\"type\": \"object\",\n\t\t\t\t\"properties\": {\n\t\t\t\t\t\"spec\": {\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\"replicas\": {\n\t\t\t\t\t\t\t\t\"type\": \"number\",\n\t\t\t\t\t\t\t\t\"minimum\": 1,\n\t\t\t\t\t\t\t\t\"default\": 1\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t},\n\t\t\t\t\t\"status\": {\n\t\t\t\t\t\t\"type\": \"object\",\n\t\t\t\t\t\t\"properties\": {\n\t\t\t\t\t\t\t\"readyReplicas\": {\n\t\t\t\t\t\t\t\t\"type\": \"number\",\n\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tkubernetes_resources = set(app[\"spec\"][\"manifest\"].keys())\n\tif \"customResourceDefinitions\" in app[\"spec\"][\"manifest\"]:\n\t\tfor crd in app[\"spec\"][\"manifest\"][\"customResourceDefinitions\"]:\n\t\t\tkubernetes_resources.add(crd[\"spec\"][\"names\"][\"kind\"].lower())\n\tfor resource_kind in kubernetes_resources:\n\t\tif resource_kind not in app[\"spec\"][\"observers\"]:\n\t\t\tapp[\"spec\"][\"observers\"][resource_kind] = default_observer_schema(resource_kind)\n\treturn app\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\t# Generate the default observer schema for each Kubernetes resource.\n\tfor k8s_resource in app.spec.manifest.k8s_resources:\n\t\t# Skip Kubernetes resources for which a custom observer schema is specified.\n\t\tif app.spec.manifest.observer_schema.get(k8s_resource.kind) is not None:\n\t\t\tcontinue\n\n\t\tapp.spec.manifest.observer_schema[k8s_resource.kind] = generate_default_observer_schema_for_k8s_resource(k8s_resource)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\tresources_with_default_observer_schema = {}\n\n\tfor resource in app.spec.manifest:\n\t\tif resource.kind not in resources_with_default_observer_schema:\n\t\t\tresources_with_default_observer_schema[resource.kind] = generate_default_observer_schema_for_resource(resource)\n\n\treturn resources_with_default_observer_schema\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def generate_default_observer_schema(app):\n\ttry:\n\t\tfor manifest in app.spec.manifest:\n\t\t\tif manifest.schema is None:\n\t\t\t\tmanifest.schema = generate_default_schema(manifest.resource)\n\texcept AttributeError:\n\t\tpass\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "format", "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).", "ques_id": "62b43427903eeb48555d3ea5", "code": ["def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif not params:\n\t\treturn sql, params\n\n\tif isinstance(params, dict):\n\t\tparams = {\n\t\t\tk: self._converter.convert(v) for k, v in params.items()\n\t\t}\n\n\telse:\n\t\tparams = [self._converter.convert(v) for v in params]\n\n\treturn sql, params\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif isinstance(sql, str):\n\t\tif isinstance(params, (list, tuple)):\n\t\t\treturn self._converter.convert(sql, tuple(params))\n\t\telse:\n\t\t\treturn self._converter.convert(sql, params)\n\telse:\n\t\treturn sql, params\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif isinstance(params, (list, tuple)):\n\t\tif self._converter.is_placeholder:\n\t\t\tparams = tuple(self._converter.convert(param) for param in params)\n\t\telse:\n\t\t\tparams = tuple(self._converter.convert(param) for param in params)\n\t\t\tsql = self._converter.convert(sql)\n\telse:\n\t\tif self._converter.is_placeholder:\n\t\t\tparams = {\n\t\t\t\tkey: self._converter.convert(params[key]) for key in params\n\t\t\t}\n\t\telse:\n\t\t\tparams = {\n\t\t\t\tkey: self._converter.convert(params[key]) for key in params\n\t\t\t}\n\t\t\tsql = self._converter.convert(sql)\n\n\treturn sql, params\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\treturn self._converter.convert(sql, params)\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tsql, params = self._converter.convert(sql, params)\n\tsql, params = super().format(sql, params)\n\treturn sql, params\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif isinstance(params, dict):\n\t\tparams = {k: self.db_api.Binary(self.db_api.Binary(v).hex()) if isinstance(v, bytes) else v for k, v in params.items()}\n\telif isinstance(params, tuple):\n\t\tparams = tuple(self.db_api.Binary(self.db_api.Binary(v).hex()) if isinstance(v, bytes) else v for v in params)\n\telse:\n\t\traise errors.InterfaceError(\n\t\t\terrno=errors.ER_INVALID_PARAMETER_DATA_TYPES_ERROR,\n\t\t\tmsg=\"params must be a dict or tuple\",\n\t\t)\n\treturn self._converter.convert(sql), params\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tsql = sql.strip()\n\tif isinstance(params, dict):\n\t\tparams = params.copy()\n\t\tfor k, v in params.items():\n\t\t\tif isinstance(k, int):\n\t\t\t\tparams[k] = self._converter.convert(v)\n\t\t\telif k in self._fields:\n\t\t\t\tparams[k] = self._converter.convert(v)\n\t\treturn sql, params\n\telse:\n\t\tsql, params = sql.strip(), tuple(params)\n\t\tfor i, v in enumerate(params):\n\t\t\tparams[i] = self._converter.convert(v)\n\t\treturn sql, params\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\treturn self._converter.convert(sql, params)\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif params is None:\n\t\treturn sql, []\n\n\tif isinstance(params, dict):\n\t\tparams = tuple(params.values())\n\n\tsql = self._converter.convert(sql)\n\treturn sql, params\n\n\n", "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif sql is None:\n\t\treturn None, params\n\n\tif isinstance(sql, str):\n\t\tsql = sql.encode(\"utf-8\")\n\n\tif isinstance(params, dict):\n\t\tparams = self._converter.convert(params)\n\n\treturn sql, params\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif not params:\n\t\treturn sql, params\n\n\tif isinstance(params, dict):\n\t\tparams = {\n\t\t\tk: self._converter.convert(v) for k, v in params.items()\n\t\t}\n\n\telse:\n\t\tparams = [self._converter.convert(v) for v in params]\n\n\treturn sql, params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif isinstance(sql, str):\n\t\tif isinstance(params, (list, tuple)):\n\t\t\treturn self._converter.convert(sql, tuple(params))\n\t\telse:\n\t\t\treturn self._converter.convert(sql, params)\n\telse:\n\t\treturn sql, params\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif isinstance(params, (list, tuple)):\n\t\tif self._converter.is_placeholder:\n\t\t\tparams = tuple(self._converter.convert(param) for param in params)\n\t\telse:\n\t\t\tparams = tuple(self._converter.convert(param) for param in params)\n\t\t\tsql = self._converter.convert(sql)\n\telse:\n\t\tif self._converter.is_placeholder:\n\t\t\tparams = {\n\t\t\t\tkey: self._converter.convert(params[key]) for key in params\n\t\t\t}\n\t\telse:\n\t\t\tparams = {\n\t\t\t\tkey: self._converter.convert(params[key]) for key in params\n\t\t\t}\n\t\t\tsql = self._converter.convert(sql)\n\n\treturn sql, params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\treturn self._converter.convert(sql, params)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tsql, params = self._converter.convert(sql, params)\n\tsql, params = super().format(sql, params)\n\treturn sql, params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif isinstance(params, dict):\n\t\tparams = {k: self.db_api.Binary(self.db_api.Binary(v).hex()) if isinstance(v, bytes) else v for k, v in params.items()}\n\telif isinstance(params, tuple):\n\t\tparams = tuple(self.db_api.Binary(self.db_api.Binary(v).hex()) if isinstance(v, bytes) else v for v in params)\n\telse:\n\t\traise errors.InterfaceError(\n\t\t\terrno=errors.ER_INVALID_PARAMETER_DATA_TYPES_ERROR,\n\t\t\tmsg=\"params must be a dict or tuple\",\n\t\t)\n\treturn self._converter.convert(sql), params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tsql = sql.strip()\n\tif isinstance(params, dict):\n\t\tparams = params.copy()\n\t\tfor k, v in params.items():\n\t\t\tif isinstance(k, int):\n\t\t\t\tparams[k] = self._converter.convert(v)\n\t\t\telif k in self._fields:\n\t\t\t\tparams[k] = self._converter.convert(v)\n\t\treturn sql, params\n\telse:\n\t\tsql, params = sql.strip(), tuple(params)\n\t\tfor i, v in enumerate(params):\n\t\t\tparams[i] = self._converter.convert(v)\n\t\treturn sql, params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\treturn self._converter.convert(sql, params)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif params is None:\n\t\treturn sql, []\n\n\tif isinstance(params, dict):\n\t\tparams = tuple(params.values())\n\n\tsql = self._converter.convert(sql)\n\treturn sql, params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\tif sql is None:\n\t\treturn None, params\n\n\tif isinstance(sql, str):\n\t\tsql = sql.encode(\"utf-8\")\n\n\tif isinstance(params, dict):\n\t\tparams = self._converter.convert(params)\n\n\treturn sql, params\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "formatmany", "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).", "ques_id": "62b43428903eeb48555d3eaa", "code": ["def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif not many_params:\n\t\treturn sql, []\n\tif isinstance(many_params[0], dict):\n\t\treturn self._converter.convert_many(sql, many_params)\n\telse:\n\t\treturn self._converter.convert_many(sql, list(many_params))\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tconverted = self._converter.convert_many(sql, many_params)\n\treturn converted[0], converted[1]\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif not many_params:\n\t\treturn sql, []\n\treturn self._converter.convert_many(sql, many_params)\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\treturn self._converter.convert_many(sql, many_params)\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif many_params:\n\t\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tconverted_sql, converted_params = self._converter.convert_many(sql, many_params)\n\tif not converted_sql:\n\t\traise ValueError(\"sql cannot be empty\")\n\n\tif not converted_params:\n\t\treturn converted_sql, converted_params\n\n\tif isinstance(converted_params[0], dict):\n\t\treturn converted_sql, converted_params\n\n\treturn converted_sql, self._converter.convert_sequence(converted_params)\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\treturn self._converter.convert_many(sql, many_params)\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif self.async_driver:\n\t\treturn self._converter.convert_many(sql, many_params)\n\telse:\n\t\treturn self._converter.convert_many(sql, many_params)\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif self.is_dict_cursor:\n\t\tparams = list(many_params)\n\t\tmany_params = list(many_params)\n\t\tfor i, p in enumerate(params):\n\t\t\tmany_params[i] = self.format(sql, p)\n\t\treturn sql, many_params\n\telse:\n\t\treturn self._converter.convert_many(sql, many_params)\n\n\n", "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif any(isinstance(param, Mapping) for param in many_params):\n\t\treturn self._converter.convert_many(sql, many_params)\n\treturn self._converter.convert_many(sql, [dict(enumerate(params)) for params in many_params])\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif not many_params:\n\t\treturn sql, []\n\tif isinstance(many_params[0], dict):\n\t\treturn self._converter.convert_many(sql, many_params)\n\telse:\n\t\treturn self._converter.convert_many(sql, list(many_params))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tconverted = self._converter.convert_many(sql, many_params)\n\treturn converted[0], converted[1]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif not many_params:\n\t\treturn sql, []\n\treturn self._converter.convert_many(sql, many_params)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\treturn self._converter.convert_many(sql, many_params)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif many_params:\n\t\tsql, many_params = self._converter.convert_many(sql, many_params)\n\treturn sql, many_params\n", "is_pass": true, "return_code": 0}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tconverted_sql, converted_params = self._converter.convert_many(sql, many_params)\n\tif not converted_sql:\n\t\traise ValueError(\"sql cannot be empty\")\n\n\tif not converted_params:\n\t\treturn converted_sql, converted_params\n\n\tif isinstance(converted_params[0], dict):\n\t\treturn converted_sql, converted_params\n\n\treturn converted_sql, self._converter.convert_sequence(converted_params)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\treturn self._converter.convert_many(sql, many_params)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif self.async_driver:\n\t\treturn self._converter.convert_many(sql, many_params)\n\telse:\n\t\treturn self._converter.convert_many(sql, many_params)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif self.is_dict_cursor:\n\t\tparams = list(many_params)\n\t\tmany_params = list(many_params)\n\t\tfor i, p in enumerate(params):\n\t\t\tmany_params[i] = self.format(sql, p)\n\t\treturn sql, many_params\n\telse:\n\t\treturn self._converter.convert_many(sql, many_params)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\tif any(isinstance(param, Mapping) for param in many_params):\n\t\treturn self._converter.convert_many(sql, many_params)\n\treturn self._converter.convert_many(sql, [dict(enumerate(params)) for params in many_params])\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.", "ques_id": "62b45df05108cfac7f2109ce", "code": ["def validate(self, path):\n\treturn self.validate_fs_root(path) or self.validate_fs_path(path)\n\n\n\tdef validate_fs_root(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is a valid OCFL object root, False otherwise.\n\t\t\"\"\"\n\t\treturn (os.path.isdir(path) and\n\t\t\t\tos.path.isdir(os.path.join(path, self._OCFL_DIR)) and\n\t\t\t\tos.path.isfile(os.path.join(path, self._OCFL_DIR, self._OCFL_MANIFEST)))\n\n\n\tdef validate_fs_path(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is within a valid OCFL object, False otherwise.\n\t\t\"\"\"\n\t\tif os.path.isdir(path):\n\t\t\tif os.path.isdir(os.path.join(path, self._OCFL_DIR)):\n\t\t\t\t# path is OCFL object root\n\t\t\t\treturn True\n\t\t\telif os.path.isfile(\n", "def validate(self, path):\n\tif not os.path.isdir(path):\n\t\tpath = os.path.dirname(path)\n\treturn self.validate_root(path)\n\n\n\tdef validate_root(self, path):\n\t\t\"\"\"\n\t\tReturns True if OCFL object at path, False otherwise.\n\t\t\"\"\"\n\t\tif not os.path.isdir(path):\n\t\t\treturn False\n\n\t\treturn os.path.isdir(os.path.join(path, '.ocfl'))\n\n\n\tdef create(self, path):\n\t\t\"\"\"\n\t\tCreates an OCFL object at path.\n\t\t\"\"\"\n\t\tif os.path.isdir(path):\n\t\t\traise PyOCFLException(\"OCFL object already exists at path\")\n\t\tif os.path.exists(path):\n\t\t\traise PyOCFLException(\"File already exists at path\")\n\n\t\tos.makedirs(path)\n\t\tos.mkdir(os.path.join(path, '.ocfl'))\n\t\tos.mkdir(os.path.join(path, '.oc\n", "def validate(self, path):\n\t\tif os.path.exists(os.path.join(path, OCFL_ROOT_DIR)):\n\t\t\treturn True\n\t\telif os.path.exists(os.path.join(path, OCFL_OBJECT_DIR)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef get_object_id(self, path):\n\t\t\"\"\"\n\t\tReturns the OCFL object id for the OCFL object at path, or None.\n\t\t\"\"\"\n\t\tif os.path.exists(os.path.join(path, OCFL_ROOT_DIR)):\n\t\t\treturn os.path.basename(path)\n\t\telif os.path.exists(os.path.join(path, OCFL_OBJECT_DIR)):\n\t\t\treturn os.path.basename(os.path.dirname(path))\n\t\telse:\n\t\t\treturn None\n\n\tdef get_object_path(self, object_id):\n\t\t\"\"\"\n\t\tReturns the path to the OCFL object with the given object\n", "def validate(self, path):\n\tif self.is_ocfl(path):\n\t\treturn True\n\treturn False\n\n", "def validate(self, path):\n\treturn self.__validate(path) is not None\n\n\tdef __validate(self, path, root=None):\n\t\"\"\"\n\tReturns OCFL object at path if valid, None otherwise.\n\t\"\"\"\n\tocfl_obj = self.__get_ocfl_obj(path, root)\n\tif not ocfl_obj:\n\treturn\n\ttry:\n\treturn ocfl_obj.validate()\n\texcept OCFLValidationError as e:\n\tlogger.error(f'Invalid OCFL object at {ocfl_obj.root}: {e}')\n\treturn\n\n\tdef __get_ocfl_obj(self, path, root=None):\n\t\"\"\"\n\tGets OCFL object at path or pyfs root.\n\t\"\"\"\n\tif root is None:\n\troot = self.root\n\tif path.startswith(root):\n\tpath = path[len(root):]\n\tif not path:\n\tpath = ''\n\treturn OCFLObject(root, path, self.pyfs, self.pyfs_root, self.checksum_type, self.version\n", "def validate(self, path):\n\treturn self.fs.isdir(path)\n\n\n", "def validate(self, path):\n\treturn self.get_version(path) is not None\n\n\n\t###########################################################################\n\tdef _get_fsroot(self, path):\n\t\t\"\"\"\n\t\tReturns the root of the pyfs filesystem at path.\n\t\t\"\"\"\n\t\tif self._fsroot is None:\n\t\t\tself._fsroot = open_fs(path)\n\t\treturn self._fsroot\n\n\n\t###########################################################################\n\tdef get_fs(self, path):\n\t\t\"\"\"\n\t\tReturns the root of the pyfs filesystem at path.\n\t\t\"\"\"\n\t\treturn self._get_fsroot(path)\n\n\n\t###########################################################################\n\tdef get_root(self, path):\n\t\t\"\"\"\n\t\tReturns the root of the OCFL object at path.\n\t\t\"\"\"\n\t\treturn self._get_fsroot(path) / 'ocfl_object'\n\n\n\t###########################################################################\n\tdef get_version(self, path):\n\t\t\"\"\"\n\t\tReturns the version of the OCFL object at path.\n\t\t\"\"\"\n\t\tocfl_object_root = self.get_root\n", "def validate(self, path):\n\tschemaValidator = SchemaValidator()\n\treturn bool(schemaValidator.validate(path))\n\n\tdef validate_log(self):\n\t\t\"\"\"\n\t\tReturns a log of the last schema validation.\n\t\t\"\"\"\n\t\treturn self._schemaValidator.log\n\n\tdef _set_fuse_options(self, options):\n\t\tself._fuse_options = options\n\n\tdef _create_file(self, path):\n\t\t\"\"\"\n\t\tCreates a new file at the given path.  The file is added to the\n\t\tworking inventory and the content of the file is added to the\n\t\tworking directory.\n\t\t\"\"\"\n\t\tinode = self._inventory.add_file(path)\n\t\tself._working_directory.add_file(path)\n\t\tself._working_directory.add_file_content(path, inode)\n\t\treturn inode\n\n\tdef _create_directory(self, path):\n\t\t\"\"\"\n\t\tCreates a new directory at the given path.  The directory is\n\t\tadded to the working inventory and the content of the\n", "def validate(self, path):\n\tpath = pathlib.Path(path)\n\tif path.is_dir():\n\t\treturn self.validate_dir(path)\n\telif path.is_file():\n\t\treturn self.validate_file(path)\n\telse:\n\t\treturn False\n\n\tdef validate_dir(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is a valid OCFL object root, False otherwise.\n\t\t\"\"\"\n\t\treturn [x for x in path.iterdir() if x.is_dir()] and all(\n\t\t\t[x for x in (path / \"v1_0\").iterdir() if x.is_dir()]\n\t\t)\n\n\tdef validate_file(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is a valid OCFL object root, False otherwise.\n\t\t\"\"\"\n\t\treturn path.name == \"inventory.json\" and self.validate_dir(path.parent)\n\n\n", "def validate(self, path):\n\treturn self._validate(path)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, path):\n\treturn self.validate_fs_root(path) or self.validate_fs_path(path)\n\n\n\tdef validate_fs_root(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is a valid OCFL object root, False otherwise.\n\t\t\"\"\"\n\t\treturn (os.path.isdir(path) and\n\t\t\t\tos.path.isdir(os.path.join(path, self._OCFL_DIR)) and\n\t\t\t\tos.path.isfile(os.path.join(path, self._OCFL_DIR, self._OCFL_MANIFEST)))\n\n\n\tdef validate_fs_path(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is within a valid OCFL object, False otherwise.\n\t\t\"\"\"\n\t\tif os.path.isdir(path):\n\t\t\tif os.path.isdir(os.path.join(path, self._OCFL_DIR)):\n\t\t\t\t# path is OCFL object root\n\t\t\t\treturn True\n\t\t\telif os.path.isfile(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tif not os.path.isdir(path):\n\t\tpath = os.path.dirname(path)\n\treturn self.validate_root(path)\n\n\n\tdef validate_root(self, path):\n\t\t\"\"\"\n\t\tReturns True if OCFL object at path, False otherwise.\n\t\t\"\"\"\n\t\tif not os.path.isdir(path):\n\t\t\treturn False\n\n\t\treturn os.path.isdir(os.path.join(path, '.ocfl'))\n\n\n\tdef create(self, path):\n\t\t\"\"\"\n\t\tCreates an OCFL object at path.\n\t\t\"\"\"\n\t\tif os.path.isdir(path):\n\t\t\traise PyOCFLException(\"OCFL object already exists at path\")\n\t\tif os.path.exists(path):\n\t\t\traise PyOCFLException(\"File already exists at path\")\n\n\t\tos.makedirs(path)\n\t\tos.mkdir(os.path.join(path, '.ocfl'))\n\t\tos.mkdir(os.path.join(path, '.oc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\t\tif os.path.exists(os.path.join(path, OCFL_ROOT_DIR)):\n\t\t\treturn True\n\t\telif os.path.exists(os.path.join(path, OCFL_OBJECT_DIR)):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\n\tdef get_object_id(self, path):\n\t\t\"\"\"\n\t\tReturns the OCFL object id for the OCFL object at path, or None.\n\t\t\"\"\"\n\t\tif os.path.exists(os.path.join(path, OCFL_ROOT_DIR)):\n\t\t\treturn os.path.basename(path)\n\t\telif os.path.exists(os.path.join(path, OCFL_OBJECT_DIR)):\n\t\t\treturn os.path.basename(os.path.dirname(path))\n\t\telse:\n\t\t\treturn None\n\n\tdef get_object_path(self, object_id):\n\t\t\"\"\"\n\t\tReturns the path to the OCFL object with the given object\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tif self.is_ocfl(path):\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\treturn self.__validate(path) is not None\n\n\tdef __validate(self, path, root=None):\n\t\"\"\"\n\tReturns OCFL object at path if valid, None otherwise.\n\t\"\"\"\n\tocfl_obj = self.__get_ocfl_obj(path, root)\n\tif not ocfl_obj:\n\treturn\n\ttry:\n\treturn ocfl_obj.validate()\n\texcept OCFLValidationError as e:\n\tlogger.error(f'Invalid OCFL object at {ocfl_obj.root}: {e}')\n\treturn\n\n\tdef __get_ocfl_obj(self, path, root=None):\n\t\"\"\"\n\tGets OCFL object at path or pyfs root.\n\t\"\"\"\n\tif root is None:\n\troot = self.root\n\tif path.startswith(root):\n\tpath = path[len(root):]\n\tif not path:\n\tpath = ''\n\treturn OCFLObject(root, path, self.pyfs, self.pyfs_root, self.checksum_type, self.version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\treturn self.fs.isdir(path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\treturn self.get_version(path) is not None\n\n\n\t###########################################################################\n\tdef _get_fsroot(self, path):\n\t\t\"\"\"\n\t\tReturns the root of the pyfs filesystem at path.\n\t\t\"\"\"\n\t\tif self._fsroot is None:\n\t\t\tself._fsroot = open_fs(path)\n\t\treturn self._fsroot\n\n\n\t###########################################################################\n\tdef get_fs(self, path):\n\t\t\"\"\"\n\t\tReturns the root of the pyfs filesystem at path.\n\t\t\"\"\"\n\t\treturn self._get_fsroot(path)\n\n\n\t###########################################################################\n\tdef get_root(self, path):\n\t\t\"\"\"\n\t\tReturns the root of the OCFL object at path.\n\t\t\"\"\"\n\t\treturn self._get_fsroot(path) / 'ocfl_object'\n\n\n\t###########################################################################\n\tdef get_version(self, path):\n\t\t\"\"\"\n\t\tReturns the version of the OCFL object at path.\n\t\t\"\"\"\n\t\tocfl_object_root = self.get_root\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tschemaValidator = SchemaValidator()\n\treturn bool(schemaValidator.validate(path))\n\n\tdef validate_log(self):\n\t\t\"\"\"\n\t\tReturns a log of the last schema validation.\n\t\t\"\"\"\n\t\treturn self._schemaValidator.log\n\n\tdef _set_fuse_options(self, options):\n\t\tself._fuse_options = options\n\n\tdef _create_file(self, path):\n\t\t\"\"\"\n\t\tCreates a new file at the given path.  The file is added to the\n\t\tworking inventory and the content of the file is added to the\n\t\tworking directory.\n\t\t\"\"\"\n\t\tinode = self._inventory.add_file(path)\n\t\tself._working_directory.add_file(path)\n\t\tself._working_directory.add_file_content(path, inode)\n\t\treturn inode\n\n\tdef _create_directory(self, path):\n\t\t\"\"\"\n\t\tCreates a new directory at the given path.  The directory is\n\t\tadded to the working inventory and the content of the\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\tpath = pathlib.Path(path)\n\tif path.is_dir():\n\t\treturn self.validate_dir(path)\n\telif path.is_file():\n\t\treturn self.validate_file(path)\n\telse:\n\t\treturn False\n\n\tdef validate_dir(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is a valid OCFL object root, False otherwise.\n\t\t\"\"\"\n\t\treturn [x for x in path.iterdir() if x.is_dir()] and all(\n\t\t\t[x for x in (path / \"v1_0\").iterdir() if x.is_dir()]\n\t\t)\n\n\tdef validate_file(self, path):\n\t\t\"\"\"\n\t\tReturns True if path is a valid OCFL object root, False otherwise.\n\t\t\"\"\"\n\t\treturn path.name == \"inventory.json\" and self.validate_dir(path.parent)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, path):\n\treturn self._validate(path)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validation_logger.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string of validator status, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dc", "code": ["def status_str(self, prefix=''):\n\t\t\n\t\t# TODO: Define the string to be returned\n\t\t\n\t\t# The string to be returned\n\t\ts = ''\n\t\t\n\t\t# Visit the sorted self.messages list\n\t\tfor i in sorted(self.messages):\n\t\t\t# Add prefix and the element in the sorted self.messages list to the string to be returned\n\t\t\ts += prefix + i + '\\n'\n\t\t\t\n\t\t# Return the string to be returned\n\t\treturn s\n\t\t\n\tdef status_str_compact(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\t\n\t\t# TODO: Define the string to be returned\n\t\t\n\t\t# The string to be returned\n\t\ts = ''\n\t\t\n\t\t# Visit the sorted self.messages list\n\t\tfor i in sorted(self.messages):\n\t\t\t# Add prefix and the element in the sorted self.mess\n", "def status_str(self, prefix=''):\n\ts = '%s: %s' % (prefix, self.status)\n\tfor i in sorted(self.messages):\n\t\ts += '\\n' + prefix + i\n\treturn s\n\n\n", "def status_str(self, prefix=''):\n\t\tif self.messages == []:\n\t\t\treturn ''\n\t\telif self.messages == None:\n\t\t\treturn ''\n\t\telse:\n\t\t\treturn prefix + self.messages[0] + '\\n' + self.status_str(self.messages[1:], prefix)\n\t\t\t\n\t\t\n\t# def status_str(self, prefix=''):\n\t# \tif self.messages == None:\n\t# \t\treturn ''\n\t# \telif self.messages == []:\n\t# \t\treturn ''\n\t# \telif len(self.messages) == 1:\n\t# \t\treturn prefix + self.messages[0]\n\t# \telse:\n\t# \t\treturn prefix + self.messages[0] + '\\n' + self.status_str(prefix, self.messages[1:])\n\n\t# def status_str(self, prefix=''):\n\t# \tif self.messages == None:\n\t# \t\treturn ''\n\t# \t\n", "def status_str(self, prefix=''):\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n", "def status_str(self, prefix=''):\n\toutput = \"\"\n\tfor msg in sorted(self.messages):\n\t\toutput += str(prefix) + str(msg) + \"\\n\"\n\treturn output\n\n", "def status_str(self, prefix=''):\n\t\t\n\t\t\n\t\t#self.messages.sort()\n\t\t#return prefix + '\\n'.join(self.messages)\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\n", "def status_str(self, prefix=''):\n\t\t\n\t\tresult = \"\"\n\t\tfor message in self.messages:\n\t\t\tresult += prefix + message + \"\\n\"\n\t\treturn result\n\t\t\n\tdef is_success(self):\n\t\t\"\"\"\n\t\tReturn True if no error messages have been reported, otherwise return False.\n\t\t\"\"\"\n\t\t\n\t\tfor message in self.messages:\n\t\t\tif message[:5] == 'ERROR':\n\t\t\t\treturn False\n\t\treturn True\n\t\t\n\tdef clear_messages(self):\n\t\t\"\"\"\n\t\tClear all messages from the self.messages list.\n\t\t\"\"\"\n\t\t\n\t\tself.messages = []\n\t\t\n\tdef add_message(self, message):\n\t\t\"\"\"\n\t\tAdd a message to the self.messages list.\n\t\t\"\"\"\n\t\t\n\t\tself.messages.append(message)\n\t\t\n\tdef add_error(self, message):\n\t\t\"\"\"\n\t\tAdd an error message to the self.messages list.\n\t\t\"\"\"\n\t\t\n\t\tself.\n", "def status_str(self, prefix=''):\n\t\t\n\t\t# sort the self.messages list\n\t\tself.messages.sort()\n\t\t\n\t\t# create a new list for storing the string\n\t\tstatus_str = []\n\t\t\n\t\t# concatenate the prefix and the string in the self.messages list\n\t\tfor msg in self.messages:\n\t\t\tstatus_str.append(prefix + msg)\n\t\t\n\t\t# return the new list\n\t\treturn status_str\n\t\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add a newline and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\t\n\t\t# sort the self.messages list\n\t\tself.messages.sort()\n\t\t\n\t\t# create a new list for storing the string\n\t\treturn '\\n'.join(self.messages)\n\t\t\n\t\t\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit\n", "def status_str(self, prefix=''):\n\tt = ''\n\tfor m in sorted(self.messages):\n\t\tt += prefix + m + '\\n'\n\treturn t\n\n", "def status_str(self, prefix=''):\n\treturn '\\n'.join(prefix + str(s) for s in sorted(self.messages))\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\t\t\n\t\t# TODO: Define the string to be returned\n\t\t\n\t\t# The string to be returned\n\t\ts = ''\n\t\t\n\t\t# Visit the sorted self.messages list\n\t\tfor i in sorted(self.messages):\n\t\t\t# Add prefix and the element in the sorted self.messages list to the string to be returned\n\t\t\ts += prefix + i + '\\n'\n\t\t\t\n\t\t# Return the string to be returned\n\t\treturn s\n\t\t\n\tdef status_str_compact(self, prefix=''):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add prefix and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\t\n\t\t# TODO: Define the string to be returned\n\t\t\n\t\t# The string to be returned\n\t\ts = ''\n\t\t\n\t\t# Visit the sorted self.messages list\n\t\tfor i in sorted(self.messages):\n\t\t\t# Add prefix and the element in the sorted self.mess\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\ts = '%s: %s' % (prefix, self.status)\n\tfor i in sorted(self.messages):\n\t\ts += '\\n' + prefix + i\n\treturn s\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\tif self.messages == []:\n\t\t\treturn ''\n\t\telif self.messages == None:\n\t\t\treturn ''\n\t\telse:\n\t\t\treturn prefix + self.messages[0] + '\\n' + self.status_str(self.messages[1:], prefix)\n\t\t\t\n\t\t\n\t# def status_str(self, prefix=''):\n\t# \tif self.messages == None:\n\t# \t\treturn ''\n\t# \telif self.messages == []:\n\t# \t\treturn ''\n\t# \telif len(self.messages) == 1:\n\t# \t\treturn prefix + self.messages[0]\n\t# \telse:\n\t# \t\treturn prefix + self.messages[0] + '\\n' + self.status_str(prefix, self.messages[1:])\n\n\t# def status_str(self, prefix=''):\n\t# \tif self.messages == None:\n\t# \t\treturn ''\n\t# \t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\toutput = \"\"\n\tfor msg in sorted(self.messages):\n\t\toutput += str(prefix) + str(msg) + \"\\n\"\n\treturn output\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\t\n\t\t\n\t\t#self.messages.sort()\n\t\t#return prefix + '\\n'.join(self.messages)\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\t\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\t\n\t\tresult = \"\"\n\t\tfor message in self.messages:\n\t\t\tresult += prefix + message + \"\\n\"\n\t\treturn result\n\t\t\n\tdef is_success(self):\n\t\t\"\"\"\n\t\tReturn True if no error messages have been reported, otherwise return False.\n\t\t\"\"\"\n\t\t\n\t\tfor message in self.messages:\n\t\t\tif message[:5] == 'ERROR':\n\t\t\t\treturn False\n\t\treturn True\n\t\t\n\tdef clear_messages(self):\n\t\t\"\"\"\n\t\tClear all messages from the self.messages list.\n\t\t\"\"\"\n\t\t\n\t\tself.messages = []\n\t\t\n\tdef add_message(self, message):\n\t\t\"\"\"\n\t\tAdd a message to the self.messages list.\n\t\t\"\"\"\n\t\t\n\t\tself.messages.append(message)\n\t\t\n\tdef add_error(self, message):\n\t\t\"\"\"\n\t\tAdd an error message to the self.messages list.\n\t\t\"\"\"\n\t\t\n\t\tself.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\t\t\n\t\t# sort the self.messages list\n\t\tself.messages.sort()\n\t\t\n\t\t# create a new list for storing the string\n\t\tstatus_str = []\n\t\t\n\t\t# concatenate the prefix and the string in the self.messages list\n\t\tfor msg in self.messages:\n\t\t\tstatus_str.append(prefix + msg)\n\t\t\n\t\t# return the new list\n\t\treturn status_str\n\t\t\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit add a newline and the element in the sorted self.messages list.\n\t\t\"\"\"\n\t\t\n\t\t# sort the self.messages list\n\t\tself.messages.sort()\n\t\t\n\t\t# create a new list for storing the string\n\t\treturn '\\n'.join(self.messages)\n\t\t\n\t\t\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string with visiting the sorted self.messages list, each visit\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\tt = ''\n\tfor m in sorted(self.messages):\n\t\tt += prefix + m + '\\n'\n\treturn t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '\\n'.join(prefix + str(s) for s in sorted(self.messages))\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "status_str", "docstring": "Return string representation of validation log, with optional prefix.", "ques_id": "62b45df15108cfac7f2109dd", "code": ["def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n", "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n\tdef add_log(self, log):\n\t\"\"\"\n\tAdd a new log to the current log. This is useful for debugging when a\n\tsubprocess is used.\n\t\"\"\"\n\tself.log.add_log(log)\n\n\tdef set_log(self, log):\n\t\"\"\"\n\tSet the log to the given log.\n\t\"\"\"\n\tself.log = log\n\n\tdef set_log_level(self, level):\n\t\"\"\"\n\tSet the log level.\n\t\"\"\"\n\tself.log.set_level(level)\n\n\tdef set_log_max_level(self, level):\n\t\"\"\"\n\tSet the maximum log level.\n\t\"\"\"\n\tself.log.set_max_level(level)\n\n\tdef get_log_level(self):\n\t\"\"\"\n\tGet the current log level.\n\t\"\"\"\n\treturn self.log.get_level()\n\n\tdef get_log_max_level(self):\n\t\"\"\"\n\tGet the maximum log level.\n\t\"\"\"\n\treturn self.log.get_max_level()\n", "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix=prefix)\n\n", "def status_str(self, prefix=''):\n\treturn \"%s%s\" % (prefix, self.log.status_str)\n\n", "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix=prefix)\n\n\n", "def status_str(self, prefix=''):\n\treturn '%s%s' % (prefix, self.log.status_str(self))\n\n", "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n\n", "def status_str(self, prefix=''):\n\treturn prefix + self.log.status_str()\n\n", "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n\t#---------------------------------------------------------------------------\n\t#\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#---------------------------------------------------------------------------\n\t@property\n\tdef log(self):\n\t\t\"\"\"\n\t\tReturn self.log.  If not defined, create it.\n\t\t\"\"\"\n\t\tif not hasattr(self, '_log'):\n\t\t\tself._log = Log()\n\n\t\treturn self._log\n\n\t#---------------------------------------------------------------------------\n\t#\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#---------------------------------------------------------------------------\n\tdef _log_info(\n", "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n\tdef add_log(self, log):\n\t\"\"\"\n\tAdd a new log to the current log. This is useful for debugging when a\n\tsubprocess is used.\n\t\"\"\"\n\tself.log.add_log(log)\n\n\tdef set_log(self, log):\n\t\"\"\"\n\tSet the log to the given log.\n\t\"\"\"\n\tself.log = log\n\n\tdef set_log_level(self, level):\n\t\"\"\"\n\tSet the log level.\n\t\"\"\"\n\tself.log.set_level(level)\n\n\tdef set_log_max_level(self, level):\n\t\"\"\"\n\tSet the maximum log level.\n\t\"\"\"\n\tself.log.set_max_level(level)\n\n\tdef get_log_level(self):\n\t\"\"\"\n\tGet the current log level.\n\t\"\"\"\n\treturn self.log.get_level()\n\n\tdef get_log_max_level(self):\n\t\"\"\"\n\tGet the maximum log level.\n\t\"\"\"\n\treturn self.log.get_max_level()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix=prefix)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\treturn \"%s%s\" % (prefix, self.log.status_str)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix=prefix)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\treturn '%s%s' % (prefix, self.log.status_str(self))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def status_str(self, prefix=''):\n\treturn prefix + self.log.status_str()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n\t#---------------------------------------------------------------------------\n\t#\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#---------------------------------------------------------------------------\n\t@property\n\tdef log(self):\n\t\t\"\"\"\n\t\tReturn self.log.  If not defined, create it.\n\t\t\"\"\"\n\t\tif not hasattr(self, '_log'):\n\t\t\tself._log = Log()\n\n\t\treturn self._log\n\n\t#---------------------------------------------------------------------------\n\t#\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  #\n\t#---------------------------------------------------------------------------\n\tdef _log_info(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def status_str(self, prefix=''):\n\treturn self.log.status_str(prefix)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/dispositor.py", "project": "zimeon/ocfl-py", "name": "is_valid", "docstring": "Return True if identifier is valid, always True in this base implementation.", "ques_id": "62b45e135108cfac7f2109f4", "code": ["def is_valid(self, identifier):\n\treturn True\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\t\n\tdef get_description(self, identifier):\n\t\"\"\"\n\tReturn a string that describes this identifier. In this base implementation, return the identifier itself. (D)\n\t\"\"\"\n\treturn identifier\n\t\n\tdef get_identifier(self, description):\n\t\"\"\"\n\tReturn a identifier that corresponds to the description. In this base implementation, return the description itself. (D)\n\t\"\"\"\n\treturn description\n\n\tdef get_identifiers(self):\n\t\"\"\"\n\tReturn a list of all valid identifiers.  (D)\n\t\"\"\"\n\treturn []\n\t\n\tdef get_descriptions(self):\n\t\"\"\"\n\tReturn a list of all descriptions.  (D)\n\t\"\"\"\n\treturn []\n\t\n\tdef get_identifier_description_map(self):\n\t\"\"\"\n\tReturn a dictionary that maps identifiers to descriptions.  (D)\n\t\"\"\"\n\treturn {}\n\t\n\tdef get_description_identifier_map(self):\n\t\"\"\"\n\tReturn a dictionary that maps descriptions to identifiers.  (D)\n\t\"\"\"\n\treturn {}\n\t\n\tdef get_ident\n", "def is_valid(self, identifier):\n\treturn True\n\n\n", "def is_valid(self, identifier):\n\treturn True\n\n\n", "def is_valid(self, identifier):\n\treturn True\n\n", "def is_valid(self, identifier):\n\treturn True\n\n\tdef translate(self, identifier):\n\t\"\"\"\n\tReturn display name. In this base implementation, simply return identifier. (D)\n\t\"\"\"\n\treturn identifier\n\n\tdef translate_reverse(self, name):\n\t\"\"\"\n\tReturn identifier. In this base implementation, simply return name. (D)\n\t\"\"\"\n\treturn name\n", "def is_valid(self, identifier):\n\treturn True\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_valid(self, identifier):\n\treturn True\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\t\n\tdef get_description(self, identifier):\n\t\"\"\"\n\tReturn a string that describes this identifier. In this base implementation, return the identifier itself. (D)\n\t\"\"\"\n\treturn identifier\n\t\n\tdef get_identifier(self, description):\n\t\"\"\"\n\tReturn a identifier that corresponds to the description. In this base implementation, return the description itself. (D)\n\t\"\"\"\n\treturn description\n\n\tdef get_identifiers(self):\n\t\"\"\"\n\tReturn a list of all valid identifiers.  (D)\n\t\"\"\"\n\treturn []\n\t\n\tdef get_descriptions(self):\n\t\"\"\"\n\tReturn a list of all descriptions.  (D)\n\t\"\"\"\n\treturn []\n\t\n\tdef get_identifier_description_map(self):\n\t\"\"\"\n\tReturn a dictionary that maps identifiers to descriptions.  (D)\n\t\"\"\"\n\treturn {}\n\t\n\tdef get_description_identifier_map(self):\n\t\"\"\"\n\tReturn a dictionary that maps descriptions to identifiers.  (D)\n\t\"\"\"\n\treturn {}\n\t\n\tdef get_ident\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n\tdef translate(self, identifier):\n\t\"\"\"\n\tReturn display name. In this base implementation, simply return identifier. (D)\n\t\"\"\"\n\treturn identifier\n\n\tdef translate_reverse(self, name):\n\t\"\"\"\n\tReturn identifier. In this base implementation, simply return name. (D)\n\t\"\"\"\n\treturn name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_valid(self, identifier):\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate", "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.", "ques_id": "62b45e145108cfac7f210a07", "code": ["def validate(self, inventory, extract_spec_version=False):\n\treturn\n\n\n\t#\n\t# \tdef __init__(self, spec_version=None):\n\t# \t\t\"\"\"\n\t# \t\tSet the spec_version to a default value unless one is provided. (D)\n\t# \t\t\"\"\"\n\t# \t\tself.spec_version = spec_version or '1.0.0'\n\t#\n\t# \tdef validate(self, inventory, extract_spec_version=False):\n\t# \t\t\"\"\"\n\t# \t\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t# \t\t\"\"\"\n\t# \t\tif extract_spec_version:\n\t# \t\t\tif inventory.get('type', None):\n\t# \t\t\t\t# TODO: validate the type value\n\t# \t\t\t\tself.spec_version = inventory.get('type')\n\t#\n", "def validate(self, inventory, extract_spec_version=False):\n\tif not self.spec_version:\n\t\traise RuntimeError(\"Must set the spec_version member before calling validate.\")\n\tif extract_spec_version and inventory.get(\"type\", None):\n\t\tif inventory[\"type\"] == \"https://metadata.data.gov/inventory\":\n\t\t\tself.spec_version = \"1.1\"\n\t\telif inventory[\"type\"] == \"https://metadata.data.gov/inventory/v1.1\":\n\t\t\tself.spec_version = \"1.1\"\n\t\telif inventory[\"type\"] == \"https://metadata.data.gov/inventory/v1.0\":\n\t\t\tself.spec_version = \"1.0\"\n\tif self.spec_version == \"1.0\":\n\t\treturn self._validate_1_0(inventory)\n\tif self.spec_version == \"1.1\":\n\t\treturn self._validate_1_1(inventory)\n\n\traise RuntimeError(\"Unknown spec_version %s\" % self.spec_version)\n\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif \"type\" in inventory:\n\t\t\tversion = inventory.get(\"type\", \"\").split(\"/\")[-1]\n\t\t\tif version in [\"0.0.0\", \"0.1.0\", \"0.1.1\", \"0.1.2\", \"0.2.0\", \"0.2.1\", \"0.2.2\", \"0.2.3\", \"0.2.4\", \"0.2.5\", \"0.2.6\", \"0.2.7\", \"0.2.8\", \"0.2.9\", \"0.2.10\", \"0.2.11\", \"0.2.12\", \"0.2.13\", \"0.2.14\"]:\n\t\t\t\tself.spec_version = version\n\t\t\telse:\n\t\t\t\tself.logger.warning(\"Attempted to set spec_version to '%s' but not in valid range.\" % version)\n\t\telse:\n\t\t\tself.logger.warning(\"Attempted to extract spec_version from inventory\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version == True:\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] in self.spec_version_mapping:\n\t\t\t\tself.spec_version = self.spec_version_mapping[inventory['type']]\n\t\t\telse:\n\t\t\t\t# Error, type value given is not valid.\n\t\t\t\treturn False\n\t\telse:\n\t\t\t# Error, no type given.\n\t\t\treturn False\n\t\n\t# Check that the inventory is a dictionary.\n\tif not isinstance(inventory, dict):\n\t\t# Error, inventory is not a dictionary.\n\t\treturn False\n\t\n\t# Check that the inventory has the required keys.\n\tif not self.has_required_keys(inventory):\n\t\t# Error, inventory does not have the required keys.\n\t\treturn False\n\t\n\t# Check that the inventory has at least one of the required items.\n\tif not self.has_required_items(inventory):\n\t\t# Error, inventory does not have the required items.\n\t\treturn False\n\t\n\t\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\t# attempt to extract the spec_version from the inventory\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] == 'inventory.json':\n\t\t\t\tself.spec_version = '1.1.0'\n\t\t\telif inventory['type'] == 'inventory.jsonld':\n\t\t\t\tself.spec_version = '1.1.1'\n\t\t\telse:\n\t\t\t\tself.spec_version = '1.1.1'\n\t\t\t\tself.add_issue('invalid', 'type', 'value is not a valid inventory type')\n\n\t\telse:\n\t\t\tself.add_issue('missing', 'type', 'type is required')\n\t\t\tself.spec_version = '1.1.1'\n\n\tif self.spec_version == '1.1.0':\n\t\tself.validate_1_1_0(inventory)\n\telif self.spec_version == '1.1.1':\n\t\tself.validate_1_1_1(inventory)\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] in self.spec_version_map:\n\t\t\t\tspec_version = self.spec_version_map[inventory['type']]\n\t\t\telse:\n\t\t\t\traise ValidationError(\"Unknown type value: \"+inventory['type'])\n\t\telse:\n\t\t\traise ValidationError(\"No type value found in inventory\")\n\telse:\n\t\tspec_version = self.spec_version\n\t\n\tif spec_version == '1.0':\n\t\tif not isinstance(inventory, dict):\n\t\t\traise ValidationError(\"Inventory must be a dict\")\n\t\tif not self.valid_spec_version(spec_version):\n\t\t\traise ValidationError(\"Unknown spec version: \"+spec_version)\n\t\t\n\t\tif not 'type' in inventory:\n\t\t\traise ValidationError(\"Inventory missing required 'type' field\")\n\t\tif not inventory['type'] in self.spec_version_map:\n\t\t\traise ValidationError(\"\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tspec_version = self.extract_spec_version(inventory)\n\t\tif not spec_version:\n\t\t\t# no version found in inventory, so use self.spec_version\n\t\t\tspec_version = self.spec_version\n\telse:\n\t\tspec_version = self.spec_version\n\t\t\n\tif spec_version not in self.specifications:\n\t\traise OcgisException('Specification version {0} not supported.'.format(spec_version))\n\t\n\t# extract the root keyword\n\troot_keyword = self.get_root_keyword(spec_version)\n\t\n\t# get the dictionary of required keywords\n\trequired_keys = self.specifications[spec_version]['required_keys']\n\t# get the dictionary of valid keywords\n\tvalid_keys = self.specifications[spec_version]['valid_keys']\n\t\n\t# validate the inventory\n\tself.validate_inventory_structure(inventory, root_keyword, required_keys, valid_keys)\n\t\n\tif 'type' in\n", "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif \"type\" in inventory and inventory[\"type\"] in self.spec_versions:\n\t\t\tself.spec_version = inventory[\"type\"]\n\t\telse:\n\t\t\tself.spec_version = self.spec_versions[\"default\"]\n\t\t\tself.error(\"No type value found in inventory, using default specification version: %s\" % self.spec_version)\n\t\t\n\t\t\n\tif \"type\" in inventory and inventory[\"type\"] in self.spec_versions:\n\t\tif inventory[\"type\"] != self.spec_version:\n\t\t\tself.error(\"Inventory type value '%s' does not match specification version '%s'\" % (inventory[\"type\"], self.spec_version))\n\telse:\n\t\tself.error(\"Inventory is missing required field 'type'\")\n\t\n\tif \"version\" in inventory:\n\t\tif not isinstance(inventory[\"version\"], str):\n\t\t\tself.error(\"Inventory version value is not a string\")\n\telse:\n\t\tself.error(\"Inventory is missing required field 'version'\n", "def validate(self, inventory, extract_spec_version=False):\n\t# Check the type value.\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\ttype_value = inventory['type']\n\t\t\tif type_value in self.valid_type_values:\n\t\t\t\tself.spec_version = self.valid_type_values[type_value]\n\t\t\telse:\n\t\t\t\tself.add_error('Invalid type value in inventory.')\n\t\telse:\n\t\t\tself.add_error('No type value in inventory.')\n\n\t# Run the appropriate tests.\n\tif self.spec_version == '1.0.0':\n\t\tself.validate_v1(inventory)\n\telse:\n\t\tself.add_error('Unknown specification version.')\n\treturn self.errors\n\n\n\tdef validate_v1(self, inventory):\n\t\t\"\"\"\n\t\tValidate the inventory against the 1.0.0 specification. (D)\n\t\t\"\"\"\n\t\t# Check that the inventory has the required fields.\n\t\tfor field in self.required_fields_v1:\n\t\t\t\n", "def validate(self, inventory, extract_spec_version=False):\n\t\t\n\t\tself.errors = []\n\t\tself.warnings = []\n\t\t\n\t\t# check that inventory is a dict\n\t\tif not isinstance(inventory, dict):\n\t\t\tself.errors.append(\"Inventory is not a dict.\")\n\t\t\treturn False\n\t\t\n\t\t# check for required keys in inventory\n\t\tfor key in self.required_keys:\n\t\t\tif key not in inventory:\n\t\t\t\tself.errors.append(\"Inventory does not have required key: %s\" % key)\n\t\t\n\t\t# check for unexpected keys in inventory\n\t\tfor key in inventory:\n\t\t\tif key not in self.allowed_keys:\n\t\t\t\tself.warnings.append(\"Inventory has unexpected key: %s\" % key)\n\t\t\n\t\t# check that type is valid\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] not in self.allowed_types:\n\t\t\t\tself.errors.append(\"Inventory type is not valid: %s\" % inventory['type'])\n\t\t\t\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\treturn\n\n\n\t#\n\t# \tdef __init__(self, spec_version=None):\n\t# \t\t\"\"\"\n\t# \t\tSet the spec_version to a default value unless one is provided. (D)\n\t# \t\t\"\"\"\n\t# \t\tself.spec_version = spec_version or '1.0.0'\n\t#\n\t# \tdef validate(self, inventory, extract_spec_version=False):\n\t# \t\t\"\"\"\n\t# \t\tValidate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)\n\t# \t\t\"\"\"\n\t# \t\tif extract_spec_version:\n\t# \t\t\tif inventory.get('type', None):\n\t# \t\t\t\t# TODO: validate the type value\n\t# \t\t\t\tself.spec_version = inventory.get('type')\n\t#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif not self.spec_version:\n\t\traise RuntimeError(\"Must set the spec_version member before calling validate.\")\n\tif extract_spec_version and inventory.get(\"type\", None):\n\t\tif inventory[\"type\"] == \"https://metadata.data.gov/inventory\":\n\t\t\tself.spec_version = \"1.1\"\n\t\telif inventory[\"type\"] == \"https://metadata.data.gov/inventory/v1.1\":\n\t\t\tself.spec_version = \"1.1\"\n\t\telif inventory[\"type\"] == \"https://metadata.data.gov/inventory/v1.0\":\n\t\t\tself.spec_version = \"1.0\"\n\tif self.spec_version == \"1.0\":\n\t\treturn self._validate_1_0(inventory)\n\tif self.spec_version == \"1.1\":\n\t\treturn self._validate_1_1(inventory)\n\n\traise RuntimeError(\"Unknown spec_version %s\" % self.spec_version)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif \"type\" in inventory:\n\t\t\tversion = inventory.get(\"type\", \"\").split(\"/\")[-1]\n\t\t\tif version in [\"0.0.0\", \"0.1.0\", \"0.1.1\", \"0.1.2\", \"0.2.0\", \"0.2.1\", \"0.2.2\", \"0.2.3\", \"0.2.4\", \"0.2.5\", \"0.2.6\", \"0.2.7\", \"0.2.8\", \"0.2.9\", \"0.2.10\", \"0.2.11\", \"0.2.12\", \"0.2.13\", \"0.2.14\"]:\n\t\t\t\tself.spec_version = version\n\t\t\telse:\n\t\t\t\tself.logger.warning(\"Attempted to set spec_version to '%s' but not in valid range.\" % version)\n\t\telse:\n\t\t\tself.logger.warning(\"Attempted to extract spec_version from inventory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version == True:\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] in self.spec_version_mapping:\n\t\t\t\tself.spec_version = self.spec_version_mapping[inventory['type']]\n\t\t\telse:\n\t\t\t\t# Error, type value given is not valid.\n\t\t\t\treturn False\n\t\telse:\n\t\t\t# Error, no type given.\n\t\t\treturn False\n\t\n\t# Check that the inventory is a dictionary.\n\tif not isinstance(inventory, dict):\n\t\t# Error, inventory is not a dictionary.\n\t\treturn False\n\t\n\t# Check that the inventory has the required keys.\n\tif not self.has_required_keys(inventory):\n\t\t# Error, inventory does not have the required keys.\n\t\treturn False\n\t\n\t# Check that the inventory has at least one of the required items.\n\tif not self.has_required_items(inventory):\n\t\t# Error, inventory does not have the required items.\n\t\treturn False\n\t\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\t# attempt to extract the spec_version from the inventory\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] == 'inventory.json':\n\t\t\t\tself.spec_version = '1.1.0'\n\t\t\telif inventory['type'] == 'inventory.jsonld':\n\t\t\t\tself.spec_version = '1.1.1'\n\t\t\telse:\n\t\t\t\tself.spec_version = '1.1.1'\n\t\t\t\tself.add_issue('invalid', 'type', 'value is not a valid inventory type')\n\n\t\telse:\n\t\t\tself.add_issue('missing', 'type', 'type is required')\n\t\t\tself.spec_version = '1.1.1'\n\n\tif self.spec_version == '1.1.0':\n\t\tself.validate_1_1_0(inventory)\n\telif self.spec_version == '1.1.1':\n\t\tself.validate_1_1_1(inventory)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] in self.spec_version_map:\n\t\t\t\tspec_version = self.spec_version_map[inventory['type']]\n\t\t\telse:\n\t\t\t\traise ValidationError(\"Unknown type value: \"+inventory['type'])\n\t\telse:\n\t\t\traise ValidationError(\"No type value found in inventory\")\n\telse:\n\t\tspec_version = self.spec_version\n\t\n\tif spec_version == '1.0':\n\t\tif not isinstance(inventory, dict):\n\t\t\traise ValidationError(\"Inventory must be a dict\")\n\t\tif not self.valid_spec_version(spec_version):\n\t\t\traise ValidationError(\"Unknown spec version: \"+spec_version)\n\t\t\n\t\tif not 'type' in inventory:\n\t\t\traise ValidationError(\"Inventory missing required 'type' field\")\n\t\tif not inventory['type'] in self.spec_version_map:\n\t\t\traise ValidationError(\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tspec_version = self.extract_spec_version(inventory)\n\t\tif not spec_version:\n\t\t\t# no version found in inventory, so use self.spec_version\n\t\t\tspec_version = self.spec_version\n\telse:\n\t\tspec_version = self.spec_version\n\t\t\n\tif spec_version not in self.specifications:\n\t\traise OcgisException('Specification version {0} not supported.'.format(spec_version))\n\t\n\t# extract the root keyword\n\troot_keyword = self.get_root_keyword(spec_version)\n\t\n\t# get the dictionary of required keywords\n\trequired_keys = self.specifications[spec_version]['required_keys']\n\t# get the dictionary of valid keywords\n\tvalid_keys = self.specifications[spec_version]['valid_keys']\n\t\n\t# validate the inventory\n\tself.validate_inventory_structure(inventory, root_keyword, required_keys, valid_keys)\n\t\n\tif 'type' in\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\tif extract_spec_version:\n\t\tif \"type\" in inventory and inventory[\"type\"] in self.spec_versions:\n\t\t\tself.spec_version = inventory[\"type\"]\n\t\telse:\n\t\t\tself.spec_version = self.spec_versions[\"default\"]\n\t\t\tself.error(\"No type value found in inventory, using default specification version: %s\" % self.spec_version)\n\t\t\n\t\t\n\tif \"type\" in inventory and inventory[\"type\"] in self.spec_versions:\n\t\tif inventory[\"type\"] != self.spec_version:\n\t\t\tself.error(\"Inventory type value '%s' does not match specification version '%s'\" % (inventory[\"type\"], self.spec_version))\n\telse:\n\t\tself.error(\"Inventory is missing required field 'type'\")\n\t\n\tif \"version\" in inventory:\n\t\tif not isinstance(inventory[\"version\"], str):\n\t\t\tself.error(\"Inventory version value is not a string\")\n\telse:\n\t\tself.error(\"Inventory is missing required field 'version'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t# Check the type value.\n\tif extract_spec_version:\n\t\tif 'type' in inventory:\n\t\t\ttype_value = inventory['type']\n\t\t\tif type_value in self.valid_type_values:\n\t\t\t\tself.spec_version = self.valid_type_values[type_value]\n\t\t\telse:\n\t\t\t\tself.add_error('Invalid type value in inventory.')\n\t\telse:\n\t\t\tself.add_error('No type value in inventory.')\n\n\t# Run the appropriate tests.\n\tif self.spec_version == '1.0.0':\n\t\tself.validate_v1(inventory)\n\telse:\n\t\tself.add_error('Unknown specification version.')\n\treturn self.errors\n\n\n\tdef validate_v1(self, inventory):\n\t\t\"\"\"\n\t\tValidate the inventory against the 1.0.0 specification. (D)\n\t\t\"\"\"\n\t\t# Check that the inventory has the required fields.\n\t\tfor field in self.required_fields_v1:\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate(self, inventory, extract_spec_version=False):\n\t\t\n\t\tself.errors = []\n\t\tself.warnings = []\n\t\t\n\t\t# check that inventory is a dict\n\t\tif not isinstance(inventory, dict):\n\t\t\tself.errors.append(\"Inventory is not a dict.\")\n\t\t\treturn False\n\t\t\n\t\t# check for required keys in inventory\n\t\tfor key in self.required_keys:\n\t\t\tif key not in inventory:\n\t\t\t\tself.errors.append(\"Inventory does not have required key: %s\" % key)\n\t\t\n\t\t# check for unexpected keys in inventory\n\t\tfor key in inventory:\n\t\t\tif key not in self.allowed_keys:\n\t\t\t\tself.warnings.append(\"Inventory has unexpected key: %s\" % key)\n\t\t\n\t\t# check that type is valid\n\t\tif 'type' in inventory:\n\t\t\tif inventory['type'] not in self.allowed_types:\n\t\t\t\tself.errors.append(\"Inventory type is not valid: %s\" % inventory['type'])\n\t\t\t\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "check_digests_present_and_used", "docstring": "Check all digests in manifest that are needed are present and used.", "ques_id": "62b45e145108cfac7f210a09", "code": ["def check_digests_present_and_used(self, manifest_files, digests_used):\n\terror = False\n\tfor f in manifest_files:\n\t\tif f.digest_type:\n\t\t\tif not f.digest_type in digests_used:\n\t\t\t\tself.error(\"Digest type %s is not in use\" % f.digest_type)\n\t\t\t\terror = True\n\treturn error\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t# check that all digests that are present in the manifest are used\n\tfor digest in self.digests:\n\t\tif digest not in digests_used:\n\t\t\tself.error(\n\t\t\t\t\"Digest '%s' for file '%s' is not used\" % (digest, self.digests[digest]),\n\t\t\t\t\"digests-unused\",\n\t\t\t\t\"Digest '%s' for file '%s' is not used\" % (digest, self.digests[digest]),\n\t\t\t\tmanifest_files)\n\n\t# check that all digests that are needed are present in the manifest\n\tfor digest in digests_used:\n\t\tif digest not in self.digests:\n\t\t\tself.error(\n\t\t\t\t\"Digest '%s' for file '%s' is not present\" % (digest, digests_used[digest]),\n\t\t\t\t\"digests-missing\",\n\t\t\t\t\"Digest '%s' for file '%s' is not present\" % (digest, dig\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tself.error = False\n\tfor file in manifest_files:\n\t\tfor digest in digests_used:\n\t\t\tif file.has_key(digest) and file[digest] != \"\":\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tself.error = True\n\t\t\t\tprint \"File \" + file[\"filename\"] + \" is missing digest \" + digest + \".\"\n\t\t\t\tbreak\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t# Make sure all digests are used.\n\tfor mf in manifest_files:\n\t\tmanifest_digests = mf.get_manifest_digests()\n\t\tfor md in manifest_digests:\n\t\t\tif md not in digests_used:\n\t\t\t\tself.error(\"Digest %s is not used in the manifest.\" % md)\n\t\t\t\treturn False\n\t# Make sure all digests are present.\n\tfor d in digests_used:\n\t\tif not self.my_digests[d]:\n\t\t\tself.error(\"Digest %s is not present in the manifest.\" % d)\n\t\t\treturn False\n\treturn True\n\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tself.error = None\n\tself.error_msg = \"\"\n\tfor x in manifest_files:\n\t\tif x.digest_info is not None:\n\t\t\tdigest_present = False\n\t\t\tdigest_used = False\n\t\t\tfor y in x.digest_info:\n\t\t\t\tif y.digest_type in digests_used:\n\t\t\t\t\tdigest_used = True\n\t\t\t\tif y.digest_type not in digests_used:\n\t\t\t\t\tself.error_msg += \"Digest type %s not used.\\n\" % (y.digest_type)\n\t\t\t\tif y.digest_value != \"\":\n\t\t\t\t\tdigest_present = True\n\t\t\tif digest_present is False:\n\t\t\t\tself.error_msg += \"No digests present for %s.\\n\" % (x.file_name)\n\t\t\tif digest_used is False:\n\t\t\t\tself.error_msg += \"No digests used for %s.\\n\"\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t# check all digests that are needed are present\n\tfor mfile in manifest_files:\n\t\tfor dtype in ('MD5', 'SHA1', 'SHA256'):\n\t\t\tif dtype in mfile['digests']:\n\t\t\t\tif dtype not in digests_used:\n\t\t\t\t\tself.error('Digest type \"%s\" is present but not used' % dtype)\n\n\t# check all digests that are used are present\n\tfor dtype in digests_used:\n\t\tfor mfile in manifest_files:\n\t\t\tif dtype not in mfile['digests']:\n\t\t\t\tself.error('Digest type \"%s\" is used but not present' % dtype)\n\n\t# check all digests that are present are used\n\tfor dtype in ('MD5', 'SHA1', 'SHA256'):\n\t\tfor mfile in manifest_files:\n\t\t\tif dtype in mfile['digests']:\n\t\t\t\tif dtype not in digests_used:\n\t\t\t\t\tself.error('Digest type\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor mf in manifest_files:\n\t\tif (mf.required_digests != None):\n\t\t\tfor d in mf.required_digests:\n\t\t\t\tif d not in self.digests:\n\t\t\t\t\tself.error(\"Digest '%s' required by '%s' is missing.\" % (d, mf.filename))\n\t\t\t\telif d not in digests_used:\n\t\t\t\t\tself.error(\"Digest '%s' required by '%s' is not used.\" % (d, mf.filename))\n\t\t\t\telse:\n\t\t\t\t\tself.info(\"Digest '%s' required by '%s' is present and used.\" % (d, mf.filename))\n\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tself.error = \"\"\n\tself.warning = \"\"\n\tself.info = \"\"\n\tfor manifest_file in manifest_files:\n\t\tfor digest_in_manifest in manifest_file.digests_needed:\n\t\t\tif digest_in_manifest not in digests_used:\n\t\t\t\tself.error += \"Digest %s is needed by %s, but not found.\\n\" % (digest_in_manifest, manifest_file.filename)\n\t\t\telif not manifest_file.uses_digest(digest_in_manifest):\n\t\t\t\tself.warning += \"Digest %s is needed by %s, but not used.\\n\" % (digest_in_manifest, manifest_file.filename)\n\t\n\treturn self.error + self.warning + self.info\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor file_path in manifest_files:\n\t\tif file_path not in digests_used.keys():\n\t\t\terror(\"File {} not found in the downloaded files\".format(file_path))\n\t\telse:\n\t\t\tif digests_used[file_path] == False:\n\t\t\t\terror(\"File {} was downloaded, but digest in manifest was not used\".format(file_path))\n\t\t\telse:\n\t\t\t\tif digests_used[file_path] == True:\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\terror(\"File {} was downloaded, but digest in manifest was not used\".format(file_path))\n\n\treturn\n\n\n", "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\t\n\t\t# Check manifest isn't empty, if so return error() in class.\n\tif len(manifest_files) == 0:\n\t\terror(\"The manifest file is empty\")\n\t\t\n\t\t# Loop through files in manifest and check if digests are present and used.\n\tfor file_name in manifest_files:\n\t\tdigests = count_digests(manifest_files[file_name])\n\t\t# Check all digests are present\n\t\tif len(digests) < len(digest_names):\n\t\t\terror(\"Missing digests for \" + file_name)\n\t\t# Check all digests are used\n\t\tif digests != digests_used:\n\t\t\terror(\"Extra digests for \" + file_name)\n\t\t\t\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\terror = False\n\tfor f in manifest_files:\n\t\tif f.digest_type:\n\t\t\tif not f.digest_type in digests_used:\n\t\t\t\tself.error(\"Digest type %s is not in use\" % f.digest_type)\n\t\t\t\terror = True\n\treturn error\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t# check that all digests that are present in the manifest are used\n\tfor digest in self.digests:\n\t\tif digest not in digests_used:\n\t\t\tself.error(\n\t\t\t\t\"Digest '%s' for file '%s' is not used\" % (digest, self.digests[digest]),\n\t\t\t\t\"digests-unused\",\n\t\t\t\t\"Digest '%s' for file '%s' is not used\" % (digest, self.digests[digest]),\n\t\t\t\tmanifest_files)\n\n\t# check that all digests that are needed are present in the manifest\n\tfor digest in digests_used:\n\t\tif digest not in self.digests:\n\t\t\tself.error(\n\t\t\t\t\"Digest '%s' for file '%s' is not present\" % (digest, digests_used[digest]),\n\t\t\t\t\"digests-missing\",\n\t\t\t\t\"Digest '%s' for file '%s' is not present\" % (digest, dig\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tself.error = False\n\tfor file in manifest_files:\n\t\tfor digest in digests_used:\n\t\t\tif file.has_key(digest) and file[digest] != \"\":\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tself.error = True\n\t\t\t\tprint \"File \" + file[\"filename\"] + \" is missing digest \" + digest + \".\"\n\t\t\t\tbreak\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t# Make sure all digests are used.\n\tfor mf in manifest_files:\n\t\tmanifest_digests = mf.get_manifest_digests()\n\t\tfor md in manifest_digests:\n\t\t\tif md not in digests_used:\n\t\t\t\tself.error(\"Digest %s is not used in the manifest.\" % md)\n\t\t\t\treturn False\n\t# Make sure all digests are present.\n\tfor d in digests_used:\n\t\tif not self.my_digests[d]:\n\t\t\tself.error(\"Digest %s is not present in the manifest.\" % d)\n\t\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tself.error = None\n\tself.error_msg = \"\"\n\tfor x in manifest_files:\n\t\tif x.digest_info is not None:\n\t\t\tdigest_present = False\n\t\t\tdigest_used = False\n\t\t\tfor y in x.digest_info:\n\t\t\t\tif y.digest_type in digests_used:\n\t\t\t\t\tdigest_used = True\n\t\t\t\tif y.digest_type not in digests_used:\n\t\t\t\t\tself.error_msg += \"Digest type %s not used.\\n\" % (y.digest_type)\n\t\t\t\tif y.digest_value != \"\":\n\t\t\t\t\tdigest_present = True\n\t\t\tif digest_present is False:\n\t\t\t\tself.error_msg += \"No digests present for %s.\\n\" % (x.file_name)\n\t\t\tif digest_used is False:\n\t\t\t\tself.error_msg += \"No digests used for %s.\\n\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t# check all digests that are needed are present\n\tfor mfile in manifest_files:\n\t\tfor dtype in ('MD5', 'SHA1', 'SHA256'):\n\t\t\tif dtype in mfile['digests']:\n\t\t\t\tif dtype not in digests_used:\n\t\t\t\t\tself.error('Digest type \"%s\" is present but not used' % dtype)\n\n\t# check all digests that are used are present\n\tfor dtype in digests_used:\n\t\tfor mfile in manifest_files:\n\t\t\tif dtype not in mfile['digests']:\n\t\t\t\tself.error('Digest type \"%s\" is used but not present' % dtype)\n\n\t# check all digests that are present are used\n\tfor dtype in ('MD5', 'SHA1', 'SHA256'):\n\t\tfor mfile in manifest_files:\n\t\t\tif dtype in mfile['digests']:\n\t\t\t\tif dtype not in digests_used:\n\t\t\t\t\tself.error('Digest type\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor mf in manifest_files:\n\t\tif (mf.required_digests != None):\n\t\t\tfor d in mf.required_digests:\n\t\t\t\tif d not in self.digests:\n\t\t\t\t\tself.error(\"Digest '%s' required by '%s' is missing.\" % (d, mf.filename))\n\t\t\t\telif d not in digests_used:\n\t\t\t\t\tself.error(\"Digest '%s' required by '%s' is not used.\" % (d, mf.filename))\n\t\t\t\telse:\n\t\t\t\t\tself.info(\"Digest '%s' required by '%s' is present and used.\" % (d, mf.filename))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tself.error = \"\"\n\tself.warning = \"\"\n\tself.info = \"\"\n\tfor manifest_file in manifest_files:\n\t\tfor digest_in_manifest in manifest_file.digests_needed:\n\t\t\tif digest_in_manifest not in digests_used:\n\t\t\t\tself.error += \"Digest %s is needed by %s, but not found.\\n\" % (digest_in_manifest, manifest_file.filename)\n\t\t\telif not manifest_file.uses_digest(digest_in_manifest):\n\t\t\t\tself.warning += \"Digest %s is needed by %s, but not used.\\n\" % (digest_in_manifest, manifest_file.filename)\n\t\n\treturn self.error + self.warning + self.info\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\tfor file_path in manifest_files:\n\t\tif file_path not in digests_used.keys():\n\t\t\terror(\"File {} not found in the downloaded files\".format(file_path))\n\t\telse:\n\t\t\tif digests_used[file_path] == False:\n\t\t\t\terror(\"File {} was downloaded, but digest in manifest was not used\".format(file_path))\n\t\t\telse:\n\t\t\t\tif digests_used[file_path] == True:\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\terror(\"File {} was downloaded, but digest in manifest was not used\".format(file_path))\n\n\treturn\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def check_digests_present_and_used(self, manifest_files, digests_used):\n\t\t\n\t\t# Check manifest isn't empty, if so return error() in class.\n\tif len(manifest_files) == 0:\n\t\terror(\"The manifest file is empty\")\n\t\t\n\t\t# Loop through files in manifest and check if digests are present and used.\n\tfor file_name in manifest_files:\n\t\tdigests = count_digests(manifest_files[file_name])\n\t\t# Check all digests are present\n\t\tif len(digests) < len(digest_names):\n\t\t\terror(\"Missing digests for \" + file_name)\n\t\t# Check all digests are used\n\t\tif digests != digests_used:\n\t\t\terror(\"Extra digests for \" + file_name)\n\t\t\t\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_as_prior_version", "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.", "ques_id": "62b45e165108cfac7f210a16", "code": ["def validate_as_prior_version(self, prior):\n\terr = self.error()\n\tif not isinstance(prior, InventoryValidator):\n\t\terr.log(\"Passed object is not an InventoryValidator object\")\n\t\treturn err\n\tif not self.is_valid():\n\t\terr.log(\"The current inventory object is not valid\")\n\t\treturn err\n\tif not prior.is_valid():\n\t\terr.log(\"The prior inventory object is not valid\")\n\t\treturn err\n\n\tif prior.version_major > self.version_major:\n\t\terr.log(\"Prior version major is greater than the current version major\")\n\t\treturn err\n\tif prior.version_major == self.version_major:\n\t\tif prior.version_minor > self.version_minor:\n\t\t\terr.log(\"Prior version minor is greater than the current version minor\")\n\t\t\treturn err\n\t\tif prior.version_minor == self.version_minor:\n\t\t\tif prior.version_patch > self.version_patch:\n\t\t\t\terr.log(\"Prior version patch is greater than the current version patch\")\n\t\n", "def validate_as_prior_version(self, prior):\n\terror = None\n\tif self.is_valid_prior_version(prior):\n\t\t# check that the inventory has not been modified (i.e. items added or removed)\n\t\tif not self.has_same_items(prior):\n\t\t\terror = self.error(self.ITEMS_DIFFER_FROM_PRIOR)\n\t\t# check that all items in the current inventory have the same quantity as the prior inventory\n\t\tif error is None and not self.has_same_quantities(prior):\n\t\t\terror = self.error(self.QUANTITIES_DIFFER_FROM_PRIOR)\n\telse:\n\t\terror = self.error(self.INVALID_PRIOR_VERSION)\n\treturn error\n\n\tdef is_valid_prior_version(self, prior):\n\t\t\"\"\"\n\t\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency.\n\t\t\"\"\"\n\t\treturn (self\n", "def validate_as_prior_version(self, prior):\n\tif not prior.is_valid:\n\t\tself.error = \"Prior version of inventory must be valid.\"\n\t\treturn False\n\tif self.error:\n\t\treturn False\n\telif (self.inventory.get_version() - prior.inventory.get_version() != 1):\n\t\tself.error = \"Current inventory version must be incremented by one from prior version\"\n\t\treturn False\n\telif self.inventory.get_date_version() <= prior.inventory.get_date_version():\n\t\tself.error = \"Current inventory date must be later than prior inventory date\"\n\t\treturn False\n\telif self.inventory.get_date_version() > datetime.datetime.utcnow():\n\t\tself.error = \"Current inventory date cannot be in the future\"\n\t\treturn False\n\telif self.inventory.get_node_id() != prior.inventory.get_node_id():\n\t\tself.error = \"Current inventory node ID must be the same as prior inventory node ID\"\n\t\treturn False\n\telif self.inventory.get_transaction_id() <=\n", "def validate_as_prior_version(self, prior):\n\tif not self.check_as_prior_version(prior):\n\t\tself.error(\"The prior version of this inventory is not valid.\")\n\t\treturn False\n\treturn True\n\t\n", "def validate_as_prior_version(self, prior):\n\terror = self.error()\n\tif prior.version == self.version:\n\t\terror.add(\"Inventory version has not changed.\")\n\telif prior.version > self.version:\n\t\terror.add(\"Inventory version has been downgraded.\")\n\telif prior.version < self.version:\n\t\tif self.version != prior.version + 1:\n\t\t\terror.add(\"Inventory version has been increased by more than one.\")\n\t\telif self.version < 5:\n\t\t\terror.add(\"Inventory version less than 5.\")\n\t\telif self.version >= 6:\n\t\t\t# Check that any changes are legitimate\n\t\t\tif prior.v6_national_accounting_identifier != self.v6_national_accounting_identifier:\n\t\t\t\terror.add(\"National accounting identifier has been changed.\")\n\t\t\tif prior.v6_national_accounting_identifier == self.v6_national_accounting_identifier and (prior.v6_national_accounting_identifier_description != self.v6_national_accounting_\n", "def validate_as_prior_version(self, prior):\n\t# Check that prior is a valid prior version of the current inventory object.\n\tif not prior.is_prior_version(self):\n\t\terror(prior.error())\n\t\treturn False\n\treturn True\n\n\t###\n\n", "def validate_as_prior_version(self, prior):\n\t# Check that the current inventory has everything from the prior inventory and that it has not been changed.\n\tfor item in prior.inventory:\n\t\tif not item in self.inventory:\n\t\t\tself.error.append(\"Item {} is missing\".format(item))\n\t\telse:\n\t\t\tif not prior.inventory[item] == self.inventory[item]:\n\t\t\t\tself.error.append(\"Item {} has changed\".format(item))\n\t\t\t\t\n\t# Check that the current inventory is the same size as the prior inventory.\n\tif not len(prior.inventory) == len(self.inventory):\n\t\tself.error.append(\"Inventory has changed size\")\n\t\t\n\t# Check that the current inventory has the same number of items in each category as the prior inventory.\n\tfor category in prior.categories:\n\t\tif not len(prior.categories[category]) == len(self.categories[category]):\n\t\t\tself.error.append(\"Category {} has changed size\".format(category))\n\t\t\t\n\t# Check that the current inventory has the same number of\n", "def validate_as_prior_version(self, prior):\n\t# The prior version should be of the same type as the current version\n\tif self.type != prior.type:\n\t\terror('Inventory type mismatch: ' + str(self.type) + ' != ' + str(prior.type))\n\t# The prior inventory should be a valid version of the current inventory\n\tif self.version < prior.version:\n\t\terror('Inventory version mismatch: ' + str(self.version) + ' < ' + str(prior.version))\n\telif self.version == prior.version:\n\t\terror('Inventory version mismatch: ' + str(self.version) + ' == ' + str(prior.version))\n\telif self.version - prior.version > 1:\n\t\terror('Inventory version mismatch: ' + str(self.version) + ' - ' + str(prior.version) + ' > 1')\n\t# The prior inventory should have the same simulation parameters as the current inventory\n\tif self.parameters != prior.parameters:\n\t\terror('Inventory parameter mismatch: ' + str(self.parameters) + ' != ' + str\n", "def validate_as_prior_version(self, prior):\n\tif prior.get_version() is None:\n\t\treturn self.error(\"Unversioned prior versions are not allowed.\")\n\tif self.get_version() is None:\n\t\treturn self.error(\"Unversioned current versions are not allowed.\")\n\tif not self.get_version().is_ancestor_of(prior.get_version()):\n\t\treturn self.error(\"Prior version {} is not a valid prior version of current version {}.\".format(prior.get_version(), self.get_version()))\n\tfor prior_element in prior.get_elements():\n\t\tif prior_element.get_name() not in self.get_element_names():\n\t\t\treturn self.error(\"Element {} in prior version {} is not present in current version {}.\".format(prior_element.get_name(), prior.get_version(), self.get_version()))\n\t\tcurrent_element = self.get_element(prior_element.get_name())\n\t\tif current_element.get_type() != prior_element.get_type():\n\t\t\treturn self.error\n", "def validate_as_prior_version(self, prior):\n\tif self.version_number == prior.version_number:\n\t\tself.error = \"Inconsistent version numbers\"\n\t\treturn self.error\n\tif self.version_number < prior.version_number:\n\t\tself.error = \"Version number of prior is higher than current\"\n\t\treturn self.error\n\tif self.version_number != prior.version_number + 1:\n\t\tself.error = \"Version number of prior is not exactly one less than current\"\n\t\treturn self.error\n\tif self.inventory_date < prior.inventory_date:\n\t\tself.error = \"Inventory date of prior is later than current\"\n\t\treturn self.error\n\tif self.inventory_date == prior.inventory_date and self.version_number != prior.version_number + 1:\n\t\tself.error = \"Inventory date of prior is same as current and version number is not exactly one less than current\"\n\t\treturn self.error\n\tif self.inventory_date == prior.inventory_date and self.version_number == prior.version_number + 1 and self.version_date <\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_as_prior_version(self, prior):\n\terr = self.error()\n\tif not isinstance(prior, InventoryValidator):\n\t\terr.log(\"Passed object is not an InventoryValidator object\")\n\t\treturn err\n\tif not self.is_valid():\n\t\terr.log(\"The current inventory object is not valid\")\n\t\treturn err\n\tif not prior.is_valid():\n\t\terr.log(\"The prior inventory object is not valid\")\n\t\treturn err\n\n\tif prior.version_major > self.version_major:\n\t\terr.log(\"Prior version major is greater than the current version major\")\n\t\treturn err\n\tif prior.version_major == self.version_major:\n\t\tif prior.version_minor > self.version_minor:\n\t\t\terr.log(\"Prior version minor is greater than the current version minor\")\n\t\t\treturn err\n\t\tif prior.version_minor == self.version_minor:\n\t\t\tif prior.version_patch > self.version_patch:\n\t\t\t\terr.log(\"Prior version patch is greater than the current version patch\")\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\terror = None\n\tif self.is_valid_prior_version(prior):\n\t\t# check that the inventory has not been modified (i.e. items added or removed)\n\t\tif not self.has_same_items(prior):\n\t\t\terror = self.error(self.ITEMS_DIFFER_FROM_PRIOR)\n\t\t# check that all items in the current inventory have the same quantity as the prior inventory\n\t\tif error is None and not self.has_same_quantities(prior):\n\t\t\terror = self.error(self.QUANTITIES_DIFFER_FROM_PRIOR)\n\telse:\n\t\terror = self.error(self.INVALID_PRIOR_VERSION)\n\treturn error\n\n\tdef is_valid_prior_version(self, prior):\n\t\t\"\"\"\n\t\tCheck that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency.\n\t\t\"\"\"\n\t\treturn (self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tif not prior.is_valid:\n\t\tself.error = \"Prior version of inventory must be valid.\"\n\t\treturn False\n\tif self.error:\n\t\treturn False\n\telif (self.inventory.get_version() - prior.inventory.get_version() != 1):\n\t\tself.error = \"Current inventory version must be incremented by one from prior version\"\n\t\treturn False\n\telif self.inventory.get_date_version() <= prior.inventory.get_date_version():\n\t\tself.error = \"Current inventory date must be later than prior inventory date\"\n\t\treturn False\n\telif self.inventory.get_date_version() > datetime.datetime.utcnow():\n\t\tself.error = \"Current inventory date cannot be in the future\"\n\t\treturn False\n\telif self.inventory.get_node_id() != prior.inventory.get_node_id():\n\t\tself.error = \"Current inventory node ID must be the same as prior inventory node ID\"\n\t\treturn False\n\telif self.inventory.get_transaction_id() <=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tif not self.check_as_prior_version(prior):\n\t\tself.error(\"The prior version of this inventory is not valid.\")\n\t\treturn False\n\treturn True\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\terror = self.error()\n\tif prior.version == self.version:\n\t\terror.add(\"Inventory version has not changed.\")\n\telif prior.version > self.version:\n\t\terror.add(\"Inventory version has been downgraded.\")\n\telif prior.version < self.version:\n\t\tif self.version != prior.version + 1:\n\t\t\terror.add(\"Inventory version has been increased by more than one.\")\n\t\telif self.version < 5:\n\t\t\terror.add(\"Inventory version less than 5.\")\n\t\telif self.version >= 6:\n\t\t\t# Check that any changes are legitimate\n\t\t\tif prior.v6_national_accounting_identifier != self.v6_national_accounting_identifier:\n\t\t\t\terror.add(\"National accounting identifier has been changed.\")\n\t\t\tif prior.v6_national_accounting_identifier == self.v6_national_accounting_identifier and (prior.v6_national_accounting_identifier_description != self.v6_national_accounting_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t# Check that prior is a valid prior version of the current inventory object.\n\tif not prior.is_prior_version(self):\n\t\terror(prior.error())\n\t\treturn False\n\treturn True\n\n\t###\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t# Check that the current inventory has everything from the prior inventory and that it has not been changed.\n\tfor item in prior.inventory:\n\t\tif not item in self.inventory:\n\t\t\tself.error.append(\"Item {} is missing\".format(item))\n\t\telse:\n\t\t\tif not prior.inventory[item] == self.inventory[item]:\n\t\t\t\tself.error.append(\"Item {} has changed\".format(item))\n\t\t\t\t\n\t# Check that the current inventory is the same size as the prior inventory.\n\tif not len(prior.inventory) == len(self.inventory):\n\t\tself.error.append(\"Inventory has changed size\")\n\t\t\n\t# Check that the current inventory has the same number of items in each category as the prior inventory.\n\tfor category in prior.categories:\n\t\tif not len(prior.categories[category]) == len(self.categories[category]):\n\t\t\tself.error.append(\"Category {} has changed size\".format(category))\n\t\t\t\n\t# Check that the current inventory has the same number of\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\t# The prior version should be of the same type as the current version\n\tif self.type != prior.type:\n\t\terror('Inventory type mismatch: ' + str(self.type) + ' != ' + str(prior.type))\n\t# The prior inventory should be a valid version of the current inventory\n\tif self.version < prior.version:\n\t\terror('Inventory version mismatch: ' + str(self.version) + ' < ' + str(prior.version))\n\telif self.version == prior.version:\n\t\terror('Inventory version mismatch: ' + str(self.version) + ' == ' + str(prior.version))\n\telif self.version - prior.version > 1:\n\t\terror('Inventory version mismatch: ' + str(self.version) + ' - ' + str(prior.version) + ' > 1')\n\t# The prior inventory should have the same simulation parameters as the current inventory\n\tif self.parameters != prior.parameters:\n\t\terror('Inventory parameter mismatch: ' + str(self.parameters) + ' != ' + str\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tif prior.get_version() is None:\n\t\treturn self.error(\"Unversioned prior versions are not allowed.\")\n\tif self.get_version() is None:\n\t\treturn self.error(\"Unversioned current versions are not allowed.\")\n\tif not self.get_version().is_ancestor_of(prior.get_version()):\n\t\treturn self.error(\"Prior version {} is not a valid prior version of current version {}.\".format(prior.get_version(), self.get_version()))\n\tfor prior_element in prior.get_elements():\n\t\tif prior_element.get_name() not in self.get_element_names():\n\t\t\treturn self.error(\"Element {} in prior version {} is not present in current version {}.\".format(prior_element.get_name(), prior.get_version(), self.get_version()))\n\t\tcurrent_element = self.get_element(prior_element.get_name())\n\t\tif current_element.get_type() != prior_element.get_type():\n\t\t\treturn self.error\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_as_prior_version(self, prior):\n\tif self.version_number == prior.version_number:\n\t\tself.error = \"Inconsistent version numbers\"\n\t\treturn self.error\n\tif self.version_number < prior.version_number:\n\t\tself.error = \"Version number of prior is higher than current\"\n\t\treturn self.error\n\tif self.version_number != prior.version_number + 1:\n\t\tself.error = \"Version number of prior is not exactly one less than current\"\n\t\treturn self.error\n\tif self.inventory_date < prior.inventory_date:\n\t\tself.error = \"Inventory date of prior is later than current\"\n\t\treturn self.error\n\tif self.inventory_date == prior.inventory_date and self.version_number != prior.version_number + 1:\n\t\tself.error = \"Inventory date of prior is same as current and version number is not exactly one less than current\"\n\t\treturn self.error\n\tif self.inventory_date == prior.inventory_date and self.version_number == prior.version_number + 1 and self.version_date <\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "get_logical_path_map", "docstring": "Get a map of logical paths in state to files on disk for version in inventory.\n\nReturns a dictionary: logical_path_in_state -> set(content_files)\n\nThe set of content_files may includes references to duplicate files in\nlater versions than the version being described.", "ques_id": "62b45e165108cfac7f210a17", "code": ["def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor state_code, state_name in inventory.items():\n\t\tlogical_path_map[state_code] = \\\n\t\t\tos.path.join(os.path.sep, \"data\", \"states\", version, state_code + \".geojson\")\n\treturn logical_path_map\n\n\n", "def get_logical_path_map(inventory, version):\n\t# Get the inventory file\n\tfile_path = inventory.get_inventory_file_path(version)\n\n\t# Store the file path of the states in the list type.\n\tlogical_path_map = {}\n\twith open(file_path, 'r') as f:\n\t\treader = csv.reader(f)\n\t\tfor row in reader:\n\t\t\tlogical_path_map[row[0]] = row[1]\n\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_paths = {}\n\tfor state in inventory:\n\t\tlogical_paths[state] = get_logical_path(state, version)\n\treturn logical_paths\n\n", "def get_logical_path_map(inventory, version):\n\tif version == 1:\n\t\treturn get_logical_path_map_v1(inventory)\n\telif version == 2:\n\t\treturn get_logical_path_map_v2(inventory)\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_map = {}\n\tfor state in inventory:\n\t\tlogical_map[state[\"title\"]] = state[\"file\"]\n\t# print(logical_map)\n\treturn logical_map\n\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor state in inventory['states']:\n\t\tlogical_path_map[state['path']] = os.path.join(version['path'], state['file_path'])\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor inventory_path in inventory.keys():\n\t\tinventory_dict = inventory[inventory_path]\n\t\tstate_name = inventory_dict['state_name']\n\t\tlogical_path = inventory_dict['logical_path']\n\t\tif logical_path_map.has_key(logical_path):\n\t\t\tlogical_path_map[logical_path].append((inventory_path, state_name))\n\t\telse:\n\t\t\tlogical_path_map[logical_path] = [(inventory_path, state_name)]\n\treturn logical_path_map\n\n\n", "def get_logical_path_map(inventory, version):\n\t\n\tlogical_path_map = {}\n\t\n\tfor state in inventory.states:\n\t\tlogical_path_map[state.name] = get_logical_path(state.name, version)\n\t\n\treturn logical_path_map\n\n", "def get_logical_path_map(inventory, version):\n\tpath_map = {}\n\n\tfor i in range(len(inventory)):\n\t\tif version == 'v1':\n\t\t\tpath_map[inventory[i]['state']] = inventory[i]['path']\n\t\telif version == 'v2':\n\t\t\tpath_map[inventory[i]['name']] = inventory[i]['path']\n\t\telse:\n\t\t\traise ValueError('invalid version')\n\n\treturn path_map\n\n\n", "def get_logical_path_map(inventory, version):\n\tlogical_path_map = dict()\n\tfor state in inventory.states:\n\t\tpath = state.get_logical_path(version)\n\t\tlogical_path_map[path] = state.path\n\treturn logical_path_map\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor state_code, state_name in inventory.items():\n\t\tlogical_path_map[state_code] = \\\n\t\t\tos.path.join(os.path.sep, \"data\", \"states\", version, state_code + \".geojson\")\n\treturn logical_path_map\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t# Get the inventory file\n\tfile_path = inventory.get_inventory_file_path(version)\n\n\t# Store the file path of the states in the list type.\n\tlogical_path_map = {}\n\twith open(file_path, 'r') as f:\n\t\treader = csv.reader(f)\n\t\tfor row in reader:\n\t\t\tlogical_path_map[row[0]] = row[1]\n\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_paths = {}\n\tfor state in inventory:\n\t\tlogical_paths[state] = get_logical_path(state, version)\n\treturn logical_paths\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tif version == 1:\n\t\treturn get_logical_path_map_v1(inventory)\n\telif version == 2:\n\t\treturn get_logical_path_map_v2(inventory)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_map = {}\n\tfor state in inventory:\n\t\tlogical_map[state[\"title\"]] = state[\"file\"]\n\t# print(logical_map)\n\treturn logical_map\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor state in inventory['states']:\n\t\tlogical_path_map[state['path']] = os.path.join(version['path'], state['file_path'])\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = {}\n\tfor inventory_path in inventory.keys():\n\t\tinventory_dict = inventory[inventory_path]\n\t\tstate_name = inventory_dict['state_name']\n\t\tlogical_path = inventory_dict['logical_path']\n\t\tif logical_path_map.has_key(logical_path):\n\t\t\tlogical_path_map[logical_path].append((inventory_path, state_name))\n\t\telse:\n\t\t\tlogical_path_map[logical_path] = [(inventory_path, state_name)]\n\treturn logical_path_map\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\t\n\tlogical_path_map = {}\n\t\n\tfor state in inventory.states:\n\t\tlogical_path_map[state.name] = get_logical_path(state.name, version)\n\t\n\treturn logical_path_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tpath_map = {}\n\n\tfor i in range(len(inventory)):\n\t\tif version == 'v1':\n\t\t\tpath_map[inventory[i]['state']] = inventory[i]['path']\n\t\telif version == 'v2':\n\t\t\tpath_map[inventory[i]['name']] = inventory[i]['path']\n\t\telse:\n\t\t\traise ValueError('invalid version')\n\n\treturn path_map\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_logical_path_map(inventory, version):\n\tlogical_path_map = dict()\n\tfor state in inventory.states:\n\t\tpath = state.get_logical_path(version)\n\t\tlogical_path_map[path] = state.path\n\treturn logical_path_map\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/inventory_validator.py", "project": "zimeon/ocfl-py", "name": "validate_fixity", "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.", "ques_id": "62b45e175108cfac7f210a19", "code": ["def validate_fixity(self, fixity, manifest_files):\n\tif fixity is None:\n\t\t# No fixity block\n\t\treturn\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"Fixity block is not a dictionary\")\n\t\treturn\n\tfor file_id in fixity:\n\t\tfile_entry = fixity[file_id]\n\t\tif not isinstance(file_entry, dict):\n\t\t\tself.error(\"Fixity entry for %s is not a dictionary\" % file_id)\n\t\t\tcontinue\n\t\tfor algorithm in file_entry:\n\t\t\tdigest = file_entry[algorithm]\n\t\t\tif not isinstance(digest, basestring):\n\t\t\t\tself.error(\"Digest for %s and algorithm %s is not a string\" % (file_id, algorithm))\n\t\t\t\tcontinue\n\t\t\tif not is_valid_digest(algorithm, digest):\n\t\t\t\tself.error(\"Digest for %s and algorithm %s is not a valid %s digest\" % (file_id, algorithm, algorithm))\n\t\t\t\tcontinue\n\t\t\n", "def validate_fixity(self, fixity, manifest_files):\n\t\n\tif not isinstance(fixity, dict):\n\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity block is not a dictionary.')\n\t\treturn\n\n\tif fixity.get('checksum') == None:\n\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity block does not contain a checksum.')\n\t\treturn\n\n\tif not isinstance(fixity['checksum'], dict):\n\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity checksum block is not a dictionary.')\n\t\treturn\n\n\tfor algorithm in fixity['checksum']:\n\t\tif algorithm not in ('md5', 'sha1', 'sha256', 'sha512'):\n\t\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity checksum algorithm \"' + algorithm + '\" is not allowed.')\n\t\t\treturn\n\n\t\tfor filepath, file_checksum in fixity['checksum'][algorithm].items():\n\t\t\tif\n", "def validate_fixity(self, fixity, manifest_files):\n\t\n\tfor line in fixity:\n\t\tif line[0] == \"manifest\":\n\t\t\tif line[1] == \"1\":\n\t\t\t\tself.fixity_manifest = True\n\t\t\telse:\n\t\t\t\tself.fixity_manifest = False\n\t\t\t\tself.error(\"Fixity manifest must be 1\")\n\t\t\t\treturn\n\t\telif line[0] == \"block_size\":\n\t\t\ttry:\n\t\t\t\tself.block_size = int(line[1])\n\t\t\texcept ValueError:\n\t\t\t\tself.error(\"Block size must be an integer\")\n\t\t\t\treturn\n\t\telif line[0] == \"hash\":\n\t\t\tif line[1] in self.supported_hashes:\n\t\t\t\tself.hash_type = line[1]\n\t\t\telse:\n\t\t\t\tself.error(\"Hash type {} not supported\".format(line[1]))\n\t\t\t\treturn\n\t\telif line[0] == \"file\":\n\t\t\tif line\n", "def validate_fixity(self, fixity, manifest_files):\n\tself.validate_fixity_structure(fixity)\n\tself.validate_fixity_files(fixity, manifest_files)\n\n\treturn self.error\n\n\n", "def validate_fixity(self, fixity, manifest_files):\n\t# Check if the fixity block is a list\n\tif not isinstance(fixity, list):\n\t\tself.error = \"Fixity block not a list.\"\n\t\treturn False\n\t# Check if each element in the fixity block is a dictionary\n\tfor f in fixity:\n\t\tif not isinstance(f, dict):\n\t\t\tself.error = \"Elements in fixity block not dictionaries.\"\n\t\t\treturn False\n\t# Check if each element in the fixity block has only two keys\n\tfor f in fixity:\n\t\tif len(f) != 2:\n\t\t\tself.error = \"Elements in fixity block do not have two keys.\"\n\t\t\treturn False\n\t# Check if each element in the fixity block has the keys 'filename' and 'digest'\n\tfor f in fixity:\n\t\tif 'filename' not in f:\n\t\t\tself.error = \"Key 'filename' not in fixity block.\"\n\t\t\treturn False\n\t\tif 'digest' not in f:\n\t\t\tself.error = \"Key 'digest\n", "def validate_fixity(self, fixity, manifest_files):\n\t\n\t# Make sure it's a dictionary\n\tif type(fixity) != dict:\n\t\tself.errors += 1\n\t\tself.errmsg.append(\"Fixity block is not a dictionary.\")\n\t\treturn\n\t\n\t# Check that it's the right size\n\tif len(fixity) == 0:\n\t\tself.errors += 1\n\t\tself.errmsg.append(\"Fixity block is empty.\")\n\t\treturn\n\t\n\t# Check that the checksums are in the manifest\n\tfor checksum in fixity.keys():\n\t\tif checksum not in manifest_files.keys():\n\t\t\tself.errors += 1\n\t\t\tself.errmsg.append(\"Fixity block refers to checksum that is not in the manifest: \" + checksum)\n\t\t\treturn\n\t\t\n\t\t# Check that the checksum's value is a list\n\t\tif type(fixity[checksum]) != list:\n\t\t\tself.errors += 1\n\t\t\tself.errmsg.append(\"Fixity block value for checksum is not a list: \" + checksum)\n\t\t\n", "def validate_fixity(self, fixity, manifest_files):\n\ttry:\n\t\tif fixity:\n\t\t\tif 'fixity-algorithm' in fixity:\n\t\t\t\tif fixity['fixity-algorithm'] in constants.fixity_algos:\n\t\t\t\t\tif 'fixity-value' in fixity:\n\t\t\t\t\t\tif type(fixity['fixity-value']) == str or type(fixity['fixity-value']) == unicode:\n\t\t\t\t\t\t\tif 'fixity-date' in fixity:\n\t\t\t\t\t\t\t\tif type(fixity['fixity-date']) == str or type(fixity['fixity-date']) == unicode:\n\t\t\t\t\t\t\t\t\tif 'filename' in fixity:\n\t\t\t\t\t\t\t\t\t\tif fixity['filename'] in manifest_files:\n\t\t\t\t\t\t\t\t\t\t\treturn True\n\t\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\tself.error('Fixity filename is not listed in manifest file.')\n\n", "def validate_fixity(self, fixity, manifest_files):\n\t\n\t# validate fixity block\n\tif fixity is None:\n\t\terror('no fixity block')\n\tif not isinstance(fixity, dict):\n\t\terror('fixity is not a dictionary')\n\tif 'algorithm' not in fixity:\n\t\terror('no fixity algorithm listed')\n\t\t\n\t# validate fixity algorithm\n\talgorithm = fixity['algorithm']\n\tif algorithm not in ['md5', 'sha1']:\n\t\terror('fixity algorithm not supported: '+algorithm)\n\t\n\t# validate fixity files\n\tif 'files' not in fixity:\n\t\terror('no fixity files listed')\n\tif not isinstance(fixity['files'], list):\n\t\terror('fixity files is not a list')\n\t\n\t# validate each fixity file entry\n\tfor file in fixity['files']:\n\t\tif not isinstance(file, dict):\n\t\t\terror('fixity file is not a dictionary')\n\t\tif 'filename' not in file:\n\t\t\terror('no fixity filename listed')\n\t\n", "def validate_fixity(self, fixity, manifest_files):\n\tif 'files' not in fixity:\n\t\tself.error('Fixity block is missing the \"files\" element.')\n\t\treturn False\n\tif 'checksums' not in fixity:\n\t\tself.error('Fixity block is missing the \"checksums\" element.')\n\t\treturn False\n\n\tif not isinstance(fixity['files'], list):\n\t\tself.error('Fixity \"files\" element is not a list.')\n\t\treturn False\n\tif not isinstance(fixity['checksums'], dict):\n\t\tself.error('Fixity \"checksums\" element is not a dictionary.')\n\t\treturn False\n\n\tfor file in fixity['files']:\n\t\tif not isinstance(file, dict):\n\t\t\tself.error('Fixity file element is not a dictionary.')\n\t\t\treturn False\n\t\tif 'filename' not in file:\n\t\t\tself.error('Fixity file element is missing the \"filename\" element.')\n\t\t\treturn False\n\t\tif 'checksum' not in file:\n\t\t\tself.error('Fix\n", "def validate_fixity(self, fixity, manifest_files):\n\t# Verify that the fixity block is a list\n\tif type(fixity) != list:\n\t\tself.error(msg='Error: Fixity block is not a list.', code=40)\n\n\t# Verify that fixity block is not empty\n\tif not fixity:\n\t\tself.error(msg='Error: Fixity block is empty.', code=41)\n\n\t# Verify that each entry in the fixity block is a dictionary\n\tfor entry in fixity:\n\t\tif type(entry) != dict:\n\t\t\tself.error(msg='Error: Fixity block entry is not a dictionary.', code=42)\n\n\t# Verify that each entry in the fixity block has the correct keys\n\tfor entry in fixity:\n\t\tif sorted(list(entry.keys())) != sorted(['algorithm', 'digest', 'path']):\n\t\t\tself.error(msg='Error: Fixity block entry is missing a required key.', code=43)\n\n\t# Verify that each entry in the fixity block has a path that is in the manifest\n\tfor entry in fixity:\n\t\tif\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tif fixity is None:\n\t\t# No fixity block\n\t\treturn\n\tif not isinstance(fixity, dict):\n\t\tself.error(\"Fixity block is not a dictionary\")\n\t\treturn\n\tfor file_id in fixity:\n\t\tfile_entry = fixity[file_id]\n\t\tif not isinstance(file_entry, dict):\n\t\t\tself.error(\"Fixity entry for %s is not a dictionary\" % file_id)\n\t\t\tcontinue\n\t\tfor algorithm in file_entry:\n\t\t\tdigest = file_entry[algorithm]\n\t\t\tif not isinstance(digest, basestring):\n\t\t\t\tself.error(\"Digest for %s and algorithm %s is not a string\" % (file_id, algorithm))\n\t\t\t\tcontinue\n\t\t\tif not is_valid_digest(algorithm, digest):\n\t\t\t\tself.error(\"Digest for %s and algorithm %s is not a valid %s digest\" % (file_id, algorithm, algorithm))\n\t\t\t\tcontinue\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\n\tif not isinstance(fixity, dict):\n\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity block is not a dictionary.')\n\t\treturn\n\n\tif fixity.get('checksum') == None:\n\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity block does not contain a checksum.')\n\t\treturn\n\n\tif not isinstance(fixity['checksum'], dict):\n\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity checksum block is not a dictionary.')\n\t\treturn\n\n\tfor algorithm in fixity['checksum']:\n\t\tif algorithm not in ('md5', 'sha1', 'sha256', 'sha512'):\n\t\t\tself.error(self.inventory_filepath, 'fixity', 'Inventory fixity checksum algorithm \"' + algorithm + '\" is not allowed.')\n\t\t\treturn\n\n\t\tfor filepath, file_checksum in fixity['checksum'][algorithm].items():\n\t\t\tif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\n\tfor line in fixity:\n\t\tif line[0] == \"manifest\":\n\t\t\tif line[1] == \"1\":\n\t\t\t\tself.fixity_manifest = True\n\t\t\telse:\n\t\t\t\tself.fixity_manifest = False\n\t\t\t\tself.error(\"Fixity manifest must be 1\")\n\t\t\t\treturn\n\t\telif line[0] == \"block_size\":\n\t\t\ttry:\n\t\t\t\tself.block_size = int(line[1])\n\t\t\texcept ValueError:\n\t\t\t\tself.error(\"Block size must be an integer\")\n\t\t\t\treturn\n\t\telif line[0] == \"hash\":\n\t\t\tif line[1] in self.supported_hashes:\n\t\t\t\tself.hash_type = line[1]\n\t\t\telse:\n\t\t\t\tself.error(\"Hash type {} not supported\".format(line[1]))\n\t\t\t\treturn\n\t\telif line[0] == \"file\":\n\t\t\tif line\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tself.validate_fixity_structure(fixity)\n\tself.validate_fixity_files(fixity, manifest_files)\n\n\treturn self.error\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t# Check if the fixity block is a list\n\tif not isinstance(fixity, list):\n\t\tself.error = \"Fixity block not a list.\"\n\t\treturn False\n\t# Check if each element in the fixity block is a dictionary\n\tfor f in fixity:\n\t\tif not isinstance(f, dict):\n\t\t\tself.error = \"Elements in fixity block not dictionaries.\"\n\t\t\treturn False\n\t# Check if each element in the fixity block has only two keys\n\tfor f in fixity:\n\t\tif len(f) != 2:\n\t\t\tself.error = \"Elements in fixity block do not have two keys.\"\n\t\t\treturn False\n\t# Check if each element in the fixity block has the keys 'filename' and 'digest'\n\tfor f in fixity:\n\t\tif 'filename' not in f:\n\t\t\tself.error = \"Key 'filename' not in fixity block.\"\n\t\t\treturn False\n\t\tif 'digest' not in f:\n\t\t\tself.error = \"Key 'digest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\n\t# Make sure it's a dictionary\n\tif type(fixity) != dict:\n\t\tself.errors += 1\n\t\tself.errmsg.append(\"Fixity block is not a dictionary.\")\n\t\treturn\n\t\n\t# Check that it's the right size\n\tif len(fixity) == 0:\n\t\tself.errors += 1\n\t\tself.errmsg.append(\"Fixity block is empty.\")\n\t\treturn\n\t\n\t# Check that the checksums are in the manifest\n\tfor checksum in fixity.keys():\n\t\tif checksum not in manifest_files.keys():\n\t\t\tself.errors += 1\n\t\t\tself.errmsg.append(\"Fixity block refers to checksum that is not in the manifest: \" + checksum)\n\t\t\treturn\n\t\t\n\t\t# Check that the checksum's value is a list\n\t\tif type(fixity[checksum]) != list:\n\t\t\tself.errors += 1\n\t\t\tself.errmsg.append(\"Fixity block value for checksum is not a list: \" + checksum)\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\ttry:\n\t\tif fixity:\n\t\t\tif 'fixity-algorithm' in fixity:\n\t\t\t\tif fixity['fixity-algorithm'] in constants.fixity_algos:\n\t\t\t\t\tif 'fixity-value' in fixity:\n\t\t\t\t\t\tif type(fixity['fixity-value']) == str or type(fixity['fixity-value']) == unicode:\n\t\t\t\t\t\t\tif 'fixity-date' in fixity:\n\t\t\t\t\t\t\t\tif type(fixity['fixity-date']) == str or type(fixity['fixity-date']) == unicode:\n\t\t\t\t\t\t\t\t\tif 'filename' in fixity:\n\t\t\t\t\t\t\t\t\t\tif fixity['filename'] in manifest_files:\n\t\t\t\t\t\t\t\t\t\t\treturn True\n\t\t\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\t\t\tself.error('Fixity filename is not listed in manifest file.')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t\n\t# validate fixity block\n\tif fixity is None:\n\t\terror('no fixity block')\n\tif not isinstance(fixity, dict):\n\t\terror('fixity is not a dictionary')\n\tif 'algorithm' not in fixity:\n\t\terror('no fixity algorithm listed')\n\t\t\n\t# validate fixity algorithm\n\talgorithm = fixity['algorithm']\n\tif algorithm not in ['md5', 'sha1']:\n\t\terror('fixity algorithm not supported: '+algorithm)\n\t\n\t# validate fixity files\n\tif 'files' not in fixity:\n\t\terror('no fixity files listed')\n\tif not isinstance(fixity['files'], list):\n\t\terror('fixity files is not a list')\n\t\n\t# validate each fixity file entry\n\tfor file in fixity['files']:\n\t\tif not isinstance(file, dict):\n\t\t\terror('fixity file is not a dictionary')\n\t\tif 'filename' not in file:\n\t\t\terror('no fixity filename listed')\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\tif 'files' not in fixity:\n\t\tself.error('Fixity block is missing the \"files\" element.')\n\t\treturn False\n\tif 'checksums' not in fixity:\n\t\tself.error('Fixity block is missing the \"checksums\" element.')\n\t\treturn False\n\n\tif not isinstance(fixity['files'], list):\n\t\tself.error('Fixity \"files\" element is not a list.')\n\t\treturn False\n\tif not isinstance(fixity['checksums'], dict):\n\t\tself.error('Fixity \"checksums\" element is not a dictionary.')\n\t\treturn False\n\n\tfor file in fixity['files']:\n\t\tif not isinstance(file, dict):\n\t\t\tself.error('Fixity file element is not a dictionary.')\n\t\t\treturn False\n\t\tif 'filename' not in file:\n\t\t\tself.error('Fixity file element is missing the \"filename\" element.')\n\t\t\treturn False\n\t\tif 'checksum' not in file:\n\t\t\tself.error('Fix\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_fixity(self, fixity, manifest_files):\n\t# Verify that the fixity block is a list\n\tif type(fixity) != list:\n\t\tself.error(msg='Error: Fixity block is not a list.', code=40)\n\n\t# Verify that fixity block is not empty\n\tif not fixity:\n\t\tself.error(msg='Error: Fixity block is empty.', code=41)\n\n\t# Verify that each entry in the fixity block is a dictionary\n\tfor entry in fixity:\n\t\tif type(entry) != dict:\n\t\t\tself.error(msg='Error: Fixity block entry is not a dictionary.', code=42)\n\n\t# Verify that each entry in the fixity block has the correct keys\n\tfor entry in fixity:\n\t\tif sorted(list(entry.keys())) != sorted(['algorithm', 'digest', 'path']):\n\t\t\tself.error(msg='Error: Fixity block entry is missing a required key.', code=43)\n\n\t# Verify that each entry in the fixity block has a path that is in the manifest\n\tfor entry in fixity:\n\t\tif\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list", "docstring": "Return the files in `path`", "ques_id": "62b463153879012d19481498", "code": ["def files_list(path):\n\tfiles = []\n\tfor f in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, f)):\n\t\t\tfiles.append(f)\n\treturn files\n\n\n", "def files_list(path):\n\tif not os.path.exists(path):\n\t\traise Exception('Path doesn\\'t exists: {}'.format(path))\n\treturn os.listdir(path)\n\n\n", "def files_list(path):\n\t\n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\n", "def files_list(path):\n\treturn listdir(path)\n\n\n", "def files_list(path):\n\tfile_list = []\n\tfor file in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, file)):\n\t\t\tfile_list.append(file)\n\treturn file_list\n\n", "def files_list(path):\n\tfile_list = []\n\tfor root, dirs, files in os.walk(path):\n\t    for file in files:\n\t        file_list.append(os.path.join(root, file))\n\treturn file_list\n\n\n", "def files_list(path):\n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\n\n", "def files_list(path):\n\treturn [ f for f in listdir(path) if isfile(join(path,f)) ]\n\n", "def files_list(path):\n\treturn os.listdir(path)\n\n", "def files_list(path):\n\tfiles = os.listdir(path)\n\tfiles = [file for file in files if file[0] != '.']\n\treturn files\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list(path):\n\tfiles = []\n\tfor f in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, f)):\n\t\t\tfiles.append(f)\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\tif not os.path.exists(path):\n\t\traise Exception('Path doesn\\'t exists: {}'.format(path))\n\treturn os.listdir(path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\t\n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn listdir(path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\tfile_list = []\n\tfor file in os.listdir(path):\n\t\tif os.path.isfile(os.path.join(path, file)):\n\t\t\tfile_list.append(file)\n\treturn file_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\tfile_list = []\n\tfor root, dirs, files in os.walk(path):\n\t    for file in files:\n\t        file_list.append(os.path.join(root, file))\n\treturn file_list\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn [ f for f in listdir(path) if isfile(join(path,f)) ]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\treturn os.listdir(path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list(path):\n\tfiles = os.listdir(path)\n\tfiles = [file for file in files if file[0] != '.']\n\treturn files\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_group_files_by_xml_filename", "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package", "ques_id": "62b463153879012d1948149a", "code": ["def _group_files_by_xml_filename(source, xmls, files):\n\txml_files = {}\n\tfor xml in xmls:\n\t\txml_files[xml] = []\n\tfor key, value in files.items():\n\t\tfor v in value:\n\t\t\tif v in xmls:\n\t\t\t\txml_files[v].append(key)\n\treturn xml_files\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t# group files by xml filename\n\tfiles_by_xml = {}\n\t\n\tfor xml in xmls:\n\t\t# get the xml filename\n\t\txml_filename = os.path.splitext(os.path.basename(xml))[0]\n\t\t\n\t\t# find all files that match the xml filename\n\t\tmatching_files = glob.glob(os.path.join(source, xml_filename + '.*'))\n\t\t\n\t\t# remove the xml from the list of matching files\n\t\tmatching_files = [x for x in matching_files if x != xml]\n\t\t\n\t\t# add the matching files to the dictionary\n\t\tfiles_by_xml[xml] = matching_files\n\t\n\t# combine the files_by_xml dictionary with the files dictionary\n\tfor xml, matching_files in files_by_xml.items():\n\t\tif xml not in files.keys():\n\t\t\tfiles[xml] = matching_files\n\t\telse:\n\t\t\tfiles[xml] += matching_files\n\t\n\treturn files\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\txml_files = []\n\tfor xml in xmls:\n\t\txml_name = xml.split(\"/\")[-1]\n\t\txml_name = xml_name.split(\"\\\\\")[-1]\n\t\txml_name = xml_name.split(\".\")[0]\n\t\tfile_list = []\n\t\tfor file in files:\n\t\t\tfile_name = file.split(\"/\")[-1]\n\t\t\tfile_name = file_name.split(\"\\\\\")[-1]\n\t\t\tfile_name = file_name.split(\".\")[0]\n\t\t\tif file_name == xml_name:\n\t\t\t\tfile_list.append(file)\n\t\tif len(file_list) == 0:\n\t\t\tprint(\"ERROR: No file found for xml: {}\".format(source + xml))\n\t\telse:\n\t\t\txml_files.append((source + xml, file_list))\n\treturn xml_files\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\tlogging.debug(\"Grouping files for: {}\".format(source))\n\tresult = {}\n\tfor xml in xmls:\n\t\tfile_name = os.path.basename(xml).split(\".\")[0]\n\t\tresult[file_name] = []\n\t\tfor file in files:\n\t\t\tif file_name in file:\n\t\t\t\tresult[file_name].append(file)\n\n\treturn result\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\tresult = []\n\tfor xml in xmls:\n\t\txml_path = os.path.join(source, xml)\n\t\txml_name = os.path.splitext(os.path.basename(xml_path))[0]\n\t\tfiles_in_xml = filter(lambda x:os.path.splitext(os.path.basename(x))[0] == xml_name, files)\n\t\tresult.append({\"xml\":xml_path, \"files\":files_in_xml})\n\n\treturn result\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\tdata = {}\n\tfor xml in xmls:\n\t\tif xml.startswith(source):\n\t\t\tfiles_grouped = []\n\t\t\tfor f in files:\n\t\t\t\tif f.startswith(source):\n\t\t\t\t\tfiles_grouped.append(f)\n\t\t\tdata[xml] = files_grouped\n\treturn data\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t# Group files by xmls.\n\tgrouped_files = {}\n\tfor file in files:\n\t\tif file.endswith(\".xml\"):\n\t\t\tcontinue\n\t\t# Find corresponding xml.\n\t\txml = file.replace(source, \"\")\n\t\txml = xml[0:xml.rindex(\"/\")]\n\t\txml = xml + \".xml\"\n\t\txml = source + xml\n\t\t# Check if xml exists.\n\t\tif not xml in xmls:\n\t\t\tcontinue\n\t\t# Add file to grouped files.\n\t\tif not xml in grouped_files:\n\t\t\tgrouped_files[xml] = []\n\t\tgrouped_files[xml].append(file)\n\n\treturn grouped_files\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t# Set default values\n\tgroups = {}\n\tfor xml in xmls:\n\t\tgroups[xml.name] = []\n\t\n\t# Add files to groups\n\tfor f in files:\n\t\t# Check if file is in xml\n\t\txml_name = f.split(os.sep)[-2]\n\t\tif xml_name not in groups:\n\t\t\tsys.exit(\"File '{}' not found in xml.\".format(f))\n\t\t\n\t\tgroups[xml_name].append(f)\n\t\n\t# Check if all files are in xml\n\tfor xml_name in groups:\n\t\t# Check if files are in xml\n\t\tif len(groups[xml_name]) == 0:\n\t\t\tsys.exit(\"Files from '{}' not found in xml.\".format(xml_name))\n\t\n\treturn groups\n\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\t# Get the xmls and files.\n\txmls = _get_files_from_source(source, 'xml')\n\tfiles = _get_files_from_source(source, 'file')\n\t\n\t# Group files by xmls.\n\tgroup = {}\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tif xml.split('.')[0] in file:\n\t\t\t\tgroup.setdefault(xml, [])\n\t\t\t\tgroup[xml].append(file)\n\t\n\treturn group\n\n", "def _group_files_by_xml_filename(source, xmls, files):\n\txml_file_dict = {}\n\tfor xml in xmls:\n\t\txml_file_dict[xml] = {}\n\t\txml_file_dict[xml]['xml'] = xml\n\t\txml_file_dict[xml]['source'] = source\n\t\txml_file_dict[xml]['files'] = {}\n\t\tfor file in files:\n\t\t\tif file.split('/')[-1] in xml:\n\t\t\t\txml_file_dict[xml]['files'][file] = file\n\treturn xml_file_dict\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\txml_files = {}\n\tfor xml in xmls:\n\t\txml_files[xml] = []\n\tfor key, value in files.items():\n\t\tfor v in value:\n\t\t\tif v in xmls:\n\t\t\t\txml_files[v].append(key)\n\treturn xml_files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t# group files by xml filename\n\tfiles_by_xml = {}\n\t\n\tfor xml in xmls:\n\t\t# get the xml filename\n\t\txml_filename = os.path.splitext(os.path.basename(xml))[0]\n\t\t\n\t\t# find all files that match the xml filename\n\t\tmatching_files = glob.glob(os.path.join(source, xml_filename + '.*'))\n\t\t\n\t\t# remove the xml from the list of matching files\n\t\tmatching_files = [x for x in matching_files if x != xml]\n\t\t\n\t\t# add the matching files to the dictionary\n\t\tfiles_by_xml[xml] = matching_files\n\t\n\t# combine the files_by_xml dictionary with the files dictionary\n\tfor xml, matching_files in files_by_xml.items():\n\t\tif xml not in files.keys():\n\t\t\tfiles[xml] = matching_files\n\t\telse:\n\t\t\tfiles[xml] += matching_files\n\t\n\treturn files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\txml_files = []\n\tfor xml in xmls:\n\t\txml_name = xml.split(\"/\")[-1]\n\t\txml_name = xml_name.split(\"\\\\\")[-1]\n\t\txml_name = xml_name.split(\".\")[0]\n\t\tfile_list = []\n\t\tfor file in files:\n\t\t\tfile_name = file.split(\"/\")[-1]\n\t\t\tfile_name = file_name.split(\"\\\\\")[-1]\n\t\t\tfile_name = file_name.split(\".\")[0]\n\t\t\tif file_name == xml_name:\n\t\t\t\tfile_list.append(file)\n\t\tif len(file_list) == 0:\n\t\t\tprint(\"ERROR: No file found for xml: {}\".format(source + xml))\n\t\telse:\n\t\t\txml_files.append((source + xml, file_list))\n\treturn xml_files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\tlogging.debug(\"Grouping files for: {}\".format(source))\n\tresult = {}\n\tfor xml in xmls:\n\t\tfile_name = os.path.basename(xml).split(\".\")[0]\n\t\tresult[file_name] = []\n\t\tfor file in files:\n\t\t\tif file_name in file:\n\t\t\t\tresult[file_name].append(file)\n\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\tresult = []\n\tfor xml in xmls:\n\t\txml_path = os.path.join(source, xml)\n\t\txml_name = os.path.splitext(os.path.basename(xml_path))[0]\n\t\tfiles_in_xml = filter(lambda x:os.path.splitext(os.path.basename(x))[0] == xml_name, files)\n\t\tresult.append({\"xml\":xml_path, \"files\":files_in_xml})\n\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\tdata = {}\n\tfor xml in xmls:\n\t\tif xml.startswith(source):\n\t\t\tfiles_grouped = []\n\t\t\tfor f in files:\n\t\t\t\tif f.startswith(source):\n\t\t\t\t\tfiles_grouped.append(f)\n\t\t\tdata[xml] = files_grouped\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t# Group files by xmls.\n\tgrouped_files = {}\n\tfor file in files:\n\t\tif file.endswith(\".xml\"):\n\t\t\tcontinue\n\t\t# Find corresponding xml.\n\t\txml = file.replace(source, \"\")\n\t\txml = xml[0:xml.rindex(\"/\")]\n\t\txml = xml + \".xml\"\n\t\txml = source + xml\n\t\t# Check if xml exists.\n\t\tif not xml in xmls:\n\t\t\tcontinue\n\t\t# Add file to grouped files.\n\t\tif not xml in grouped_files:\n\t\t\tgrouped_files[xml] = []\n\t\tgrouped_files[xml].append(file)\n\n\treturn grouped_files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t# Set default values\n\tgroups = {}\n\tfor xml in xmls:\n\t\tgroups[xml.name] = []\n\t\n\t# Add files to groups\n\tfor f in files:\n\t\t# Check if file is in xml\n\t\txml_name = f.split(os.sep)[-2]\n\t\tif xml_name not in groups:\n\t\t\tsys.exit(\"File '{}' not found in xml.\".format(f))\n\t\t\n\t\tgroups[xml_name].append(f)\n\t\n\t# Check if all files are in xml\n\tfor xml_name in groups:\n\t\t# Check if files are in xml\n\t\tif len(groups[xml_name]) == 0:\n\t\t\tsys.exit(\"Files from '{}' not found in xml.\".format(xml_name))\n\t\n\treturn groups\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\t# Get the xmls and files.\n\txmls = _get_files_from_source(source, 'xml')\n\tfiles = _get_files_from_source(source, 'file')\n\t\n\t# Group files by xmls.\n\tgroup = {}\n\tfor xml in xmls:\n\t\tfor file in files:\n\t\t\tif xml.split('.')[0] in file:\n\t\t\t\tgroup.setdefault(xml, [])\n\t\t\t\tgroup[xml].append(file)\n\t\n\treturn group\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _group_files_by_xml_filename(source, xmls, files):\n\txml_file_dict = {}\n\tfor xml in xmls:\n\t\txml_file_dict[xml] = {}\n\t\txml_file_dict[xml]['xml'] = xml\n\t\txml_file_dict[xml]['source'] = source\n\t\txml_file_dict[xml]['files'] = {}\n\t\tfor file in files:\n\t\t\tif file.split('/')[-1] in xml:\n\t\t\t\txml_file_dict[xml]['files'][file] = file\n\treturn xml_file_dict\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "match_file_by_prefix", "docstring": "Identify if a `file_path` belongs to a document package by a given `prefix`\n\nRetorna `True` para documentos pertencentes a um pacote.\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfile_path : str\n    File path\nReturns\n-------\nbool\n    True - file belongs to the package", "ques_id": "62b463153879012d1948149b", "code": ["def match_file_by_prefix(prefix, file_path):\n\tfname = os.path.basename(file_path)\n\treturn ((fname.startswith(prefix + \"-\") or fname.startswith(prefix + \".\")))\n\n", "def match_file_by_prefix(prefix, file_path):\n\tfile_basename = os.path.basename(file_path)\n\treturn file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\")\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix + \"-\") or file_name.startswith(prefix + \".\")\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn os.path.basename(file_path).startswith(prefix + \"-\" or prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn file_path.endswith(\".out\") or file_path.endswith(\".err\") or file_path.endswith(\".out.json\") or file_path.endswith(\".err.json\") or file_path.endswith(\".out.json.err\") or file_path.endswith(\".err.json.err\") or file_path.endswith(\".log\") or file_path.endswith(\".log.err\") or file_path.endswith(\".data\") or file_path.endswith(\".data.err\") or file_path.endswith(\".data.json\") or file_path.endswith(\".data.json.err\") or file_path.endswith(\".data.txt\") or file_path.endswith(\".data.txt.err\") or file_path.endswith(\".data.txt.json\") or file_path.endswith(\".data.txt.json.err\") or file_path.endswith(\".data.txt.json.json\") or file_path.endswith\n", "def match_file_by_prefix(prefix, file_path):\n\tfile_basename = os.path.basename(file_path)\n\treturn file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\tfile_name = os.path.basename(file_path)\n\tprefix_plus_dash = prefix + \"-\"\n\tprefix_plus_dot = prefix + \".\"\n\treturn file_name.startswith(prefix_plus_dash) or file_name.startswith(prefix_plus_dot)\n\n\n", "def match_file_by_prefix(prefix, file_path):\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n\n", "def match_file_by_prefix(prefix, file_path):\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tfname = os.path.basename(file_path)\n\treturn ((fname.startswith(prefix + \"-\") or fname.startswith(prefix + \".\")))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tfile_basename = os.path.basename(file_path)\n\treturn file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tfile_name = os.path.basename(file_path)\n\treturn file_name.startswith(prefix + \"-\") or file_name.startswith(prefix + \".\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn os.path.basename(file_path).startswith(prefix + \"-\" or prefix + \".\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn file_path.endswith(\".out\") or file_path.endswith(\".err\") or file_path.endswith(\".out.json\") or file_path.endswith(\".err.json\") or file_path.endswith(\".out.json.err\") or file_path.endswith(\".err.json.err\") or file_path.endswith(\".log\") or file_path.endswith(\".log.err\") or file_path.endswith(\".data\") or file_path.endswith(\".data.err\") or file_path.endswith(\".data.json\") or file_path.endswith(\".data.json.err\") or file_path.endswith(\".data.txt\") or file_path.endswith(\".data.txt.err\") or file_path.endswith(\".data.txt.json\") or file_path.endswith(\".data.txt.json.err\") or file_path.endswith(\".data.txt.json.json\") or file_path.endswith\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tfile_basename = os.path.basename(file_path)\n\treturn file_basename.startswith(prefix + \"-\") or file_basename.startswith(prefix + \".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tfile_name = os.path.basename(file_path)\n\tprefix_plus_dash = prefix + \"-\"\n\tprefix_plus_dot = prefix + \".\"\n\treturn file_name.startswith(prefix_plus_dash) or file_name.startswith(prefix_plus_dot)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\treturn os.path.basename(file_path).startswith(prefix + \"-\") or os.path.basename(file_path).startswith(prefix + \".\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match_file_by_prefix(prefix, file_path):\n\tbasename = os.path.basename(file_path)\n\treturn basename.startswith(prefix + \"-\") or basename.startswith(prefix + \".\")\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "select_filenames_by_prefix", "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix", "ques_id": "62b463153879012d1948149c", "code": ["def select_filenames_by_prefix(prefix, files):\n\treturn [filename for filename in files if filename.startswith(prefix)]\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if f.startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\tprefix_len = len(prefix)\n\treturn [x for x in files if x[:prefix_len] == prefix]\n\n", "def select_filenames_by_prefix(prefix, files):\n\tfiltered_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfiltered_files.append(file)\n\treturn filtered_files\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if f.startswith(prefix)]\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn [file for file in files if file.startswith(prefix)]\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\treturn list(filter(lambda x: x.startswith(prefix), files))\n\n", "def select_filenames_by_prefix(prefix, files):\n\tmatching = []\n\tfor filename in files:\n\t\tif filename.startswith(prefix):\n\t\t\tmatching.append(filename)\n\treturn matching\n\n\n", "def select_filenames_by_prefix(prefix, files):\n\toutput_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\toutput_files.append(file)\n\treturn output_files\n\n", "def select_filenames_by_prefix(prefix, files):\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [filename for filename in files if filename.startswith(prefix)]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if f.startswith(prefix)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\tprefix_len = len(prefix)\n\treturn [x for x in files if x[:prefix_len] == prefix]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\tfiltered_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfiltered_files.append(file)\n\treturn filtered_files\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [f for f in files if f.startswith(prefix)]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn [file for file in files if file.startswith(prefix)]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\treturn list(filter(lambda x: x.startswith(prefix), files))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\tmatching = []\n\tfor filename in files:\n\t\tif filename.startswith(prefix):\n\t\t\tmatching.append(filename)\n\treturn matching\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\toutput_files = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\toutput_files.append(file)\n\treturn output_files\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def select_filenames_by_prefix(prefix, files):\n\tfilenames = []\n\tfor file in files:\n\t\tif file.startswith(prefix):\n\t\t\tfilenames.append(file)\n\treturn filenames\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_folder", "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149d", "code": ["def _explore_folder(folder):\n\tprint(\"Exploring folder {}\".format(folder))\n\tfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n\tfiles = [f for f in files if f.endswith('.xml')]\n\tfiles = [join(folder, f) for f in files]\n\tif not files:\n\t\tprint(\"No xml files found in {}\".format(folder))\n\t\treturn\n\tprint(\"Found {} xml files.\".format(len(files)))\n\t_process_files(files)\n\n\n", "def _explore_folder(folder):\n\tfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n\tfiles = _group_files_by_xml_filename(files)\n\treturn files\n\n\n", "def _explore_folder(folder):\n\tgrouped_files = _group_files_by_xml_filename(folder)\n\tfor grouped_files in grouped_files.values():\n\t\ttry:\n\t\t\tyield _extract_values_from_files(grouped_files)\n\t\texcept ValueError as e:\n\t\t\tlogger.warning(e)\n\n\n", "def _explore_folder(folder):\n\tfiles = filter(lambda x: x[-4:] == '.xml', os.listdir(folder))\n\tfiles = map(lambda x: os.path.join(folder, x), files)\n\treturn _group_files_by_xml_filename(files)\n\n", "def _explore_folder(folder):\n\tprint \"Exploring folder %s\" % folder\n\tfor root, dirs, files in os.walk(folder):\n\t\t# Get the name of the folder\n\t\tfolder_name = os.path.basename(root)\n\t\tfolder_name = folder_name.replace(\"_\", \" \")\n\t\tprint \"Current folder: %s\" % folder_name\n\t\t\n\t\t# Get a list of XML files\n\t\txml_files = [f for f in files if _is_xml_file(f)]\n\t\tif len(xml_files) > 0:\n\t\t\t# Group files by XML filename\n\t\t\tgroups = _group_files_by_xml_filename(xml_files)\n\t\t\t\n\t\t\t# Move files\n\t\t\tfor group in groups:\n\t\t\t\t_move_files(root, folder_name, group)\n\n\tprint \"Finished exploring folder %s\" % folder\n\n", "def _explore_folder(folder):\n\t# get all files in the folder\n\tfiles = glob.glob(os.path.join(folder, \"*.xml\"))\n\t# group them\n\treturn _group_files_by_xml_filename(files)\n\n\n", "def _explore_folder(folder):\n\tfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n\treturn _group_files_by_xml_filename(files)\n\n", "def _explore_folder(folder):\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor filename in files:\n\t\t\tif filename.endswith(\".xml\"):\n\t\t\t\t_group_files_by_xml_filename(root, filename)\n\t\t\t\t\n", "def _explore_folder(folder):\n\tfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f))]\n\treturn _group_files_by_xml_filename(files)\n\n", "def _explore_folder(folder):\n\tfiles = list_files(folder)\n\tif len(files) == 0:\n\t\treturn []\n\tgroups = _group_files_by_xml_filename(files)\n\treturn groups\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_folder(folder):\n\tprint(\"Exploring folder {}\".format(folder))\n\tfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n\tfiles = [f for f in files if f.endswith('.xml')]\n\tfiles = [join(folder, f) for f in files]\n\tif not files:\n\t\tprint(\"No xml files found in {}\".format(folder))\n\t\treturn\n\tprint(\"Found {} xml files.\".format(len(files)))\n\t_process_files(files)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n\tfiles = _group_files_by_xml_filename(files)\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tgrouped_files = _group_files_by_xml_filename(folder)\n\tfor grouped_files in grouped_files.values():\n\t\ttry:\n\t\t\tyield _extract_values_from_files(grouped_files)\n\t\texcept ValueError as e:\n\t\t\tlogger.warning(e)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfiles = filter(lambda x: x[-4:] == '.xml', os.listdir(folder))\n\tfiles = map(lambda x: os.path.join(folder, x), files)\n\treturn _group_files_by_xml_filename(files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tprint \"Exploring folder %s\" % folder\n\tfor root, dirs, files in os.walk(folder):\n\t\t# Get the name of the folder\n\t\tfolder_name = os.path.basename(root)\n\t\tfolder_name = folder_name.replace(\"_\", \" \")\n\t\tprint \"Current folder: %s\" % folder_name\n\t\t\n\t\t# Get a list of XML files\n\t\txml_files = [f for f in files if _is_xml_file(f)]\n\t\tif len(xml_files) > 0:\n\t\t\t# Group files by XML filename\n\t\t\tgroups = _group_files_by_xml_filename(xml_files)\n\t\t\t\n\t\t\t# Move files\n\t\t\tfor group in groups:\n\t\t\t\t_move_files(root, folder_name, group)\n\n\tprint \"Finished exploring folder %s\" % folder\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\t# get all files in the folder\n\tfiles = glob.glob(os.path.join(folder, \"*.xml\"))\n\t# group them\n\treturn _group_files_by_xml_filename(files)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n\treturn _group_files_by_xml_filename(files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfor root, dirs, files in os.walk(folder):\n\t\tfor filename in files:\n\t\t\tif filename.endswith(\".xml\"):\n\t\t\t\t_group_files_by_xml_filename(root, filename)\n\t\t\t\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _explore_folder(folder):\n\tfiles = [join(folder, f) for f in listdir(folder) if isfile(join(folder, f))]\n\treturn _group_files_by_xml_filename(files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_folder(folder):\n\tfiles = list_files(folder)\n\tif len(files) == 0:\n\t\treturn []\n\tgroups = _group_files_by_xml_filename(files)\n\treturn groups\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_eval_file", "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endere\u00e7o do arquivo em an\u00e1lise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extens\u00e3o\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict", "ques_id": "62b463153879012d1948149f", "code": ["def _eval_file(prefix, file_path):\n\tcomponent_id = None\n\tftype = None\n\tif prefix in file_path:\n\t\tif os.path.splitext(file_path)[1] == \".xml\":\n\t\t\treturn None\n\t\telse:\n\t\t\tif os.path.splitext(file_path)[1] == \".pdf\":\n\t\t\t\tcomponent_id = os.path.splitext(os.path.basename(file_path))[0]\n\t\t\telse:\n\t\t\t\tcomponent_id = os.path.splitext(os.path.basename(file_path))[0]\n\t\t\t\tftype = os.path.splitext(file_path)[1]\n\t\treturn {\"component_id\":component_id, \"file_path\":file_path, \"ftype\":ftype}\n\treturn None\n\n", "def _eval_file(prefix, file_path):\n\tif not file_path.startswith(prefix + '/'):\n\t\treturn None\n\t\n\tfile_path_split = file_path.split('/')\n\tif len(file_path_split) < 2:\n\t\treturn None\n\t\n\tcomponent_id = file_path_split[1]\n\t\n\tif file_path_split[-1] == 'xml':\n\t\treturn None\n\t\n\tif file_path_split[-1] == 'pdf':\n\t\treturn {'component_id': component_id, 'file_path': file_path}\n\t\n\tfile_path_split = file_path.split('.')\n\tif len(file_path_split) < 2:\n\t\treturn None\n\t\n\tftype = file_path_split[-1]\n\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n\n", "def _eval_file(prefix, file_path):\n\t# if the file is not pdf, only identify the component id\n\tif not file_path.endswith('.pdf'):\n\t\tcomponent_id = _identify_component_id(prefix, file_path)\n\t\tif component_id:\n\t\t\treturn {'component_id': component_id, 'file_path': file_path}\n\t\telse:\n\t\t\treturn None\n\t# if the file is pdf, identify the component id, file type and file path\n\telse:\n\t\tftype = _identify_ftype(prefix, file_path)\n\t\tcomponent_id = _identify_component_id(prefix, file_path)\n\t\tif component_id and ftype:\n\t\t\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n\t\telse:\n\t\t\treturn None\n\n", "def _eval_file(prefix, file_path):\n\tcomponent_id = _get_component_id(file_path)\n\tif not component_id.startswith(prefix):\n\t\treturn None\n\tftype = _get_file_type(file_path)\n\tif ftype == \"xml\":\n\t\treturn None\n\telif ftype == \"pdf\":\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\n\n", "def _eval_file(prefix, file_path):\n\tcomponent_id = file_path.split(\"/\")[-1].split(\".\")[0]\n\tfile_type = file_path.split(\".\")[-1]\n\tif component_id.startswith(prefix):\n\t\tif file_type == \"pdf\":\n\t\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\t\telse:\n\t\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": file_type}\n\telse:\n\t\treturn None\n\n", "def _eval_file(prefix, file_path):\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\n\tif file_path.endswith('.xml'):\n\t\treturn None\n\n\tfile_path = file_path.replace('\\\\', '/')\n\tcomponent_id = file_path.replace(prefix, '')[1:-4]\n\tr = {'component_id': component_id, 'file_path': file_path}\n\n\tif file_path.endswith('.pdf'):\n\t\treturn r\n\n\tr['ftype'] = file_path.split('.')[-1]\n\tr['file_path'] = file_path.replace('.%s' % r['ftype'], '.pdf')\n\treturn r\n\n\n", "def _eval_file(prefix, file_path):\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\t\n\tfile_name = file_path.split(\"/\")[-1]\n\tcomponent_id = file_name[len(prefix):].split(\".\")[0]\n\tftype = file_name.split(\".\")[-1]\n\t\n\tif ftype == \"pdf\":\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\n", "def _eval_file(prefix, file_path):\n\tcomponent_id = os.path.basename(file_path).split('.')[0]\n\tif component_id.startswith(prefix) and component_id != prefix:\n\t\tftype = os.path.basename(file_path).split('.')[-1]\n\t\tif ftype == 'xml':\n\t\t\treturn None\n\t\telse:\n\t\t\tif ftype == 'pdf':\n\t\t\t\treturn {'component_id': component_id, 'file_path': file_path}\n\t\t\telse:\n\t\t\t\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n\telse:\n\t\treturn None\n\n\n", "def _eval_file(prefix, file_path):\n\tftype = None\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tif not os.path.isfile(file_path):\n\t\treturn None\n\tif file_path.lower().endswith(\".xml\"):\n\t\treturn None\n\n\tcomponent_id = file_path[len(prefix):-4]\n\tif file_path.lower().endswith(\".pdf\"):\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\telse:\n\t\tftype = os.path.splitext(file_path)[1][1:]\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\n\n", "def _eval_file(prefix, file_path):\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\t\n\tfile_type = __get_file_type(file_path)\n\tif file_type == 'xml':\n\t\treturn None\n\t\n\treturn {'component_id':file_path.replace(prefix, '').replace('/', '_'), 'file_path':file_path, 'ftype':file_type}\n\t\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _eval_file(prefix, file_path):\n\tcomponent_id = None\n\tftype = None\n\tif prefix in file_path:\n\t\tif os.path.splitext(file_path)[1] == \".xml\":\n\t\t\treturn None\n\t\telse:\n\t\t\tif os.path.splitext(file_path)[1] == \".pdf\":\n\t\t\t\tcomponent_id = os.path.splitext(os.path.basename(file_path))[0]\n\t\t\telse:\n\t\t\t\tcomponent_id = os.path.splitext(os.path.basename(file_path))[0]\n\t\t\t\tftype = os.path.splitext(file_path)[1]\n\t\treturn {\"component_id\":component_id, \"file_path\":file_path, \"ftype\":ftype}\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tif not file_path.startswith(prefix + '/'):\n\t\treturn None\n\t\n\tfile_path_split = file_path.split('/')\n\tif len(file_path_split) < 2:\n\t\treturn None\n\t\n\tcomponent_id = file_path_split[1]\n\t\n\tif file_path_split[-1] == 'xml':\n\t\treturn None\n\t\n\tif file_path_split[-1] == 'pdf':\n\t\treturn {'component_id': component_id, 'file_path': file_path}\n\t\n\tfile_path_split = file_path.split('.')\n\tif len(file_path_split) < 2:\n\t\treturn None\n\t\n\tftype = file_path_split[-1]\n\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\t# if the file is not pdf, only identify the component id\n\tif not file_path.endswith('.pdf'):\n\t\tcomponent_id = _identify_component_id(prefix, file_path)\n\t\tif component_id:\n\t\t\treturn {'component_id': component_id, 'file_path': file_path}\n\t\telse:\n\t\t\treturn None\n\t# if the file is pdf, identify the component id, file type and file path\n\telse:\n\t\tftype = _identify_ftype(prefix, file_path)\n\t\tcomponent_id = _identify_component_id(prefix, file_path)\n\t\tif component_id and ftype:\n\t\t\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n\t\telse:\n\t\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tcomponent_id = _get_component_id(file_path)\n\tif not component_id.startswith(prefix):\n\t\treturn None\n\tftype = _get_file_type(file_path)\n\tif ftype == \"xml\":\n\t\treturn None\n\telif ftype == \"pdf\":\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tcomponent_id = file_path.split(\"/\")[-1].split(\".\")[0]\n\tfile_type = file_path.split(\".\")[-1]\n\tif component_id.startswith(prefix):\n\t\tif file_type == \"pdf\":\n\t\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\t\telse:\n\t\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": file_type}\n\telse:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\n\tif file_path.endswith('.xml'):\n\t\treturn None\n\n\tfile_path = file_path.replace('\\\\', '/')\n\tcomponent_id = file_path.replace(prefix, '')[1:-4]\n\tr = {'component_id': component_id, 'file_path': file_path}\n\n\tif file_path.endswith('.pdf'):\n\t\treturn r\n\n\tr['ftype'] = file_path.split('.')[-1]\n\tr['file_path'] = file_path.replace('.%s' % r['ftype'], '.pdf')\n\treturn r\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tif file_path.endswith(\".xml\"):\n\t\treturn None\n\t\n\tfile_name = file_path.split(\"/\")[-1]\n\tcomponent_id = file_name[len(prefix):].split(\".\")[0]\n\tftype = file_name.split(\".\")[-1]\n\t\n\tif ftype == \"pdf\":\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\telse:\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tcomponent_id = os.path.basename(file_path).split('.')[0]\n\tif component_id.startswith(prefix) and component_id != prefix:\n\t\tftype = os.path.basename(file_path).split('.')[-1]\n\t\tif ftype == 'xml':\n\t\t\treturn None\n\t\telse:\n\t\t\tif ftype == 'pdf':\n\t\t\t\treturn {'component_id': component_id, 'file_path': file_path}\n\t\t\telse:\n\t\t\t\treturn {'component_id': component_id, 'file_path': file_path, 'ftype': ftype}\n\telse:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tftype = None\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\tif not os.path.isfile(file_path):\n\t\treturn None\n\tif file_path.lower().endswith(\".xml\"):\n\t\treturn None\n\n\tcomponent_id = file_path[len(prefix):-4]\n\tif file_path.lower().endswith(\".pdf\"):\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path}\n\telse:\n\t\tftype = os.path.splitext(file_path)[1][1:]\n\t\treturn {\"component_id\": component_id, \"file_path\": file_path, \"ftype\": ftype}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _eval_file(prefix, file_path):\n\tif not file_path.startswith(prefix):\n\t\treturn None\n\t\n\tfile_type = __get_file_type(file_path)\n\tif file_type == 'xml':\n\t\treturn None\n\t\n\treturn {'component_id':file_path.replace(prefix, '').replace('/', '_'), 'file_path':file_path, 'ftype':file_type}\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_rendition", "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}", "ques_id": "62b463153879012d194814a1", "code": ["def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\t\n\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\t\t\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n\n", "def add_rendition(self, lang, file_path):\n\tif lang in self._renditions:\n\t\traise Exception('Language %s already exists' % lang)\n\n\tself._renditions[lang] = self.filepath(file_path)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\t\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tself._renditions[lang] = file_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_rendition(self, lang, file_path):\n\tif lang in self._renditions:\n\t\traise Exception('Language %s already exists' % lang)\n\n\tself._renditions[lang] = self.filepath(file_path)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "add_asset", "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}", "ques_id": "62b463163879012d194814a2", "code": ["def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\t\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = filepath(file_path)\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\tdef __getitem__(self, name):\n\t\"\"\"\n\tReturn the filepath invoke by filepath() in the class.\n\t\"\"\"\n\treturn self._assets[name]\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn all the basename invoke by filepath() in the class.\n\t\"\"\"\n\treturn iter(self._assets)\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\tif self.DEBUG:\n\t\tprint(\"[%s][%s][%s]\" % (basename, file_path, self._assets[basename]))\n\n\treturn self\n\n\n", "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = filepath(file_path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\tdef __getitem__(self, name):\n\t\"\"\"\n\tReturn the filepath invoke by filepath() in the class.\n\t\"\"\"\n\treturn self._assets[name]\n\n\tdef __iter__(self):\n\t\"\"\"\n\tReturn all the basename invoke by filepath() in the class.\n\t\"\"\"\n\treturn iter(self._assets)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\tif self.DEBUG:\n\t\tprint(\"[%s][%s][%s]\" % (basename, file_path, self._assets[basename]))\n\n\treturn self\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def add_asset(self, basename, file_path):\n\tself._assets[basename] = file_path\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/packages.py", "project": "scieloorg/packtools", "name": "_explore_zipfile", "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict", "ques_id": "62b463163879012d194814a4", "code": ["def _explore_zipfile(zip_path):\n\tfiles_by_xml_filename = _group_files_by_xml_filename(zip_path)\n\t\n\t# Create file groups for each XML file\n\tfile_groups = []\n\tfor xml_filename, filenames in files_by_xml_filename.items():\n\t\tfile_groups.append(FileGroup(xml_filename, filenames))\n\t\n\treturn file_groups\n\n\n", "def _explore_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn _group_files_by_xml_filename(zip_file.namelist())\n\n", "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\t# Go through all the files in the zip file.\n\t\tfor zipped_filename in zf.namelist():\n\t\t\t# If the file is a directory, skip it.\n\t\t\tif zipped_filename.endswith('/'):\n\t\t\t\tcontinue\n\n\t\t\t# Unzip and group the file.\n\t\t\t_group_files_by_xml_filename(zipped_filename, zf.open(zipped_filename))\n\n", "def _explore_zipfile(zip_path):\n\tzip = zipfile.ZipFile(zip_path)\n\tfiles = zip.namelist()\n\tfiles_by_xml_filename = _group_files_by_xml_filename(files)\n\tzip.close()\n\treturn files_by_xml_filename\n\n", "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, 'r') as _zip:\n\t\tfiles = _zip.namelist()\n\t\treturn _group_files_by_xml_filename(files)\n\n", "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as z:\n\t\tfor info in z.infolist():\n\t\t\tif info.filename.endswith(\".xml\"):\n\t\t\t\tyield from _group_files_by_xml_filename(z, info)\n\n", "def _explore_zipfile(zip_path):\n\tzip_files = _get_files_from_zip(zip_path)\n\treturn _group_files_by_xml_filename(zip_files)\n\n", "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfiles = []\n\t\tfor info in zip_file.infolist():\n\t\t\twith zip_file.open(info.filename, 'r') as f:\n\t\t\t\tfiles.append(f)\n\n\t\tfor group in _group_files_by_xml_filename(files):\n\t\t\tyield group\n\n\n", "def _explore_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\tfile_list = zip_file.namelist()\n\txml_file_list = filter(lambda filename: filename.endswith('.xml'), file_list)\n\treturn _group_files_by_xml_filename(xml_file_list)\n\n", "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\treturn _group_files_by_xml_filename(\n\t\t\tzip_file,\n\t\t\tzip_file.namelist(),\n\t\t\t_is_xml_filename)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _explore_zipfile(zip_path):\n\tfiles_by_xml_filename = _group_files_by_xml_filename(zip_path)\n\t\n\t# Create file groups for each XML file\n\tfile_groups = []\n\tfor xml_filename, filenames in files_by_xml_filename.items():\n\t\tfile_groups.append(FileGroup(xml_filename, filenames))\n\t\n\treturn file_groups\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn _group_files_by_xml_filename(zip_file.namelist())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\t# Go through all the files in the zip file.\n\t\tfor zipped_filename in zf.namelist():\n\t\t\t# If the file is a directory, skip it.\n\t\t\tif zipped_filename.endswith('/'):\n\t\t\t\tcontinue\n\n\t\t\t# Unzip and group the file.\n\t\t\t_group_files_by_xml_filename(zipped_filename, zf.open(zipped_filename))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tzip = zipfile.ZipFile(zip_path)\n\tfiles = zip.namelist()\n\tfiles_by_xml_filename = _group_files_by_xml_filename(files)\n\tzip.close()\n\treturn files_by_xml_filename\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, 'r') as _zip:\n\t\tfiles = _zip.namelist()\n\t\treturn _group_files_by_xml_filename(files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as z:\n\t\tfor info in z.infolist():\n\t\t\tif info.filename.endswith(\".xml\"):\n\t\t\t\tyield from _group_files_by_xml_filename(z, info)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tzip_files = _get_files_from_zip(zip_path)\n\treturn _group_files_by_xml_filename(zip_files)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, 'r') as zip_file:\n\t\tfiles = []\n\t\tfor info in zip_file.infolist():\n\t\t\twith zip_file.open(info.filename, 'r') as f:\n\t\t\t\tfiles.append(f)\n\n\t\tfor group in _group_files_by_xml_filename(files):\n\t\t\tyield group\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path, 'r')\n\tfile_list = zip_file.namelist()\n\txml_file_list = filter(lambda filename: filename.endswith('.xml'), file_list)\n\treturn _group_files_by_xml_filename(xml_file_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _explore_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zip_file:\n\t\treturn _group_files_by_xml_filename(\n\t\t\tzip_file,\n\t\t\tzip_file.namelist(),\n\t\t\t_is_xml_filename)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/file_utils.py", "project": "scieloorg/packtools", "name": "files_list_from_zipfile", "docstring": "Return the files in `zip_path`\n\nExample:\n\n```\n[\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200069.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.pdf',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071.xml',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf01.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf02.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf03.tif',\n    '2318-0889-tinf-33-0421/2318-0889-tinf-33-e200071-gf04.tif',\n]\n```", "ques_id": "62b463163879012d194814a6", "code": ["def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\treturn zf.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\tzf = zipfile.ZipFile(zip_path)\n\treturn zf.namelist()\n\n\n", "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfiles = zf.namelist()\n\treturn files\n\n", "def files_list_from_zipfile(zip_path):\n\tzf = zipfile.ZipFile(zip_path, 'r')\n\treturn zf.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as z:\n\t\tfor f in z.namelist():\n\t\t\tfiles.append(f)\n\treturn files\n\n\n", "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, 'r') as z:\n\t\treturn z.namelist()\n\n\n", "def files_list_from_zipfile(zip_path):\n\treturn [f.filename for f in zipfile.ZipFile(zip_path).filelist]\n\n", "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as z:\n\t\treturn z.namelist()\n\n\n", "def files_list_from_zipfile(zip_path):\n\tz = zipfile.ZipFile(zip_path)\n\tfiles = []\n\tfor name in z.namelist():\n\t\tif name.endswith('/'):\n\t\t\tcontinue\n\t\tfiles.append(name)\n\treturn files\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\treturn zf.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tzf = zipfile.ZipFile(zip_path)\n\treturn zf.namelist()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as zf:\n\t\tfiles = zf.namelist()\n\treturn files\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tzf = zipfile.ZipFile(zip_path, 'r')\n\treturn zf.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tzip_file = zipfile.ZipFile(zip_path)\n\treturn zip_file.namelist()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tfiles = []\n\twith zipfile.ZipFile(zip_path) as z:\n\t\tfor f in z.namelist():\n\t\t\tfiles.append(f)\n\treturn files\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path, 'r') as z:\n\t\treturn z.namelist()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\treturn [f.filename for f in zipfile.ZipFile(zip_path).filelist]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\twith zipfile.ZipFile(zip_path) as z:\n\t\treturn z.namelist()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def files_list_from_zipfile(zip_path):\n\tz = zipfile.ZipFile(zip_path)\n\tfiles = []\n\tfor name in z.namelist():\n\t\tif name.endswith('/'):\n\t\t\tcontinue\n\t\tfiles.append(name)\n\treturn files\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "fix_namespace_prefix_w", "docstring": "Convert text that defaults to 'w:st=\"' to 'w-st=\"'", "ques_id": "62b4631b3879012d194814dd", "code": ["def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=')\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=').replace('w:lang=', 'w-lang=')\n\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace(\"w:st=\", \"w-st=\")\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace(\"w:st=\", \"w-st=\")\n\n\n", "def fix_namespace_prefix_w(content):\n\tcontent = content.replace('w:st=', 'w-st=')\n\treturn content\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=')\n\t\n", "def fix_namespace_prefix_w(content):\n\treturn re.sub(r'(<[^ >]+)(xmlns:w=\"[^\"]*\")([^>]*>)', r'\\1\\3', content)\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace(u'w:st=', u'w-st=')\n\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace(\"w:st=\", \"w-st=\")\n\n\n", "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=')\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=').replace('w:lang=', 'w-lang=')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace(\"w:st=\", \"w-st=\")\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace(\"w:st=\", \"w-st=\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\tcontent = content.replace('w:st=', 'w-st=')\n\treturn content\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=')\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn re.sub(r'(<[^ >]+)(xmlns:w=\"[^\"]*\")([^>]*>)', r'\\1\\3', content)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace(u'w:st=', u'w-st=')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace(\"w:st=\", \"w-st=\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fix_namespace_prefix_w(content):\n\treturn content.replace('w:st=', 'w-st=')\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "packtools/sps/utils/xml_utils.py", "project": "scieloorg/packtools", "name": "match_pubdate", "docstring": "Returns the first match in the pubdate_xpaths list", "ques_id": "62b463283879012d1948153d", "code": ["def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_node = node.find(pubdate_xpath)\n\t\tif pubdate_node is not None:\n\t\t\treturn pubdate_node.text\n\n\treturn None\n\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_match = node.xpath(pubdate_xpath)\n\t\tif pubdate_match:\n\t\t\tpubdate_match = pubdate_match[0]\n\t\t\tbreak\n\telse:\n\t\tpubdate_match = None\n\n\treturn pubdate_match\n\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = get_text_content(node, pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate\n\treturn None\n\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_match = node.xpath(pubdate_xpath, namespaces=NAMESPACES)\n\t\tif pubdate_match:\n\t\t\treturn pubdate_match[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tresult = node.xpath(pubdate_xpath)\n\t\tif len(result) > 0:\n\t\t\treturn result[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "def match_pubdate(node, pubdate_xpaths):\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.find(xpath)\n\t\tif pubdate is not None:\n\t\t\treturn pubdate.text\n\treturn None\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_node = node.find(pubdate_xpath)\n\t\tif pubdate_node is not None:\n\t\t\treturn pubdate_node.text\n\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_match = node.xpath(pubdate_xpath)\n\t\tif pubdate_match:\n\t\t\tpubdate_match = pubdate_match[0]\n\t\t\tbreak\n\telse:\n\t\tpubdate_match = None\n\n\treturn pubdate_match\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = get_text_content(node, pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate_match = node.xpath(pubdate_xpath, namespaces=NAMESPACES)\n\t\tif pubdate_match:\n\t\t\treturn pubdate_match[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tresult = node.xpath(pubdate_xpath)\n\t\tif len(result) > 0:\n\t\t\treturn result[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor pubdate_xpath in pubdate_xpaths:\n\t\tpubdate = node.xpath(pubdate_xpath)\n\t\tif pubdate:\n\t\t\treturn pubdate[0]\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match_pubdate(node, pubdate_xpaths):\n\tfor xpath in pubdate_xpaths:\n\t\tpubdate = node.find(xpath)\n\t\tif pubdate is not None:\n\t\t\treturn pubdate.text\n\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "packtools/sps/models/front_articlemeta_issue.py", "project": "scieloorg/packtools", "name": "_extract_number_and_supplment_from_issue_element", "docstring": "Extract the possible values of number and suppl from the contents of issue.", "ques_id": "62b463303879012d19481579", "code": ["def _extract_number_and_supplment_from_issue_element(issue):\n\tcontents = issue.contents\n\tnumber_parts = None\n\tsup = None\n\tfor i in range(len(contents)):\n\t\tcontent = contents[i]\n\t\tif isinstance(content, BeautifulSoup.Tag) and content.name == 'sup':\n\t\t\tsup = content.string\n\t\t\tcontent.extract()\n\t\telif isinstance(content, BeautifulSoup.NavigableString):\n\t\t\tnumber_parts = content.string.strip().split()\n\t\t\tbreak\n\tif number_parts is None:\n\t\treturn None, None\n\tnumber = number_parts[0]\n\tif len(number_parts) > 1:\n\t\tsupplement = number_parts[1]\n\telse:\n\t\tsupplement = None\n\treturn number, supplement\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber_and_supplement = re.findall(r'^\\s*(\\d+)\\s*(.*)$', issue.text, re.I)\n\tif number_and_supplement:\n\t\treturn number_and_supplement[0]\n\telse:\n\t\treturn (None, None)\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\tif 'issue' not in issue:\n\t\treturn number, supplement\n\tfor child in issue.issue.children:\n\t\tif child.tag == 'issue':\n\t\t\tnumber = child.text\n\t\telif child.tag == 'supplement':\n\t\t\tsupplement = child.text\n\treturn number, supplement\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsup = None\n\tfor content in issue.contents:\n\t\tif isinstance(content, NavigableString):\n\t\t\tnumber = content\n\t\telif isinstance(content, Tag):\n\t\t\tif content.name == 'sup':\n\t\t\t\tsup = content.text\n\treturn number, sup\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\t# We don't know the type of the number and supplement elements, so we have to\n\t# check for all the possibilities\n\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number[@type=\"issue\"]')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number[@type=\"issueNumber\"]')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number[@type=\"issueNumberStart\"]')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\n\tif issue is not None:\n\t\tif issue.text is not None:\n\t\t\tnumber = issue.text\n\n\t\tsupplement_el = issue.find(\".//supplement\")\n\t\tif supplement_el is not None:\n\t\t\tif supplement_el.text is not None:\n\t\t\t\tsupplement = supplement_el.text\n\n\treturn number, supplement\n\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\tissue_text = issue.text\n\tif issue_text is None:\n\t\tissue_text = \"\"\n\telif issue.get('number') is not None:\n\t\tnumber = issue.get('number')\n\t\tissue_text = issue_text.replace(number, \"\")\n\tissue_text = issue_text.strip()\n\tif issue_text is not None and issue_text != \"\":\n\t\tif issue_text.startswith(\"suppl\"):\n\t\t\tsupplement = issue_text\n\t\telse:\n\t\t\tnumber = issue_text\n\treturn (number, supplement)\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsup = None\n\tissue_children = get_children(issue)\n\tfor child in issue_children:\n\t\tif child.tag == \"number\":\n\t\t\tnumber = get_text_from_element(child)\n\t\telif child.tag == \"supplement\":\n\t\t\tsup = get_text_from_element(child)\n\treturn (number, sup)\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_regex = re.compile(r'(?P<number>\\d+)[ ]*(?P<supplement>\\w*)')\n\tnumber = None\n\tsupplement = None\n\tmatch = issue_regex.search(issue.text)\n\tif match:\n\t\tnumber = match.group('number')\n\t\tsupplement = match.group('supplement')\n\treturn [number, supplement]\n\n", "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\n\ttry:\n\t\tnumber = int(issue.find(\"number\").text)\n\t\tif number < 1:\n\t\t\tnumber = None\n\texcept AttributeError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tsupplement = int(issue.find(\"supplement\").text)\n\t\tif supplement < 1:\n\t\t\tsupplement = None\n\texcept AttributeError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\texcept ValueError:\n\t\tpass\n\n\treturn (number, supplement)\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tcontents = issue.contents\n\tnumber_parts = None\n\tsup = None\n\tfor i in range(len(contents)):\n\t\tcontent = contents[i]\n\t\tif isinstance(content, BeautifulSoup.Tag) and content.name == 'sup':\n\t\t\tsup = content.string\n\t\t\tcontent.extract()\n\t\telif isinstance(content, BeautifulSoup.NavigableString):\n\t\t\tnumber_parts = content.string.strip().split()\n\t\t\tbreak\n\tif number_parts is None:\n\t\treturn None, None\n\tnumber = number_parts[0]\n\tif len(number_parts) > 1:\n\t\tsupplement = number_parts[1]\n\telse:\n\t\tsupplement = None\n\treturn number, supplement\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber_and_supplement = re.findall(r'^\\s*(\\d+)\\s*(.*)$', issue.text, re.I)\n\tif number_and_supplement:\n\t\treturn number_and_supplement[0]\n\telse:\n\t\treturn (None, None)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\tif 'issue' not in issue:\n\t\treturn number, supplement\n\tfor child in issue.issue.children:\n\t\tif child.tag == 'issue':\n\t\t\tnumber = child.text\n\t\telif child.tag == 'supplement':\n\t\t\tsupplement = child.text\n\treturn number, supplement\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsup = None\n\tfor content in issue.contents:\n\t\tif isinstance(content, NavigableString):\n\t\t\tnumber = content\n\t\telif isinstance(content, Tag):\n\t\t\tif content.name == 'sup':\n\t\t\t\tsup = content.text\n\treturn number, sup\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\t# We don't know the type of the number and supplement elements, so we have to\n\t# check for all the possibilities\n\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number[@type=\"issue\"]')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number[@type=\"issueNumber\"]')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/namespaces/basic/2.0/}number[@type=\"issueNumberStart\"]')\n\tif len(number_elements) == 0:\n\t\tnumber_elements = issue.findall('{http://prismstandard.org/\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\n\tif issue is not None:\n\t\tif issue.text is not None:\n\t\t\tnumber = issue.text\n\n\t\tsupplement_el = issue.find(\".//supplement\")\n\t\tif supplement_el is not None:\n\t\t\tif supplement_el.text is not None:\n\t\t\t\tsupplement = supplement_el.text\n\n\treturn number, supplement\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\tissue_text = issue.text\n\tif issue_text is None:\n\t\tissue_text = \"\"\n\telif issue.get('number') is not None:\n\t\tnumber = issue.get('number')\n\t\tissue_text = issue_text.replace(number, \"\")\n\tissue_text = issue_text.strip()\n\tif issue_text is not None and issue_text != \"\":\n\t\tif issue_text.startswith(\"suppl\"):\n\t\t\tsupplement = issue_text\n\t\telse:\n\t\t\tnumber = issue_text\n\treturn (number, supplement)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsup = None\n\tissue_children = get_children(issue)\n\tfor child in issue_children:\n\t\tif child.tag == \"number\":\n\t\t\tnumber = get_text_from_element(child)\n\t\telif child.tag == \"supplement\":\n\t\t\tsup = get_text_from_element(child)\n\treturn (number, sup)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tissue_regex = re.compile(r'(?P<number>\\d+)[ ]*(?P<supplement>\\w*)')\n\tnumber = None\n\tsupplement = None\n\tmatch = issue_regex.search(issue.text)\n\tif match:\n\t\tnumber = match.group('number')\n\t\tsupplement = match.group('supplement')\n\treturn [number, supplement]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _extract_number_and_supplment_from_issue_element(issue):\n\tnumber = None\n\tsupplement = None\n\n\ttry:\n\t\tnumber = int(issue.find(\"number\").text)\n\t\tif number < 1:\n\t\t\tnumber = None\n\texcept AttributeError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tsupplement = int(issue.find(\"supplement\").text)\n\t\tif supplement < 1:\n\t\t\tsupplement = None\n\texcept AttributeError:\n\t\tpass\n\texcept TypeError:\n\t\tpass\n\texcept ValueError:\n\t\tpass\n\n\treturn (number, supplement)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "pretty", "docstring": "Return a pretty formatted representation of self.", "ques_id": "62b46740d2f69a53b466171a", "code": ["def pretty(self, indent=0, debug=False):\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of this object.\n\t\t\"\"\"\n\t\treturn f\"<{self.__class__.__name__} {self.obj}>\"\n\n\t@property\n\tdef obj(self):\n\t\t\"\"\"\n\t\tThe object we are wrapping.\n\t\t\"\"\"\n\t\treturn self.__dict__[\"obj\"]\n\n\t@obj.setter\n\tdef obj(self, value):\n\t\t\"\"\"\n\t\tSet the object we are wrapping.\n\t\t\"\"\"\n\t\tself.__dict__[\"obj\"] = value\n\n\n", "def pretty(self, indent=0, debug=False):\n\tpass\n", "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (indent * \" \") + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\n", "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({self.obj})\"\n\n", "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tif debug:\n\t\tdebug_details = f\"({self.obj_type}, {self.type_arg}, {self.dim})\"\n\telse:\n\t\tdebug_details = \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}{debug_details}{obj}\"\n\n", "def pretty(self, indent=0, debug=False):\n\treturn self.__class__.__name__ + '(' + ')'\n", "def pretty(self, indent=0, debug=False):\n\treturn \"\"\n\n", "def pretty(self, indent=0, debug=False):\n\tpass\n\n", "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"  <{id(self)}> \" if debug else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pretty(self, indent=0, debug=False):\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\tReturn a string representation of this object.\n\t\t\"\"\"\n\t\treturn f\"<{self.__class__.__name__} {self.obj}>\"\n\n\t@property\n\tdef obj(self):\n\t\t\"\"\"\n\t\tThe object we are wrapping.\n\t\t\"\"\"\n\t\treturn self.__dict__[\"obj\"]\n\n\t@obj.setter\n\tdef obj(self, value):\n\t\t\"\"\"\n\t\tSet the object we are wrapping.\n\t\t\"\"\"\n\t\tself.__dict__[\"obj\"] = value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tpass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (indent * \" \") + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({self.obj})\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tif debug:\n\t\tdebug_details = f\"({self.obj_type}, {self.type_arg}, {self.dim})\"\n\telse:\n\t\tdebug_details = \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}{debug_details}{obj}\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\treturn self.__class__.__name__ + '(' + ')'\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\treturn \"\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pretty(self, indent=0, debug=False):\n\tobj = f\"'{self.obj}'\" if isinstance(self.obj, str) else repr(self.obj)\n\tdebug_details = f\"  <{id(self)}> \" if debug else \"\"\n\treturn (\" \" * indent) + f\"{self.__class__.__name__}({debug_details}{obj})\"\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "boolean/boolean.py", "project": "bastikr/boolean", "name": "absorb", "docstring": "Given an `args` sequence of expressions, return a new list of expression\napplying absorption and negative absorption.\n\nSee https://en.wikipedia.org/wiki/Absorption_law\n\nAbsorption::\n\n    A & (A | B) = A, A | (A & B) = A\n\nNegative absorption::\n\n    A & (~A | B) = A & B, A | (~A & B) = A | B", "ques_id": "62b46746d2f69a53b4661722", "code": ["def absorb(self, args):\n\tterms = args\n\tresult = []\n\t\n\twhile terms:\n\t\tterm, terms = terms[0], terms[1:]\n\t\t\n\t\tif term in result: continue\n\t\t\n\t\tresult.append(term)\n\t\t\n\t\tif term.is_zero: continue\n\t\t\n\t\tif term.is_one:\n\t\t\tresult = [term]\n\t\t\tbreak\n\t\t\n\t\tif term.is_minus_one:\n\t\t\tresult = [term]\n\t\t\tbreak\n\t\t\n\t\tif term.is_mul:\n\t\t\t# 0*x = 0\n\t\t\tif term.args[0].is_zero:\n\t\t\t\tresult = [term.args[0]]\n\t\t\t\tbreak\n\t\t\t# x*0 = 0\n\t\t\tif term.args[1].is_zero:\n\t\t\t\tresult = [term.args[1]]\n\t\t\t\tbreak\n\t\t\n\t\tif term.is_add:\n\t\t\t# 0+x = x\n\t\t\tif\n", "def absorb(self, args):\n\treturn [self.absorb_expr(expr) for expr in args]\n\n", "def absorb(self, args):\n\tnewargs = []\n\tfor arg in args:\n\t\tif not arg:\n\t\t\tcontinue\n\t\tif arg in self.absorbing:\n\t\t\treturn [arg]\n\t\tif arg not in self.absorbing_map:\n\t\t\tself.absorbing_map[arg] = []\n\t\tnewargs.append(arg)\n\t\tself.absorbing_map[arg].append(newargs)\n\treturn newargs\n\n\n", "def absorb(self, args):\n\tdef absorb_in(self, arg):\n\t\t\"\"\"\n\t\tAbsorb an expression `arg` in this expression.\n\t\t\"\"\"\n\t\tif isinstance(arg, Add):\n\t\t\treturn self.add(arg.args)\n\t\telif isinstance(arg, Mul):\n\t\t\treturn self.mul(arg.args)\n\t\telif isinstance(arg, Pow):\n\t\t\treturn self.pow(arg.args)\n\t\telse:\n\t\t\treturn self.mul((arg,))\n\n\treturn [ absorb_in(self, arg) for arg in args ]\n\n\n", "def absorb(self, args):\n\treturn [self.absorb_one(a) for a in args]\n\n\tdef absorb_one(self, a):\n\t\"\"\"\n\tGiven an expression `a`, apply absorption to it.\n\t\"\"\"\n\treturn self.absorb_inner(a)\n\n\tdef absorb_inner(self, a):\n\t\"\"\"\n\tGiven an expression `a`, apply absorption to it.\n\t\"\"\"\n\tassert isinstance(a, Expression)\n\tif a.is_atom():\n\t\treturn a\n\telif a.is_call():\n\t\tif a.is_absorbable():\n\t\t\treturn self.absorb_call(a.functor(), a.args())\n\t\telse:\n\t\t\treturn a\n\telif a.is_quantified():\n\t\tif a.is_absorbable():\n\t\t\treturn self.absorb_quant(a.quant(), a.bounds(), a.body())\n\t\telse:\n\t\t\treturn a\n\telse:\n\t\tassert False, \"Unexpected expression type: \" + str(a)\n\n\tdef absorb_quant(self,\n", "def absorb(self, args):\n\treturn [self._absorb(arg) for arg in args]\n\n", "def absorb(self, args):\n\t# Absorption of Null\n\tif len(args) == 0:\n\t\treturn [Null]\n\n\t# Absorption of One\n\tif len(args) == 1:\n\t\treturn [args[0]]\n\n\t# Absorption of Zero\n\tif Zero in args:\n\t\treturn [Zero]\n\n\t# Absorption of One\n\tif One in args:\n\t\treturn [One]\n\n\t# Absorption of Null\n\tif Null in args:\n\t\treturn [Null]\n\n\t# Absorption of Zero\n\tif Zero in args:\n\t\treturn [Zero]\n\n\t# Absorption of One\n\tif One in args:\n\t\treturn [One]\n\n\t# Absorption of Sum over Product\n\tif Sum in args and Product in args:\n\t\treturn [args[0]]\n\n\t# Absorption of Sum over One\n\tif Sum in args and One in args:\n\t\treturn [Sum]\n\n\t# Absorption of Product over Zero\n\tif Product in args and Zero in args:\n\t\treturn [Zero]\n\n\t# Absorption of Product over One\n", "def absorb(self, args):\n\treturn [self.absorb_expr(expr) for expr in args]\n\n\tdef absorb_expr(self, expr):\n\t\"\"\"\n\tGiven an `expr` expression, return a new expression applying absorption.\n\t\"\"\"\n\tif isinstance(expr, (list, tuple)):\n\t\treturn self.absorb(expr)\n\telif isinstance(expr, Absorption):\n\t\treturn expr.apply()\n\telif isinstance(expr, Variable):\n\t\treturn expr\n\telif isinstance(expr, (int, float)):\n\t\treturn expr\n\telif isinstance(expr, str):\n\t\treturn expr\n\telif isinstance(expr, (Fraction, Decimal)):\n\t\treturn expr\n\telse:\n\t\traise Exception(\"Invalid expression: \" + str(expr))\n", "def absorb(self, args):\n\treturn [self.absorb_one(e) for e in args]\n\n\n", "def absorb(self, args):\n\tresult = []\n\tfor e in args:\n\t\tif e.is_zero():\n\t\t\treturn [zero]\n\t\telif e.is_one():\n\t\t\treturn [one]\n\t\telse:\n\t\t\tresult.append(e)\n\treturn result\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def absorb(self, args):\n\tterms = args\n\tresult = []\n\t\n\twhile terms:\n\t\tterm, terms = terms[0], terms[1:]\n\t\t\n\t\tif term in result: continue\n\t\t\n\t\tresult.append(term)\n\t\t\n\t\tif term.is_zero: continue\n\t\t\n\t\tif term.is_one:\n\t\t\tresult = [term]\n\t\t\tbreak\n\t\t\n\t\tif term.is_minus_one:\n\t\t\tresult = [term]\n\t\t\tbreak\n\t\t\n\t\tif term.is_mul:\n\t\t\t# 0*x = 0\n\t\t\tif term.args[0].is_zero:\n\t\t\t\tresult = [term.args[0]]\n\t\t\t\tbreak\n\t\t\t# x*0 = 0\n\t\t\tif term.args[1].is_zero:\n\t\t\t\tresult = [term.args[1]]\n\t\t\t\tbreak\n\t\t\n\t\tif term.is_add:\n\t\t\t# 0+x = x\n\t\t\tif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\treturn [self.absorb_expr(expr) for expr in args]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tnewargs = []\n\tfor arg in args:\n\t\tif not arg:\n\t\t\tcontinue\n\t\tif arg in self.absorbing:\n\t\t\treturn [arg]\n\t\tif arg not in self.absorbing_map:\n\t\t\tself.absorbing_map[arg] = []\n\t\tnewargs.append(arg)\n\t\tself.absorbing_map[arg].append(newargs)\n\treturn newargs\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tdef absorb_in(self, arg):\n\t\t\"\"\"\n\t\tAbsorb an expression `arg` in this expression.\n\t\t\"\"\"\n\t\tif isinstance(arg, Add):\n\t\t\treturn self.add(arg.args)\n\t\telif isinstance(arg, Mul):\n\t\t\treturn self.mul(arg.args)\n\t\telif isinstance(arg, Pow):\n\t\t\treturn self.pow(arg.args)\n\t\telse:\n\t\t\treturn self.mul((arg,))\n\n\treturn [ absorb_in(self, arg) for arg in args ]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\treturn [self.absorb_one(a) for a in args]\n\n\tdef absorb_one(self, a):\n\t\"\"\"\n\tGiven an expression `a`, apply absorption to it.\n\t\"\"\"\n\treturn self.absorb_inner(a)\n\n\tdef absorb_inner(self, a):\n\t\"\"\"\n\tGiven an expression `a`, apply absorption to it.\n\t\"\"\"\n\tassert isinstance(a, Expression)\n\tif a.is_atom():\n\t\treturn a\n\telif a.is_call():\n\t\tif a.is_absorbable():\n\t\t\treturn self.absorb_call(a.functor(), a.args())\n\t\telse:\n\t\t\treturn a\n\telif a.is_quantified():\n\t\tif a.is_absorbable():\n\t\t\treturn self.absorb_quant(a.quant(), a.bounds(), a.body())\n\t\telse:\n\t\t\treturn a\n\telse:\n\t\tassert False, \"Unexpected expression type: \" + str(a)\n\n\tdef absorb_quant(self,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\treturn [self._absorb(arg) for arg in args]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\t# Absorption of Null\n\tif len(args) == 0:\n\t\treturn [Null]\n\n\t# Absorption of One\n\tif len(args) == 1:\n\t\treturn [args[0]]\n\n\t# Absorption of Zero\n\tif Zero in args:\n\t\treturn [Zero]\n\n\t# Absorption of One\n\tif One in args:\n\t\treturn [One]\n\n\t# Absorption of Null\n\tif Null in args:\n\t\treturn [Null]\n\n\t# Absorption of Zero\n\tif Zero in args:\n\t\treturn [Zero]\n\n\t# Absorption of One\n\tif One in args:\n\t\treturn [One]\n\n\t# Absorption of Sum over Product\n\tif Sum in args and Product in args:\n\t\treturn [args[0]]\n\n\t# Absorption of Sum over One\n\tif Sum in args and One in args:\n\t\treturn [Sum]\n\n\t# Absorption of Product over Zero\n\tif Product in args and Zero in args:\n\t\treturn [Zero]\n\n\t# Absorption of Product over One\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\treturn [self.absorb_expr(expr) for expr in args]\n\n\tdef absorb_expr(self, expr):\n\t\"\"\"\n\tGiven an `expr` expression, return a new expression applying absorption.\n\t\"\"\"\n\tif isinstance(expr, (list, tuple)):\n\t\treturn self.absorb(expr)\n\telif isinstance(expr, Absorption):\n\t\treturn expr.apply()\n\telif isinstance(expr, Variable):\n\t\treturn expr\n\telif isinstance(expr, (int, float)):\n\t\treturn expr\n\telif isinstance(expr, str):\n\t\treturn expr\n\telif isinstance(expr, (Fraction, Decimal)):\n\t\treturn expr\n\telse:\n\t\traise Exception(\"Invalid expression: \" + str(expr))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\treturn [self.absorb_one(e) for e in args]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def absorb(self, args):\n\tresult = []\n\tfor e in args:\n\t\tif e.is_zero():\n\t\t\treturn [zero]\n\t\telif e.is_one():\n\t\t\treturn [one]\n\t\telse:\n\t\t\tresult.append(e)\n\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "krake/krake/controller/kubernetes/hooks.py", "project": "rak-n-rok/Krake", "name": "on", "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.", "ques_id": "62b86707b4d922cb0e688c2a", "code": ["def on(self, hook):\n\tdef decorator(func):\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n\n", "def on(self, hook):\n\tdef decorator(f):\n\t\told_handlers = self._handlers.get(hook, [])\n\t\told_handlers.append(f)\n\t\tself._handlers[hook] = old_handlers\n\t\treturn f\n\treturn decorator\n\n", "def on(self, hook):\n\tdef on_decorator(handler):\n\t\tself.handlers[hook].append(handler)\n\t\treturn handler\n\treturn on_decorator\n\n", "def on(self, hook):\n\tdef decorator(func):\n\t\tif hook not in self.registry:\n\t\t\tself.registry[hook] = []\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n\tdef emit(self, hook, *args, **kwargs):\n\t\t\"\"\"\n\t\tEmit a signal on a certain hook, which will call all the handler functions for that hook.\n\t\t\"\"\"\n\t\thandlers = self.registry.get(hook, [])\n\t\tfor func in handlers:\n\t\t\tfunc(*args, **kwargs)\n", "def on(self, hook):\n\tdef add_handler(func):\n\t\tself.registry[hook].add(func)\n\t\treturn func\n\treturn add_handler\n\n", "def on(self, hook):\n\tdef decorator(handler):\n\t\tself.registry[hook].append(handler)\n\t\treturn handler\n\treturn decorator\n\n", "def on(self, hook):\n\t\n\tdef decorator(handler):\n\t\tif not isinstance(handler, HookHandler):\n\t\t\traise TypeError(\"Handler must be a subclass of HookHandler\")\n\t\tif not isinstance(hook, str):\n\t\t\traise TypeError(\"Hook must be a string\")\n\t\tif not hasattr(self, \"_handlers\"):\n\t\t\tself._handlers = {}\n\t\tif not hook in self._handlers:\n\t\t\tself._handlers[hook] = []\n\t\tself._handlers[hook].append(handler)\n\t\treturn handler\n\treturn decorator\n\n", "def on(self, hook):\n\tdef decorator(handler):\n\t\ttry:\n\t\t\tself.registry[hook].append(handler)\n\t\texcept KeyError:\n\t\t\tself.registry[hook] = [handler]\n\t\treturn handler\n\treturn decorator\n\n", "def on(self, hook):\n\tdef decorator(handler):\n\t\tself.register(hook, handler)\n\t\treturn handler\n\treturn decorator\n\n\n", "def on(self, hook):\n\t\n\tdef decorator(func):\n\t\tif hook not in self.registry:\n\t\t\tself.registry[hook] = []\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\t\n\treturn decorator\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def on(self, hook):\n\tdef decorator(func):\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\tdef decorator(f):\n\t\told_handlers = self._handlers.get(hook, [])\n\t\told_handlers.append(f)\n\t\tself._handlers[hook] = old_handlers\n\t\treturn f\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef on_decorator(handler):\n\t\tself.handlers[hook].append(handler)\n\t\treturn handler\n\treturn on_decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorator(func):\n\t\tif hook not in self.registry:\n\t\t\tself.registry[hook] = []\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\treturn decorator\n\n\tdef emit(self, hook, *args, **kwargs):\n\t\t\"\"\"\n\t\tEmit a signal on a certain hook, which will call all the handler functions for that hook.\n\t\t\"\"\"\n\t\thandlers = self.registry.get(hook, [])\n\t\tfor func in handlers:\n\t\t\tfunc(*args, **kwargs)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\tdef add_handler(func):\n\t\tself.registry[hook].add(func)\n\t\treturn func\n\treturn add_handler\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorator(handler):\n\t\tself.registry[hook].append(handler)\n\t\treturn handler\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\t\n\tdef decorator(handler):\n\t\tif not isinstance(handler, HookHandler):\n\t\t\traise TypeError(\"Handler must be a subclass of HookHandler\")\n\t\tif not isinstance(hook, str):\n\t\t\traise TypeError(\"Hook must be a string\")\n\t\tif not hasattr(self, \"_handlers\"):\n\t\t\tself._handlers = {}\n\t\tif not hook in self._handlers:\n\t\t\tself._handlers[hook] = []\n\t\tself._handlers[hook].append(handler)\n\t\treturn handler\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\tdef decorator(handler):\n\t\ttry:\n\t\t\tself.registry[hook].append(handler)\n\t\texcept KeyError:\n\t\t\tself.registry[hook] = [handler]\n\t\treturn handler\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def on(self, hook):\n\tdef decorator(handler):\n\t\tself.register(hook, handler)\n\t\treturn handler\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def on(self, hook):\n\t\n\tdef decorator(func):\n\t\tif hook not in self.registry:\n\t\t\tself.registry[hook] = []\n\t\tself.registry[hook].append(func)\n\t\treturn func\n\t\n\treturn decorator\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "krake/tests/conftest.py", "project": "rak-n-rok/Krake", "name": "base_config", "docstring": "Creates a configuration with some simple parameters, which have a default value\nthat can be set.\n\nArgs:\n    user (str): the name of the user for the static authentication\n    etcd_host (str): the host for the database.\n    etcd_port (int): the port for the database.\n\nReturns:\n    dict: the created configuration.", "ques_id": "62b86729b4d922cb0e688c2f", "code": ["def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"key\": \"./test/data/{0}_key.pem\".format(user),\n\t\t\t\"cert\": \"./test/data/{0}_cert.pem\".format(user),\n\t\t\t\"ca\": \"./test/data/ca_cert.pem\",\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"method\": \"token\",\n\t\t\t\"token\": \"testtoken\",\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"method\": \"static\",\n\t\t\t\"groups\": {\n\t\t\t\t\"group1\": [\"user1\", \"user2\"],\n\t\t\t\t\"group2\": [\"user3\"]\n\t\t\t},\n\t\t\t\"permissions\": {\n\t\t\t\t\"user1\": [\"write\", \"read\"],\n\t\t\t\t\"user2\": [\"read\"],\n\t\t\t\t\"user3\": [\"read\"]\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t'log': {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"format\": \"%(asctime)s %(levelname)s [%(name)s] %(message)s\"\n\t\t},\n\t\t'tls': {\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"address\": \"\",\n\t\t\t\"port\": \"\",\n\t\t\t\"key_passphrase\": None\n\t\t},\n\t\t'authentication': {\n\t\t\t'user': user,\n\t\t\t'password': '',\n\t\t\t'jwt': {\n\t\t\t\t'key': '',\n\t\t\t\t'issuer': '',\n\t\t\t\t'algorithm': '',\n\t\t\t\t'expires_in': 60\n\t\t\t}\n\t\t},\n\t\t'authorization': {\n\t\t\t'enabled': True,\n\t\t\t'roles': [\n\t\t\t\t{\n\t\t\t\t\t'name': \"admin\",\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"key\": user.key,\n\t\t\t\"cert\": user.cert,\n\t\t\t\"ca_cert\": ca.cert\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"certificate\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"root_ca\": ca.cert\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": True\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"endpoints\": \"https://{}:{}\".format(etcd_host, etcd_port),\n\t\t\t\"key_file\": user.key,\n\t\t\t\"cert_file\": user.cert,\n\t\t\t\"ca_file\": ca.cert,\n\t\t\t\"quota_backend_bytes\": 1073741824,\n\t\t\t\"username\": user.name,\n\t\t\t\"password\": user.passwd\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True\n\t\t},\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig['log'] = {\n\t\t'level': 'info',\n\t\t'path': 'stdout',\n\t\t'format': 'json'\n\t}\n\tconfig['tls'] = {\n\t\t'cert': 'tests/test_certs/server/cert.pem',\n\t\t'key': 'tests/test_certs/server/key.pem',\n\t\t'ca': 'tests/test_certs/ca/cacert.pem'\n\t}\n\tconfig['authentication'] = {\n\t\t'jwt': {\n\t\t\t'algorithm': 'HS256',\n\t\t\t'key': 'secret'\n\t\t}\n\t}\n\tconfig['authorization'] = {\n\t\t'roles': {\n\t\t\t'admin': {\n\t\t\t\t'permissions': '*'\n\t\t\t},\n\t\t\t'user': {\n\t\t\t\t'permissions': [\n\t\t\t\t\t'user.read',\n\t\t\t\t\t'user.write'\n\t\t\t\t]\n\t\t\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"authentication\": {\n\t\t\t\"token\": {\n\t\t\t\t\"user\": user,\n\t\t\t\t\"secret\": \"user-password\"\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"allow_all\": True\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"url\": \"http://localhost\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"pretty\": True\n\t\t}\n\t}\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig['tls'] = {\n\t\t\"enabled\": True,\n\t\t\"cert_file\": \"certs/%s.crt\" % user,\n\t\t\"key_file\": \"certs/%s.key\" % user,\n\t\t\"ca_file\": \"certs/ca.crt\"\n\t}\n\tconfig['authentication'] = {\n\t\t\"enabled\": True,\n\t\t\"methods\": [\"x509\"],\n\t\t\"users\": [\n\t\t\t{\n\t\t\t\t\"name\": user,\n\t\t\t\t\"groups\": [\"system:masters\"],\n\t\t\t\t\"extra\": {\n\t\t\t\t\t\"email\": \"%s@example.com\" % user\n\t\t\t\t}\n\t\t\t}\n\t\t]\n\t}\n\tconfig['authorization'] = {\n\t\t\"enabled\": True,\n\t\t\"mode\": \"AlwaysAllow\",\n\t\t\"groups\": [\"system:masters\"],\n\t}\n\tconfig['etcd'] = {\n\t\t\"host\": etcd_host,\n\t\t\"port\":\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\",\n\t\t\t\"ca\": \"\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"etcd\",\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": \"etcd\",\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8080,\n\t\t\t\"path\": \"/docs\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\",\n\t\t\t\"ca\": \"\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t\t\"key\": \"api-gateway\"\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8080\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"path\": \"./logs\"\n\t\t}\n\t}\n\treturn config\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enable\": False,\n\t\t\t\"cert_file\": \"\",\n\t\t\t\"key_file\": \"\"\n\t\t}, \n\t\t\"authentication\": {\n\t\t\t\"enable\": False,\n\t\t\t\"key\": \"\",\n\t\t\t\"expiration\": 3600\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enable\": False,\n\t\t\t\"key\": \"\",\n\t\t\t\"expiration\": 3600\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t\t\"prefix\": \"/etcd/{}\".format(user),\n\t\t\t\"timeout\": 3\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enable\": False,\n\t\t\t\"path\": \"\",\n\t\t\t\"expiration\": 3600\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"path\": \"server.log\"\n\n", "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig[\"tls\"] = {\n\t\t\"cert\": \"certs/%s.crt\" % user,\n\t\t\"key\": \"certs/%s.key\" % user,\n\t\t\"ca\": \"certs/ca.crt\"\n\t}\n\tconfig[\"authentication\"] = {\n\t\t\"username\": user,\n\t\t\"password\": user\n\t}\n\tconfig[\"authorization\"] = {\n\t\t\"username\": user\n\t}\n\tconfig[\"etcd\"] = {\n\t\t\"host\": etcd_host,\n\t\t\"port\": etcd_port\n\t}\n\tconfig[\"docs\"] = \"docs\"\n\tconfig[\"log\"] = {\n\t\t\"level\": \"debug\"\n\t}\n\treturn config\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"key\": \"./test/data/{0}_key.pem\".format(user),\n\t\t\t\"cert\": \"./test/data/{0}_cert.pem\".format(user),\n\t\t\t\"ca\": \"./test/data/ca_cert.pem\",\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"method\": \"token\",\n\t\t\t\"token\": \"testtoken\",\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"method\": \"static\",\n\t\t\t\"groups\": {\n\t\t\t\t\"group1\": [\"user1\", \"user2\"],\n\t\t\t\t\"group2\": [\"user3\"]\n\t\t\t},\n\t\t\t\"permissions\": {\n\t\t\t\t\"user1\": [\"write\", \"read\"],\n\t\t\t\t\"user2\": [\"read\"],\n\t\t\t\t\"user3\": [\"read\"]\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t'log': {\n\t\t\t\"level\": \"debug\",\n\t\t\t\"format\": \"%(asctime)s %(levelname)s [%(name)s] %(message)s\"\n\t\t},\n\t\t'tls': {\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\",\n\t\t\t\"ca\": \"\",\n\t\t\t\"address\": \"\",\n\t\t\t\"port\": \"\",\n\t\t\t\"key_passphrase\": None\n\t\t},\n\t\t'authentication': {\n\t\t\t'user': user,\n\t\t\t'password': '',\n\t\t\t'jwt': {\n\t\t\t\t'key': '',\n\t\t\t\t'issuer': '',\n\t\t\t\t'algorithm': '',\n\t\t\t\t'expires_in': 60\n\t\t\t}\n\t\t},\n\t\t'authorization': {\n\t\t\t'enabled': True,\n\t\t\t'roles': [\n\t\t\t\t{\n\t\t\t\t\t'name': \"admin\",\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"key\": user.key,\n\t\t\t\"cert\": user.cert,\n\t\t\t\"ca_cert\": ca.cert\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"certificate\": {\n\t\t\t\t\"enabled\": True,\n\t\t\t\t\"root_ca\": ca.cert\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": True\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"endpoints\": \"https://{}:{}\".format(etcd_host, etcd_port),\n\t\t\t\"key_file\": user.key,\n\t\t\t\"cert_file\": user.cert,\n\t\t\t\"ca_file\": ca.cert,\n\t\t\t\"quota_backend_bytes\": 1073741824,\n\t\t\t\"username\": user.name,\n\t\t\t\"password\": user.passwd\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": True\n\t\t},\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig['log'] = {\n\t\t'level': 'info',\n\t\t'path': 'stdout',\n\t\t'format': 'json'\n\t}\n\tconfig['tls'] = {\n\t\t'cert': 'tests/test_certs/server/cert.pem',\n\t\t'key': 'tests/test_certs/server/key.pem',\n\t\t'ca': 'tests/test_certs/ca/cacert.pem'\n\t}\n\tconfig['authentication'] = {\n\t\t'jwt': {\n\t\t\t'algorithm': 'HS256',\n\t\t\t'key': 'secret'\n\t\t}\n\t}\n\tconfig['authorization'] = {\n\t\t'roles': {\n\t\t\t'admin': {\n\t\t\t\t'permissions': '*'\n\t\t\t},\n\t\t\t'user': {\n\t\t\t\t'permissions': [\n\t\t\t\t\t'user.read',\n\t\t\t\t\t'user.write'\n\t\t\t\t]\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"authentication\": {\n\t\t\t\"token\": {\n\t\t\t\t\"user\": user,\n\t\t\t\t\"secret\": \"user-password\"\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"allow_all\": True\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"url\": \"http://localhost\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"pretty\": True\n\t\t}\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig['tls'] = {\n\t\t\"enabled\": True,\n\t\t\"cert_file\": \"certs/%s.crt\" % user,\n\t\t\"key_file\": \"certs/%s.key\" % user,\n\t\t\"ca_file\": \"certs/ca.crt\"\n\t}\n\tconfig['authentication'] = {\n\t\t\"enabled\": True,\n\t\t\"methods\": [\"x509\"],\n\t\t\"users\": [\n\t\t\t{\n\t\t\t\t\"name\": user,\n\t\t\t\t\"groups\": [\"system:masters\"],\n\t\t\t\t\"extra\": {\n\t\t\t\t\t\"email\": \"%s@example.com\" % user\n\t\t\t\t}\n\t\t\t}\n\t\t]\n\t}\n\tconfig['authorization'] = {\n\t\t\"enabled\": True,\n\t\t\"mode\": \"AlwaysAllow\",\n\t\t\"groups\": [\"system:masters\"],\n\t}\n\tconfig['etcd'] = {\n\t\t\"host\": etcd_host,\n\t\t\"port\":\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\",\n\t\t\t\"ca\": \"\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"type\": \"etcd\",\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t}\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"type\": \"etcd\",\n\t\t\t\"etcd\": {\n\t\t\t\t\"host\": etcd_host,\n\t\t\t\t\"port\": etcd_port\n\t\t\t}\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8080,\n\t\t\t\"path\": \"/docs\"\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"debug\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {\n\t\t\"tls\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"cert\": \"\",\n\t\t\t\"key\": \"\",\n\t\t\t\"ca\": \"\"\n\t\t},\n\t\t\"authentication\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"type\": \"none\"\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t\t\"key\": \"api-gateway\"\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enabled\": False,\n\t\t\t\"host\": \"localhost\",\n\t\t\t\"port\": 8080\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"path\": \"./logs\"\n\t\t}\n\t}\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\treturn {\n\t\t\"tls\": {\n\t\t\t\"enable\": False,\n\t\t\t\"cert_file\": \"\",\n\t\t\t\"key_file\": \"\"\n\t\t}, \n\t\t\"authentication\": {\n\t\t\t\"enable\": False,\n\t\t\t\"key\": \"\",\n\t\t\t\"expiration\": 3600\n\t\t},\n\t\t\"authorization\": {\n\t\t\t\"enable\": False,\n\t\t\t\"key\": \"\",\n\t\t\t\"expiration\": 3600\n\t\t},\n\t\t\"etcd\": {\n\t\t\t\"host\": etcd_host,\n\t\t\t\"port\": etcd_port,\n\t\t\t\"prefix\": \"/etcd/{}\".format(user),\n\t\t\t\"timeout\": 3\n\t\t},\n\t\t\"docs\": {\n\t\t\t\"enable\": False,\n\t\t\t\"path\": \"\",\n\t\t\t\"expiration\": 3600\n\t\t},\n\t\t\"log\": {\n\t\t\t\"level\": \"info\",\n\t\t\t\"path\": \"server.log\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def base_config(user, etcd_host=\"localhost\", etcd_port=2379):\n\tconfig = {}\n\tconfig[\"tls\"] = {\n\t\t\"cert\": \"certs/%s.crt\" % user,\n\t\t\"key\": \"certs/%s.key\" % user,\n\t\t\"ca\": \"certs/ca.crt\"\n\t}\n\tconfig[\"authentication\"] = {\n\t\t\"username\": user,\n\t\t\"password\": user\n\t}\n\tconfig[\"authorization\"] = {\n\t\t\"username\": user\n\t}\n\tconfig[\"etcd\"] = {\n\t\t\"host\": etcd_host,\n\t\t\"port\": etcd_port\n\t}\n\tconfig[\"docs\"] = \"docs\"\n\tconfig[\"log\"] = {\n\t\t\"level\": \"debug\"\n\t}\n\treturn config\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "_fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d2", "code": ["def _fromutc(self, dt):\n\treturn dt.astimezone(self.tz)\n\n", "def _fromutc(self, dt):\n\treturn dt.replace(tzinfo=self.tzinfo).astimezone(self.tzinfo)\n\n", "def _fromutc(self, dt):\n\treturn dt + self.utcoffset(dt)\n\n", "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"Given datetime is not timezone-aware\")\n\treturn dt.replace(tzinfo=self.tz)\n\n", "def _fromutc(self, dt):\n\tdt = dt.replace(tzinfo=self.tz)\n\treturn dt.astimezone(self.newtz)\n\n\n\n", "def _fromutc(self, dt):\n\treturn dt.replace(tzinfo=self)\n\n\n", "def _fromutc(self, dt):\n\treturn dt.replace(tzinfo=self.tzfrom).astimezone(self.tzto)\n\n", "def _fromutc(self, dt):\n\tdt = dt.replace(tzinfo=None) - dt.utcoffset()\n\tdt = dt.replace(tzinfo=self.timezone)\n\treturn dt\n\n", "def _fromutc(self, dt):\n\t#dt = self.utc_to_local(dt)\n\t#dt = self.local_to_utc(dt)\n\t\n\tdt = self.utc_to_local(dt)\n\tdt = self.local_to_utc(dt)\n\treturn dt\n\t\n", "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\treturn dt\n\telse:\n\t\t#return dt.astimezone(self.tz)\n\t\treturn dt.replace(tzinfo=self.tz)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _fromutc(self, dt):\n\treturn dt.astimezone(self.tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\treturn dt.replace(tzinfo=self.tzinfo).astimezone(self.tzinfo)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\treturn dt + self.utcoffset(dt)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"Given datetime is not timezone-aware\")\n\treturn dt.replace(tzinfo=self.tz)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tdt = dt.replace(tzinfo=self.tz)\n\treturn dt.astimezone(self.newtz)\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\treturn dt.replace(tzinfo=self)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\treturn dt.replace(tzinfo=self.tzfrom).astimezone(self.tzto)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tdt = dt.replace(tzinfo=None) - dt.utcoffset()\n\tdt = dt.replace(tzinfo=self.timezone)\n\treturn dt\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\t#dt = self.utc_to_local(dt)\n\t#dt = self.local_to_utc(dt)\n\t\n\tdt = self.utc_to_local(dt)\n\tdt = self.local_to_utc(dt)\n\treturn dt\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\treturn dt\n\telse:\n\t\t#return dt.astimezone(self.tz)\n\t\treturn dt.replace(tzinfo=self.tz)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/relativedelta.py", "project": "pexip/os-python-dateutil", "name": "normalized", "docstring": "Return a version of this object represented entirely using integer\nvalues for the relative attributes.\n\n>>> relativedelta(days=1.5, hours=2).normalized()\nrelativedelta(days=+1, hours=+14)\n\n:return:\n    Returns a :class:`dateutil.relativedelta.relativedelta` object.", "ques_id": "62b8982f755ee91dce50a241", "code": ["def normalized(self):\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n\treturn self\n\n", "def normalized(self):\n\tfor key in self.__dict__:\n\t\tif key.endswith('_units'):\n\t\t\tunits = getattr(self, key)\n\t\t\tif units:\n\t\t\t\ttime = getattr(self, key.split('_')[0])\n\t\t\t\tsetattr(self, key.split('_')[0], time * units)\n\t\t\t\tsetattr(self, key, 1)\n\t\t\t\t\n", "def normalized(self):\n\tself.year = int(self.year)\n\tself.month = int(self.month)\n\tself.day = int(self.day)\n\tself.hour = int(self.hour)\n\tself.minute = int(self.minute)\n\tself.second = int(self.second)\n\tself.microsecond = int(self.microsecond)\n\treturn self\n\n", "def normalized(self):\n\tself.years   = int(self.years)\n\tself.months  = int(self.months)\n\tself.weeks   = int(self.weeks)\n\tself.days    = int(self.days)\n\tself.hours   = int(self.hours)\n\tself.minutes = int(self.minutes)\n\tself.seconds = int(self.seconds)\n\treturn self\n\n", "def normalized(self):\n\tdef _normalized(self):\n\t\treturn self.to_seconds()\n\treturn _normalized\n\n\n", "def normalized(self):\n\tif self.seconds < 0:\n\t\tsign = -1\n\telse:\n\t\tsign = 1\n\tself.seconds = abs(self.seconds)\n\tminutes, self.seconds = divmod(self.seconds, 60)\n\thours, minutes = divmod(minutes, 60)\n\tdays, hours = divmod(hours, 24)\n\tyears, days = divmod(days, 365)\n\tself.years = years\n\tself.days = days\n\tself.hours = hours\n\tself.minutes = minutes\n\tself.seconds = sign * self.seconds\n\treturn self\n\n", "def normalized(self):\n\treturn Time(self.years,\n\t\t    self.months,\n\t\t    self.days,\n\t\t    self.hours,\n\t\t    self.minutes,\n\t\t    self.seconds)\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn the string representation of this time.\n\t\t\"\"\"\n\t\treturn str(self.years) + 'y ' + \\\n\t\t\tstr(self.months) + 'm ' + \\\n\t\t\tstr(self.days) + 'd ' + \\\n\t\t\tstr(self.hours) + 'h ' + \\\n\t\t\tstr(self.minutes) + 'm ' + \\\n\t\t\tstr(self.seconds) + 's '\n\n\tdef __add__(self, other):\n\t\t\"\"\"\n\t\tAdd two times together.\n\t\t\"\"\"\n\t\tif not isinstance(other, Time):\n\t\t\treturn NotImplemented\n\t\treturn self.normalized + other.normalized\n\n\tdef __sub__(self, other):\n\t\t\"\"\"\n\t\tSubtract two times.\n", "def normalized(self):\n\tif self.unit in ['seconds', 'second', 'sec', 'secs', 's']:\n\t\treturn self.value\n\telif self.unit in ['minutes', 'minute', 'min', 'mins', 'm']:\n\t\treturn self.value * 60\n\telif self.unit in ['hours', 'hour', 'hr', 'hrs', 'h']:\n\t\treturn self.value * 3600\n\telif self.unit in ['days', 'day', 'd']:\n\t\treturn self.value * 86400\n\telif self.unit in ['weeks', 'week', 'w']:\n\t\treturn self.value * 604800\n\telif self.unit in ['months', 'month', 'months']:\n\t\treturn self.value * 2592000\n\telif self.unit in ['years', 'year', 'yrs', 'yr']:\n\t\treturn self.value * 31536000\n\telse:\n\t\treturn self.value\n\n", "def normalized(self):\n\tself.days\t= self.days // 1\n\tself.hours\t= self.hours // 1\n\tself.minutes\t= self.minutes // 1\n\tself.seconds\t= self.seconds // 1\n\tself.microseconds = self.microseconds // 1\n\treturn self\n\n", "def normalized(self):\n\tresult = self.copy()\n\tresult.weeks = result.weeks * 7\n\tresult.days = result.days + result.weeks * 7\n\tresult.weeks = 0\n\tresult.hours = result.hours + result.days * 24\n\tresult.days = 0\n\tresult.minutes = result.minutes + result.hours * 60\n\tresult.hours = 0\n\tresult.seconds = result.seconds + result.minutes * 60\n\tresult.minutes = 0\n\treturn result\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def normalized(self):\n\tself.seconds = int(self.seconds)\n\tself.minutes = int(self.minutes)\n\tself.hours = int(self.hours)\n\tself.days = int(self.days)\n\tself.weeks = int(self.weeks)\n\tself.months = int(self.months)\n\tself.years = int(self.years)\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tfor key in self.__dict__:\n\t\tif key.endswith('_units'):\n\t\t\tunits = getattr(self, key)\n\t\t\tif units:\n\t\t\t\ttime = getattr(self, key.split('_')[0])\n\t\t\t\tsetattr(self, key.split('_')[0], time * units)\n\t\t\t\tsetattr(self, key, 1)\n\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tself.year = int(self.year)\n\tself.month = int(self.month)\n\tself.day = int(self.day)\n\tself.hour = int(self.hour)\n\tself.minute = int(self.minute)\n\tself.second = int(self.second)\n\tself.microsecond = int(self.microsecond)\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tself.years   = int(self.years)\n\tself.months  = int(self.months)\n\tself.weeks   = int(self.weeks)\n\tself.days    = int(self.days)\n\tself.hours   = int(self.hours)\n\tself.minutes = int(self.minutes)\n\tself.seconds = int(self.seconds)\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tdef _normalized(self):\n\t\treturn self.to_seconds()\n\treturn _normalized\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tif self.seconds < 0:\n\t\tsign = -1\n\telse:\n\t\tsign = 1\n\tself.seconds = abs(self.seconds)\n\tminutes, self.seconds = divmod(self.seconds, 60)\n\thours, minutes = divmod(minutes, 60)\n\tdays, hours = divmod(hours, 24)\n\tyears, days = divmod(days, 365)\n\tself.years = years\n\tself.days = days\n\tself.hours = hours\n\tself.minutes = minutes\n\tself.seconds = sign * self.seconds\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\treturn Time(self.years,\n\t\t    self.months,\n\t\t    self.days,\n\t\t    self.hours,\n\t\t    self.minutes,\n\t\t    self.seconds)\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\tReturn the string representation of this time.\n\t\t\"\"\"\n\t\treturn str(self.years) + 'y ' + \\\n\t\t\tstr(self.months) + 'm ' + \\\n\t\t\tstr(self.days) + 'd ' + \\\n\t\t\tstr(self.hours) + 'h ' + \\\n\t\t\tstr(self.minutes) + 'm ' + \\\n\t\t\tstr(self.seconds) + 's '\n\n\tdef __add__(self, other):\n\t\t\"\"\"\n\t\tAdd two times together.\n\t\t\"\"\"\n\t\tif not isinstance(other, Time):\n\t\t\treturn NotImplemented\n\t\treturn self.normalized + other.normalized\n\n\tdef __sub__(self, other):\n\t\t\"\"\"\n\t\tSubtract two times.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tif self.unit in ['seconds', 'second', 'sec', 'secs', 's']:\n\t\treturn self.value\n\telif self.unit in ['minutes', 'minute', 'min', 'mins', 'm']:\n\t\treturn self.value * 60\n\telif self.unit in ['hours', 'hour', 'hr', 'hrs', 'h']:\n\t\treturn self.value * 3600\n\telif self.unit in ['days', 'day', 'd']:\n\t\treturn self.value * 86400\n\telif self.unit in ['weeks', 'week', 'w']:\n\t\treturn self.value * 604800\n\telif self.unit in ['months', 'month', 'months']:\n\t\treturn self.value * 2592000\n\telif self.unit in ['years', 'year', 'yrs', 'yr']:\n\t\treturn self.value * 31536000\n\telse:\n\t\treturn self.value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tself.days\t= self.days // 1\n\tself.hours\t= self.hours // 1\n\tself.minutes\t= self.minutes // 1\n\tself.seconds\t= self.seconds // 1\n\tself.microseconds = self.microseconds // 1\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def normalized(self):\n\tresult = self.copy()\n\tresult.weeks = result.weeks * 7\n\tresult.days = result.days + result.weeks * 7\n\tresult.weeks = 0\n\tresult.hours = result.hours + result.days * 24\n\tresult.days = 0\n\tresult.minutes = result.minutes + result.hours * 60\n\tresult.hours = 0\n\tresult.seconds = result.seconds + result.minutes * 60\n\tresult.minutes = 0\n\treturn result\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "tzname_in_python2", "docstring": "Change unicode output into bytestrings in Python 2\n\ntzname() API changed in Python 3. It used to return bytes, but was changed\nto unicode strings", "ques_id": "62b89640755ee91dce50a114", "code": ["def tzname_in_python2(namefunc):\n\tif PY2:\n\t\tnamefunc.__name__ = namefunc.__name__.encode(\"ascii\")\n\treturn namefunc\n\n", "def tzname_in_python2(namefunc):\n\tdef wrapper(self, dt):\n\t\tnames = namefunc(self, dt)\n\t\tif isinstance(names, tuple):\n\t\t\treturn tuple(self.encode(x) for x in names)\n\t\telse:\n\t\t\treturn self.encode(names)\n\treturn wrapper\n\n\n", "def tzname_in_python2(namefunc):\n\t@functools.wraps(namefunc)\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif not isinstance(result, str):\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\n\n\n", "def tzname_in_python2(namefunc):\n\t@functools.wraps(namefunc)\n\tdef wrapper(dt, *args, **kwargs):\n\t\tresult = namefunc(dt, *args, **kwargs)\n\t\tif isinstance(result, unicode):\n\t\t\treturn result.encode(\"utf-8\")\n\t\treturn result\n\treturn wrapper\n\n", "def tzname_in_python2(namefunc):\n\tif not PY2:\n\t\treturn namefunc\n\treturn lambda: [force_bytes(x) for x in namefunc()]\n\n\n", "def tzname_in_python2(namefunc):\n\t@wraps(namefunc)\n\tdef wrapper(self, dt):\n\t\tname = namefunc(self, dt)\n\t\tif name and PY2:\n\t\t\tname = name.encode('ascii')\n\t\treturn name\n\treturn wrapper\n\n\n", "def tzname_in_python2(namefunc):\n\t@wraps(namefunc)\n\tdef inner(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif not isinstance(result, tuple):\n\t\t\tresult = (result,)\n\t\tif PYTHON2:\n\t\t\treturn tuple(x.encode('utf-8') if isinstance(x, unicode) else x for x in result)\n\t\treturn result\n\treturn inner\n\n", "def tzname_in_python2(namefunc):\n\tif PY2:\n\t\tdef wrapped(*args):\n\t\t\treturn namefunc(*args).encode('utf-8')\n\telse:\n\t\twrapped = namefunc\n\treturn wrapped\n\n\n", "def tzname_in_python2(namefunc):\n\t@wraps(namefunc)\n\tdef wrapper(self, *args, **kwargs):\n\t\tresult = namefunc(self, *args, **kwargs)\n\t\tif isinstance(result, unicode):\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\n\n\n", "def tzname_in_python2(namefunc):\n\tdef wrapper(self, *args, **kwargs):\n\t\tres = namefunc(self, *args, **kwargs)\n\t\tif not isinstance(res, str):\n\t\t\tres = res.encode('utf-8')\n\t\treturn res\n\treturn wrapper\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def tzname_in_python2(namefunc):\n\tif PY2:\n\t\tnamefunc.__name__ = namefunc.__name__.encode(\"ascii\")\n\treturn namefunc\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\tdef wrapper(self, dt):\n\t\tnames = namefunc(self, dt)\n\t\tif isinstance(names, tuple):\n\t\t\treturn tuple(self.encode(x) for x in names)\n\t\telse:\n\t\t\treturn self.encode(names)\n\treturn wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t@functools.wraps(namefunc)\n\tdef wrapper(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif not isinstance(result, str):\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\t@functools.wraps(namefunc)\n\tdef wrapper(dt, *args, **kwargs):\n\t\tresult = namefunc(dt, *args, **kwargs)\n\t\tif isinstance(result, unicode):\n\t\t\treturn result.encode(\"utf-8\")\n\t\treturn result\n\treturn wrapper\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\tif not PY2:\n\t\treturn namefunc\n\treturn lambda: [force_bytes(x) for x in namefunc()]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t@wraps(namefunc)\n\tdef wrapper(self, dt):\n\t\tname = namefunc(self, dt)\n\t\tif name and PY2:\n\t\t\tname = name.encode('ascii')\n\t\treturn name\n\treturn wrapper\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t@wraps(namefunc)\n\tdef inner(*args, **kwargs):\n\t\tresult = namefunc(*args, **kwargs)\n\t\tif not isinstance(result, tuple):\n\t\t\tresult = (result,)\n\t\tif PYTHON2:\n\t\t\treturn tuple(x.encode('utf-8') if isinstance(x, unicode) else x for x in result)\n\t\treturn result\n\treturn inner\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\tif PY2:\n\t\tdef wrapped(*args):\n\t\t\treturn namefunc(*args).encode('utf-8')\n\telse:\n\t\twrapped = namefunc\n\treturn wrapped\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def tzname_in_python2(namefunc):\n\t@wraps(namefunc)\n\tdef wrapper(self, *args, **kwargs):\n\t\tresult = namefunc(self, *args, **kwargs)\n\t\tif isinstance(result, unicode):\n\t\t\tresult = result.encode('utf-8')\n\t\treturn result\n\treturn wrapper\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def tzname_in_python2(namefunc):\n\tdef wrapper(self, *args, **kwargs):\n\t\tres = namefunc(self, *args, **kwargs)\n\t\tif not isinstance(res, str):\n\t\t\tres = res.encode('utf-8')\n\t\treturn res\n\treturn wrapper\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_versions", "docstring": "Get version information or return default if unable to do so.", "ques_id": "62b87d24d292efb640a55670", "code": ["def get_versions():\n\tglobal version, version_info, version_date, version_author, version_copyright, version_license\n\n\ttry:\n\t\timport pkg_resources\n\n\t\tversion = pkg_resources.get_distribution(\"pyspectools\").version  # @UndefinedVariable\n\t\tversion_info = tuple(int(v) for v in version.split(\".\"))\n\n\texcept:\n\t\tversion = \"0.0.0\"\n\t\tversion_info = (0, 0, 0)\n\n\tversion_date = \"2018-07-18\"\n\tversion_author = \"Christopher Simpkins\"\n\tversion_copyright = \"Copyright (c) 2018 Christopher Simpkins\"\n\tversion_license = \"MIT License\"\n\n", "def get_versions():\n\ttry:\n\t\treturn _get_versions()\n\texcept:\n\t\treturn {\n\t\t\t\"version\": \"0.0.0\",\n\t\t\t\"build_date\": \"1970-01-01T00:00:00.000+00:00\",\n\t\t\t\"commit_hash\": \"\",\n\t\t\t\"commit_dirty\": False\n\t\t}\n\n", "def get_versions():\n\tversions = {}\n\ttry:\n\t\tversions['platform'] = platform.platform()\n\texcept:\n\t\tversions['platform'] = 'unknown platform'\n\n\ttry:\n\t\tversions['python'] = platform.python_version()\n\texcept:\n\t\tversions['python'] = 'unknown python version'\n\n\ttry:\n\t\tversions['pip'] = pip.__version__\n\texcept:\n\t\tversions['pip'] = 'unknown pip version'\n\n\ttry:\n\t\tversions['pip-review'] = pkg_resources.get_distribution('pip-review').version\n\texcept:\n\t\tversions['pip-review'] = 'unknown pip-review version'\n\n\ttry:\n\t\tversions['pipdeptree'] = pkg_resources.get_distribution('pipdeptree').version\n\texcept:\n\t\tversions['pipdeptree'] = 'unknown pipdeptree version'\n\n\ttry:\n\t\tversions['pipdeptree-local'] = pkg_resources.get_distribution('pipdeptree').version\n\texcept:\n\n", "def get_versions():\n\ttry:\n\t\tversions = {}\n\t\tversions['python'] = platform.python_version()\n\t\tversions['numpy'] = np.__version__\n\t\tversions['scikit-learn'] = sklearn.__version__\n\t\tversions['pandas'] = pd.__version__\n\t\tversions['scipy'] = sp.__version__\n\t\tversions['xgboost'] = xgb.__version__\n\t\tversions['lightgbm'] = lgb.__version__\n\t\tversions['catboost'] = cb.__version__\n\texcept Exception as e:\n\t\tversions = None\n\t\tprint(\"Version information cannot be obtained.\")\n\t\tprint(e)\n\treturn versions\n\n\n", "def get_versions():\n\treturn {\n\t\t\"kaitaistruct\": kaitaistruct_version.__version__,\n\t\t\"kaitai-struct-compiler\": kaitai_struct_compiler_version.__version__,\n\t\t\"kaitai-struct-formats\": kaitai_struct_formats_version.__version__,\n\t}\n\n\n", "def get_versions():\n\ttry:\n\t\tversion = _get_versions()\n\texcept:\n\t\tversion = {}\n\t\tversion['version'] = 'unknown'\n\t\tversion['gitrev'] = 'unknown'\n\t\tversion['giturl'] = 'unknown'\n\t\tversion['gitauthor'] = 'unknown'\n\t\tversion['gitdate'] = 'unknown'\n\treturn version\n\n", "def get_versions():\n\ttry:\n\t\tversions = json.loads(urllib.request.urlopen(\"https://raw.githubusercontent.com/M4GNV5/M4GNV5-Mod-Manager/master/version.json\").read().decode(\"utf-8\"))\n\texcept urllib.error.URLError:\n\t\tversions = {\n\t\t\t\"app\": {\n\t\t\t\t\"version\": \"1.0.0\",\n\t\t\t\t\"url\": \"https://github.com/M4GNV5/M4GNV5-Mod-Manager/releases/latest\"\n\t\t\t},\n\t\t\t\"mods\": {\n\t\t\t\t\"version\": \"1.0.0\",\n\t\t\t\t\"url\": \"https://raw.githubusercontent.com/M4GNV5/M4GNV5-Mod-Manager/master/mods.json\"\n\t\t\t}\n\t\t}\n\treturn versions\n\n", "def get_versions():\n\ttry:\n\t\tversion = requests.get('https://raw.githubusercontent.com/TomasJohansson/adventofcode/master/version.json').json()\n\t\treturn version\n\texcept Exception as e:\n\t\tprint('The version information could not be obtained. Using default version.')\n\t\treturn {\n\t\t\t'challenges': {\n\t\t\t\t'latest': '2015-12-25'\n\t\t\t},\n\t\t\t'solutions': {\n\t\t\t\t'latest': '2015-12-25'\n\t\t\t}\n\t\t}\n\n", "def get_versions():\n\tversions = {}\n\ttry:\n\t\twith open(VERSION_FILE, \"r\") as version_file:\n\t\t\tversions = json.load(version_file)\n\texcept FileNotFoundError:\n\t\tpass\n\texcept ValueError:\n\t\tpass\n\treturn versions\n\n", "def get_versions():\n\ttry:\n\t\treturn version.get_versions(verbose=True)\n\texcept Exception:\n\t\treturn {'version': '0.0.0', 'full-revisionid': '0', 'error': 'Unable to obtain version information.'}\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_versions():\n\tglobal version, version_info, version_date, version_author, version_copyright, version_license\n\n\ttry:\n\t\timport pkg_resources\n\n\t\tversion = pkg_resources.get_distribution(\"pyspectools\").version  # @UndefinedVariable\n\t\tversion_info = tuple(int(v) for v in version.split(\".\"))\n\n\texcept:\n\t\tversion = \"0.0.0\"\n\t\tversion_info = (0, 0, 0)\n\n\tversion_date = \"2018-07-18\"\n\tversion_author = \"Christopher Simpkins\"\n\tversion_copyright = \"Copyright (c) 2018 Christopher Simpkins\"\n\tversion_license = \"MIT License\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\treturn _get_versions()\n\texcept:\n\t\treturn {\n\t\t\t\"version\": \"0.0.0\",\n\t\t\t\"build_date\": \"1970-01-01T00:00:00.000+00:00\",\n\t\t\t\"commit_hash\": \"\",\n\t\t\t\"commit_dirty\": False\n\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\tversions = {}\n\ttry:\n\t\tversions['platform'] = platform.platform()\n\texcept:\n\t\tversions['platform'] = 'unknown platform'\n\n\ttry:\n\t\tversions['python'] = platform.python_version()\n\texcept:\n\t\tversions['python'] = 'unknown python version'\n\n\ttry:\n\t\tversions['pip'] = pip.__version__\n\texcept:\n\t\tversions['pip'] = 'unknown pip version'\n\n\ttry:\n\t\tversions['pip-review'] = pkg_resources.get_distribution('pip-review').version\n\texcept:\n\t\tversions['pip-review'] = 'unknown pip-review version'\n\n\ttry:\n\t\tversions['pipdeptree'] = pkg_resources.get_distribution('pipdeptree').version\n\texcept:\n\t\tversions['pipdeptree'] = 'unknown pipdeptree version'\n\n\ttry:\n\t\tversions['pipdeptree-local'] = pkg_resources.get_distribution('pipdeptree').version\n\texcept:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\tversions = {}\n\t\tversions['python'] = platform.python_version()\n\t\tversions['numpy'] = np.__version__\n\t\tversions['scikit-learn'] = sklearn.__version__\n\t\tversions['pandas'] = pd.__version__\n\t\tversions['scipy'] = sp.__version__\n\t\tversions['xgboost'] = xgb.__version__\n\t\tversions['lightgbm'] = lgb.__version__\n\t\tversions['catboost'] = cb.__version__\n\texcept Exception as e:\n\t\tversions = None\n\t\tprint(\"Version information cannot be obtained.\")\n\t\tprint(e)\n\treturn versions\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\treturn {\n\t\t\"kaitaistruct\": kaitaistruct_version.__version__,\n\t\t\"kaitai-struct-compiler\": kaitai_struct_compiler_version.__version__,\n\t\t\"kaitai-struct-formats\": kaitai_struct_formats_version.__version__,\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\tversion = _get_versions()\n\texcept:\n\t\tversion = {}\n\t\tversion['version'] = 'unknown'\n\t\tversion['gitrev'] = 'unknown'\n\t\tversion['giturl'] = 'unknown'\n\t\tversion['gitauthor'] = 'unknown'\n\t\tversion['gitdate'] = 'unknown'\n\treturn version\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\tversions = json.loads(urllib.request.urlopen(\"https://raw.githubusercontent.com/M4GNV5/M4GNV5-Mod-Manager/master/version.json\").read().decode(\"utf-8\"))\n\texcept urllib.error.URLError:\n\t\tversions = {\n\t\t\t\"app\": {\n\t\t\t\t\"version\": \"1.0.0\",\n\t\t\t\t\"url\": \"https://github.com/M4GNV5/M4GNV5-Mod-Manager/releases/latest\"\n\t\t\t},\n\t\t\t\"mods\": {\n\t\t\t\t\"version\": \"1.0.0\",\n\t\t\t\t\"url\": \"https://raw.githubusercontent.com/M4GNV5/M4GNV5-Mod-Manager/master/mods.json\"\n\t\t\t}\n\t\t}\n\treturn versions\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\tversion = requests.get('https://raw.githubusercontent.com/TomasJohansson/adventofcode/master/version.json').json()\n\t\treturn version\n\texcept Exception as e:\n\t\tprint('The version information could not be obtained. Using default version.')\n\t\treturn {\n\t\t\t'challenges': {\n\t\t\t\t'latest': '2015-12-25'\n\t\t\t},\n\t\t\t'solutions': {\n\t\t\t\t'latest': '2015-12-25'\n\t\t\t}\n\t\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\tversions = {}\n\ttry:\n\t\twith open(VERSION_FILE, \"r\") as version_file:\n\t\t\tversions = json.load(version_file)\n\texcept FileNotFoundError:\n\t\tpass\n\texcept ValueError:\n\t\tpass\n\treturn versions\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_versions():\n\ttry:\n\t\treturn version.get_versions(verbose=True)\n\texcept Exception:\n\t\treturn {'version': '0.0.0', 'full-revisionid': '0', 'error': 'Unable to obtain version information.'}\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "render", "docstring": "Render the given version pieces into the requested style.", "ques_id": "62b87d24d292efb640a5566f", "code": ["def render(pieces, style):\n\tpass\n\n", "def render(pieces, style):\n\tif style == 'HTML':\n\t\treturn render_html(pieces)\n\telif style == 'PDF':\n\t\treturn render_pdf(pieces)\n\telif style == 'TXT':\n\t\treturn render_txt(pieces)\n\telse:\n\t\traise Exception(\"Unknown style: %s\" % style)\n\n", "def render(pieces, style):\n\tif style == 1:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style1()\n\telif style == 2:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style2()\n\telif style == 3:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style3()\n\telif style == 4:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style4()\n\telif style == 5:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style5()\n\telse:\n\t\tpass\n\n\n", "def render(pieces, style):\n\tif style == 'csv':\n\t\trender_csv(pieces)\n\telif style == 'json':\n\t\trender_json(pieces)\n\telif style == 'xml':\n\t\trender_xml(pieces)\n\telse:\n\t\trender_table(pieces)\n\n", "def render(pieces, style):\n\tif style == \"html\":\n\t\treturn render_html(pieces)\n\telif style == \"html-custom\":\n\t\treturn render_html_custom(pieces)\n\telif style == \"markdown\":\n\t\treturn render_markdown(pieces)\n\telif style == \"latex\":\n\t\treturn render_latex(pieces)\n\telse:\n\t\traise Exception(\"Unrecognized style: \" + style)\n\n", "def render(pieces, style):\n\tif style == \"html\":\n\t\treturn render_html(pieces)\n\telif style == \"latex\":\n\t\treturn render_latex(pieces)\n\telif style == \"plain\":\n\t\treturn render_plain(pieces)\n\telse:\n\t\traise ValueError(\"Invalid style\")\n\n", "def render(pieces, style):\n\tif style == \"r\":\n\t\treturn render_r(pieces)\n\telif style == \"n\":\n\t\treturn render_n(pieces)\n\telse:\n\t\treturn render_s(pieces)\n\n", "def render(pieces, style):\n\tfor piece in pieces:\n\t\tprint(piece)\n\t\tprint()\n\t\ttime.sleep(3)\n\t\tprint(\"\\033c\")\n\n", "def render(pieces, style):\n\t# Get the width and height of the image\n\twidth = 0\n\theight = 0\n\tfor piece in pieces:\n\t\twidth = max(width, piece[0] + piece[2])\n\t\theight = max(height, piece[1] + piece[3])\n\n\t# Create the image\n\timg = Image.new('RGB', (width, height), (255, 255, 255))\n\tdraw = ImageDraw.Draw(img)\n\tfor piece in pieces:\n\t\tdraw_piece(draw, piece, style)\n\n\t# Show the image\n\timg.show()\n\n", "def render(pieces, style):\n\tif style == 's':\n\t\trender_simple(pieces)\n\telif style == 'c':\n\t\trender_complex(pieces)\n\telse:\n\t\tpass\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def render(pieces, style):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == 'HTML':\n\t\treturn render_html(pieces)\n\telif style == 'PDF':\n\t\treturn render_pdf(pieces)\n\telif style == 'TXT':\n\t\treturn render_txt(pieces)\n\telse:\n\t\traise Exception(\"Unknown style: %s\" % style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == 1:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style1()\n\telif style == 2:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style2()\n\telif style == 3:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style3()\n\telif style == 4:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style4()\n\telif style == 5:\n\t\tfor piece in pieces:\n\t\t\tpiece.render_style5()\n\telse:\n\t\tpass\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == 'csv':\n\t\trender_csv(pieces)\n\telif style == 'json':\n\t\trender_json(pieces)\n\telif style == 'xml':\n\t\trender_xml(pieces)\n\telse:\n\t\trender_table(pieces)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == \"html\":\n\t\treturn render_html(pieces)\n\telif style == \"html-custom\":\n\t\treturn render_html_custom(pieces)\n\telif style == \"markdown\":\n\t\treturn render_markdown(pieces)\n\telif style == \"latex\":\n\t\treturn render_latex(pieces)\n\telse:\n\t\traise Exception(\"Unrecognized style: \" + style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == \"html\":\n\t\treturn render_html(pieces)\n\telif style == \"latex\":\n\t\treturn render_latex(pieces)\n\telif style == \"plain\":\n\t\treturn render_plain(pieces)\n\telse:\n\t\traise ValueError(\"Invalid style\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == \"r\":\n\t\treturn render_r(pieces)\n\telif style == \"n\":\n\t\treturn render_n(pieces)\n\telse:\n\t\treturn render_s(pieces)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tfor piece in pieces:\n\t\tprint(piece)\n\t\tprint()\n\t\ttime.sleep(3)\n\t\tprint(\"\\033c\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\t# Get the width and height of the image\n\twidth = 0\n\theight = 0\n\tfor piece in pieces:\n\t\twidth = max(width, piece[0] + piece[2])\n\t\theight = max(height, piece[1] + piece[3])\n\n\t# Create the image\n\timg = Image.new('RGB', (width, height), (255, 255, 255))\n\tdraw = ImageDraw.Draw(img)\n\tfor piece in pieces:\n\t\tdraw_piece(draw, piece, style)\n\n\t# Show the image\n\timg.show()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def render(pieces, style):\n\tif style == 's':\n\t\trender_simple(pieces)\n\telif style == 'c':\n\t\trender_complex(pieces)\n\telse:\n\t\tpass\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "plus_or_dot", "docstring": "Return a + if we don't already have one, else return a .", "ques_id": "62b87d24d292efb640a5566d", "code": ["def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n", "def plus_or_dot(pieces):\n\tfor p in pieces:\n\t\tif \"+\" in p:\n\t\t\treturn \".\"\n\treturn \"+\"\n\n\n", "def plus_or_dot(pieces):\n\tif '+' in pieces[0]:\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "def plus_or_dot(pieces):\n\tcloset_tag = pieces[-1]\n\tif closet_tag[0] == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\t# Find the closet tag.\n\tcloset_tag = pieces[0]\n\tfor piece in pieces:\n\t\tif len(closet_tag) > len(piece):\n\t\t\tcloset_tag = piece\n\t# Return \"+\" if the closet tag contains \"+\", otherwise, return \".\"\n\tif \"+\" in closet_tag:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\tfor piece in pieces:\n\t\tif \"+\" in piece:\n\t\t\treturn \".\"\n\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\treturn \"+\"\n\n", "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tfor p in pieces:\n\t\tif \"+\" in p:\n\t\t\treturn \".\"\n\treturn \"+\"\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n\tif '+' in pieces[0]:\n\t\treturn '.'\n\telse:\n\t\treturn '+'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tcloset_tag = pieces[-1]\n\tif closet_tag[0] == \"+\":\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\t# Find the closet tag.\n\tcloset_tag = pieces[0]\n\tfor piece in pieces:\n\t\tif len(closet_tag) > len(piece):\n\t\t\tcloset_tag = piece\n\t# Return \"+\" if the closet tag contains \"+\", otherwise, return \".\"\n\tif \"+\" in closet_tag:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tfor piece in pieces:\n\t\tif \"+\" in piece:\n\t\t\treturn \".\"\n\treturn \"+\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def plus_or_dot(pieces):\n\tif \"+\" in pieces[-1]:\n\t\treturn \".\"\n\telse:\n\t\treturn \"+\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62b87d23d292efb640a5566b", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, list)\n\tp = None\n\tfor c in commands:\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = subprocess.Popen(c + args, cwd=cwd, stdout=subprocess.PIPE,\n\t\t\t                     stderr=(subprocess.PIPE if hide_stderr\n\t\t\t                             else None), env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = get_exception()\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tcontinue\n\t\t\tif verbose:\n\t\t\t\tstderr_write('%s\\n' % e)\n\t\t\treturn None\n\telse:\n\t\tif verbose:\n\t\t\tstderr_write('Failed to execute command: %s\\n' % args[0])\n\t\treturn None\n\tstdout = p.communicate()[0].strip()\n\tif sys.version_\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, list)\n\tif env is None:\n\t\tenv = os.environ.copy()\n\tenv['LC_ALL'] = 'C'\n\tif hide_stderr:\n\t\tstderr = subprocess.DEVNULL\n\telse:\n\t\tstderr = subprocess.STDOUT\n\tif len(commands) == 1:\n\t\tcommand = commands[0]\n\telse:\n\t\tcommand = '{ ' + '; '.join(commands) + '; }'\n\tif verbose:\n\t\tprint(\"Running command:\", command)\n\tsubprocess.check_call(command, shell=True, cwd=cwd, stderr=stderr, env=env)\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tresult = None\n\tfor command in commands:\n\t\ttry:\n\t\t\tsubprocess.check_call(\n\t\t\t\tcommand + args,\n\t\t\t\tshell=True,\n\t\t\t\tcwd=cwd,\n\t\t\t\tstdout=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tenv=env)\n\t\t\tresult = 0\n\t\t\tbreak\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tresult = e.returncode\n\t\t\tif verbose:\n\t\t\t\tprint('Command failed: %s' % command)\n\treturn result\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\" \".join(list(map(lambda x: shlex.quote(str(x)), [command] + args))))\n\n\t\tif hide_stderr:\n\t\t\twith open(os.devnull, 'w') as devnull:\n\t\t\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env, stdout=devnull)\n\t\telse:\n\t\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env)\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tif not env:\n\t\tenv = os.environ\n\t# Prepare the command to be executed.\n\tcommands = [shlex.split(c % args) for c in commands]\n\t# Print the command(s).\n\tif verbose:\n\t\tprint(\"Running: %s\" % \" && \".join([\" \".join(c) for c in commands]))\n\tif not hide_stderr:\n\t\tstderr = None\n\telse:\n\t\tstderr = subprocess.STDOUT\n\t# Execute the command(s).\n\tfor command in commands:\n\t\ttry:\n\t\t\tsubprocess.check_call(command, cwd=cwd, env=env, stderr=stderr)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tprint(\"Command failed: %s\" % \" \".join(command))\n\t\t\tsys.exit(e.\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, list)\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tp = None\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tlog.info('Running: %s' % ' '.join(command))\n\t\t\tsys.stdout.flush()\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = subprocess.Popen(command, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = sys.exc_info()[1]\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tcontinue\n\t\t\tif verbose:\n\t\t\t\tlog.exception('Failed to execute command %s' % command)\n\t\t\telse:\n\t\t\t\tlog.error('Failed to execute command %s' % command)\n\t\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif len(commands) == 1:\n\t\tif hide_stderr:\n\t\t\tstderr = subprocess.DEVNULL\n\t\telse:\n\t\t\tstderr = subprocess.STDOUT\n\t\tcommand = commands[0]\n\t\tif verbose:\n\t\t\tprint(command)\n\t\tsubprocess.check_call(command, cwd=cwd, shell=True, stdout=sys.stdout, stderr=stderr, env=env)\n\t\treturn\n\telse:\n\t\tfor command in commands:\n\t\t\trun_command(command, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# default the cwd to the current directory\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\t# default the env to the current process environment\n\tif env is None:\n\t\tenv = os.environ\n\n\t# make sure we have a list of commands\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(\"Running command(s) in '%s': %s\" % (cwd, commands))\n\n\t# construct the command to run\n\tcommand = ' '.join(commands)\n\n\t# run the command\n\tif verbose:\n\t\tstdout = None\n\telse:\n\t\tstdout = subprocess.PIPE\n\n\tif hide_stderr:\n\t\tstderr = subprocess.PIPE\n\telif verbose:\n\t\tstderr = None\n\telse:\n\t\tstderr = subprocess.STDOUT\n\n\tproc = subprocess.Popen(command, cwd=cwd, stdout=stdout, stderr=stderr\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"running '{command}' in '{cwd}'\".format(\n\t\t\t\tcommand=command,\n\t\t\t\tcwd=cwd or os.getcwd(),\n\t\t\t))\n\t\tsubprocess_args = {\n\t\t\t\"args\": [command] + args,\n\t\t\t\"stdout\": subprocess.PIPE,\n\t\t\t\"cwd\": cwd,\n\t\t\t\"env\": os.environ if env is None else env,\n\t\t}\n\t\tif hide_stderr:\n\t\t\tsubprocess_args[\"stderr\"] = subprocess.PIPE\n\t\telse:\n\t\t\tsubprocess_args[\"stderr\"] = subprocess.STDOUT\n\n\t\tprocess = subprocess.Popen(**subprocess_args)\n\t\toutput, _ = process.communicate()\n\t\tif process.returncode != 0:\n\t\t\traise Exception\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\t# Hide stderr if requested.\n\tstderr = None if hide_stderr else subprocess.STDOUT\n\n\tfor command in commands:\n\t\tcommand_line = command.format(**args)\n\n\t\tif verbose:\n\t\t\tprint(command_line)\n\n\t\tif not env:\n\t\t\tenv = dict(os.environ)\n\n\t\tif sys.platform == 'win32':\n\t\t\tshell = True\n\t\t\tcommand_line = command_line.replace('\\\\', '\\\\\\\\')\n\t\telse:\n\t\t\tshell = False\n\n\t\tsubprocess.check_call(command_line, cwd=cwd, env=env, shell=shell, stderr=stderr)\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, list)\n\tp = None\n\tfor c in commands:\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = subprocess.Popen(c + args, cwd=cwd, stdout=subprocess.PIPE,\n\t\t\t                     stderr=(subprocess.PIPE if hide_stderr\n\t\t\t                             else None), env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = get_exception()\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tcontinue\n\t\t\tif verbose:\n\t\t\t\tstderr_write('%s\\n' % e)\n\t\t\treturn None\n\telse:\n\t\tif verbose:\n\t\t\tstderr_write('Failed to execute command: %s\\n' % args[0])\n\t\treturn None\n\tstdout = p.communicate()[0].strip()\n\tif sys.version_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, list)\n\tif env is None:\n\t\tenv = os.environ.copy()\n\tenv['LC_ALL'] = 'C'\n\tif hide_stderr:\n\t\tstderr = subprocess.DEVNULL\n\telse:\n\t\tstderr = subprocess.STDOUT\n\tif len(commands) == 1:\n\t\tcommand = commands[0]\n\telse:\n\t\tcommand = '{ ' + '; '.join(commands) + '; }'\n\tif verbose:\n\t\tprint(\"Running command:\", command)\n\tsubprocess.check_call(command, shell=True, cwd=cwd, stderr=stderr, env=env)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tresult = None\n\tfor command in commands:\n\t\ttry:\n\t\t\tsubprocess.check_call(\n\t\t\t\tcommand + args,\n\t\t\t\tshell=True,\n\t\t\t\tcwd=cwd,\n\t\t\t\tstdout=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tenv=env)\n\t\t\tresult = 0\n\t\t\tbreak\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tresult = e.returncode\n\t\t\tif verbose:\n\t\t\t\tprint('Command failed: %s' % command)\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\" \".join(list(map(lambda x: shlex.quote(str(x)), [command] + args))))\n\n\t\tif hide_stderr:\n\t\t\twith open(os.devnull, 'w') as devnull:\n\t\t\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env, stdout=devnull)\n\t\telse:\n\t\t\tsubprocess.check_call([command] + args, cwd=cwd, env=env)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tif not env:\n\t\tenv = os.environ\n\t# Prepare the command to be executed.\n\tcommands = [shlex.split(c % args) for c in commands]\n\t# Print the command(s).\n\tif verbose:\n\t\tprint(\"Running: %s\" % \" && \".join([\" \".join(c) for c in commands]))\n\tif not hide_stderr:\n\t\tstderr = None\n\telse:\n\t\tstderr = subprocess.STDOUT\n\t# Execute the command(s).\n\tfor command in commands:\n\t\ttry:\n\t\t\tsubprocess.check_call(command, cwd=cwd, env=env, stderr=stderr)\n\t\texcept subprocess.CalledProcessError as e:\n\t\t\tprint(\"Command failed: %s\" % \" \".join(command))\n\t\t\tsys.exit(e.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tassert isinstance(commands, list)\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\tp = None\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tlog.info('Running: %s' % ' '.join(command))\n\t\t\tsys.stdout.flush()\n\t\ttry:\n\t\t\t# remember shell=False, so use git.cmd on windows, not just git\n\t\t\tp = subprocess.Popen(command, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\t\tbreak\n\t\texcept EnvironmentError:\n\t\t\te = sys.exc_info()[1]\n\t\t\tif e.errno == errno.ENOENT:\n\t\t\t\tcontinue\n\t\t\tif verbose:\n\t\t\t\tlog.exception('Failed to execute command %s' % command)\n\t\t\telse:\n\t\t\t\tlog.error('Failed to execute command %s' % command)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif len(commands) == 1:\n\t\tif hide_stderr:\n\t\t\tstderr = subprocess.DEVNULL\n\t\telse:\n\t\t\tstderr = subprocess.STDOUT\n\t\tcommand = commands[0]\n\t\tif verbose:\n\t\t\tprint(command)\n\t\tsubprocess.check_call(command, cwd=cwd, shell=True, stdout=sys.stdout, stderr=stderr, env=env)\n\t\treturn\n\telse:\n\t\tfor command in commands:\n\t\t\trun_command(command, args, cwd=cwd, verbose=verbose, hide_stderr=hide_stderr, env=env)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\t# default the cwd to the current directory\n\tif cwd is None:\n\t\tcwd = os.getcwd()\n\n\t# default the env to the current process environment\n\tif env is None:\n\t\tenv = os.environ\n\n\t# make sure we have a list of commands\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tif verbose:\n\t\tprint(\"Running command(s) in '%s': %s\" % (cwd, commands))\n\n\t# construct the command to run\n\tcommand = ' '.join(commands)\n\n\t# run the command\n\tif verbose:\n\t\tstdout = None\n\telse:\n\t\tstdout = subprocess.PIPE\n\n\tif hide_stderr:\n\t\tstderr = subprocess.PIPE\n\telif verbose:\n\t\tstderr = None\n\telse:\n\t\tstderr = subprocess.STDOUT\n\n\tproc = subprocess.Popen(command, cwd=cwd, stdout=stdout, stderr=stderr\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"running '{command}' in '{cwd}'\".format(\n\t\t\t\tcommand=command,\n\t\t\t\tcwd=cwd or os.getcwd(),\n\t\t\t))\n\t\tsubprocess_args = {\n\t\t\t\"args\": [command] + args,\n\t\t\t\"stdout\": subprocess.PIPE,\n\t\t\t\"cwd\": cwd,\n\t\t\t\"env\": os.environ if env is None else env,\n\t\t}\n\t\tif hide_stderr:\n\t\t\tsubprocess_args[\"stderr\"] = subprocess.PIPE\n\t\telse:\n\t\t\tsubprocess_args[\"stderr\"] = subprocess.STDOUT\n\n\t\tprocess = subprocess.Popen(**subprocess_args)\n\t\toutput, _ = process.communicate()\n\t\tif process.returncode != 0:\n\t\t\traise Exception\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\t# Hide stderr if requested.\n\tstderr = None if hide_stderr else subprocess.STDOUT\n\n\tfor command in commands:\n\t\tcommand_line = command.format(**args)\n\n\t\tif verbose:\n\t\t\tprint(command_line)\n\n\t\tif not env:\n\t\t\tenv = dict(os.environ)\n\n\t\tif sys.platform == 'win32':\n\t\t\tshell = True\n\t\t\tcommand_line = command_line.replace('\\\\', '\\\\\\\\')\n\t\telse:\n\t\t\tshell = False\n\n\t\tsubprocess.check_call(command_line, cwd=cwd, env=env, shell=shell, stderr=stderr)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "get_config", "docstring": "Create, populate and return the VersioneerConfig() object.", "ques_id": "62b87d23d292efb640a55668", "code": ["def get_config():\n\tconfig = VersioneerConfig()\n\n\t# set this to the root of your project, the place where setup.py lives\n\tproject_dir = dirname(dirname(dirname(abspath(__file__))))\n\tconfig.topdir = project_dir\n\n\t# these variables are filled in using the values of git-describe\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"kartothek-\"\n\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"  # or 'pep440-pre' or 'pep440-post' or 'git-describe'\n\tconfig.versionfile_source = join(\n\t    project_dir, \"src\", \"kartothek\", \"_version.py\"\n\t)\n\tconfig.versionfile_build = join(\n\t    project_dir, \"src\", \"kartothek\", \"_version.py\"\n\t)\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"kartothek-\"\n\n\t# these templates are filled in using the values of git-describe\n\t\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"\n\tconfig.versionfile_source = \"src/versioneer/_version.py\"\n\tconfig.versionfile_build = \"versioneer/_version.py\"\n\tconfig.tag_prefix = None\n\tconfig.parentdir_prefix = \"versioneer-\"\n\tconfig.vcs_tag_ignore = []\n\tconfig.vcs_tag_skip = []\n\tconfig.tag_format = \"v{tag}\"\n\tconfig.tag_date_format = \"{tag_prefix}.%Y%m%d\"\n\tconfig.verbose = False\n\treturn config\n\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = VCS\n\tconfig.versionfile_source = versionfile_source\n\tconfig.versionfile_build = versionfile_build\n\tconfig.tag_prefix = tag_prefix\n\tconfig.parentdir_prefix = parentdir_prefix\n\tconfig.verbose = verbose\n\treturn config\n\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = os.environ.get('VERSIO\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"scrapinghub.client-\"\n\tconfig.versionfile_source = os.path.join(config.root,\n\t\t\t\t\t\t\t\t\t\t\t \"shub\", \"__init__.py\")\n\tconfig.versionfile_build = None\n\tconfig.versionfile_install = None\n\tconfig.verbose = True\n\tconfig.identity = \"tungwaiyip <tungwaiyip@gmail.com>\"\n\tconfig.version_prefix = \"v\"\n\tconfig.tag_prefix = \"v\"\n\tconfig.parentdir_prefix = \"scrapinghub.client-\"\n\tconfig.use_system_git = True\n\tconfig.branch_name = \"master\"\n\tconfig.tag_name = \"v{version}\"\n\tconfig.tag_message = \"Version {version}\"\n\tconfig.parentdir_format = \"shub-{version}\"\n\tconfig.version\n", "def get_config():\n\tconfig = VersioneerConfig()\n\t# set up the Cache object\n\tconfig.cache = VcsCache()\n\t# find the root of the source tree\n\tconfig.root = find_root(os.path.dirname(__file__))\n\t# find the actual VCS command-line client, and verify that it works\n\tconfig.vcs = get_keyed_setting(config, \"VCS\", \"vcs\")\n\t# determine the VCS version\n\tconfig.vcs_version = run_command([config.vcs, \"--version\"]).splitlines()[0]\n\t# find the actual VCS command-line client, and verify that it works\n\tconfig.versionfile_source = get_keyed_setting(config,\n\t\t\t\t\t\t\t\t\t\t\t\t  \"versionfile_source\",\n\t\t\t\t\t\t\t\t\t\t\t\t  \"versionfile_source\")\n\tconfig.versionfile_build = get_keyed_setting(config,\n\t\t\t\t\t\t\t\t\t\t\t\t \"versionfile_build\",\n\t\t\t\t\t\t\t\t\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.run_command(\"git\", \"rev-parse\", \"--show-toplevel\", cwd=config.root)\n\tif config.git_dir is None:\n\t\tconfig.run_command(\"git\", \"rev-parse\", \"--git-dir\", cwd=config.root)\n\tif config.git_dir is not None:\n\t\tconfig.git_dir = os.path.normpath(os.path.join(config.root, config.git_dir))\n\t\tconfig.git_work_tree = config.git_dir\n\t\tif not os.path.exists(config.git_dir):\n\t\t\tconfig.git_dir = None\n\t\t\tconfig.git_work_tree = None\n\t\telse:\n\t\t\tconfig.run_command(\"git\", \"rev-parse\", \"--show-toplevel\", cwd=config.git_dir)\n\tif config.git_work_tree is not None:\n\t\tconfig.git_work_tree = os.path.normpath(os.path.join(config\n", "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# find the project's root directory (the parent of the directory containing this file)\n\tconfig.root = os.path.dirname(os.path.dirname(__file__))\n\n\t# look for VCS system config files\n\tconfig.get_vcs()\n\n\t# look for versioneer config file\n\tconfig.parse(read_config_py())\n\n\t# read setup.cfg, if any\n\tconfig.parse(get_config_from_setup_cfg())\n\n\t# other config files may be specified in the environment, in setup.cfg, or on\n\t# the command line\n\tfor path in os.environ.get(\"VERSIOEER_CONFIG_FILES\", \"\").split():\n\t\tconfig.parse(read_config_file(path))\n\tfor path in config.config_files:\n\t\tconfig.parse(read_config_file(path))\n\n\t# there may be a [versioneer] section in setup.cfg\n\tconfig.parse(get_config_from_setup_cfg())\n\n\t# command-line flags may override all of the above\n\n", "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.srcdir = os.path.realpath(os.path.dirname(sys.argv[0]))\n\tconfig.versionfile_source = os.path.join(config.srcdir, \"VERSION\")\n\tconfig.versionfile_build = os.path.join(config.srcdir, \"VERSION\")\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"versioneer-example-\"\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"\n\tconfig.verbose = True\n\treturn config\n\n", "def get_config():\n\tconfig = VersioneerConfig()\n\t# if we are running inside of a git repo, then get the branch name\n\ttry:\n\t\tconfig.VCS = \"git\"\n\t\t# 'git describe' returns something like '0.11-1-g9c5b1c0'.\n\t\tconfig.versionfile_source = get_keywords()[\"tag\"]\n\t\t# this will be something like '0.11'\n\t\tconfig.versionfile_build = re.sub(r\"-\\d+-g[0-9a-f]*$\", \"\", config.versionfile_source)\n\t\t# this will be something like '1'\n\t\tconfig.versionfile_revision = get_keywords()[\"distance\"]\n\t\t# if we're building on a non-master branch, add a suffix\n\t\tbranch = get_keywords()[\"branch\"]\n\t\tif branch != \"master\":\n\t\t\tconfig.versionfile_build += \"-%s\" % branch\n\t\t# if we're building a git-archive, use the version that 'git-archive'\n\t\t# uses\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# set this to the root of your project, the place where setup.py lives\n\tproject_dir = dirname(dirname(dirname(abspath(__file__))))\n\tconfig.topdir = project_dir\n\n\t# these variables are filled in using the values of git-describe\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"kartothek-\"\n\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"  # or 'pep440-pre' or 'pep440-post' or 'git-describe'\n\tconfig.versionfile_source = join(\n\t    project_dir, \"src\", \"kartothek\", \"_version.py\"\n\t)\n\tconfig.versionfile_build = join(\n\t    project_dir, \"src\", \"kartothek\", \"_version.py\"\n\t)\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"kartothek-\"\n\n\t# these templates are filled in using the values of git-describe\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"\n\tconfig.versionfile_source = \"src/versioneer/_version.py\"\n\tconfig.versionfile_build = \"versioneer/_version.py\"\n\tconfig.tag_prefix = None\n\tconfig.parentdir_prefix = \"versioneer-\"\n\tconfig.vcs_tag_ignore = []\n\tconfig.vcs_tag_skip = []\n\tconfig.tag_format = \"v{tag}\"\n\tconfig.tag_date_format = \"{tag_prefix}.%Y%m%d\"\n\tconfig.verbose = False\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = VCS\n\tconfig.versionfile_source = versionfile_source\n\tconfig.versionfile_build = versionfile_build\n\tconfig.tag_prefix = tag_prefix\n\tconfig.parentdir_prefix = parentdir_prefix\n\tconfig.verbose = verbose\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = os.environ.get('VERSIO\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"scrapinghub.client-\"\n\tconfig.versionfile_source = os.path.join(config.root,\n\t\t\t\t\t\t\t\t\t\t\t \"shub\", \"__init__.py\")\n\tconfig.versionfile_build = None\n\tconfig.versionfile_install = None\n\tconfig.verbose = True\n\tconfig.identity = \"tungwaiyip <tungwaiyip@gmail.com>\"\n\tconfig.version_prefix = \"v\"\n\tconfig.tag_prefix = \"v\"\n\tconfig.parentdir_prefix = \"scrapinghub.client-\"\n\tconfig.use_system_git = True\n\tconfig.branch_name = \"master\"\n\tconfig.tag_name = \"v{version}\"\n\tconfig.tag_message = \"Version {version}\"\n\tconfig.parentdir_format = \"shub-{version}\"\n\tconfig.version\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\t# set up the Cache object\n\tconfig.cache = VcsCache()\n\t# find the root of the source tree\n\tconfig.root = find_root(os.path.dirname(__file__))\n\t# find the actual VCS command-line client, and verify that it works\n\tconfig.vcs = get_keyed_setting(config, \"VCS\", \"vcs\")\n\t# determine the VCS version\n\tconfig.vcs_version = run_command([config.vcs, \"--version\"]).splitlines()[0]\n\t# find the actual VCS command-line client, and verify that it works\n\tconfig.versionfile_source = get_keyed_setting(config,\n\t\t\t\t\t\t\t\t\t\t\t\t  \"versionfile_source\",\n\t\t\t\t\t\t\t\t\t\t\t\t  \"versionfile_source\")\n\tconfig.versionfile_build = get_keyed_setting(config,\n\t\t\t\t\t\t\t\t\t\t\t\t \"versionfile_build\",\n\t\t\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.run_command(\"git\", \"rev-parse\", \"--show-toplevel\", cwd=config.root)\n\tif config.git_dir is None:\n\t\tconfig.run_command(\"git\", \"rev-parse\", \"--git-dir\", cwd=config.root)\n\tif config.git_dir is not None:\n\t\tconfig.git_dir = os.path.normpath(os.path.join(config.root, config.git_dir))\n\t\tconfig.git_work_tree = config.git_dir\n\t\tif not os.path.exists(config.git_dir):\n\t\t\tconfig.git_dir = None\n\t\t\tconfig.git_work_tree = None\n\t\telse:\n\t\t\tconfig.run_command(\"git\", \"rev-parse\", \"--show-toplevel\", cwd=config.git_dir)\n\tif config.git_work_tree is not None:\n\t\tconfig.git_work_tree = os.path.normpath(os.path.join(config\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\n\t# find the project's root directory (the parent of the directory containing this file)\n\tconfig.root = os.path.dirname(os.path.dirname(__file__))\n\n\t# look for VCS system config files\n\tconfig.get_vcs()\n\n\t# look for versioneer config file\n\tconfig.parse(read_config_py())\n\n\t# read setup.cfg, if any\n\tconfig.parse(get_config_from_setup_cfg())\n\n\t# other config files may be specified in the environment, in setup.cfg, or on\n\t# the command line\n\tfor path in os.environ.get(\"VERSIOEER_CONFIG_FILES\", \"\").split():\n\t\tconfig.parse(read_config_file(path))\n\tfor path in config.config_files:\n\t\tconfig.parse(read_config_file(path))\n\n\t# there may be a [versioneer] section in setup.cfg\n\tconfig.parse(get_config_from_setup_cfg())\n\n\t# command-line flags may override all of the above\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\tconfig.srcdir = os.path.realpath(os.path.dirname(sys.argv[0]))\n\tconfig.versionfile_source = os.path.join(config.srcdir, \"VERSION\")\n\tconfig.versionfile_build = os.path.join(config.srcdir, \"VERSION\")\n\tconfig.tag_prefix = \"\"\n\tconfig.parentdir_prefix = \"versioneer-example-\"\n\tconfig.VCS = \"git\"\n\tconfig.style = \"pep440\"\n\tconfig.verbose = True\n\treturn config\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_config():\n\tconfig = VersioneerConfig()\n\t# if we are running inside of a git repo, then get the branch name\n\ttry:\n\t\tconfig.VCS = \"git\"\n\t\t# 'git describe' returns something like '0.11-1-g9c5b1c0'.\n\t\tconfig.versionfile_source = get_keywords()[\"tag\"]\n\t\t# this will be something like '0.11'\n\t\tconfig.versionfile_build = re.sub(r\"-\\d+-g[0-9a-f]*$\", \"\", config.versionfile_source)\n\t\t# this will be something like '1'\n\t\tconfig.versionfile_revision = get_keywords()[\"distance\"]\n\t\t# if we're building on a non-master branch, add a suffix\n\t\tbranch = get_keywords()[\"branch\"]\n\t\tif branch != \"master\":\n\t\t\tconfig.versionfile_build += \"-%s\" % branch\n\t\t# if we're building a git-archive, use the version that 'git-archive'\n\t\t# uses\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/prestoplot/_version.py", "project": "eykd/prestoplot", "name": "register_vcs_handler", "docstring": "Create decorator to mark a method as the handler of a VCS.", "ques_id": "62b87d23d292efb640a55667", "code": ["def register_vcs_handler(vcs, method):\n\tdef deco(f):\n\t\tvcs.handlers[method] = f\n\t\treturn f\n\treturn deco\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\thandlers[vcs] = f\n\t\treturn f\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(handler):\n\t\t\"\"\"\n\t\tRegister a handler for a given vcs\n\t\t\"\"\"\n\t\tif vcs not in VCS_HANDLERS:\n\t\t\traise KeyError(\"VCS %s does not exist\" % vcs)\n\t\tif method not in VCS_HANDLERS[vcs]:\n\t\t\traise KeyError(\"Method %s does not exist for VCS %s\" % (method, vcs))\n\t\tVCS_HANDLERS[vcs][method] = handler\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(fn):\n\t\tif vcs not in vcs_handlers:\n\t\t\tvcs_handlers[vcs] = []\n\t\tvcs_handlers[vcs].append((method, fn))\n\t\treturn fn\n\treturn decorator\n\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(func):\n\t\tfunc.is_handler = True\n\t\tfunc.vcs = vcs\n\t\tfunc.method = method\n\t\treturn func\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\tf.__vcs_handler__ = vcs\n\t\tf.__vcs_method__ = method\n\t\treturn f\n\treturn decorator\n\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\tmethods = getattr(vcs, 'handlers', {})\n\t\tmethods[method] = f\n\t\tsetattr(vcs, 'handlers', methods)\n\t\treturn f\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorator(fun):\n\t\t\"\"\"\n\t\tDecorator function\n\t\t\"\"\"\n\t\tfun.vcs_handler = (vcs, method)\n\t\treturn fun\n\treturn decorator\n\n", "def register_vcs_handler(vcs, method):\n\treturn lambda func: setattr(func, '__vcs_handler', vcs)\n\n\n", "def register_vcs_handler(vcs, method):\n\tdef decorate(handler):\n\t\t__VCS_HANDLERS[vcs] = handler\n\t\treturn handler\n\treturn decorate\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def register_vcs_handler(vcs, method):\n\tdef deco(f):\n\t\tvcs.handlers[method] = f\n\t\treturn f\n\treturn deco\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\thandlers[vcs] = f\n\t\treturn f\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(handler):\n\t\t\"\"\"\n\t\tRegister a handler for a given vcs\n\t\t\"\"\"\n\t\tif vcs not in VCS_HANDLERS:\n\t\t\traise KeyError(\"VCS %s does not exist\" % vcs)\n\t\tif method not in VCS_HANDLERS[vcs]:\n\t\t\traise KeyError(\"Method %s does not exist for VCS %s\" % (method, vcs))\n\t\tVCS_HANDLERS[vcs][method] = handler\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(fn):\n\t\tif vcs not in vcs_handlers:\n\t\t\tvcs_handlers[vcs] = []\n\t\tvcs_handlers[vcs].append((method, fn))\n\t\treturn fn\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(func):\n\t\tfunc.is_handler = True\n\t\tfunc.vcs = vcs\n\t\tfunc.method = method\n\t\treturn func\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\tf.__vcs_handler__ = vcs\n\t\tf.__vcs_method__ = method\n\t\treturn f\n\treturn decorator\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(f):\n\t\tmethods = getattr(vcs, 'handlers', {})\n\t\tmethods[method] = f\n\t\tsetattr(vcs, 'handlers', methods)\n\t\treturn f\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorator(fun):\n\t\t\"\"\"\n\t\tDecorator function\n\t\t\"\"\"\n\t\tfun.vcs_handler = (vcs, method)\n\t\treturn fun\n\treturn decorator\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\treturn lambda func: setattr(func, '__vcs_handler', vcs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def register_vcs_handler(vcs, method):\n\tdef decorate(handler):\n\t\t__VCS_HANDLERS[vcs] = handler\n\t\treturn handler\n\treturn decorate\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "validate_hierarchy", "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid", "ques_id": "62b45e945108cfac7f210a4a", "code": ["def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.all_objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tif validate_objects:\n\t\t\t\tobj.validate(check_digests=check_digests)\n\t\t\tgood_objects += 1\n\t\texcept ObjectException as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint(\"%s: %s\" % (obj.path, str(e)))\n\treturn num_objects, good_objects\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t# this function is currently not being used, because it is slow\n\t# and not very useful\n\t# TODO: delete this function?\n\tsys.stdout.write('Validating root hierarchy...')\n\tsys.stdout.flush()\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor o in self._storage.list_objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj = self._storage.get_object(o)\n\t\t\tobj.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tsys.stdout.write('\\nError: %s\\n' % e)\n\tsys.stdout.write('done\\n')\n\treturn (num_objects, good_objects)\n\n\n\tdef _create_object_from_path(self, path, parent, object_type='auto'):\n\t\t\"\"\"\n\t\tCreate a new object from\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\ttry:\n\t\tfor root, dirs, files in os.walk(self.storage_root):\n\t\t\tfor d in dirs:\n\t\t\t\td = os.path.join(root, d)\n\t\t\t\tif os.path.islink(d):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"%s is a symbolic link\", d)\n\t\t\t\t\tcontinue\n\t\t\t\tif not os.path.isdir(d):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"%s is not a directory\", d)\n\t\t\t\t\tcontinue\n\t\t\t\tif not os.access(d, os.W_OK):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"No write access to %s\", d)\n\t\t\t\t\tcontinue\n\t\t\t\tif validate_objects:\n\t\t\t\t\t\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tobj_count = 0\n\tgood_objs = 0\n\tfor k, v in self.object_cache.items():\n\t\tobj_count += 1\n\t\tif v.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\tgood_objs += 1\n\t\telse:\n\t\t\tself.logger.error(\"Validation failed for object %s\", v.key)\n\tself.logger.info(\"Validated %i objects, %i were good\", obj_count, good_objs)\n\treturn obj_count, good_objs\n\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor (dirpath, dirnames, filenames) in os.walk(self.root):\n\t\tfor filename in filenames:\n\t\t\tif filename == 'info':\n\t\t\t\tcontinue\n\t\t\tif re.match(r'.+\\.\\d+$', filename):\n\t\t\t\tcontinue\n\t\t\tnum_objects += 1\n\t\t\tobj_path = os.path.join(dirpath, filename)\n\t\t\ttry:\n\t\t\t\tobj = self.get_object(obj_path, check_digests=check_digests)\n\t\t\t\tif validate_objects:\n\t\t\t\t\tobj.validate()\n\t\t\t\tgood_objects += 1\n\t\t\texcept ObjectException as e:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint e\n\treturn (num_objects, good_objects)\n\n\n\tdef _object_root_path(self, obj_id):\n\t\treturn os.path.join(self.root, self._get_object\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif self.root is None:\n\t\t\traise Exception(\"Storage root not set\")\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tfor dirpath, dirnames, filenames in os.walk(self.root):\n\t\t\tif dirpath == self.root:\n\t\t\t\tfor d in dirnames:\n\t\t\t\t\tif d not in self.valid_top_level_dirs:\n\t\t\t\t\t\traise Exception(\"Invalid top-level directory: %s\" % d)\n\t\t\telse:\n\t\t\t\tfor d in dirnames:\n\t\t\t\t\tif d not in self.valid_dirs:\n\t\t\t\t\t\traise Exception(\"Invalid directory: %s\" % d)\n\t\t\tfor f in filenames:\n\t\t\t\tnum_objects += 1\n\t\t\t\tif validate_objects:\n\t\t\t\t\tpath = os.path.join(dirpath, f)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tobj = self.read_object(path)\n\t\t\t\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t# check that the root of the storage is a valid storage root\n\t\tif not self.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\treturn False, 0, 0\n\n\t\t# check that all subdirectories are also valid storage roots.\n\t\tnum_objects, good_objects = 0, 0\n\t\tfor name in os.listdir(self.path):\n\t\t\tsubdir = os.path.join(self.path, name)\n\t\t\tif os.path.isdir(subdir):\n\t\t\t\tstorage = Storage(subdir)\n\t\t\t\tif storage.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\t\tnum_objects += storage.num_objects\n\t\t\t\t\tgood_objects += storage.good_objects\n\t\t\t\telse:\n\t\t\t\t\treturn False, 0, 0\n\n\t\treturn True, num_objects, good\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif self.state != StorageState.OPEN:\n\t\t\traise RuntimeError(\"Storage is not open\")\n\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\n\t\tfor dirpath, dirnames, filenames in os.walk(self.root_dir):\n\t\t\t# Ignore .git directories\n\t\t\tif dirpath.endswith('.git'):\n\t\t\t\tcontinue\n\n\t\t\t# Ignore directories that are not object directories\n\t\t\tif not dirnames and not filenames:\n\t\t\t\tcontinue\n\n\t\t\t# Ignore .git directories\n\t\t\tif dirpath.endswith('.git'):\n\t\t\t\tcontinue\n\n\t\t\t# Ignore files that are not objects\n\t\t\tfor filename in filenames:\n\t\t\t\tif not filename.endswith('.json'):\n\t\t\t\t\tcontinue\n\n\t\t\t\tnum_objects += 1\n\n\t\t\t\tfilepath = os.path.join(dirpath, filename)\n\n\t\t\t\tif check_digests:\n\t\t\t\t\t# Compute digest of\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif self.storage_root is None:\n\t\t\traise ValueError(\"Storage root not set\")\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tfor root, dirs, files in os.walk(self.storage_root):\n\t\t\tfor d in dirs:\n\t\t\t\tif d not in self.hierarchy_levels:\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint(\"WARNING:\", os.path.join(root, d), \"is not a valid hierarchy level\")\n\t\t\t\t\tcontinue\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(\"INFO:\", os.path.join(root, d), \"is a valid hierarchy level\")\n\t\t\tfor f in files:\n\t\t\t\tif not f.endswith(\".json\"):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint(\"WARNING:\", os.path.join(root, f), \"is not a json file\")\n\t\t\t\t\tcontinue\n\t\t\t\tif show\n", "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\treturn self._validate_hierarchy(validate_objects, check_digests, show_warnings)\n\n\tdef _validate_hierarchy(self, validate_objects, check_digests, show_warnings):\n\t\t\"\"\"\n\t\tValidate the storage root hierarchy by checking all objects in the storage root.\n\t\tRaises an exception if any object is not found to be valid.\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor obj in self.all_objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tif validate_objects:\n\t\t\t\tobj.validate(check_digests=check_digests)\n\t\t\tgood_objects += 1\n\t\texcept ObjectException as e:\n\t\t\tif show_warnings:\n\t\t\t\tprint(\"%s: %s\" % (obj.path, str(e)))\n\treturn num_objects, good_objects\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t# this function is currently not being used, because it is slow\n\t# and not very useful\n\t# TODO: delete this function?\n\tsys.stdout.write('Validating root hierarchy...')\n\tsys.stdout.flush()\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor o in self._storage.list_objects():\n\t\tnum_objects += 1\n\t\ttry:\n\t\t\tobj = self._storage.get_object(o)\n\t\t\tobj.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n\t\t\tgood_objects += 1\n\t\texcept Exception as e:\n\t\t\tsys.stdout.write('\\nError: %s\\n' % e)\n\tsys.stdout.write('done\\n')\n\treturn (num_objects, good_objects)\n\n\n\tdef _create_object_from_path(self, path, parent, object_type='auto'):\n\t\t\"\"\"\n\t\tCreate a new object from\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\ttry:\n\t\tfor root, dirs, files in os.walk(self.storage_root):\n\t\t\tfor d in dirs:\n\t\t\t\td = os.path.join(root, d)\n\t\t\t\tif os.path.islink(d):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"%s is a symbolic link\", d)\n\t\t\t\t\tcontinue\n\t\t\t\tif not os.path.isdir(d):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"%s is not a directory\", d)\n\t\t\t\t\tcontinue\n\t\t\t\tif not os.access(d, os.W_OK):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tself.logger.warning(\"No write access to %s\", d)\n\t\t\t\t\tcontinue\n\t\t\t\tif validate_objects:\n\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tobj_count = 0\n\tgood_objs = 0\n\tfor k, v in self.object_cache.items():\n\t\tobj_count += 1\n\t\tif v.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\tgood_objs += 1\n\t\telse:\n\t\t\tself.logger.error(\"Validation failed for object %s\", v.key)\n\tself.logger.info(\"Validated %i objects, %i were good\", obj_count, good_objs)\n\treturn obj_count, good_objs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\tnum_objects = 0\n\tgood_objects = 0\n\tfor (dirpath, dirnames, filenames) in os.walk(self.root):\n\t\tfor filename in filenames:\n\t\t\tif filename == 'info':\n\t\t\t\tcontinue\n\t\t\tif re.match(r'.+\\.\\d+$', filename):\n\t\t\t\tcontinue\n\t\t\tnum_objects += 1\n\t\t\tobj_path = os.path.join(dirpath, filename)\n\t\t\ttry:\n\t\t\t\tobj = self.get_object(obj_path, check_digests=check_digests)\n\t\t\t\tif validate_objects:\n\t\t\t\t\tobj.validate()\n\t\t\t\tgood_objects += 1\n\t\t\texcept ObjectException as e:\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint e\n\treturn (num_objects, good_objects)\n\n\n\tdef _object_root_path(self, obj_id):\n\t\treturn os.path.join(self.root, self._get_object\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif self.root is None:\n\t\t\traise Exception(\"Storage root not set\")\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tfor dirpath, dirnames, filenames in os.walk(self.root):\n\t\t\tif dirpath == self.root:\n\t\t\t\tfor d in dirnames:\n\t\t\t\t\tif d not in self.valid_top_level_dirs:\n\t\t\t\t\t\traise Exception(\"Invalid top-level directory: %s\" % d)\n\t\t\telse:\n\t\t\t\tfor d in dirnames:\n\t\t\t\t\tif d not in self.valid_dirs:\n\t\t\t\t\t\traise Exception(\"Invalid directory: %s\" % d)\n\t\t\tfor f in filenames:\n\t\t\t\tnum_objects += 1\n\t\t\t\tif validate_objects:\n\t\t\t\t\tpath = os.path.join(dirpath, f)\n\t\t\t\t\ttry:\n\t\t\t\t\t\tobj = self.read_object(path)\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\t# check that the root of the storage is a valid storage root\n\t\tif not self.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\treturn False, 0, 0\n\n\t\t# check that all subdirectories are also valid storage roots.\n\t\tnum_objects, good_objects = 0, 0\n\t\tfor name in os.listdir(self.path):\n\t\t\tsubdir = os.path.join(self.path, name)\n\t\t\tif os.path.isdir(subdir):\n\t\t\t\tstorage = Storage(subdir)\n\t\t\t\tif storage.validate(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings):\n\t\t\t\t\tnum_objects += storage.num_objects\n\t\t\t\t\tgood_objects += storage.good_objects\n\t\t\t\telse:\n\t\t\t\t\treturn False, 0, 0\n\n\t\treturn True, num_objects, good\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif self.state != StorageState.OPEN:\n\t\t\traise RuntimeError(\"Storage is not open\")\n\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\n\t\tfor dirpath, dirnames, filenames in os.walk(self.root_dir):\n\t\t\t# Ignore .git directories\n\t\t\tif dirpath.endswith('.git'):\n\t\t\t\tcontinue\n\n\t\t\t# Ignore directories that are not object directories\n\t\t\tif not dirnames and not filenames:\n\t\t\t\tcontinue\n\n\t\t\t# Ignore .git directories\n\t\t\tif dirpath.endswith('.git'):\n\t\t\t\tcontinue\n\n\t\t\t# Ignore files that are not objects\n\t\t\tfor filename in filenames:\n\t\t\t\tif not filename.endswith('.json'):\n\t\t\t\t\tcontinue\n\n\t\t\t\tnum_objects += 1\n\n\t\t\t\tfilepath = os.path.join(dirpath, filename)\n\n\t\t\t\tif check_digests:\n\t\t\t\t\t# Compute digest of\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\tif self.storage_root is None:\n\t\t\traise ValueError(\"Storage root not set\")\n\t\tnum_objects = 0\n\t\tgood_objects = 0\n\t\tfor root, dirs, files in os.walk(self.storage_root):\n\t\t\tfor d in dirs:\n\t\t\t\tif d not in self.hierarchy_levels:\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint(\"WARNING:\", os.path.join(root, d), \"is not a valid hierarchy level\")\n\t\t\t\t\tcontinue\n\t\t\t\tif show_warnings:\n\t\t\t\t\tprint(\"INFO:\", os.path.join(root, d), \"is a valid hierarchy level\")\n\t\t\tfor f in files:\n\t\t\t\tif not f.endswith(\".json\"):\n\t\t\t\t\tif show_warnings:\n\t\t\t\t\t\tprint(\"WARNING:\", os.path.join(root, f), \"is not a json file\")\n\t\t\t\t\tcontinue\n\t\t\t\tif show\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n\t\treturn self._validate_hierarchy(validate_objects, check_digests, show_warnings)\n\n\tdef _validate_hierarchy(self, validate_objects, check_digests, show_warnings):\n\t\t\"\"\"\n\t\tValidate the storage root hierarchy by checking all objects in the storage root.\n\t\tRaises an exception if any object is not found to be valid.\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/store.py", "project": "zimeon/ocfl-py", "name": "initialize", "docstring": "Create and initialize a new OCFL storage root.", "ques_id": "62b45e515108cfac7f210a3c", "code": ["def initialize(self):\n\t# Create the storage root directory\n\tos.mkdir(self.storage_root)\n\tos.chmod(self.storage_root, 0o2775)\n\tos.chown(self.storage_root, os.getuid(), os.getgid())\n\n\t# Create the objects directory\n\tos.mkdir(self.objects_dir)\n\tos.chmod(self.objects_dir, 0o2775)\n\tos.chown(self.objects_dir, os.getuid(), os.getgid())\n\n\t# Create the staging directory\n\tos.mkdir(self.staging_dir)\n\tos.chmod(self.staging_dir, 0o2775)\n\tos.chown(self.staging_dir, os.getuid(), os.getgid())\n\n\t# Create the inventory file\n\tself.inventory = Inventory(self, \"inventory.json\")\n\tself.inventory.write()\n\n\t# Write the storage information file\n\tself.write_storage_info()\n\n\t# TODO: create the staging file\n\t# TODO: create the staging inventory\n", "def initialize(self):\n\t# Create the storage root directory.\n\tcreate_dir(self.path)\n\n\t# Create the inventory file.\n\tinventory_path = os.path.join(self.path, INVENTORY_FILE)\n\tcreate_file(inventory_path)\n\twrite_json(inventory_path, {\n\t\t\"id\": self.id,\n\t\t\"type\": \"InventoryFile\",\n\t\t\"version\": INVENTORY_VERSION,\n\t\t\"inventoryFileVersion\": INVENTORY_FILE_VERSION,\n\t\t\"manifest\": {}\n\t})\n\n\t# Create the storage root's state directory.\n\tstate_path = os.path.join(self.path, STATE_DIR)\n\tcreate_dir(state_path)\n\n\t# Create a new state file.\n\tstate_file_path = os.path.join(state_path, STATE_FILE)\n\tcreate_file(state_file_path)\n\twrite_json(state_file_path, {\n\t\t\"state\": \"new\"\n\t})\n\n\t# Create a new object directory.\n\tobject_path = os.path.\n", "def initialize(self):\n\tself.create()\n\tself.stage_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n", "def initialize(self):\n\tself.check_not_exists()\n\tself.storage_root.mkdir()\n\tself.create_inventory()\n\n\tself.create_inventory_file()\n\tself.create_inventory_digests_file()\n\n\tif self.inventory.digest_algorithm != 'sha512':\n\t\traise ValueError('OCFL only supports SHA-512 digests')\n\n\tself.inventory.digests['sha512'] = {\n\t\t'sha512': self.inventory.digest,\n\t\t}\n\n\tself.inventory.save()\n\n\n", "def initialize(self):\n\traise NotImplementedError('initialize')\n\n\tdef is_initialized(self):\n\t\"\"\"\n\tReturn True if this storage root is initialized.\n\t\"\"\"\n\traise NotImplementedError('is_initialized')\n\n\tdef exists(self, path):\n\t\"\"\"\n\tReturn True if the object identified by path exists in this storage root.\n\t\"\"\"\n\traise NotImplementedError('exists')\n\n\tdef get_version(self, path):\n\t\"\"\"\n\tReturn the version of the object identified by path in this storage root.\n\t\"\"\"\n\traise NotImplementedError('get_version')\n\n\tdef get_versions(self, path):\n\t\"\"\"\n\tReturn a list of version IDs for the object identified by path in this\n\tstorage root.\n\t\"\"\"\n\traise NotImplementedError('get_versions')\n\n\tdef get_head(self, path):\n\t\"\"\"\n\tReturn the head version ID for the object identified by path in this\n\tstorage root.\n\t\"\"\"\n\traise NotImplementedError('get_head')\n\n\tdef get_inventory(self, path, version=None):\n\t\n", "def initialize(self):\n\tself._logger.debug(\"Initializing OCFL storage root: %s\", self._path)\n\n\tif self._path.exists():\n\t\traise OcflStorageRootAlreadyExistsError(self._path)\n\n\tself._path.mkdir(parents=True, exist_ok=False)\n\tself._path.joinpath(OCFL_INVENTORY_FILENAME).touch()\n\tself._logger.info(\"OCFL storage root initialized: %s\", self._path)\n\n\n", "def initialize(self):\n\tos.mkdir(self.path)\n\tself.write_inventory()\n\n\tself.write_inventory()\n\twith open(self.path + '/ocfl_object_1.0.txt', 'w') as f:\n\t\tf.write(\"OCFL object version: 1.0\")\n\n\twith open(self.path + '/inventory.json', 'w') as f:\n\t\tjson.dump({'ocflVersion': '1.0'}, f)\n\n\twith open(self.path + '/inventory.json', 'r') as f:\n\t\tinventory = json.load(f)\n\n\treturn inventory\n\n", "def initialize(self):\n\tlogger.debug(\"Creating new OCFL storage root at %s\", self.path)\n\tos.makedirs(self.path, exist_ok=True)\n\tos.makedirs(self.inventory_path, exist_ok=True)\n\tos.makedirs(self.objects_path, exist_ok=True)\n\tos.makedirs(self.staging_path, exist_ok=True)\n\tself._write_inventory()\n\tself._write_inventory_versions()\n\n\t# XXX: Set up the root inventory.\n\n\t# XXX: Set up the root inventory version.\n\n\t# XXX: Set up the root inventory version.\n\n\t# XXX: Set up the root inventory version.\n\n", "def initialize(self):\n\traise NotImplementedError()\n\n", "def initialize(self):\n\tself.create_root_dir()\n\tself.create_inventory()\n\tself.create_fixity_dir()\n\n\t# Create 'inventory' and 'fixity' versions\n\tinv_version = self.create_version(version_id=OCFLVersion.INVENTORY_VERSION_ID)\n\tinv_version.write_file(OCFLVersion.INVENTORY_FILENAME, self.inventory.serialize())\n\tfixity_version = self.create_version(version_id=OCFLVersion.FIXITY_VERSION_ID)\n\tfixity_version.write_file(OCFLVersion.FIXITY_FILENAME, self.fixity.serialize())\n\n\t# Update the inventory with the initial versions\n\tself.inventory.add_version(inv_version)\n\tself.inventory.add_version(fixity_version)\n\n\t# Update the inventory and fixity files\n\tinv_version.write_file(OCFLVersion.INVENTORY_FILENAME, self.inventory.serialize())\n\tfixity_version.write_file(OCFLVersion.FIXITY_FILEN\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def initialize(self):\n\t# Create the storage root directory\n\tos.mkdir(self.storage_root)\n\tos.chmod(self.storage_root, 0o2775)\n\tos.chown(self.storage_root, os.getuid(), os.getgid())\n\n\t# Create the objects directory\n\tos.mkdir(self.objects_dir)\n\tos.chmod(self.objects_dir, 0o2775)\n\tos.chown(self.objects_dir, os.getuid(), os.getgid())\n\n\t# Create the staging directory\n\tos.mkdir(self.staging_dir)\n\tos.chmod(self.staging_dir, 0o2775)\n\tos.chown(self.staging_dir, os.getuid(), os.getgid())\n\n\t# Create the inventory file\n\tself.inventory = Inventory(self, \"inventory.json\")\n\tself.inventory.write()\n\n\t# Write the storage information file\n\tself.write_storage_info()\n\n\t# TODO: create the staging file\n\t# TODO: create the staging inventory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\t# Create the storage root directory.\n\tcreate_dir(self.path)\n\n\t# Create the inventory file.\n\tinventory_path = os.path.join(self.path, INVENTORY_FILE)\n\tcreate_file(inventory_path)\n\twrite_json(inventory_path, {\n\t\t\"id\": self.id,\n\t\t\"type\": \"InventoryFile\",\n\t\t\"version\": INVENTORY_VERSION,\n\t\t\"inventoryFileVersion\": INVENTORY_FILE_VERSION,\n\t\t\"manifest\": {}\n\t})\n\n\t# Create the storage root's state directory.\n\tstate_path = os.path.join(self.path, STATE_DIR)\n\tcreate_dir(state_path)\n\n\t# Create a new state file.\n\tstate_file_path = os.path.join(state_path, STATE_FILE)\n\tcreate_file(state_file_path)\n\twrite_json(state_file_path, {\n\t\t\"state\": \"new\"\n\t})\n\n\t# Create a new object directory.\n\tobject_path = os.path.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself.create()\n\tself.stage_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n\tself.commit_inventory_metadata()\n\n\tself.stage_inventory()\n\tself.commit_inventory()\n\tself.stage_inventory_metadata()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself.check_not_exists()\n\tself.storage_root.mkdir()\n\tself.create_inventory()\n\n\tself.create_inventory_file()\n\tself.create_inventory_digests_file()\n\n\tif self.inventory.digest_algorithm != 'sha512':\n\t\traise ValueError('OCFL only supports SHA-512 digests')\n\n\tself.inventory.digests['sha512'] = {\n\t\t'sha512': self.inventory.digest,\n\t\t}\n\n\tself.inventory.save()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\traise NotImplementedError('initialize')\n\n\tdef is_initialized(self):\n\t\"\"\"\n\tReturn True if this storage root is initialized.\n\t\"\"\"\n\traise NotImplementedError('is_initialized')\n\n\tdef exists(self, path):\n\t\"\"\"\n\tReturn True if the object identified by path exists in this storage root.\n\t\"\"\"\n\traise NotImplementedError('exists')\n\n\tdef get_version(self, path):\n\t\"\"\"\n\tReturn the version of the object identified by path in this storage root.\n\t\"\"\"\n\traise NotImplementedError('get_version')\n\n\tdef get_versions(self, path):\n\t\"\"\"\n\tReturn a list of version IDs for the object identified by path in this\n\tstorage root.\n\t\"\"\"\n\traise NotImplementedError('get_versions')\n\n\tdef get_head(self, path):\n\t\"\"\"\n\tReturn the head version ID for the object identified by path in this\n\tstorage root.\n\t\"\"\"\n\traise NotImplementedError('get_head')\n\n\tdef get_inventory(self, path, version=None):\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself._logger.debug(\"Initializing OCFL storage root: %s\", self._path)\n\n\tif self._path.exists():\n\t\traise OcflStorageRootAlreadyExistsError(self._path)\n\n\tself._path.mkdir(parents=True, exist_ok=False)\n\tself._path.joinpath(OCFL_INVENTORY_FILENAME).touch()\n\tself._logger.info(\"OCFL storage root initialized: %s\", self._path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tos.mkdir(self.path)\n\tself.write_inventory()\n\n\tself.write_inventory()\n\twith open(self.path + '/ocfl_object_1.0.txt', 'w') as f:\n\t\tf.write(\"OCFL object version: 1.0\")\n\n\twith open(self.path + '/inventory.json', 'w') as f:\n\t\tjson.dump({'ocflVersion': '1.0'}, f)\n\n\twith open(self.path + '/inventory.json', 'r') as f:\n\t\tinventory = json.load(f)\n\n\treturn inventory\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tlogger.debug(\"Creating new OCFL storage root at %s\", self.path)\n\tos.makedirs(self.path, exist_ok=True)\n\tos.makedirs(self.inventory_path, exist_ok=True)\n\tos.makedirs(self.objects_path, exist_ok=True)\n\tos.makedirs(self.staging_path, exist_ok=True)\n\tself._write_inventory()\n\tself._write_inventory_versions()\n\n\t# XXX: Set up the root inventory.\n\n\t# XXX: Set up the root inventory version.\n\n\t# XXX: Set up the root inventory version.\n\n\t# XXX: Set up the root inventory version.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def initialize(self):\n\tself.create_root_dir()\n\tself.create_inventory()\n\tself.create_fixity_dir()\n\n\t# Create 'inventory' and 'fixity' versions\n\tinv_version = self.create_version(version_id=OCFLVersion.INVENTORY_VERSION_ID)\n\tinv_version.write_file(OCFLVersion.INVENTORY_FILENAME, self.inventory.serialize())\n\tfixity_version = self.create_version(version_id=OCFLVersion.FIXITY_VERSION_ID)\n\tfixity_version.write_file(OCFLVersion.FIXITY_FILENAME, self.fixity.serialize())\n\n\t# Update the inventory with the initial versions\n\tself.inventory.add_version(inv_version)\n\tself.inventory.add_version(fixity_version)\n\n\t# Update the inventory and fixity files\n\tinv_version.write_file(OCFLVersion.INVENTORY_FILENAME, self.inventory.serialize())\n\tfixity_version.write_file(OCFLVersion.FIXITY_FILEN\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "next_version", "docstring": "Next version identifier following existing pattern.\n\nMust deal with both zero-prefixed and non-zero prefixed versions.", "ques_id": "62b45e2eb89c9fd354170232", "code": ["def next_version(version):\n\treturn str(int(version[1:])+1)\n\n", "def next_version(version):\n\tversion_list = version.split('.')\n\tlast_index = len(version_list) - 1\n\tversion_list[last_index] = str(int(version_list[last_index]) + 1)\n\treturn '.'.join(version_list)\n\n\n", "def next_version(version):\n\tversion_part = version.split('.')\n\tversion_part[2] = str(int(version_part[2]) + 1)\n\treturn '.'.join(version_part)\n\n\n", "def next_version(version):\n\t# Split existing version into components\n\tversion_parts = version.split('.')\n\n\t# Increment last version component\n\tversion_parts[-1] = str(int(version_parts[-1]) + 1)\n\n\t# Return string representation of next version\n\treturn '.'.join(version_parts)\n\n\n", "def next_version(version):\n\tpattern = re.compile(r'\\d{1,3}')\n\t# get all digits\n\tdigits = [int(x) for x in pattern.findall(version)]\n\t# add one to last digit\n\tdigits[-1] += 1\n\t# if last digit is 10, add 1 to second last digit\n\tif digits[-1] == 10:\n\t\tdigits[-2] += 1\n\t\tdigits[-1] = 0\n\t# if second last digit is 10, add 1 to third last digit\n\tif digits[-2] == 10:\n\t\tdigits[-3] += 1\n\t\tdigits[-2] = 0\n\t# update the version\n\tversion = '.'.join([str(x) for x in digits])\n\treturn version\n\n", "def next_version(version):\n\t# version = version.replace(\"v\", \"\")\n\tversion_list = version.split(\".\")\n\t# print(version_list)\n\t# version_list = [int(x) for x in version_list]\n\t# print(version_list)\n\tif version_list[-1] == \"\":\n\t\tversion_list = version_list[:-1]\n\tversion_list[-1] = str(int(version_list[-1]) + 1)\n\t# print(version_list)\n\treturn \".\".join(version_list)\n\n\n", "def next_version(version):\n\tversion_list = version.split('.')\n\tversion_list[-1] = str(int(version_list[-1])+1)\n\treturn '.'.join(version_list)\n\n", "def next_version(version):\n\tversion_list = [int(i) for i in version.split('.')]\n\tversion_list[-1] += 1\n\tfor i in range(len(version_list)-1,0,-1):\n\t\tif version_list[i] == 1000:\n\t\t\tversion_list[i] = 0\n\t\t\tversion_list[i-1] += 1\n\treturn \".\".join(str(i) for i in version_list)\n", "def next_version(version):\n\t# Split version into list of integers\n\tv = [int(x) for x in version.split('.')]\n\n\t# Add 1 to last digit of version\n\tv[-1] += 1\n\n\t# If last digit is 10, then increase previous digit by 1 and set last digit to 0\n\tif v[-1] == 10:\n\t\tv[-2] += 1\n\t\tv[-1] = 0\n\n\t# If first digit is 10, then increase second digit by 1 and set first digit to 0\n\tif v[0] == 10:\n\t\tv[1] += 1\n\t\tv[0] = 0\n\n\t# Return new version identifier\n\treturn '.'.join([str(x) for x in v])\n", "def next_version(version):\n\tif isinstance(version, int):\n\t\tversion = str(version)\n\tversion_list = version.split('.')\n\tif len(version_list) == 1:\n\t\tversion_list.append('0')\n\telif len(version_list) > 2:\n\t\tversion_list = version_list[:2]\n\tversion_list[-1] = str(int(version_list[-1]) + 1)\n\treturn '.'.join(version_list)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def next_version(version):\n\treturn str(int(version[1:])+1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tversion_list = version.split('.')\n\tlast_index = len(version_list) - 1\n\tversion_list[last_index] = str(int(version_list[last_index]) + 1)\n\treturn '.'.join(version_list)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tversion_part = version.split('.')\n\tversion_part[2] = str(int(version_part[2]) + 1)\n\treturn '.'.join(version_part)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t# Split existing version into components\n\tversion_parts = version.split('.')\n\n\t# Increment last version component\n\tversion_parts[-1] = str(int(version_parts[-1]) + 1)\n\n\t# Return string representation of next version\n\treturn '.'.join(version_parts)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tpattern = re.compile(r'\\d{1,3}')\n\t# get all digits\n\tdigits = [int(x) for x in pattern.findall(version)]\n\t# add one to last digit\n\tdigits[-1] += 1\n\t# if last digit is 10, add 1 to second last digit\n\tif digits[-1] == 10:\n\t\tdigits[-2] += 1\n\t\tdigits[-1] = 0\n\t# if second last digit is 10, add 1 to third last digit\n\tif digits[-2] == 10:\n\t\tdigits[-3] += 1\n\t\tdigits[-2] = 0\n\t# update the version\n\tversion = '.'.join([str(x) for x in digits])\n\treturn version\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t# version = version.replace(\"v\", \"\")\n\tversion_list = version.split(\".\")\n\t# print(version_list)\n\t# version_list = [int(x) for x in version_list]\n\t# print(version_list)\n\tif version_list[-1] == \"\":\n\t\tversion_list = version_list[:-1]\n\tversion_list[-1] = str(int(version_list[-1]) + 1)\n\t# print(version_list)\n\treturn \".\".join(version_list)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tversion_list = version.split('.')\n\tversion_list[-1] = str(int(version_list[-1])+1)\n\treturn '.'.join(version_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tversion_list = [int(i) for i in version.split('.')]\n\tversion_list[-1] += 1\n\tfor i in range(len(version_list)-1,0,-1):\n\t\tif version_list[i] == 1000:\n\t\t\tversion_list[i] = 0\n\t\t\tversion_list[i-1] += 1\n\treturn \".\".join(str(i) for i in version_list)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\t# Split version into list of integers\n\tv = [int(x) for x in version.split('.')]\n\n\t# Add 1 to last digit of version\n\tv[-1] += 1\n\n\t# If last digit is 10, then increase previous digit by 1 and set last digit to 0\n\tif v[-1] == 10:\n\t\tv[-2] += 1\n\t\tv[-1] = 0\n\n\t# If first digit is 10, then increase second digit by 1 and set first digit to 0\n\tif v[0] == 10:\n\t\tv[1] += 1\n\t\tv[0] = 0\n\n\t# Return new version identifier\n\treturn '.'.join([str(x) for x in v])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def next_version(version):\n\tif isinstance(version, int):\n\t\tversion = str(version)\n\tversion_list = version.split('.')\n\tif len(version_list) == 1:\n\t\tversion_list.append('0')\n\telif len(version_list) > 2:\n\t\tversion_list = version_list[:2]\n\tversion_list[-1] = str(int(version_list[-1]) + 1)\n\treturn '.'.join(version_list)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/validator.py", "project": "zimeon/ocfl-py", "name": "validate_version_inventories", "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).", "ques_id": "62b45e23e0d4551b0392c90a", "code": ["def validate_version_inventories(self, version_dirs):\n\tversion_inventories = {}\n\tcontent_digest_count = {}\n\tfor version_dir in version_dirs:\n\t\tversion_num = int(version_dir)\n\t\t# Each version must have an inventory\n\t\tinventory_path = os.path.join(self.path, version_dir, \"inventory\")\n\t\tif not os.path.exists(inventory_path):\n\t\t\traise ValidationError(\"%s does not exist\" % inventory_path)\n\t\tversion_inventories[version_num] = inventory_path\n\t\t# Also check that all content digests are the same as in the root inventory\n\t\tinventory = Inventory(self.path, version_num)\n\t\tfor entry in inventory.entries:\n\t\t\tif entry.digest in content_digest_count:\n\t\t\t\tcontent_digest_count[entry.digest] += 1\n\t\t\telse:\n\t\t\t\tcontent_digest_count[entry.digest] = 1\n\tfor digest in content_digest_count:\n\t\tif content_digest_\n", "def validate_version_inventories(self, version_dirs):\n\tself.content_digests = {}\n\tself.version_inventories = {}\n\tfor dir in version_dirs:\n\t\tversion_num = int(dir[1:])\n\t\twith open(os.path.join(dir, 'inventory.xml')) as inv:\n\t\t\tinv_xml = inv.read()\n\t\tinv_dom = parseString(inv_xml)\n\t\tnum_files = len(inv_dom.getElementsByTagName('file'))\n\t\tif num_files != version_num:\n\t\t\tprint >> sys.stderr, 'ERROR: Version ' + dir + ' has ' + str(num_files) + ' files in inventory, should have ' + str(version_num)\n\t\t\tself.valid = False\n\t\t# TODO - check that files are in version sequence\n\t\tfor file in inv_dom.getElementsByTagName('file'):\n\t\t\tcontent_digest = file.getAttribute('content-digest')\n\t\t\tself.content_digests[content_digest] = True\n\t\t\n", "def validate_version_inventories(self, version_dirs):\n\tversion_inventory_names = []\n\tfor version_dir in version_dirs:\n\t\tversion_number = version_dir[-1]\n\t\tinventory_name = \"inventory_v\" + version_number + \".xml\"\n\t\tversion_inventory_names.append(inventory_name)\n\t\tinventory_path = os.path.join(self.path, version_dir, inventory_name)\n\t\tif not os.path.isfile(inventory_path):\n\t\t\tself.errors.append(inventory_path + \" missing\")\n\t\telse:\n\t\t\tinventory_file = self.load_file(inventory_path)\n\t\t\tinventory = self.parse_xml(inventory_file)\n\t\t\tself.validate_inventory(inventory)\n\t\t\tself.validate_inventory_version_number(inventory_path, inventory, version_number)\n\t\t\tself.validate_inventory_version_sequence(inventory_path, inventory, version_number)\n\t\t\tself.validate_inventory_content_digests(inventory)\n\t\t\tself.validate_inventory\n", "def validate_version_inventories(self, version_dirs):\n\t# FIXME: we should use a \"with\" block and close the inventory files.\n\tfor version in version_dirs:\n\t\tinventory_path = self.path.append(\"version-\" + version).append(\"inventory.xml\")\n\t\ttry:\n\t\t\tinventory = load_inventory_from_path(inventory_path, _validate_inventory_file)\n\t\texcept:\n\t\t\traise InvalidRepository(\"inventory in version \" + version + \" is invalid\")\n\t\tfor content_digest in inventory.content_digests():\n\t\t\tif content_digest not in self.root_inventory.content_digests():\n\t\t\t\tself.content_digests.add(content_digest)\n\t# FIXME: we should also check that the inventory digests in the versions\n\t# match the inventory digest in the root inventory.\n\t# FIXME: we should also check that the content digests in the root inventory\n\t# match the content digests in the content directory.\n\n\tdef validate_version_content(self):\n\t\t\"\"\"\n\t\tEach version SHOULD have the content that it refers to.\n", "def validate_version_inventories(self, version_dirs):\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir)\n\t\tinventory_path = os.path.join(version_dir, \"inventory\")\n\t\tif not os.path.exists(inventory_path):\n\t\t\tself.add_error(\"Version %d is missing inventory %s\" % (version, inventory_path))\n\t\t\tcontinue\n\n\t\twith open(inventory_path, 'rb') as f:\n\t\t\tinventory_content = f.read()\n\t\tinventory_digest = get_digest(inventory_content, \"MD5\")\n\t\tself.version_inventories[version] = inventory_digest\n\t\tself.version_inventory_content[version] = inventory_content\n\t\tif version == 1:\n\t\t\t# The first inventory should be the same as the root inventory\n\t\t\tif self.root_inventory_digest != inventory_digest:\n\t\t\t\tself.add_error(\"Version %d inventory %s does not match root inventory %s\" % (version, inventory_digest, self.root_inventory_\n", "def validate_version_inventories(self, version_dirs):\n\tversion_digests = {\n\t\t\"root\": self.digests[\"root\"]\n\t}\n\tcontent_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion_dir_path = os.path.join(self.dir_path, version_dir)\n\t\tversion_inventory_path = os.path.join(version_dir_path, \"inventory.json\")\n\t\tif not os.path.exists(version_inventory_path):\n\t\t\tprint(\"ERROR: version directory %s lacks inventory.json\" % version_dir)\n\t\t\tself.errors += 1\n\t\t\tcontinue\n\t\tversion_inventory = json.load(open(version_inventory_path, \"r\"))\n\t\tversion_digest = version_inventory[\"versionDigest\"]\n\t\tversion_digests[version_dir] = version_digest\n\t\tif version_digest == version_digests[\"root\"]:\n\t\t\tprint(\"ERROR: version inventory %s has same digest as the root inventory\" % version_dir)\n\t\t\tself.errors += 1\n\t\t\n", "def validate_version_inventories(self, version_dirs):\n\tassert isinstance(version_dirs, list)\n\n\tversion = 0\n\tfor version_dir in version_dirs:\n\t\tversion += 1\n\t\tinventory_path = os.path.join(self.version_dir, version_dir, 'inventory')\n\t\ttry:\n\t\t\tinventory = self.repository.get_inventory_from_path(inventory_path)\n\t\texcept NoSuchFile:\n\t\t\tself.errors.append('Version %d is missing inventory file %s'\n\t\t\t\t\t% (version, inventory_path))\n\t\t\tcontinue\n\n\t\t# Check that the version's inventory is a subset of the previous version\n\t\t# (or of the root inventory if this is the first version).\n\t\tprevious_inventory = self.root_inventory\n\t\tif version > 1:\n\t\t\tprevious_inventory_path = os.path.join(self.version_dir, version_dirs[version - 2], 'inventory')\n\t\t\ttry:\n\t\t\t\tprevious_inventory = self.repository.get_inventory_from_path(\n", "def validate_version_inventories(self, version_dirs):\n\tcontent_digests_list = [] # list of content digests for each version\n\tcontent_digests_set = set() # for checking that each content digest is unique\n\troot_inventory = None\n\troot_inventory_content_digests = set() # to check that each version inventory\n\t\t# has the same content digests as the root inventory\n\ti = 0\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_filename = os.path.join(repo_dir, version_dir,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"inventory\")\n\t\tif not os.path.isfile(version_inventory_filename):\n\t\t\tprint(\"Bad: Version %s has no inventory.\" % version_dir)\n\t\t\treturn False\n\t\tversion_inventory = json.load(open(version_inventory_filename))\n\t\tversion_inventory_content_digests = set()\n\t\tfor entry in version_inventory:\n\t\t\tversion_inventory_content_digests.add(entry[\"content_digest\"])\n", "def validate_version_inventories(self, version_dirs):\n\tversion_inventories = []\n\tcontent_diffs = []\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir)\n\t\tversion_inventory_file = os.path.join(version_dir, \"inventory.xml\")\n\t\tif not os.path.isfile(version_inventory_file):\n\t\t\tprint(\"Missing inventory.xml in version %d\" % version)\n\t\t\tcontinue\n\t\tversion_inventory = etree.parse(version_inventory_file).getroot()\n\t\tversion_inventories.append(version_inventory)\n\t\tif version == 1:\n\t\t\tcontinue\n\n\t\tprevious_inventory = version_inventories[-2]\n\t\tcontent_diffs.append(self.compare_inventories(previous_inventory, version_inventory))\n\n\treturn version_inventories, content_diffs\n\n\tdef compare_inventories(self, previous_inventory, version_inventory):\n\t\"\"\"\n\tCompare the two invetories and return a dict of any content digests that are\n\tdifferent.\n\n", "def validate_version_inventories(self, version_dirs):\n\tversion_inventory_files = [\n\t\t\"%s/%s\" % (version_dir, self.inventory_file) for version_dir in version_dirs\n\t]\n\tfor version_inventory_file in version_inventory_files:\n\t\tif not os.path.isfile(version_inventory_file):\n\t\t\tself.log_error(\"%s: missing\" % version_inventory_file)\n\t\t\tself.valid = False\n\t\t\tcontinue\n\t\twith open(version_inventory_file, \"rb\") as f:\n\t\t\tversion_inventory = json.load(f)\n\t\t\tif self.validate_inventory(version_inventory, version_inventory_file):\n\t\t\t\tfor key in version_inventory:\n\t\t\t\t\tif key not in self.root_inventory:\n\t\t\t\t\t\tself.log_error(\"%s: %s: missing from root inventory\" % (version_inventory_file, key))\n\t\t\t\t\t\tself.valid = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tif (\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_version_inventories(self, version_dirs):\n\tversion_inventories = {}\n\tcontent_digest_count = {}\n\tfor version_dir in version_dirs:\n\t\tversion_num = int(version_dir)\n\t\t# Each version must have an inventory\n\t\tinventory_path = os.path.join(self.path, version_dir, \"inventory\")\n\t\tif not os.path.exists(inventory_path):\n\t\t\traise ValidationError(\"%s does not exist\" % inventory_path)\n\t\tversion_inventories[version_num] = inventory_path\n\t\t# Also check that all content digests are the same as in the root inventory\n\t\tinventory = Inventory(self.path, version_num)\n\t\tfor entry in inventory.entries:\n\t\t\tif entry.digest in content_digest_count:\n\t\t\t\tcontent_digest_count[entry.digest] += 1\n\t\t\telse:\n\t\t\t\tcontent_digest_count[entry.digest] = 1\n\tfor digest in content_digest_count:\n\t\tif content_digest_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tself.content_digests = {}\n\tself.version_inventories = {}\n\tfor dir in version_dirs:\n\t\tversion_num = int(dir[1:])\n\t\twith open(os.path.join(dir, 'inventory.xml')) as inv:\n\t\t\tinv_xml = inv.read()\n\t\tinv_dom = parseString(inv_xml)\n\t\tnum_files = len(inv_dom.getElementsByTagName('file'))\n\t\tif num_files != version_num:\n\t\t\tprint >> sys.stderr, 'ERROR: Version ' + dir + ' has ' + str(num_files) + ' files in inventory, should have ' + str(version_num)\n\t\t\tself.valid = False\n\t\t# TODO - check that files are in version sequence\n\t\tfor file in inv_dom.getElementsByTagName('file'):\n\t\t\tcontent_digest = file.getAttribute('content-digest')\n\t\t\tself.content_digests[content_digest] = True\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tversion_inventory_names = []\n\tfor version_dir in version_dirs:\n\t\tversion_number = version_dir[-1]\n\t\tinventory_name = \"inventory_v\" + version_number + \".xml\"\n\t\tversion_inventory_names.append(inventory_name)\n\t\tinventory_path = os.path.join(self.path, version_dir, inventory_name)\n\t\tif not os.path.isfile(inventory_path):\n\t\t\tself.errors.append(inventory_path + \" missing\")\n\t\telse:\n\t\t\tinventory_file = self.load_file(inventory_path)\n\t\t\tinventory = self.parse_xml(inventory_file)\n\t\t\tself.validate_inventory(inventory)\n\t\t\tself.validate_inventory_version_number(inventory_path, inventory, version_number)\n\t\t\tself.validate_inventory_version_sequence(inventory_path, inventory, version_number)\n\t\t\tself.validate_inventory_content_digests(inventory)\n\t\t\tself.validate_inventory\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\t# FIXME: we should use a \"with\" block and close the inventory files.\n\tfor version in version_dirs:\n\t\tinventory_path = self.path.append(\"version-\" + version).append(\"inventory.xml\")\n\t\ttry:\n\t\t\tinventory = load_inventory_from_path(inventory_path, _validate_inventory_file)\n\t\texcept:\n\t\t\traise InvalidRepository(\"inventory in version \" + version + \" is invalid\")\n\t\tfor content_digest in inventory.content_digests():\n\t\t\tif content_digest not in self.root_inventory.content_digests():\n\t\t\t\tself.content_digests.add(content_digest)\n\t# FIXME: we should also check that the inventory digests in the versions\n\t# match the inventory digest in the root inventory.\n\t# FIXME: we should also check that the content digests in the root inventory\n\t# match the content digests in the content directory.\n\n\tdef validate_version_content(self):\n\t\t\"\"\"\n\t\tEach version SHOULD have the content that it refers to.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir)\n\t\tinventory_path = os.path.join(version_dir, \"inventory\")\n\t\tif not os.path.exists(inventory_path):\n\t\t\tself.add_error(\"Version %d is missing inventory %s\" % (version, inventory_path))\n\t\t\tcontinue\n\n\t\twith open(inventory_path, 'rb') as f:\n\t\t\tinventory_content = f.read()\n\t\tinventory_digest = get_digest(inventory_content, \"MD5\")\n\t\tself.version_inventories[version] = inventory_digest\n\t\tself.version_inventory_content[version] = inventory_content\n\t\tif version == 1:\n\t\t\t# The first inventory should be the same as the root inventory\n\t\t\tif self.root_inventory_digest != inventory_digest:\n\t\t\t\tself.add_error(\"Version %d inventory %s does not match root inventory %s\" % (version, inventory_digest, self.root_inventory_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tversion_digests = {\n\t\t\"root\": self.digests[\"root\"]\n\t}\n\tcontent_digests = {}\n\tfor version_dir in version_dirs:\n\t\tversion_dir_path = os.path.join(self.dir_path, version_dir)\n\t\tversion_inventory_path = os.path.join(version_dir_path, \"inventory.json\")\n\t\tif not os.path.exists(version_inventory_path):\n\t\t\tprint(\"ERROR: version directory %s lacks inventory.json\" % version_dir)\n\t\t\tself.errors += 1\n\t\t\tcontinue\n\t\tversion_inventory = json.load(open(version_inventory_path, \"r\"))\n\t\tversion_digest = version_inventory[\"versionDigest\"]\n\t\tversion_digests[version_dir] = version_digest\n\t\tif version_digest == version_digests[\"root\"]:\n\t\t\tprint(\"ERROR: version inventory %s has same digest as the root inventory\" % version_dir)\n\t\t\tself.errors += 1\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tassert isinstance(version_dirs, list)\n\n\tversion = 0\n\tfor version_dir in version_dirs:\n\t\tversion += 1\n\t\tinventory_path = os.path.join(self.version_dir, version_dir, 'inventory')\n\t\ttry:\n\t\t\tinventory = self.repository.get_inventory_from_path(inventory_path)\n\t\texcept NoSuchFile:\n\t\t\tself.errors.append('Version %d is missing inventory file %s'\n\t\t\t\t\t% (version, inventory_path))\n\t\t\tcontinue\n\n\t\t# Check that the version's inventory is a subset of the previous version\n\t\t# (or of the root inventory if this is the first version).\n\t\tprevious_inventory = self.root_inventory\n\t\tif version > 1:\n\t\t\tprevious_inventory_path = os.path.join(self.version_dir, version_dirs[version - 2], 'inventory')\n\t\t\ttry:\n\t\t\t\tprevious_inventory = self.repository.get_inventory_from_path(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tcontent_digests_list = [] # list of content digests for each version\n\tcontent_digests_set = set() # for checking that each content digest is unique\n\troot_inventory = None\n\troot_inventory_content_digests = set() # to check that each version inventory\n\t\t# has the same content digests as the root inventory\n\ti = 0\n\tfor version_dir in version_dirs:\n\t\tversion_inventory_filename = os.path.join(repo_dir, version_dir,\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\"inventory\")\n\t\tif not os.path.isfile(version_inventory_filename):\n\t\t\tprint(\"Bad: Version %s has no inventory.\" % version_dir)\n\t\t\treturn False\n\t\tversion_inventory = json.load(open(version_inventory_filename))\n\t\tversion_inventory_content_digests = set()\n\t\tfor entry in version_inventory:\n\t\t\tversion_inventory_content_digests.add(entry[\"content_digest\"])\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tversion_inventories = []\n\tcontent_diffs = []\n\tfor version_dir in version_dirs:\n\t\tversion = int(version_dir)\n\t\tversion_inventory_file = os.path.join(version_dir, \"inventory.xml\")\n\t\tif not os.path.isfile(version_inventory_file):\n\t\t\tprint(\"Missing inventory.xml in version %d\" % version)\n\t\t\tcontinue\n\t\tversion_inventory = etree.parse(version_inventory_file).getroot()\n\t\tversion_inventories.append(version_inventory)\n\t\tif version == 1:\n\t\t\tcontinue\n\n\t\tprevious_inventory = version_inventories[-2]\n\t\tcontent_diffs.append(self.compare_inventories(previous_inventory, version_inventory))\n\n\treturn version_inventories, content_diffs\n\n\tdef compare_inventories(self, previous_inventory, version_inventory):\n\t\"\"\"\n\tCompare the two invetories and return a dict of any content digests that are\n\tdifferent.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_version_inventories(self, version_dirs):\n\tversion_inventory_files = [\n\t\t\"%s/%s\" % (version_dir, self.inventory_file) for version_dir in version_dirs\n\t]\n\tfor version_inventory_file in version_inventory_files:\n\t\tif not os.path.isfile(version_inventory_file):\n\t\t\tself.log_error(\"%s: missing\" % version_inventory_file)\n\t\t\tself.valid = False\n\t\t\tcontinue\n\t\twith open(version_inventory_file, \"rb\") as f:\n\t\t\tversion_inventory = json.load(f)\n\t\t\tif self.validate_inventory(version_inventory, version_inventory_file):\n\t\t\t\tfor key in version_inventory:\n\t\t\t\t\tif key not in self.root_inventory:\n\t\t\t\t\t\tself.log_error(\"%s: %s: missing from root inventory\" % (version_inventory_file, key))\n\t\t\t\t\t\tself.valid = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tif (\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "ocfl/object_utils.py", "project": "zimeon/ocfl-py", "name": "find_path_type", "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.", "ques_id": "62b45e21e0d4551b0392c8ed", "code": ["def find_path_type(path):\n\tif path == '/':\n\t\treturn \"root\"\n\telif os.path.isdir(path):\n\t\treturn \"dir\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"unknown\"\n\n", "def find_path_type(path):\n\tif os.path.exists(path):\n\t\tif os.path.isfile(path):\n\t\t\treturn \"file\"\n\t\tif os.path.isdir(path):\n\t\t\treturn \"directory\"\n\t\treturn \"unknown\"\n\telse:\n\t\treturn \"nonexistent\"\n\n", "def find_path_type(path):\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.isdir(path):\n\t\treturn \"dir\"\n\telse:\n\t\treturn \"nonexistent\"\n\n", "def find_path_type(path):\n\tif not os.path.exists(path):\n\t\treturn None\n\tif os.path.isdir(path):\n\t\treturn 'dir'\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\tif os.path.islink(path):\n\t\treturn 'link'\n\treturn None\n\n\n", "def find_path_type(path):\n\tif os.path.exists(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn 'directory'\n\t\telif os.path.isfile(path):\n\t\t\treturn 'file'\n\t\telse:\n\t\t\treturn 'other'\n\telse:\n\t\treturn 'none'\n\n", "def find_path_type(path):\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telse:\n\t\treturn 'unknown'\n\n", "def find_path_type(path):\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.isdir(path):\n\t\treturn 'dir'\n\telif os.path.islink(path):\n\t\treturn 'link'\n\telse:\n\t\treturn 'unknown'\n\n", "def find_path_type(path):\n\tif not os.path.exists(path):\n\t\treturn 'missing'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telse:\n\t\treturn 'unknown'\n\n", "def find_path_type(path):\n\tif path == \"/\" or not path:\n\t\treturn \"root\"\n\tif os.path.isdir(path):\n\t\treturn \"dir\"\n\treturn \"file\"\n\n\n", "def find_path_type(path):\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"symlink\"\n\telse:\n\t\treturn \"unknown\"\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_path_type(path):\n\tif path == '/':\n\t\treturn \"root\"\n\telif os.path.isdir(path):\n\t\treturn \"dir\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telse:\n\t\treturn \"unknown\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif os.path.exists(path):\n\t\tif os.path.isfile(path):\n\t\t\treturn \"file\"\n\t\tif os.path.isdir(path):\n\t\t\treturn \"directory\"\n\t\treturn \"unknown\"\n\telse:\n\t\treturn \"nonexistent\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.isdir(path):\n\t\treturn \"dir\"\n\telse:\n\t\treturn \"nonexistent\"\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif not os.path.exists(path):\n\t\treturn None\n\tif os.path.isdir(path):\n\t\treturn 'dir'\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\tif os.path.islink(path):\n\t\treturn 'link'\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif os.path.exists(path):\n\t\tif os.path.isdir(path):\n\t\t\treturn 'directory'\n\t\telif os.path.isfile(path):\n\t\t\treturn 'file'\n\t\telse:\n\t\t\treturn 'other'\n\telse:\n\t\treturn 'none'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telse:\n\t\treturn 'unknown'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif os.path.isfile(path):\n\t\treturn 'file'\n\telif os.path.isdir(path):\n\t\treturn 'dir'\n\telif os.path.islink(path):\n\t\treturn 'link'\n\telse:\n\t\treturn 'unknown'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif not os.path.exists(path):\n\t\treturn 'missing'\n\telif os.path.isdir(path):\n\t\treturn 'directory'\n\telif os.path.isfile(path):\n\t\treturn 'file'\n\telse:\n\t\treturn 'unknown'\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif path == \"/\" or not path:\n\t\treturn \"root\"\n\tif os.path.isdir(path):\n\t\treturn \"dir\"\n\treturn \"file\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_path_type(path):\n\tif os.path.isdir(path):\n\t\treturn \"directory\"\n\telif os.path.isfile(path):\n\t\treturn \"file\"\n\telif os.path.islink(path):\n\t\treturn \"symlink\"\n\telse:\n\t\treturn \"unknown\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "auto_nag/bzcleaner.py", "project": "mozilla/relman-auto-nag", "name": "amend_bzparams", "docstring": "Amend the Bugzilla params", "ques_id": "62b45b396decaeff903e1001", "code": ["def amend_bzparams(self, params, bug_ids):\n\tif not bug_ids:\n\t\treturn params\n\tparams[\"bug_id\"] = \",\".join(bug_ids)\n\treturn params\n\n", "def amend_bzparams(self, params, bug_ids):\n\tif bug_ids:\n\t\tparams['bug_id'] = bug_ids\n\treturn params\n\n\t@classmethod\n\tdef get_extra_args(cls):\n\t\t\"\"\"\n\t\tReturn the extra arguments for Bugzilla\n\t\t\"\"\"\n\t\treturn [\n\t\t\t(\n\t\t\t\t['-b', '--bug-ids'],\n\t\t\t\t{\n\t\t\t\t\t'action': 'store',\n\t\t\t\t\t'nargs': '+',\n\t\t\t\t\t'type': int,\n\t\t\t\t\t'help': 'Bug IDs to add to the bug report',\n\t\t\t\t},\n\t\t\t),\n\t\t]\n", "def amend_bzparams(self, params, bug_ids):\n\tif not isinstance(bug_ids, (list, tuple)):\n\t\tbug_ids = [bug_ids]\n\t# if the param is already set, we don't override it\n\tif 'bug_id' in params:\n\t\treturn\n\tif 'bug_ids' in params:\n\t\treturn\n\t# if bug_ids is set, we set the bug_ids param\n\tif bug_ids:\n\t\tparams['bug_ids'] = bug_ids\n\t\treturn\n\n\t# if bug_ids is not set, we set the bug_id param\n\tif self.options.bug_id:\n\t\tparams['bug_id'] = self.options.bug_id\n\t\treturn\n\n\t# if we have no bug_ids and no bug_id, we fail\n\traise ValueError(\"No bug_ids or bug_id specified\")\n\n", "def amend_bzparams(self, params, bug_ids):\n\tif not bug_ids:\n\t\treturn\n\n\t# amend params\n\tparams['bug_id'] = bug_ids\n\tparams['include_fields'] = '_default'\n\tparams['permissive'] = 1\n\treturn True\n\n\n", "def amend_bzparams(self, params, bug_ids):\n\tsuper(Bugzilla, self).amend_bzparams(params, bug_ids)\n\n\t# Add the bug list\n\tparams['bug_id'] = ','.join(bug_ids)\n\tparams['include_fields'] = ','.join(\n\t\t[\n\t\t\t'id', 'status', 'resolution', 'summary', 'assigned_to', 'qa_contact',\n\t\t\t'last_change_time', 'target_milestone', 'product', 'component',\n\t\t\t'priority', 'severity', 'flags', 'is_open', 'blocks', 'depends_on'\n\t\t]\n\t)\n\n\n", "def amend_bzparams(self, params, bug_ids):\n\tif 'id' in params:\n\t\tparams['id'] = bug_ids\n\telif 'bug_id' in params:\n\t\tparams['bug_id'] = bug_ids\n\telse:\n\t\tparams['ids'] = bug_ids\n\n\treturn params\n\n\n", "def amend_bzparams(self, params, bug_ids):\n\t# Only operate on the first bug\n\tif len(bug_ids) > 1:\n\t\tbug_ids = bug_ids[:1]\n\n\t# Make sure we're not going to overwrite existing params\n\tfor i in params:\n\t\tif i in self.bz_params:\n\t\t\tprint(\"Error: parameter '%s' already exists\" % i)\n\t\t\treturn False\n\n\t# Amend the params\n\tself.bz_params.update(params)\n\n\t# Update the bugs\n\tfor bug_id in bug_ids:\n\t\tself.update_bug(bug_id, params)\n\n\n", "def amend_bzparams(self, params, bug_ids):\n\tparams['id'] = bug_ids\n\treturn params\n\n\n", "def amend_bzparams(self, params, bug_ids):\n\tfor bug_id in bug_ids:\n\t\tparams['id'] = bug_id\n\t\tparams['blocks'] = self.get_blocks(bug_id)\n\t\tparams['dependson'] = self.get_dependson(bug_id)\n\t\tparams['blockedby'] = self.get_blockedby(bug_id)\n\t\tparams['keywords'] = self.get_keywords(bug_id)\n\t\tparams['attachments'] = self.get_attachments(bug_id)\n\t\tparams['comments'] = self.get_comments(bug_id)\n\t\tparams['history'] = self.get_history(bug_id)\n\t\tparams['see_also'] = self.get_see_also(bug_id)\n\t\tparams['duplicates'] = self.get_duplicates(bug_id)\n\t\tparams['external_bugs'] = self.get_external_bugs(bug_id)\n\t\tyield params\n\t\tparams.pop('id', None)\n\t\tparams.pop('blocks', None)\n\t\tparams.pop\n", "def amend_bzparams(self, params, bug_ids):\n\tquery = None\n\tif not params['bug_id']:\n\t\tquery = {'include_fields': ['id']}\n\t\tquery.update(self.get_bug_query(params))\n\t\tquery = {'query': query}\n\t\tif not bug_ids:\n\t\t\tbug_ids = self.bugzilla.search_bugs(query)\n\t\t\tbug_ids = [b['id'] for b in bug_ids['bugs']]\n\t\tparams['bug_id'] = bug_ids\n\t\tif len(bug_ids) == 1:\n\t\t\tquery = None\n\t\telse:\n\t\t\tparams['bug_id'] = ','.join(str(b) for b in bug_ids)\n\treturn query\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tif not bug_ids:\n\t\treturn params\n\tparams[\"bug_id\"] = \",\".join(bug_ids)\n\treturn params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tif bug_ids:\n\t\tparams['bug_id'] = bug_ids\n\treturn params\n\n\t@classmethod\n\tdef get_extra_args(cls):\n\t\t\"\"\"\n\t\tReturn the extra arguments for Bugzilla\n\t\t\"\"\"\n\t\treturn [\n\t\t\t(\n\t\t\t\t['-b', '--bug-ids'],\n\t\t\t\t{\n\t\t\t\t\t'action': 'store',\n\t\t\t\t\t'nargs': '+',\n\t\t\t\t\t'type': int,\n\t\t\t\t\t'help': 'Bug IDs to add to the bug report',\n\t\t\t\t},\n\t\t\t),\n\t\t]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tif not isinstance(bug_ids, (list, tuple)):\n\t\tbug_ids = [bug_ids]\n\t# if the param is already set, we don't override it\n\tif 'bug_id' in params:\n\t\treturn\n\tif 'bug_ids' in params:\n\t\treturn\n\t# if bug_ids is set, we set the bug_ids param\n\tif bug_ids:\n\t\tparams['bug_ids'] = bug_ids\n\t\treturn\n\n\t# if bug_ids is not set, we set the bug_id param\n\tif self.options.bug_id:\n\t\tparams['bug_id'] = self.options.bug_id\n\t\treturn\n\n\t# if we have no bug_ids and no bug_id, we fail\n\traise ValueError(\"No bug_ids or bug_id specified\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tif not bug_ids:\n\t\treturn\n\n\t# amend params\n\tparams['bug_id'] = bug_ids\n\tparams['include_fields'] = '_default'\n\tparams['permissive'] = 1\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tsuper(Bugzilla, self).amend_bzparams(params, bug_ids)\n\n\t# Add the bug list\n\tparams['bug_id'] = ','.join(bug_ids)\n\tparams['include_fields'] = ','.join(\n\t\t[\n\t\t\t'id', 'status', 'resolution', 'summary', 'assigned_to', 'qa_contact',\n\t\t\t'last_change_time', 'target_milestone', 'product', 'component',\n\t\t\t'priority', 'severity', 'flags', 'is_open', 'blocks', 'depends_on'\n\t\t]\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tif 'id' in params:\n\t\tparams['id'] = bug_ids\n\telif 'bug_id' in params:\n\t\tparams['bug_id'] = bug_ids\n\telse:\n\t\tparams['ids'] = bug_ids\n\n\treturn params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\t# Only operate on the first bug\n\tif len(bug_ids) > 1:\n\t\tbug_ids = bug_ids[:1]\n\n\t# Make sure we're not going to overwrite existing params\n\tfor i in params:\n\t\tif i in self.bz_params:\n\t\t\tprint(\"Error: parameter '%s' already exists\" % i)\n\t\t\treturn False\n\n\t# Amend the params\n\tself.bz_params.update(params)\n\n\t# Update the bugs\n\tfor bug_id in bug_ids:\n\t\tself.update_bug(bug_id, params)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tparams['id'] = bug_ids\n\treturn params\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tfor bug_id in bug_ids:\n\t\tparams['id'] = bug_id\n\t\tparams['blocks'] = self.get_blocks(bug_id)\n\t\tparams['dependson'] = self.get_dependson(bug_id)\n\t\tparams['blockedby'] = self.get_blockedby(bug_id)\n\t\tparams['keywords'] = self.get_keywords(bug_id)\n\t\tparams['attachments'] = self.get_attachments(bug_id)\n\t\tparams['comments'] = self.get_comments(bug_id)\n\t\tparams['history'] = self.get_history(bug_id)\n\t\tparams['see_also'] = self.get_see_also(bug_id)\n\t\tparams['duplicates'] = self.get_duplicates(bug_id)\n\t\tparams['external_bugs'] = self.get_external_bugs(bug_id)\n\t\tyield params\n\t\tparams.pop('id', None)\n\t\tparams.pop('blocks', None)\n\t\tparams.pop\n", "is_pass": false, "return_code": 1}, {"generate_code": "def amend_bzparams(self, params, bug_ids):\n\tquery = None\n\tif not params['bug_id']:\n\t\tquery = {'include_fields': ['id']}\n\t\tquery.update(self.get_bug_query(params))\n\t\tquery = {'query': query}\n\t\tif not bug_ids:\n\t\t\tbug_ids = self.bugzilla.search_bugs(query)\n\t\t\tbug_ids = [b['id'] for b in bug_ids['bugs']]\n\t\tparams['bug_id'] = bug_ids\n\t\tif len(bug_ids) == 1:\n\t\t\tquery = None\n\t\telse:\n\t\t\tparams['bug_id'] = ','.join(str(b) for b in bug_ids)\n\treturn query\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/atticmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b4567ed7d32e5b55cc83d9", "code": ["def deep_merge_nodes(nodes):\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes:\n\t\t\t\tif isinstance(merged_nodes[key], MappingNode):\n\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], MappingNode(value=value)])\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([MappingNode(value=merged_nodes[key]), MappingNode(value=value)])\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\n\treturn MappingNode(value=merged_nodes)\n\n", "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor (key, value) in node.items():\n\t\t\t\tif key in result:\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\telse:\n\t\t\tresult = node\n\treturn result\n\n\n", "def deep_merge_nodes(nodes):\n\treturn _merge_nodes(nodes, merge_func=lambda a, b: a.deep_merge(b))\n\n", "def deep_merge_nodes(nodes):\n\tnew_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in new_nodes:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tif isinstance(new_nodes[key], MappingNode):\n\t\t\t\t\t\tnew_nodes[key] = deep_merge_nodes([value, new_nodes[key]])\n\t\t\t\t\telse:\n\t\t\t\t\t\tnew_nodes[key] = value\n\t\t\telse:\n\t\t\t\tnew_nodes[key] = value\n\treturn MappingNode(new_nodes)\n\n", "def deep_merge_nodes(nodes):\n\tnode_dict = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tnode_dict.update(node.value)\n\t\telif isinstance(node, ScalarNode):\n\t\t\tnode_dict[node.value] = node.value\n\t\telse:\n\t\t\traise ValueError('Cannot merge nodes of type {}'.format(type(node)))\n\n\treturn node_dict\n\n\n", "def deep_merge_nodes(nodes):\n\tresult = {}\n\t\n\tfor node in nodes:\n\t\tfor k, v in node.items():\n\t\t\tif k in result:\n\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\tif isinstance(v, MappingNode):\n\t\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[k] = v\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = v\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\t\n\treturn result\n\n\n", "def deep_merge_nodes(nodes):\n\tif not nodes:\n\t\treturn None\n\telif len(nodes) == 1:\n\t\treturn nodes[0]\n\telse:\n\t\tresult = nodes[0]\n\t\tfor node in nodes[1:]:\n\t\t\tfor key, value in node.items():\n\t\t\t\tif key in result:\n\t\t\t\t\tif isinstance(result[key], MappingNode):\n\t\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\treturn result\n\n\n", "def deep_merge_nodes(nodes):\n\tret = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\traise TypeError(f'{node} is not a MappingNode')\n\t\tif node.key in ret:\n\t\t\tret[node.key].merge(node.value)\n\t\telse:\n\t\t\tret[node.key] = node.value\n\treturn ret\n\n\n", "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode):\n\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\tresult[key].update(deep_merge_nodes([result[key], value]))\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[key] = value\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n\n", "def deep_merge_nodes(nodes):\n\tvalue = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor k, v in node.value.items():\n\t\t\t\tif k in value:\n\t\t\t\t\tvalue[k] = deep_merge_nodes([value[k], v])\n\t\t\t\telse:\n\t\t\t\t\tvalue[k] = v\n\t\telif isinstance(node, SequenceNode):\n\t\t\tvalue = deep_merge_nodes([value, node])\n\treturn value\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\tmerged_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged_nodes:\n\t\t\t\tif isinstance(merged_nodes[key], MappingNode):\n\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], value])\n\t\t\t\t\telse:\n\t\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([merged_nodes[key], MappingNode(value=value)])\n\t\t\t\telse:\n\t\t\t\t\tmerged_nodes[key] = deep_merge_nodes([MappingNode(value=merged_nodes[key]), MappingNode(value=value)])\n\t\t\telse:\n\t\t\t\tmerged_nodes[key] = value\n\n\treturn MappingNode(value=merged_nodes)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor (key, value) in node.items():\n\t\t\t\tif key in result:\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\telse:\n\t\t\tresult = node\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\treturn _merge_nodes(nodes, merge_func=lambda a, b: a.deep_merge(b))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tnew_nodes = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in new_nodes:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tif isinstance(new_nodes[key], MappingNode):\n\t\t\t\t\t\tnew_nodes[key] = deep_merge_nodes([value, new_nodes[key]])\n\t\t\t\t\telse:\n\t\t\t\t\t\tnew_nodes[key] = value\n\t\t\telse:\n\t\t\t\tnew_nodes[key] = value\n\treturn MappingNode(new_nodes)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tnode_dict = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tnode_dict.update(node.value)\n\t\telif isinstance(node, ScalarNode):\n\t\t\tnode_dict[node.value] = node.value\n\t\telse:\n\t\t\traise ValueError('Cannot merge nodes of type {}'.format(type(node)))\n\n\treturn node_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\t\n\tfor node in nodes:\n\t\tfor k, v in node.items():\n\t\t\tif k in result:\n\t\t\t\tif isinstance(result[k], MappingNode):\n\t\t\t\t\tif isinstance(v, MappingNode):\n\t\t\t\t\t\tresult[k] = deep_merge_nodes([result[k], v])\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[k] = v\n\t\t\t\telse:\n\t\t\t\t\tresult[k] = v\n\t\t\telse:\n\t\t\t\tresult[k] = v\n\t\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tif not nodes:\n\t\treturn None\n\telif len(nodes) == 1:\n\t\treturn nodes[0]\n\telse:\n\t\tresult = nodes[0]\n\t\tfor node in nodes[1:]:\n\t\t\tfor key, value in node.items():\n\t\t\t\tif key in result:\n\t\t\t\t\tif isinstance(result[key], MappingNode):\n\t\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tret = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\traise TypeError(f'{node} is not a MappingNode')\n\t\tif node.key in ret:\n\t\t\tret[node.key].merge(node.value)\n\t\telse:\n\t\t\tret[node.key] = node.value\n\treturn ret\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tif isinstance(result[key], MappingNode):\n\t\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\t\tresult[key].update(deep_merge_nodes([result[key], value]))\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult[key] = value\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tvalue = {}\n\tfor node in nodes:\n\t\tif isinstance(node, MappingNode):\n\t\t\tfor k, v in node.value.items():\n\t\t\t\tif k in value:\n\t\t\t\t\tvalue[k] = deep_merge_nodes([value[k], v])\n\t\t\t\telse:\n\t\t\t\t\tvalue[k] = v\n\t\telif isinstance(node, SequenceNode):\n\t\t\tvalue = deep_merge_nodes([value, node])\n\treturn value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b4567ad7d32e5b55cc83af", "code": ["def parse_arguments(*arguments):\n\t# We use the \"description\" argument of ArgumentParser to print a nice-looking usage message\n\tdescription = \"Script for generating a custom /etc/hosts file for use with the Pi-hole ad-blocking software. https://pi-hole.net\"\n\n\t# Create an ArgumentParser object\n\tparser = argparse.ArgumentParser(description=description)\n\n\t# Add arguments to the ArgumentParser instance\n\t# The \"dest\" argument determines the name of the attribute that will be added to the object returned by parse_arguments()\n\tparser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", dest=\"quiet\", help=\"Silence all output\")\n\tparser.add_argument(\"-s\", \"--sources\", action=\"append\", dest=\"sources\", help=\"Specify the sources from which to pull domains (see README for full list of supported sources)\")\n\tparser.add_argument(\"-a\", \"--ad-list\", action=\"append\", dest=\"ad_lists\", help=\"Specify any additional ad-lists that should be included in the custom hosts file\")\n\tparser.add_argument(\"-d\", \"--dom\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tThis script will create a CSV file and a SQLite database containing the data from the\n\t\twebsites of the US Patent and Trademark Office (USPTO) and the European Patent Office (EPO).\n\t\tThe columns of the CSV file will contain data about the patent (patent number, patent title,\n\t\tpatent description, assignee, inventor, examiner, and examiner art unit), the patent application\n\t\t(application number, filing date, and publication date), and the patent family (the number of\n\t\tpatents in the family, the number of forward citations, and the number of backward citations).\n\t\tThe SQLite database will contain two tables: a table containing the patent data and a table\n\t\tcontaining the patent family data.\n\t\t\"\"\",\n\t\tformatter_class=argparse.RawTextHelpFormatter\n\t)\n\tparser.add_argument(\n\t\t\"-i\",\n\t\t\"--input\",\n\t\tdest=\"input_file\",\n\t\trequired=True,\n\t\thelp=\"Path to a file containing a list of\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Parse FASTA files containing sequences for which you want to calculate the GC content and output the results to a tab-delimited file.')\n\tparser.add_argument('-i','--input', help='Input file or directory containing FASTA files for which you want to calculate the GC content', required=True)\n\tparser.add_argument('-o','--output', help='Output tab-delimited file with the results', required=True)\n\tparser.add_argument('-n','--name', help='Optional column name for the output file. Defaults to \"gc_content\"', required=False, default='gc_content')\n\tparser.add_argument('-v','--verbose', help='Print all output to terminal as well as to the output file', required=False, action='store_true')\n\tparser.add_argument('-f','--force', help='Force the output file to be overwritten', required=False, action='store_true')\n\tparser.add_argument('-p','--proceed', help='Proceed if the input file or directory does\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Generate a series of plots from a given CSV file\")\n\tparser.add_argument(\"-i\", \"--input-csv\", dest=\"input_csv\", required=True, help=\"Input CSV file containing data to be plotted\")\n\tparser.add_argument(\"-o\", \"--output-directory\", dest=\"output_directory\", required=True, help=\"Output directory to which to write plots\")\n\tparser.add_argument(\"-x\", \"--x-axis\", dest=\"x_axis\", required=True, help=\"Column to use as the x-axis\")\n\tparser.add_argument(\"-y\", \"--y-axes\", dest=\"y_axes\", required=True, help=\"Columns to use as the y-axes (can be specified more than once)\")\n\tparser.add_argument(\"-t\", \"--title\", dest=\"title\", required=True, help=\"Title to use for the plot\")\n\tparser.add_argument(\"-xl\", \"--x-label\", dest=\"x_label\", required=False, help=\"Label to use for the x-axis\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"This script generates a dataset from a list of PubMed IDs and a list of PubMed IDs from which to exclude articles.\")\n\tparser.add_argument(\"--infile\", \"-i\", type=str, required=True, help=\"The file containing the list of PubMed IDs from which to generate the dataset.\")\n\tparser.add_argument(\"--outfile\", \"-o\", type=str, required=True, help=\"The path and name of the file to which to write the dataset.\")\n\tparser.add_argument(\"--exclude\", \"-e\", type=str, required=False, default=None, help=\"The file containing the list of PubMed IDs to exclude from the dataset.\")\n\tparser.add_argument(\"--format\", \"-f\", type=str, required=False, default=\"pubtator\", help=\"The format of the dataset to generate. Either 'pubtator' or 'text'.\")\n\tparser.add_argument(\"--no-progress-bar\", action=\"store_true\", required=False, default=False, help=\"Disable the progress bar.\")\n\n\t# parse the arguments\n\treturn parser.parse_args(\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--log_file\", \"-l\", help = \"the file to which the log will be written\")\n\tparser.add_argument(\"--log_level\", \"-v\", choices = [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"], default = \"INFO\", help = \"the verbosity of the logging\")\n\tparser.add_argument(\"--log_format\", \"-f\", default = \"%(asctime)s %(levelname)s [%(name)s] %(message)s\", help = \"the logging format\")\n\tparser.add_argument(\"--log_date_format\", \"-d\", default = \"%Y-%m-%d %H:%M:%S\", help = \"the format of the date in the log\")\n\t\n\treturn parser.parse_args(arguments)\n\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tThis script extracts the names of the files in a given directory that have the given extensions.\n\t\t\"\"\")\n\tparser.add_argument('dir', type=str, help=\"\"\"\n\t\tThe directory whose files are to be extracted.\n\t\t\"\"\")\n\tparser.add_argument('-ext', type=str, nargs='+', help=\"\"\"\n\t\tThe extensions of the files to be extracted.\n\t\t\"\"\")\n\tparser.add_argument('--exts', type=str, nargs='+', help=\"\"\"\n\t\tAn alias for the -ext argument.\n\t\t\"\"\")\n\tparser.add_argument('--out', type=str, help=\"\"\"\n\t\tA file to write the list of files to.\n\t\t\"\"\")\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Generate a new password for the user, given the user's name and password file\")\n\tparser.add_argument(\"user\", help=\"The name of the user for whom to generate a password\")\n\tparser.add_argument(\"password_file\", help=\"The name of the password file to use\")\n\tparser.add_argument(\"--no-write\", action=\"store_true\", help=\"Generate a new password but do not save it in the given password file\")\n\tparser.add_argument(\"--no-print\", action=\"store_true\", help=\"Generate a new password, but do not print it to the terminal\")\n\tparser.add_argument(\"--print-old\", action=\"store_true\", help=\"Print the user's old password to the terminal\")\n\tparser.add_argument(\"--no-old\", action=\"store_true\", help=\"Do not print the user's old password to the terminal\")\n\tparser.add_argument(\"--no-new\", action=\"store_true\", help=\"Do not print the user's new password to the terminal\")\n\tparser.add_argument(\"--no\n", "def parse_arguments(*arguments):\n\tdescription = '''\n\tGiven a directory containing subdirectories of raw sequencing data,\n\tthis script will perform the following functions:\n\t\n\t1. Rename each raw sequencing run subdirectory according to the sample name,\n\t   as specified in the sample sheet.\n\t2. Copy the sample sheet to a new location, and rename it according to the\n\t   sample name, as specified in the sample sheet.\n\t3. If a directory called 'fastq' exists in the current directory, move all\n\t   sample sheet-renamed subdirectories into the 'fastq' directory.\n\t4. Copy all sample sheet-renamed subdirectories to a remote server.\n\t'''\n\n\tparser = argparse.ArgumentParser(description=description)\n\tparser.add_argument('-d', '--data',\n\t\t\t\t\t\tdest='data_directory',\n\t\t\t\t\t\thelp='The directory containing the raw sequencing run subdirectories',\n\t\t\t\t\t\trequired=True)\n\tparser.add_argument('-s', '--server',\n\t\t\t\t\t\tdest='server',\n\t\t\t\n", "def parse_arguments(*arguments):\n\tparser = ArgumentParser(description=\"Delete a specific directory from all of the specified GCS buckets, optionally recursively\")\n\tparser.add_argument('--buckets', nargs='+', required=True, help=\"The names of the buckets from which the specified directories should be deleted\")\n\tparser.add_argument('--prefixes', nargs='+', required=True, help=\"The prefixes of the directories to delete from the specified buckets\")\n\tparser.add_argument('--recursive', action='store_true', help=\"Whether or not to delete the specified directories recursively\")\n\t\n\treturn parser.parse_args(arguments)\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\t# We use the \"description\" argument of ArgumentParser to print a nice-looking usage message\n\tdescription = \"Script for generating a custom /etc/hosts file for use with the Pi-hole ad-blocking software. https://pi-hole.net\"\n\n\t# Create an ArgumentParser object\n\tparser = argparse.ArgumentParser(description=description)\n\n\t# Add arguments to the ArgumentParser instance\n\t# The \"dest\" argument determines the name of the attribute that will be added to the object returned by parse_arguments()\n\tparser.add_argument(\"-q\", \"--quiet\", action=\"store_true\", dest=\"quiet\", help=\"Silence all output\")\n\tparser.add_argument(\"-s\", \"--sources\", action=\"append\", dest=\"sources\", help=\"Specify the sources from which to pull domains (see README for full list of supported sources)\")\n\tparser.add_argument(\"-a\", \"--ad-list\", action=\"append\", dest=\"ad_lists\", help=\"Specify any additional ad-lists that should be included in the custom hosts file\")\n\tparser.add_argument(\"-d\", \"--dom\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tThis script will create a CSV file and a SQLite database containing the data from the\n\t\twebsites of the US Patent and Trademark Office (USPTO) and the European Patent Office (EPO).\n\t\tThe columns of the CSV file will contain data about the patent (patent number, patent title,\n\t\tpatent description, assignee, inventor, examiner, and examiner art unit), the patent application\n\t\t(application number, filing date, and publication date), and the patent family (the number of\n\t\tpatents in the family, the number of forward citations, and the number of backward citations).\n\t\tThe SQLite database will contain two tables: a table containing the patent data and a table\n\t\tcontaining the patent family data.\n\t\t\"\"\",\n\t\tformatter_class=argparse.RawTextHelpFormatter\n\t)\n\tparser.add_argument(\n\t\t\"-i\",\n\t\t\"--input\",\n\t\tdest=\"input_file\",\n\t\trequired=True,\n\t\thelp=\"Path to a file containing a list of\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='Parse FASTA files containing sequences for which you want to calculate the GC content and output the results to a tab-delimited file.')\n\tparser.add_argument('-i','--input', help='Input file or directory containing FASTA files for which you want to calculate the GC content', required=True)\n\tparser.add_argument('-o','--output', help='Output tab-delimited file with the results', required=True)\n\tparser.add_argument('-n','--name', help='Optional column name for the output file. Defaults to \"gc_content\"', required=False, default='gc_content')\n\tparser.add_argument('-v','--verbose', help='Print all output to terminal as well as to the output file', required=False, action='store_true')\n\tparser.add_argument('-f','--force', help='Force the output file to be overwritten', required=False, action='store_true')\n\tparser.add_argument('-p','--proceed', help='Proceed if the input file or directory does\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Generate a series of plots from a given CSV file\")\n\tparser.add_argument(\"-i\", \"--input-csv\", dest=\"input_csv\", required=True, help=\"Input CSV file containing data to be plotted\")\n\tparser.add_argument(\"-o\", \"--output-directory\", dest=\"output_directory\", required=True, help=\"Output directory to which to write plots\")\n\tparser.add_argument(\"-x\", \"--x-axis\", dest=\"x_axis\", required=True, help=\"Column to use as the x-axis\")\n\tparser.add_argument(\"-y\", \"--y-axes\", dest=\"y_axes\", required=True, help=\"Columns to use as the y-axes (can be specified more than once)\")\n\tparser.add_argument(\"-t\", \"--title\", dest=\"title\", required=True, help=\"Title to use for the plot\")\n\tparser.add_argument(\"-xl\", \"--x-label\", dest=\"x_label\", required=False, help=\"Label to use for the x-axis\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"This script generates a dataset from a list of PubMed IDs and a list of PubMed IDs from which to exclude articles.\")\n\tparser.add_argument(\"--infile\", \"-i\", type=str, required=True, help=\"The file containing the list of PubMed IDs from which to generate the dataset.\")\n\tparser.add_argument(\"--outfile\", \"-o\", type=str, required=True, help=\"The path and name of the file to which to write the dataset.\")\n\tparser.add_argument(\"--exclude\", \"-e\", type=str, required=False, default=None, help=\"The file containing the list of PubMed IDs to exclude from the dataset.\")\n\tparser.add_argument(\"--format\", \"-f\", type=str, required=False, default=\"pubtator\", help=\"The format of the dataset to generate. Either 'pubtator' or 'text'.\")\n\tparser.add_argument(\"--no-progress-bar\", action=\"store_true\", required=False, default=False, help=\"Disable the progress bar.\")\n\n\t# parse the arguments\n\treturn parser.parse_args(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--log_file\", \"-l\", help = \"the file to which the log will be written\")\n\tparser.add_argument(\"--log_level\", \"-v\", choices = [\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"], default = \"INFO\", help = \"the verbosity of the logging\")\n\tparser.add_argument(\"--log_format\", \"-f\", default = \"%(asctime)s %(levelname)s [%(name)s] %(message)s\", help = \"the logging format\")\n\tparser.add_argument(\"--log_date_format\", \"-d\", default = \"%Y-%m-%d %H:%M:%S\", help = \"the format of the date in the log\")\n\t\n\treturn parser.parse_args(arguments)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\t\tThis script extracts the names of the files in a given directory that have the given extensions.\n\t\t\"\"\")\n\tparser.add_argument('dir', type=str, help=\"\"\"\n\t\tThe directory whose files are to be extracted.\n\t\t\"\"\")\n\tparser.add_argument('-ext', type=str, nargs='+', help=\"\"\"\n\t\tThe extensions of the files to be extracted.\n\t\t\"\"\")\n\tparser.add_argument('--exts', type=str, nargs='+', help=\"\"\"\n\t\tAn alias for the -ext argument.\n\t\t\"\"\")\n\tparser.add_argument('--out', type=str, help=\"\"\"\n\t\tA file to write the list of files to.\n\t\t\"\"\")\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Generate a new password for the user, given the user's name and password file\")\n\tparser.add_argument(\"user\", help=\"The name of the user for whom to generate a password\")\n\tparser.add_argument(\"password_file\", help=\"The name of the password file to use\")\n\tparser.add_argument(\"--no-write\", action=\"store_true\", help=\"Generate a new password but do not save it in the given password file\")\n\tparser.add_argument(\"--no-print\", action=\"store_true\", help=\"Generate a new password, but do not print it to the terminal\")\n\tparser.add_argument(\"--print-old\", action=\"store_true\", help=\"Print the user's old password to the terminal\")\n\tparser.add_argument(\"--no-old\", action=\"store_true\", help=\"Do not print the user's old password to the terminal\")\n\tparser.add_argument(\"--no-new\", action=\"store_true\", help=\"Do not print the user's new password to the terminal\")\n\tparser.add_argument(\"--no\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tdescription = '''\n\tGiven a directory containing subdirectories of raw sequencing data,\n\tthis script will perform the following functions:\n\t\n\t1. Rename each raw sequencing run subdirectory according to the sample name,\n\t   as specified in the sample sheet.\n\t2. Copy the sample sheet to a new location, and rename it according to the\n\t   sample name, as specified in the sample sheet.\n\t3. If a directory called 'fastq' exists in the current directory, move all\n\t   sample sheet-renamed subdirectories into the 'fastq' directory.\n\t4. Copy all sample sheet-renamed subdirectories to a remote server.\n\t'''\n\n\tparser = argparse.ArgumentParser(description=description)\n\tparser.add_argument('-d', '--data',\n\t\t\t\t\t\tdest='data_directory',\n\t\t\t\t\t\thelp='The directory containing the raw sequencing run subdirectories',\n\t\t\t\t\t\trequired=True)\n\tparser.add_argument('-s', '--server',\n\t\t\t\t\t\tdest='server',\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = ArgumentParser(description=\"Delete a specific directory from all of the specified GCS buckets, optionally recursively\")\n\tparser.add_argument('--buckets', nargs='+', required=True, help=\"The names of the buckets from which the specified directories should be deleted\")\n\tparser.add_argument('--prefixes', nargs='+', required=True, help=\"The prefixes of the directories to delete from the specified buckets\")\n\tparser.add_argument('--recursive', action='store_true', help=\"Whether or not to delete the specified directories recursively\")\n\t\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 2}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/atticmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b45679d7d32e5b55cc83a9", "code": ["def parser_flags(parser):\n\treturn \" \".join(arg for arg in sys.argv[1:] if arg.startswith(\"-\"))\n\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, _HelpAction):\n\t\t\tcontinue\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn \" \".join(flags)\n\n\n", "def parser_flags(parser):\n\treturn \" \".join(sorted([\n\t\t(\"-\" + x) if len(x) == 1 else (\"--\" + x)\n\t\tfor x in parser._option_string_actions.keys()\n\t]))\n\n", "def parser_flags(parser):\n\treturn \" \".join(flag for flag in itertools.chain(parser._option_string_actions.keys(),\n\t\tparser._positionals._actions[0].option_strings))\n\n", "def parser_flags(parser):\n\treturn ' '.join(sorted(['-%s' % (f,) if len(f) == 1 else '--%s' % (f,) for f in parser._option_string_actions]))\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(\n\t\tx.option_strings[0] if len(x.option_strings) == 1 else x.option_strings[1]\n\t\tfor x in parser._actions\n\t)\n\n\n", "def parser_flags(parser):\n\treturn ' '.join([\n\t\targ for arg in parser._actions if isinstance(arg, argparse._StoreAction)\n\t])\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(list(set(sum([list(action.option_strings) for action in parser._actions], []) +\n\t\t\t\t\t\t\t   parser.prefix_chars)))\n\n\n", "def parser_flags(parser):\n\treturn \" \".join(sorted(arg.lstrip('-').replace('-', '_') for arg in vars(parser.parse_known_args()[0])))\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(sorted(set(flag for action in parser._actions for flag in action.option_strings)))\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\treturn \" \".join(arg for arg in sys.argv[1:] if arg.startswith(\"-\"))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif isinstance(action, _HelpAction):\n\t\t\tcontinue\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn \" \".join(flags)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(sorted([\n\t\t(\"-\" + x) if len(x) == 1 else (\"--\" + x)\n\t\tfor x in parser._option_string_actions.keys()\n\t]))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(flag for flag in itertools.chain(parser._option_string_actions.keys(),\n\t\tparser._positionals._actions[0].option_strings))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(sorted(['-%s' % (f,) if len(f) == 1 else '--%s' % (f,) for f in parser._option_string_actions]))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(\n\t\tx.option_strings[0] if len(x.option_strings) == 1 else x.option_strings[1]\n\t\tfor x in parser._actions\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join([\n\t\targ for arg in parser._actions if isinstance(arg, argparse._StoreAction)\n\t])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(list(set(sum([list(action.option_strings) for action in parser._actions], []) +\n\t\t\t\t\t\t\t   parser.prefix_chars)))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(sorted(arg.lstrip('-').replace('-', '_') for arg in vars(parser.parse_known_args()[0])))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(sorted(set(flag for action in parser._actions for flag in action.option_strings)))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b45665d7d32e5b55cc8365", "code": ["def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--mode\", help=\"'train' or 'test'\", type=str, default=\"train\")\n\tparser.add_argument(\"--log_dir\", help=\"Directory where to write training logs\", type=str, default=\"logs\")\n\tparser.add_argument(\"--train_dir\", help=\"Directory where to write event logs and checkpoint\", type=str, default=\"train\")\n\tparser.add_argument(\"--max_steps\", help=\"Number of batches to run\", type=int, default=10000)\n\tparser.add_argument(\"--batch_size\", help=\"Number of samples per batch\", type=int, default=32)\n\tparser.add_argument(\"--learning_rate\", help=\"Initial learning rate\", type=float, default=1e-4)\n\tparser.add_argument(\"--dropout\", help=\"Keep probability for training dropout\", type=float, default=0.5)\n\tparser.add_argument(\"--weight_decay\", help=\"Weight decay factor\", type=float, default=0.00004)\n\tparser.add\n", "def parse_arguments(*unparsed_arguments):\n\tparameter_map = {}\n\n\t# Remove all empty strings\n\tunparsed_arguments = [x for x in unparsed_arguments if x != \"\"]\n\n\t# Iterate through all arguments and parse them\n\tfor argument in unparsed_arguments:\n\n\t\t# Parse the argument\n\t\tsplit_argument = argument.split(\"=\")\n\n\t\t# Check if the argument is parsed correctly\n\t\tif len(split_argument) != 2:\n\t\t\traise Exception(\"Could not parse argument: \" + argument)\n\n\t\t# Check if the key already exists\n\t\tif split_argument[0] in parameter_map:\n\t\t\traise Exception(\"The key \" + split_argument[0] + \" already exists!\")\n\n\t\t# Add the argument to the parameter map\n\t\tparameter_map[split_argument[0]] = split_argument[1]\n\n\treturn parameter_map\n\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Processes *.log files from given folder'\n\t)\n\tparser.add_argument(\n\t\t'-d', '--log-dir',\n\t\thelp='Path to the directory with log files'\n\t)\n\tparser.add_argument(\n\t\t'-o', '--output-dir',\n\t\thelp='Path to the directory where to store results'\n\t)\n\tparser.add_argument(\n\t\t'-t', '--top-count',\n\t\thelp='Top N results to return',\n\t\ttype=int\n\t)\n\tparser.add_argument(\n\t\t'-p', '--parallelism',\n\t\thelp='Number of parallel threads to use',\n\t\ttype=int\n\t)\n\tparser.add_argument(\n\t\t'-m', '--max-lines',\n\t\thelp='Maximum number of lines to process',\n\t\ttype=int\n\t)\n\n\targuments, _ = parser.parse_known_args(unparsed_arguments)\n\treturn arguments\n\n\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('--data-dir',\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault='/tmp/tensorflow/mnist/input_data',\n\t\t\t\t\t\thelp='Directory for storing input data')\n\n\tparser.add_argument('--job-dir',\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault='./job_dir',\n\t\t\t\t\t\thelp='Directory for storing model checkpoints')\n\n\tparser.add_argument('--train-steps',\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=1000,\n\t\t\t\t\t\thelp='Number of training steps')\n\n\tparser.add_argument('--batch-size',\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=100,\n\t\t\t\t\t\thelp='Number of batch size')\n\n\tparser.add_argument('--learning-rate',\n\t\t\t\t\t\ttype=float,\n\t\t\t\t\t\tdefault=\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"CSV to JSON converter\")\n\tparser.add_argument('-i', '--input', required=True, help='Input CSV file')\n\tparser.add_argument('-o', '--output', required=True, help='Output JSON file')\n\tparser.add_argument('-n', '--names', required=True, help='Name of JSON object to be converted')\n\tparser.add_argument('-f', '--format', required=False, help='Output JSON format', choices=['array', 'hash'], default='array')\n\tparser.add_argument('-s', '--separator', required=False, help='Separator used in CSV file', default=';')\n\tparser.add_argument('--no-header', required=False, help='CSV contains no header', action=\"store_true\")\n\tparser.add_argument('--ignore-empty', required=False, help='Ignore empty values', action=\"store_true\")\n\tparser.add_argument('--id-column', required=False, help='Name of ID column')\n\tparser\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"A simple program to auto generate CMakeLists.txt\")\n\tparser.add_argument(\n\t\t\"--files\",\n\t\thelp=\"files to include in the CMakeLists.txt\",\n\t\tdefault=\"*\"\n\t)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\thelp=\"Version of the project\",\n\t\tdefault=\"0.1.0\"\n\t)\n\tparser.add_argument(\n\t\t\"--name\",\n\t\thelp=\"Name of the project\",\n\t\tdefault=\"Project\"\n\t)\n\tparser.add_argument(\n\t\t\"--description\",\n\t\thelp=\"Description of the project\",\n\t\tdefault=\"Project description\"\n\t)\n\tparser.add_argument(\n\t\t\"--author\",\n\t\thelp=\"Author of the project\",\n\t\tdefault=\"Me\"\n\t)\n\tparser.add_argument(\n\t\t\"--author-email\",\n\t\thelp=\"Author's email\",\n\t\tdefault=\"me@example.com\"\n\t)\n\tparser.add\n", "def parse_arguments(*unparsed_arguments):\n\t# Parse input arguments\n\tparser = argparse.ArgumentParser(description='Script for retrieving information about a list of users')\n\n\t# Add arguments\n\tparser.add_argument('-i', '--input', type=str, help='File containing a list of user IDs', required=True)\n\tparser.add_argument('-o', '--output', type=str, help='Output file', required=True)\n\tparser.add_argument('-t', '--token', type=str, help='Twitter token', required=True)\n\tparser.add_argument('-s', '--secret', type=str, help='Twitter secret', required=True)\n\tparser.add_argument('-kn', '--consumer_key', type=str, help='Twitter consumer key', required=True)\n\tparser.add_argument('-ks', '--consumer_secret', type=str, help='Twitter consumer secret', required=True)\n\n\t# Parse the arguments\n\treturn vars(parser.parse_args(*unparsed_arguments))\n\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--host\", help=\"Hostname for the server. Defaults to localhost\", default=\"localhost\")\n\tparser.add_argument(\"--port\", help=\"Port for the server. Defaults to 8080\", default=8080)\n\tparser.add_argument(\"--log_file\", help=\"Log file. Defaults to stdout\", default=sys.stdout)\n\tparser.add_argument(\"--verbose\", help=\"Verbose output. Defaults to False\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--debug\", help=\"Debug mode. Defaults to False\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--database\", help=\"Database. Defaults to sqlite\", default=\"sqlite\")\n\tparser.add_argument(\"--database_file\", help=\"Database file. Defaults to db.sqlite\", default=\"db.sqlite\")\n\tparser.add_argument(\"--database_host\", help=\"Database host. Defaults to localhost\", default=\"localhost\")\n\tparser.add_\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"This script is used to create a local copy of a git repository\")\n\tparser.add_argument(\"-r\", \"--repository\", help=\"The git repository to clone\")\n\tparser.add_argument(\"-p\", \"--path\", help=\"The path in which the repository should be cloned to\")\n\treturn parser.parse_args(unparsed_arguments)\n\n\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description='Retrieves basic information about a specific user.')\n\tparser.add_argument('--user-id', type=str, required=True, help='The userId to be used to fetch the user information')\n\tparser.add_argument('--tenant-id', type=str, required=True, help='The tenantId to be used to fetch the user information')\n\tparser.add_argument('--api-version', type=str, default='1.6', help='The API version to be used to fetch the user information')\n\tparser.add_argument('--base-url', type=str, default='https://api.office.com', help='The base URL to be used to fetch the user information')\n\tparser.add_argument('--print-json', action='store_true', help='Prints the JSON response from the API as a string')\n\tparser.add_argument('--print-json-pretty', action='store_true', help='Prints the JSON response from the API as a pretty string')\n\tparser.add_argument('--print-json-pretty-with\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--mode\", help=\"'train' or 'test'\", type=str, default=\"train\")\n\tparser.add_argument(\"--log_dir\", help=\"Directory where to write training logs\", type=str, default=\"logs\")\n\tparser.add_argument(\"--train_dir\", help=\"Directory where to write event logs and checkpoint\", type=str, default=\"train\")\n\tparser.add_argument(\"--max_steps\", help=\"Number of batches to run\", type=int, default=10000)\n\tparser.add_argument(\"--batch_size\", help=\"Number of samples per batch\", type=int, default=32)\n\tparser.add_argument(\"--learning_rate\", help=\"Initial learning rate\", type=float, default=1e-4)\n\tparser.add_argument(\"--dropout\", help=\"Keep probability for training dropout\", type=float, default=0.5)\n\tparser.add_argument(\"--weight_decay\", help=\"Weight decay factor\", type=float, default=0.00004)\n\tparser.add\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparameter_map = {}\n\n\t# Remove all empty strings\n\tunparsed_arguments = [x for x in unparsed_arguments if x != \"\"]\n\n\t# Iterate through all arguments and parse them\n\tfor argument in unparsed_arguments:\n\n\t\t# Parse the argument\n\t\tsplit_argument = argument.split(\"=\")\n\n\t\t# Check if the argument is parsed correctly\n\t\tif len(split_argument) != 2:\n\t\t\traise Exception(\"Could not parse argument: \" + argument)\n\n\t\t# Check if the key already exists\n\t\tif split_argument[0] in parameter_map:\n\t\t\traise Exception(\"The key \" + split_argument[0] + \" already exists!\")\n\n\t\t# Add the argument to the parameter map\n\t\tparameter_map[split_argument[0]] = split_argument[1]\n\n\treturn parameter_map\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Processes *.log files from given folder'\n\t)\n\tparser.add_argument(\n\t\t'-d', '--log-dir',\n\t\thelp='Path to the directory with log files'\n\t)\n\tparser.add_argument(\n\t\t'-o', '--output-dir',\n\t\thelp='Path to the directory where to store results'\n\t)\n\tparser.add_argument(\n\t\t'-t', '--top-count',\n\t\thelp='Top N results to return',\n\t\ttype=int\n\t)\n\tparser.add_argument(\n\t\t'-p', '--parallelism',\n\t\thelp='Number of parallel threads to use',\n\t\ttype=int\n\t)\n\tparser.add_argument(\n\t\t'-m', '--max-lines',\n\t\thelp='Maximum number of lines to process',\n\t\ttype=int\n\t)\n\n\targuments, _ = parser.parse_known_args(unparsed_arguments)\n\treturn arguments\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument('--data-dir',\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault='/tmp/tensorflow/mnist/input_data',\n\t\t\t\t\t\thelp='Directory for storing input data')\n\n\tparser.add_argument('--job-dir',\n\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\tdefault='./job_dir',\n\t\t\t\t\t\thelp='Directory for storing model checkpoints')\n\n\tparser.add_argument('--train-steps',\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=1000,\n\t\t\t\t\t\thelp='Number of training steps')\n\n\tparser.add_argument('--batch-size',\n\t\t\t\t\t\ttype=int,\n\t\t\t\t\t\tdefault=100,\n\t\t\t\t\t\thelp='Number of batch size')\n\n\tparser.add_argument('--learning-rate',\n\t\t\t\t\t\ttype=float,\n\t\t\t\t\t\tdefault=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"CSV to JSON converter\")\n\tparser.add_argument('-i', '--input', required=True, help='Input CSV file')\n\tparser.add_argument('-o', '--output', required=True, help='Output JSON file')\n\tparser.add_argument('-n', '--names', required=True, help='Name of JSON object to be converted')\n\tparser.add_argument('-f', '--format', required=False, help='Output JSON format', choices=['array', 'hash'], default='array')\n\tparser.add_argument('-s', '--separator', required=False, help='Separator used in CSV file', default=';')\n\tparser.add_argument('--no-header', required=False, help='CSV contains no header', action=\"store_true\")\n\tparser.add_argument('--ignore-empty', required=False, help='Ignore empty values', action=\"store_true\")\n\tparser.add_argument('--id-column', required=False, help='Name of ID column')\n\tparser\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"A simple program to auto generate CMakeLists.txt\")\n\tparser.add_argument(\n\t\t\"--files\",\n\t\thelp=\"files to include in the CMakeLists.txt\",\n\t\tdefault=\"*\"\n\t)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\thelp=\"Version of the project\",\n\t\tdefault=\"0.1.0\"\n\t)\n\tparser.add_argument(\n\t\t\"--name\",\n\t\thelp=\"Name of the project\",\n\t\tdefault=\"Project\"\n\t)\n\tparser.add_argument(\n\t\t\"--description\",\n\t\thelp=\"Description of the project\",\n\t\tdefault=\"Project description\"\n\t)\n\tparser.add_argument(\n\t\t\"--author\",\n\t\thelp=\"Author of the project\",\n\t\tdefault=\"Me\"\n\t)\n\tparser.add_argument(\n\t\t\"--author-email\",\n\t\thelp=\"Author's email\",\n\t\tdefault=\"me@example.com\"\n\t)\n\tparser.add\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\t# Parse input arguments\n\tparser = argparse.ArgumentParser(description='Script for retrieving information about a list of users')\n\n\t# Add arguments\n\tparser.add_argument('-i', '--input', type=str, help='File containing a list of user IDs', required=True)\n\tparser.add_argument('-o', '--output', type=str, help='Output file', required=True)\n\tparser.add_argument('-t', '--token', type=str, help='Twitter token', required=True)\n\tparser.add_argument('-s', '--secret', type=str, help='Twitter secret', required=True)\n\tparser.add_argument('-kn', '--consumer_key', type=str, help='Twitter consumer key', required=True)\n\tparser.add_argument('-ks', '--consumer_secret', type=str, help='Twitter consumer secret', required=True)\n\n\t# Parse the arguments\n\treturn vars(parser.parse_args(*unparsed_arguments))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--host\", help=\"Hostname for the server. Defaults to localhost\", default=\"localhost\")\n\tparser.add_argument(\"--port\", help=\"Port for the server. Defaults to 8080\", default=8080)\n\tparser.add_argument(\"--log_file\", help=\"Log file. Defaults to stdout\", default=sys.stdout)\n\tparser.add_argument(\"--verbose\", help=\"Verbose output. Defaults to False\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--debug\", help=\"Debug mode. Defaults to False\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--database\", help=\"Database. Defaults to sqlite\", default=\"sqlite\")\n\tparser.add_argument(\"--database_file\", help=\"Database file. Defaults to db.sqlite\", default=\"db.sqlite\")\n\tparser.add_argument(\"--database_host\", help=\"Database host. Defaults to localhost\", default=\"localhost\")\n\tparser.add_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"This script is used to create a local copy of a git repository\")\n\tparser.add_argument(\"-r\", \"--repository\", help=\"The git repository to clone\")\n\tparser.add_argument(\"-p\", \"--path\", help=\"The path in which the repository should be cloned to\")\n\treturn parser.parse_args(unparsed_arguments)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description='Retrieves basic information about a specific user.')\n\tparser.add_argument('--user-id', type=str, required=True, help='The userId to be used to fetch the user information')\n\tparser.add_argument('--tenant-id', type=str, required=True, help='The tenantId to be used to fetch the user information')\n\tparser.add_argument('--api-version', type=str, default='1.6', help='The API version to be used to fetch the user information')\n\tparser.add_argument('--base-url', type=str, default='https://api.office.com', help='The base URL to be used to fetch the user information')\n\tparser.add_argument('--print-json', action='store_true', help='Prints the JSON response from the API as a string')\n\tparser.add_argument('--print-json-pretty', action='store_true', help='Prints the JSON response from the API as a pretty string')\n\tparser.add_argument('--print-json-pretty-with\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b45665d7d32e5b55cc8364", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_actions = set([action.dest for action in subparsers.values()._actions])\n\tsubparser_name = None\n\tsubparser_arguments = []\n\tremaining_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tif argument in subparsers:\n\t\t\tif subparser_name:\n\t\t\t\tsubparsers[subparser_name].parse_args(subparser_arguments, namespace=subparser_namespace)\n\t\t\t\tsubparser_arguments = []\n\t\t\tsubparser_name = argument\n\t\t\tsubparser_namespace = argparse.Namespace()\n\t\telif subparser_name and argument in subparser_actions:\n\t\t\tsubparsers[subparser_name].parse_args(subparser_arguments, namespace=subparser_namespace)\n\t\t\tsubparser_arguments = []\n\t\t\tsubparser_name = None\n\t\telse:\n\t\t\tsubparser_arguments.append(argument)\n\tif subparser_name:\n\t\tsubparsers[\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\targuments_per_subparser = collections.defaultdict(list)\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif subparser.can_parse(argument):\n\t\t\t\targuments_per_subparser[subparser_name].append(argument)\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise ValueError(\"unrecognized argument {!r}\".format(argument))\n\tparsed_arguments_per_subparser = {\n\t\tsubparser_name: subparser.parse_args(arguments)\n\t\tfor subparser_name, subparser in subparsers.items()\n\t\tfor arguments in [arguments_per_subparser[subparser_name]]\n\t\tif arguments\n\t}\n\tunclaimed_arguments = [\n\t\targument\n\t\tfor argument in unparsed_arguments\n\t\tif not any(\n\t\t\targuments_per_subparser[subparser_name]\n\t\t\tfor subparser_name in parsed_arguments_\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Assemble a list of all subparsers' parse_known_args() functions, so we can call them in\n\t# order.\n\tparse_functions = []\n\tfor name, subparser in subparsers.items():\n\t\tparse_functions.append(lambda name, subparser: (name, subparser.parse_known_args(unparsed_arguments)))\n\t# Call each parse function, keeping track of which arguments were consumed by each.\n\tparsed_arguments = {}\n\tconsumed_arguments = set()\n\tfor name, subparser in subparsers.items():\n\t\tname, (namespace, remaining_arguments) = subparser.parse_known_args(unparsed_arguments)\n\t\tparsed_arguments[name] = namespace\n\t\tconsumed_arguments.update(set(unparsed_arguments) - set(remaining_arguments))\n\t# Return the parsed arguments, along with the arguments that were not claimed by any subparser.\n\treturn (parsed_arguments, [arg for arg in unparsed\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_arguments = {}\n\tunparsed_arguments = list(unparsed_arguments)\n\twhile unparsed_arguments:\n\t\tfound_action = False\n\t\tfor action, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tns = subparser.parse_known_args(unparsed_arguments)\n\t\t\texcept SystemExit:\n\t\t\t\tcontinue\n\t\t\tif ns[0].action == action:\n\t\t\t\tfound_action = True\n\t\t\t\tsubparser_arguments[action] = ns[0]\n\t\t\t\tunparsed_arguments = ns[1]\n\t\t\t\tbreak\n\t\tif not found_action:\n\t\t\tbreak\n\treturn subparser_arguments, unparsed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Start with a dict mapping from subparser name to a list of arguments to be parsed by\n\t# that subparser.\n\tsubparser_arguments = {name: [] for name in subparsers}\n\t# Keep track of which subparsers have been seen.\n\tseen_subparsers = set()\n\t# Keep track of the remaining arguments.\n\tremaining_arguments = []\n\t# Iterate over the arguments, stopping when the argument is an action.\n\tfor i, argument in enumerate(unparsed_arguments):\n\t\tif argument in subparsers:\n\t\t\t# This is an action, so stop here.\n\t\t\tremaining_arguments = unparsed_arguments[i:]\n\t\t\tbreak\n\t\telse:\n\t\t\t# This is not an action, so add it to all subparsers' argument lists.\n\t\t\tfor name in subparsers:\n\t\t\t\tsubparser_arguments[name].append(argument)\n\t# Iterate over the remaining arguments, which are actions.\n\tfor argument in remaining_arguments:\n\t\t#\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# If any subparser was explicitly requested, assume that all arguments belong to it.\n\t# Otherwise, examine each subparser in turn to see if any of them can parse the\n\t# arguments.\n\tsubparser_name_to_arguments = {}\n\tsubparser_names = subparsers.keys()\n\tfor subparser_name in subparser_names:\n\t\tsubparser = subparsers[subparser_name]\n\t\tsubparser_arguments, remaining_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\tif subparser_arguments.subparser_name == subparser_name:\n\t\t\tsubparser_name_to_arguments[subparser_name] = subparser_arguments\n\t\t\tunparsed_arguments = remaining_arguments\n\t\t\tsubparser_names.remove(subparser_name)\n\t\t\tbreak\n\telse:\n\t\tsubparser_name_to_arguments = {}\n\t\tremaining_arguments = unparsed_arguments\n\t\tfor subparser_name in subparser_names:\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Parse all command-line arguments.\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif subparser.prog in argument:\n\t\t\t\tif subparser_name in parsed_arguments:\n\t\t\t\t\traise Exception(\n\t\t\t\t\t\t\"Argument %s matches more than one subparser!\" % argument)\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(\n\t\t\t\t\tunparsed_arguments)\n\t\t\t\tbreak\n\t\telse:\n\t\t\tremaining_arguments.append(argument)\n\treturn parsed_arguments, remaining_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed = {}\n\tremaining = []\n\tfor subparser_name in subparsers:\n\t\tparsed[subparser_name] = subparsers[subparser_name].parse_args(unparsed_arguments)\n\t\tif parsed[subparser_name].action == subparser_name:\n\t\t\tremaining = parsed[subparser_name].remaining\n\t\t\tbreak\n\t\n\treturn (parsed, remaining)\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tassert len(unparsed_arguments) > 0\n\tsubparser_names = subparsers.keys()\n\tfor i in range(len(unparsed_arguments)):\n\t\ttry:\n\t\t\treturn subparsers[unparsed_arguments[i]].parse_known_args(unparsed_arguments[i:])\n\t\texcept KeyError:\n\t\t\tpass\n\traise ArgumentParserError(\"Subparser not specified\", subparser_names)\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_args = {}\n\tunparsed_args = []\n\n\twhile unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_args[subparser_name] = subparser.parse_known_args(unparsed_arguments)[0]\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tunparsed_arguments = unparsed_args\n\t\t\t\tbreak\n\t\telse:\n\t\t\tunparsed_args.append(unparsed_arguments.pop(0))\n\n\treturn parsed_args, unparsed_args\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_actions = set([action.dest for action in subparsers.values()._actions])\n\tsubparser_name = None\n\tsubparser_arguments = []\n\tremaining_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tif argument in subparsers:\n\t\t\tif subparser_name:\n\t\t\t\tsubparsers[subparser_name].parse_args(subparser_arguments, namespace=subparser_namespace)\n\t\t\t\tsubparser_arguments = []\n\t\t\tsubparser_name = argument\n\t\t\tsubparser_namespace = argparse.Namespace()\n\t\telif subparser_name and argument in subparser_actions:\n\t\t\tsubparsers[subparser_name].parse_args(subparser_arguments, namespace=subparser_namespace)\n\t\t\tsubparser_arguments = []\n\t\t\tsubparser_name = None\n\t\telse:\n\t\t\tsubparser_arguments.append(argument)\n\tif subparser_name:\n\t\tsubparsers[\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\targuments_per_subparser = collections.defaultdict(list)\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif subparser.can_parse(argument):\n\t\t\t\targuments_per_subparser[subparser_name].append(argument)\n\t\t\t\tbreak\n\t\telse:\n\t\t\traise ValueError(\"unrecognized argument {!r}\".format(argument))\n\tparsed_arguments_per_subparser = {\n\t\tsubparser_name: subparser.parse_args(arguments)\n\t\tfor subparser_name, subparser in subparsers.items()\n\t\tfor arguments in [arguments_per_subparser[subparser_name]]\n\t\tif arguments\n\t}\n\tunclaimed_arguments = [\n\t\targument\n\t\tfor argument in unparsed_arguments\n\t\tif not any(\n\t\t\targuments_per_subparser[subparser_name]\n\t\t\tfor subparser_name in parsed_arguments_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Assemble a list of all subparsers' parse_known_args() functions, so we can call them in\n\t# order.\n\tparse_functions = []\n\tfor name, subparser in subparsers.items():\n\t\tparse_functions.append(lambda name, subparser: (name, subparser.parse_known_args(unparsed_arguments)))\n\t# Call each parse function, keeping track of which arguments were consumed by each.\n\tparsed_arguments = {}\n\tconsumed_arguments = set()\n\tfor name, subparser in subparsers.items():\n\t\tname, (namespace, remaining_arguments) = subparser.parse_known_args(unparsed_arguments)\n\t\tparsed_arguments[name] = namespace\n\t\tconsumed_arguments.update(set(unparsed_arguments) - set(remaining_arguments))\n\t# Return the parsed arguments, along with the arguments that were not claimed by any subparser.\n\treturn (parsed_arguments, [arg for arg in unparsed\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_arguments = {}\n\tunparsed_arguments = list(unparsed_arguments)\n\twhile unparsed_arguments:\n\t\tfound_action = False\n\t\tfor action, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tns = subparser.parse_known_args(unparsed_arguments)\n\t\t\texcept SystemExit:\n\t\t\t\tcontinue\n\t\t\tif ns[0].action == action:\n\t\t\t\tfound_action = True\n\t\t\t\tsubparser_arguments[action] = ns[0]\n\t\t\t\tunparsed_arguments = ns[1]\n\t\t\t\tbreak\n\t\tif not found_action:\n\t\t\tbreak\n\treturn subparser_arguments, unparsed_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Start with a dict mapping from subparser name to a list of arguments to be parsed by\n\t# that subparser.\n\tsubparser_arguments = {name: [] for name in subparsers}\n\t# Keep track of which subparsers have been seen.\n\tseen_subparsers = set()\n\t# Keep track of the remaining arguments.\n\tremaining_arguments = []\n\t# Iterate over the arguments, stopping when the argument is an action.\n\tfor i, argument in enumerate(unparsed_arguments):\n\t\tif argument in subparsers:\n\t\t\t# This is an action, so stop here.\n\t\t\tremaining_arguments = unparsed_arguments[i:]\n\t\t\tbreak\n\t\telse:\n\t\t\t# This is not an action, so add it to all subparsers' argument lists.\n\t\t\tfor name in subparsers:\n\t\t\t\tsubparser_arguments[name].append(argument)\n\t# Iterate over the remaining arguments, which are actions.\n\tfor argument in remaining_arguments:\n\t\t#\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# If any subparser was explicitly requested, assume that all arguments belong to it.\n\t# Otherwise, examine each subparser in turn to see if any of them can parse the\n\t# arguments.\n\tsubparser_name_to_arguments = {}\n\tsubparser_names = subparsers.keys()\n\tfor subparser_name in subparser_names:\n\t\tsubparser = subparsers[subparser_name]\n\t\tsubparser_arguments, remaining_arguments = subparser.parse_known_args(unparsed_arguments)\n\t\tif subparser_arguments.subparser_name == subparser_name:\n\t\t\tsubparser_name_to_arguments[subparser_name] = subparser_arguments\n\t\t\tunparsed_arguments = remaining_arguments\n\t\t\tsubparser_names.remove(subparser_name)\n\t\t\tbreak\n\telse:\n\t\tsubparser_name_to_arguments = {}\n\t\tremaining_arguments = unparsed_arguments\n\t\tfor subparser_name in subparser_names:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# Parse all command-line arguments.\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\tif subparser.prog in argument:\n\t\t\t\tif subparser_name in parsed_arguments:\n\t\t\t\t\traise Exception(\n\t\t\t\t\t\t\"Argument %s matches more than one subparser!\" % argument)\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_args(\n\t\t\t\t\tunparsed_arguments)\n\t\t\t\tbreak\n\t\telse:\n\t\t\tremaining_arguments.append(argument)\n\treturn parsed_arguments, remaining_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed = {}\n\tremaining = []\n\tfor subparser_name in subparsers:\n\t\tparsed[subparser_name] = subparsers[subparser_name].parse_args(unparsed_arguments)\n\t\tif parsed[subparser_name].action == subparser_name:\n\t\t\tremaining = parsed[subparser_name].remaining\n\t\t\tbreak\n\t\n\treturn (parsed, remaining)\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tassert len(unparsed_arguments) > 0\n\tsubparser_names = subparsers.keys()\n\tfor i in range(len(unparsed_arguments)):\n\t\ttry:\n\t\t\treturn subparsers[unparsed_arguments[i]].parse_known_args(unparsed_arguments[i:])\n\t\texcept KeyError:\n\t\t\tpass\n\traise ArgumentParserError(\"Subparser not specified\", subparser_names)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_args = {}\n\tunparsed_args = []\n\n\twhile unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_args[subparser_name] = subparser.parse_known_args(unparsed_arguments)[0]\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tunparsed_arguments = unparsed_args\n\t\t\t\tbreak\n\t\telse:\n\t\t\tunparsed_args.append(unparsed_arguments.pop(0))\n\n\treturn parsed_args, unparsed_args\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/atticmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b45665d7d32e5b55cc8363", "code": ["def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"Dummy\",\n\t\tdescription=\"A dummy program that does nothing\",\n\t\tepilog=\"Have fun!\"\n\t)\n\tparser.add_argument(\n\t\t\"--version\", action=\"version\",\n\t\tversion=\"%(prog)s {version}\".format(version=__version__)\n\t)\n\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"Subcommands\",\n\t\tmetavar=\"<subcommand>\",\n\t\thelp=\"Possible subcommands\"\n\t)\n\n\t# 'config' subcommand\n\tparser_config = subparsers.add_parser(\n\t\t\"config\",\n\t\thelp=\"Show configuration\"\n\t)\n\tparser_config.set_defaults(func=show_config)\n\n\t# 'dummy' subcommand\n\tparser_dummy = subparsers.add_parser(\n\t\t\"dummy\",\n\t\thelp=\"Dummy command\"\n\t)\n\tparser_dummy.add_argument(\n\t\t\"--foo\",\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\t\tprog='gol',\n\t\t\tdescription='''\n\t\t\t\tA Game of Life simulator.\n\t\t\t''',\n\t\t\tepilog='''\n\t\t\t\tThe Game of Life is a cellular automaton.  It is a zero-player\n\t\t\t\tgame, meaning that its evolution is determined by its initial\n\t\t\t\tstate, requiring no further input.  One interacts with the Game\n\t\t\t\tof Life by creating an initial configuration and observing how\n\t\t\t\tit evolves.\n\t\t\t'''\n\t\t)\n\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tparser_init = subparsers.add_parser('init', help='initialize a new game')\n\tparser_init.add_argument(\n\t\t\t'rows',\n\t\t\ttype=int,\n\t\t\thelp='number of rows in the game'\n\t\t)\n\tparser_init.add_argument(\n\t\t\t'cols',\n\t\t\ttype=int,\n\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t# parser.add_argument('-v', '--verbose', action=\"store_true\", default=False, help=\"Enable verbose output.\")\n\tparser.add_argument('-l', '--logfile', action=\"store\", default=None, help=\"Log file\")\n\tparser.add_argument('-p', '--port', action=\"store\", default=None, help=\"Port to listen on\")\n\tparser.add_argument('-b', '--bindhost', action=\"store\", default=None, help=\"Host to bind to\")\n\tparser.add_argument('-k', '--keyfile', action=\"store\", default=None, help=\"SSL key file\")\n\tparser.add_argument('-c', '--certfile', action=\"store\", default=None, help=\"SSL certificate file\")\n\tparser.add_argument('-i', '--pidfile', action=\"store\", default=None, help=\"PID file\")\n\tparser.add_argument('-a',\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(description=\"Run a script on multiple files\")\n\n\tsubparsers = parser.add_subparsers(dest='command', help=\"sub-command help\")\n\n\tdef add_subparser(name, help, **kwargs):\n\t\t\"\"\"\n\t\tAdd a subparser to the subparsers.\n\n\t\tOptions:\n\t\t\tname: The name of the subparser.\n\t\t\thelp: Help text for the subparser.\n\t\t\tkwargs: Any other options to pass to the parser.\n\t\t\"\"\"\n\t\tsubparsers.add_parser(name, help=help, **kwargs)\n\n\tadd_subparser('run', \"\"\"Run a script on multiple files.\"\"\")\n\n\treturn parser\n\n\n", "def make_parsers():\n\t\n\t# Build the main parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription=__doc__,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tadd_help=False\n\t)\n\t\n\t# Add the arguments to the main parser\n\tparser.add_argument('-h', '--help',\n\t\taction='help',\n\t\thelp='show this help message and exit'\n\t)\n\tparser.add_argument('-v', '--version',\n\t\taction='version',\n\t\tversion='%(prog)s ' + __version__,\n\t\thelp='show program\\'s version number and exit'\n\t)\n\t\n\t# Build the subparsers\n\tsubparsers = parser.add_subparsers(\n\t\ttitle='sub-commands',\n\t\tdest='subcommand'\n\t)\n\tsubparsers.required = True\n\t\n\t# Add the arguments to the subparsers\n\tsubparsers.add_parser('install',\n\t\tparents=[install_parser()],\n\t\t\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Manage your dotfiles with git.')\n\t\n\tsubparsers = parser.add_subparsers(dest='command')\n\t\n\t## init\n\tparser_init = subparsers.add_parser('init', help='initialize a new repository')\n\t\n\t## add\n\tparser_add = subparsers.add_parser('add', help='add files to the repository')\n\tparser_add.add_argument('files', metavar='FILE', nargs='+', help='files to add')\n\t\n\t## rm\n\tparser_rm = subparsers.add_parser('rm', help='remove files from the repository')\n\tparser_rm.add_argument('files', metavar='FILE', nargs='+', help='files to remove')\n\t\n\t## commit\n\tparser_commit = subparsers.add_parser('commit', help='commit changes')\n\tparser_commit.add_argument('-m', '--message', help='use the given message as the commit message')\n\t\n\t## push\n\t\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog='po2json',\n\t\tdescription='Convert a PO file to JSON.',\n\t\tepilog='See the README for more information.',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\n\tparser.add_argument('-i', '--input', type=argparse.FileType('r'), default=sys.stdin, help='The PO file to be converted.')\n\tparser.add_argument('-o', '--output', type=argparse.FileType('w'), default=sys.stdout, help='The output file.')\n\tparser.add_argument('-f', '--format', choices=['js', 'json', 'jsonp'], default='json', help='The output format.')\n\tparser.add_argument('-n', '--namespace', default='locale', help='The namespace to be used in the JSON.')\n\t\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(description='Lint and build a library')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_build = subparsers.add_parser('build', help='build a library')\n\tparser_build.set_defaults(function=build)\n\tparser_build.add_argument('-r', '--release', action='store_true',\n\t\thelp='build a release version of the library')\n\n\tparser_lint = subparsers.add_parser('lint', help='lint a library')\n\tparser_lint.set_defaults(function=lint)\n\n\treturn parser, subparsers\n\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog='{} {}'.format(PROG_NAME, PROG_VERSION),\n\t\tdescription='Automatically download, unzip and import data from the '\n\t\t\t'US Department of Labor Employment and Training Administration\\'s '\n\t\t\t'Occupational Employment Statistics (OES) program.',\n\t\tepilog='For more information, visit https://github.com/jeffknupp/oes_data_loader')\n\tparser.add_argument('-v', '--version', action='version',\n\t\tversion='%(prog)s {}'.format(PROG_VERSION))\n\tparser.add_argument('-d', '--data-dir', default='data/',\n\t\thelp='The directory in which to store downloaded datasets and data files. '\n\t\t\t'Defaults to \"data/\"')\n\tparser.add_argument('-i', '--import-dir', default='import/',\n\t\thelp='The directory in which to store data files ready to be imported. '\n\t\t\t'Defaults to \"import/\"')\n\tparser.\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\t# Make the parser for the \"init\" subcommand.\n\tparser_init = subparsers.add_parser('init', help='initialize the database')\n\tparser_init.add_argument('-q', '--quiet', action='store_true',\n\t\t\t\t\t\t\thelp='suppress non-error output')\n\tparser_init.set_defaults(func=init)\n\t\n\t# Make the parser for the \"add\" subcommand.\n\tparser_add = subparsers.add_parser('add', help='add a new package')\n\tparser_add.add_argument('name', help='name of the package')\n\tparser_add.add_argument('version', help='version of the package')\n\tparser_add.add_argument('--source', help='source package',\n\t\t\t\t\t\t\tdefault='source')\n\tparser_add.add_argument('--arch', help='arch\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog=\"Dummy\",\n\t\tdescription=\"A dummy program that does nothing\",\n\t\tepilog=\"Have fun!\"\n\t)\n\tparser.add_argument(\n\t\t\"--version\", action=\"version\",\n\t\tversion=\"%(prog)s {version}\".format(version=__version__)\n\t)\n\n\tsubparsers = parser.add_subparsers(\n\t\ttitle=\"Subcommands\",\n\t\tmetavar=\"<subcommand>\",\n\t\thelp=\"Possible subcommands\"\n\t)\n\n\t# 'config' subcommand\n\tparser_config = subparsers.add_parser(\n\t\t\"config\",\n\t\thelp=\"Show configuration\"\n\t)\n\tparser_config.set_defaults(func=show_config)\n\n\t# 'dummy' subcommand\n\tparser_dummy = subparsers.add_parser(\n\t\t\"dummy\",\n\t\thelp=\"Dummy command\"\n\t)\n\tparser_dummy.add_argument(\n\t\t\"--foo\",\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\t\tprog='gol',\n\t\t\tdescription='''\n\t\t\t\tA Game of Life simulator.\n\t\t\t''',\n\t\t\tepilog='''\n\t\t\t\tThe Game of Life is a cellular automaton.  It is a zero-player\n\t\t\t\tgame, meaning that its evolution is determined by its initial\n\t\t\t\tstate, requiring no further input.  One interacts with the Game\n\t\t\t\tof Life by creating an initial configuration and observing how\n\t\t\t\tit evolves.\n\t\t\t'''\n\t\t)\n\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tparser_init = subparsers.add_parser('init', help='initialize a new game')\n\tparser_init.add_argument(\n\t\t\t'rows',\n\t\t\ttype=int,\n\t\t\thelp='number of rows in the game'\n\t\t)\n\tparser_init.add_argument(\n\t\t\t'cols',\n\t\t\ttype=int,\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t# parser.add_argument('-v', '--verbose', action=\"store_true\", default=False, help=\"Enable verbose output.\")\n\tparser.add_argument('-l', '--logfile', action=\"store\", default=None, help=\"Log file\")\n\tparser.add_argument('-p', '--port', action=\"store\", default=None, help=\"Port to listen on\")\n\tparser.add_argument('-b', '--bindhost', action=\"store\", default=None, help=\"Host to bind to\")\n\tparser.add_argument('-k', '--keyfile', action=\"store\", default=None, help=\"SSL key file\")\n\tparser.add_argument('-c', '--certfile', action=\"store\", default=None, help=\"SSL certificate file\")\n\tparser.add_argument('-i', '--pidfile', action=\"store\", default=None, help=\"PID file\")\n\tparser.add_argument('-a',\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(description=\"Run a script on multiple files\")\n\n\tsubparsers = parser.add_subparsers(dest='command', help=\"sub-command help\")\n\n\tdef add_subparser(name, help, **kwargs):\n\t\t\"\"\"\n\t\tAdd a subparser to the subparsers.\n\n\t\tOptions:\n\t\t\tname: The name of the subparser.\n\t\t\thelp: Help text for the subparser.\n\t\t\tkwargs: Any other options to pass to the parser.\n\t\t\"\"\"\n\t\tsubparsers.add_parser(name, help=help, **kwargs)\n\n\tadd_subparser('run', \"\"\"Run a script on multiple files.\"\"\")\n\n\treturn parser\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t\n\t# Build the main parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription=__doc__,\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t\tadd_help=False\n\t)\n\t\n\t# Add the arguments to the main parser\n\tparser.add_argument('-h', '--help',\n\t\taction='help',\n\t\thelp='show this help message and exit'\n\t)\n\tparser.add_argument('-v', '--version',\n\t\taction='version',\n\t\tversion='%(prog)s ' + __version__,\n\t\thelp='show program\\'s version number and exit'\n\t)\n\t\n\t# Build the subparsers\n\tsubparsers = parser.add_subparsers(\n\t\ttitle='sub-commands',\n\t\tdest='subcommand'\n\t)\n\tsubparsers.required = True\n\t\n\t# Add the arguments to the subparsers\n\tsubparsers.add_parser('install',\n\t\tparents=[install_parser()],\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription='Manage your dotfiles with git.')\n\t\n\tsubparsers = parser.add_subparsers(dest='command')\n\t\n\t## init\n\tparser_init = subparsers.add_parser('init', help='initialize a new repository')\n\t\n\t## add\n\tparser_add = subparsers.add_parser('add', help='add files to the repository')\n\tparser_add.add_argument('files', metavar='FILE', nargs='+', help='files to add')\n\t\n\t## rm\n\tparser_rm = subparsers.add_parser('rm', help='remove files from the repository')\n\tparser_rm.add_argument('files', metavar='FILE', nargs='+', help='files to remove')\n\t\n\t## commit\n\tparser_commit = subparsers.add_parser('commit', help='commit changes')\n\tparser_commit.add_argument('-m', '--message', help='use the given message as the commit message')\n\t\n\t## push\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog='po2json',\n\t\tdescription='Convert a PO file to JSON.',\n\t\tepilog='See the README for more information.',\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument('-v', '--version', action='version', version='%(prog)s ' + __version__)\n\tparser.add_argument('-i', '--input', type=argparse.FileType('r'), default=sys.stdin, help='The PO file to be converted.')\n\tparser.add_argument('-o', '--output', type=argparse.FileType('w'), default=sys.stdout, help='The output file.')\n\tparser.add_argument('-f', '--format', choices=['js', 'json', 'jsonp'], default='json', help='The output format.')\n\tparser.add_argument('-n', '--namespace', default='locale', help='The namespace to be used in the JSON.')\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(description='Lint and build a library')\n\tsubparsers = parser.add_subparsers()\n\n\tparser_build = subparsers.add_parser('build', help='build a library')\n\tparser_build.set_defaults(function=build)\n\tparser_build.add_argument('-r', '--release', action='store_true',\n\t\thelp='build a release version of the library')\n\n\tparser_lint = subparsers.add_parser('lint', help='lint a library')\n\tparser_lint.set_defaults(function=lint)\n\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tprog='{} {}'.format(PROG_NAME, PROG_VERSION),\n\t\tdescription='Automatically download, unzip and import data from the '\n\t\t\t'US Department of Labor Employment and Training Administration\\'s '\n\t\t\t'Occupational Employment Statistics (OES) program.',\n\t\tepilog='For more information, visit https://github.com/jeffknupp/oes_data_loader')\n\tparser.add_argument('-v', '--version', action='version',\n\t\tversion='%(prog)s {}'.format(PROG_VERSION))\n\tparser.add_argument('-d', '--data-dir', default='data/',\n\t\thelp='The directory in which to store downloaded datasets and data files. '\n\t\t\t'Defaults to \"data/\"')\n\tparser.add_argument('-i', '--import-dir', default='import/',\n\t\thelp='The directory in which to store data files ready to be imported. '\n\t\t\t'Defaults to \"import/\"')\n\tparser.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(description='Process some integers.')\n\tsubparsers = parser.add_subparsers(help='sub-command help')\n\t\n\t# Make the parser for the \"init\" subcommand.\n\tparser_init = subparsers.add_parser('init', help='initialize the database')\n\tparser_init.add_argument('-q', '--quiet', action='store_true',\n\t\t\t\t\t\t\thelp='suppress non-error output')\n\tparser_init.set_defaults(func=init)\n\t\n\t# Make the parser for the \"add\" subcommand.\n\tparser_add = subparsers.add_parser('add', help='add a new package')\n\tparser_add.add_argument('name', help='name of the package')\n\tparser_add.add_argument('version', help='version of the package')\n\tparser_add.add_argument('--source', help='source package',\n\t\t\t\t\t\t\tdefault='source')\n\tparser_add.add_argument('--arch', help='arch\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/config/load.py", "project": "witten/borgmatic", "name": "deep_merge_nodes", "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.", "ques_id": "62b438ba66fea644fe22cca2", "code": ["def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n\n", "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\tcontinue\n\t\tfor key, value in node.value.items():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tif isinstance(value, MappingNode) and isinstance(result[key], MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\treturn MappingNode(result)\n\n\n", "def deep_merge_nodes(nodes):\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn merged\n\n\n", "def deep_merge_nodes(nodes):\n\tresult = MappingNode()\n\tfor node in nodes:\n\t\tif node == None:\n\t\t\tcontinue\n\t\tassert isinstance(node, MappingNode)\n\t\tfor key, value in node.iteritems():\n\t\t\tif isinstance(value, MappingNode):\n\t\t\t\tresult[key] = deep_merge_nodes([result.get(key), value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result and isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\traise TypeError('all nodes must be MappingNodes')\n\t\tresult.update(node.value)\n\treturn MappingNode(result)\n\n\n", "def deep_merge_nodes(nodes):\n\tif isinstance(nodes[0], MappingNode):\n\t\t# merge mapping nodes\n\t\tres = nodes[0]\n\t\tfor node in nodes[1:]:\n\t\t\tfor k, v in node.iter_items():\n\t\t\t\t# add if it doesn't exist, merge if it does\n\t\t\t\tif k not in res:\n\t\t\t\t\tres[k] = v\n\t\t\t\telse:\n\t\t\t\t\tres[k] = deep_merge_nodes([res[k], v])\n\t\treturn res\n\telif isinstance(nodes[0], SequenceNode):\n\t\t# merge sequence nodes\n\t\tres = nodes[0]\n\t\tfor node in nodes[1:]:\n\t\t\tfor i, v in enumerate(node):\n\t\t\t\t# if we're at the end of the current result, just append\n\t\t\t\tif i >= len(res):\n\t\t\t\t\tres.append(v)\n\t\t\t\telse:\n\t\t\t\t\tres[i] = deep_merge_nodes\n", "def deep_merge_nodes(nodes):\n\tresult_dict = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\traise TypeError(\"Can't deep merge nodes of type %s\" % node.__class__.__name__)\n\t\tfor k, v in node.value.items():\n\t\t\tif isinstance(v, MappingNode):\n\t\t\t\tresult_dict.setdefault(k, []).append(v)\n\t\t\telse:\n\t\t\t\tresult_dict[k] = [v]\n\treturn MappingNode(result_dict, nodes[0].start_mark, nodes[-1].end_mark)\n\n\n", "def deep_merge_nodes(nodes):\n\tif not nodes:\n\t\treturn DictNode()\n\tif not all(type(n) is DictNode for n in nodes):\n\t\traise ValueError(\"Can only merge dict-type nodes\")\n\treturn DictNode(reduce(lambda a, b: dict(a, **b), (n.value for n in nodes)))\n\n\n", "def deep_merge_nodes(nodes):\n\treturn MappingNode(deep_merge_dicts(nodes))\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\tcontinue\n\t\tfor key, value in node.value.items():\n\t\t\tif key not in result:\n\t\t\t\tresult[key] = value\n\t\t\telse:\n\t\t\t\tif isinstance(value, MappingNode) and isinstance(result[key], MappingNode):\n\t\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\t\telse:\n\t\t\t\t\tresult[key] = value\n\treturn MappingNode(result)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tmerged = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in merged:\n\t\t\t\tif isinstance(value, MappingNode):\n\t\t\t\t\tmerged[key] = deep_merge_nodes([merged[key], value])\n\t\t\telse:\n\t\t\t\tmerged[key] = value\n\treturn merged\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = MappingNode()\n\tfor node in nodes:\n\t\tif node == None:\n\t\t\tcontinue\n\t\tassert isinstance(node, MappingNode)\n\t\tfor key, value in node.iteritems():\n\t\t\tif isinstance(value, MappingNode):\n\t\t\t\tresult[key] = deep_merge_nodes([result.get(key), value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tfor key, value in node.items():\n\t\t\tif key in result and isinstance(result[key], MappingNode) and isinstance(value, MappingNode):\n\t\t\t\tresult[key] = deep_merge_nodes([result[key], value])\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\traise TypeError('all nodes must be MappingNodes')\n\t\tresult.update(node.value)\n\treturn MappingNode(result)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tif isinstance(nodes[0], MappingNode):\n\t\t# merge mapping nodes\n\t\tres = nodes[0]\n\t\tfor node in nodes[1:]:\n\t\t\tfor k, v in node.iter_items():\n\t\t\t\t# add if it doesn't exist, merge if it does\n\t\t\t\tif k not in res:\n\t\t\t\t\tres[k] = v\n\t\t\t\telse:\n\t\t\t\t\tres[k] = deep_merge_nodes([res[k], v])\n\t\treturn res\n\telif isinstance(nodes[0], SequenceNode):\n\t\t# merge sequence nodes\n\t\tres = nodes[0]\n\t\tfor node in nodes[1:]:\n\t\t\tfor i, v in enumerate(node):\n\t\t\t\t# if we're at the end of the current result, just append\n\t\t\t\tif i >= len(res):\n\t\t\t\t\tres.append(v)\n\t\t\t\telse:\n\t\t\t\t\tres[i] = deep_merge_nodes\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tresult_dict = {}\n\tfor node in nodes:\n\t\tif not isinstance(node, MappingNode):\n\t\t\traise TypeError(\"Can't deep merge nodes of type %s\" % node.__class__.__name__)\n\t\tfor k, v in node.value.items():\n\t\t\tif isinstance(v, MappingNode):\n\t\t\t\tresult_dict.setdefault(k, []).append(v)\n\t\t\telse:\n\t\t\t\tresult_dict[k] = [v]\n\treturn MappingNode(result_dict, nodes[0].start_mark, nodes[-1].end_mark)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\tif not nodes:\n\t\treturn DictNode()\n\tif not all(type(n) is DictNode for n in nodes):\n\t\traise ValueError(\"Can only merge dict-type nodes\")\n\treturn DictNode(reduce(lambda a, b: dict(a, **b), (n.value for n in nodes)))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def deep_merge_nodes(nodes):\n\treturn MappingNode(deep_merge_dicts(nodes))\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/generate_config.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as an ArgumentParser instance.", "ques_id": "62b438b666fea644fe22cc78", "code": ["def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Batch-renames files inside a folder.\")\n\tparser.add_argument('folder', help=\"Folder containing the file(s) to rename.\")\n\tparser.add_argument('find_pattern', help=\"Pattern to look for in file names.\")\n\tparser.add_argument('replace_pattern', help=\"Pattern to replace the found pattern with.\")\n\tparser.add_argument('--extension', help=\"File extension to search for. If not specified, all files will be searched.\")\n\tparser.add_argument('--recursive', help=\"Search for files recursively.\", action=\"store_true\")\n\tparser.add_argument('--dry-run', help=\"Do not rename the files.\", action=\"store_true\")\n\tparser.add_argument('--verbose', help=\"Print more information.\", action=\"store_true\")\n\t\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Take a list of files and a list of output paths and copy each file to its corresponding output path\")\n\tparser.add_argument('--files', '-f', type=str, required=True, nargs='+', help='The input files')\n\tparser.add_argument('--output-paths', '-o', type=str, required=True, nargs='+', help='The output paths')\n\tparser.add_argument('--overwrite', '-w', action='store_true', help='Overwrite existing files')\n\tparser.add_argument('--verbose', '-v', action='store_true', help='Print information about each file as it is processed')\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis script simulates a set of models using the specified simulator program, and saves the results.\n\t\"\"\")\n\n\tparser.add_argument(\"--simulator\", \"-s\", metavar=\"simulator\", required=True, help=\"The simulator program\")\n\tparser.add_argument(\"--input-models\", \"-i\", metavar=\"input_models\", required=True, help=\"The input models\")\n\tparser.add_argument(\"--output-results\", \"-o\", metavar=\"output_results\", required=True, help=\"The output results\")\n\tparser.add_argument(\"--print-progress\", \"-p\", action=\"store_true\", default=False, help=\"Print simulation progress\")\n\tparser.add_argument(\"--max-threads\", \"-m\", type=int, metavar=\"max_threads\", default=1, help=\"The maximum number of threads to use (default: 1)\")\n\tparser.add_argument(\"--max-trials\", \"-t\", type=int, metavar=\"max_trials\", default=1, help=\"The maximum\n", "def parse_arguments(*arguments):\n\t# Create an ArgumentParser instance \n\tparser = ArgumentParser(description=\"Determine the number of times a particular word occurs in a file\")\n\t# Add the arguments to the parser\n\tparser.add_argument(\"--file\", required=True, help=\"File to be analyzed\", type=str)\n\tparser.add_argument(\"--word\", required=True, help=\"Word to be analyzed\", type=str)\n\tparser.add_argument(\"--ignore_case\", required=False, help=\"Ignore case in the file\", action=\"store_true\")\n\tparser.add_argument(\"--ignore_punctuation\", required=False, help=\"Ignore punctuation in the file\", action=\"store_true\")\n\tparser.add_argument(\"--ignore_numbers\", required=False, help=\"Ignore numbers in the file\", action=\"store_true\")\n\t# Parse the arguments\n\targs = parser.parse_args()\n\t# Return the arguments\n\treturn args\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"A program for creating a distributed control plane for a set of mininet hosts\")\n\n\tparser.add_argument(\"--hosts\", \"-n\", action=\"store\", type=int, dest=\"hosts\",\n\t\thelp=\"The number of hosts to create in the topology\", default=3)\n\n\tparser.add_argument(\"--controller\", \"-c\", action=\"store\", type=str, dest=\"controller\",\n\t\thelp=\"The type of controller to use.  May be 'remote' or 'local'\", default=\"local\")\n\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", dest=\"debug\",\n\t\thelp=\"Turn on debugging messages\", default=False)\n\n\tparser.add_argument(\"--switch\", \"-s\", action=\"store\", type=str, dest=\"switch\",\n\t\thelp=\"The type of software switch to use.  May be 'user' or 'kernel'\", default=\"user\")\n\n\tparser.add_argument(\"--topo\", \"-t\", action=\"store\", type=str, dest=\"topo\",\n\t\thelp=\"The type of\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tGiven a list of samples and a list of genomes, check if the genomes are in the samples.\n\t\"\"\")\n\n\tparser.add_argument(\"-s\", \"--samples\", dest=\"samples\",\n\t\t\t\t\t\thelp=\"A list of sample names\",\n\t\t\t\t\t\trequired=True)\n\tparser.add_argument(\"-g\", \"--genomes\", dest=\"genomes\",\n\t\t\t\t\t\thelp=\"A list of genome names\",\n\t\t\t\t\t\trequired=True)\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\",\n\t\t\t\t\t\thelp=\"The output file to write the result to\",\n\t\t\t\t\t\trequired=True)\n\treturn parser.parse_args()\n\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Convert from the output of the TIGER/LDC parser to the CoNLL-U format\",\n\t\tepilog=\"Example usage: python3 tiger_to_conllu.py --input-file=../trees/sample.tiger.xml --output-file=../trees/sample.conllu\")\n\tparser.add_argument(\"--input-file\", type=str, required=True, help=\"The TIGER/LDC XML file to convert.\")\n\tparser.add_argument(\"--output-file\", type=str, required=True, help=\"The path to the file to which the CoNLL-U format representation of the TIGER/LDC XML file should be written.\")\n\tparser.add_argument(\"--verbose\", action=\"store_true\", help=\"If specified, print verbose output.\")\n\treturn parser.parse_args(arguments)\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"A script to plot the Ntuple data in a ROOT file. Currently only works with the data produced by the 'run_printer' program in the ntuple_tools directory.\")\n\tparser.add_argument(\"-i\", \"--infile\", \n\t\t\t\t\t\thelp=\"The name of the ROOT file to be parsed. Default: 'test.root'\", \n\t\t\t\t\t\tdefault=\"test.root\")\n\tparser.add_argument(\"-o\", \"--outfile\", \n\t\t\t\t\t\thelp=\"The name of the output file. Default: 'test.png'\", \n\t\t\t\t\t\tdefault=\"test.png\")\n\tparser.add_argument(\"-n\", \"--nentries\", \n\t\t\t\t\t\thelp=\"The number of entries to plot. Default: 100\", \n\t\t\t\t\t\tdefault=100, \n\t\t\t\t\t\ttype=int)\n\tparser.add_argument(\"-b\", \"--binning\", \n\t\t\t\t\t\thelp=\"The number of\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='This script is used to generate a report for the number of times each unique word occurs in the text')\n\tparser.add_argument(\"-i\", \"--input\", dest=\"input_file\", help=\"The file to process\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output_file\", help=\"The file to write the output to\", required=True)\n\tparser.add_argument(\"-n\", \"--top_n\", dest=\"top_n\", help=\"The number of unique words to include in the report\", required=True)\n\tparser.add_argument(\"-l\", \"--log\", dest=\"log_file\", help=\"The file to write the log to\", required=True)\n\targs = parser.parse_args()\n\treturn args\n\n", "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Downloads a file and splits it into multiple files\")\n\tparser.add_argument(\"-u\", \"--url\", dest=\"url\", help=\"the URL of the file to be downloaded and split\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", help=\"the base name of the output files\", required=True)\n\tparser.add_argument(\"-s\", \"--size\", dest=\"size\", default=DEFAULT_SPLIT_SIZE, type=int, help=\"the size of the split files (in bytes)\")\n\tparser.add_argument(\"-c\", \"--chunk-size\", dest=\"chunk_size\", default=DEFAULT_CHUNK_SIZE, type=int, help=\"the size of the chunk that is read from the file at a time\")\n\tparser.add_argument(\"-d\", \"--decompress\", dest=\"decompress\", action=\"store_true\", default=False, help=\"whether the download file should be decompressed\")\n\tparser.add_argument(\"-n\", \"--num-processes\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Batch-renames files inside a folder.\")\n\tparser.add_argument('folder', help=\"Folder containing the file(s) to rename.\")\n\tparser.add_argument('find_pattern', help=\"Pattern to look for in file names.\")\n\tparser.add_argument('replace_pattern', help=\"Pattern to replace the found pattern with.\")\n\tparser.add_argument('--extension', help=\"File extension to search for. If not specified, all files will be searched.\")\n\tparser.add_argument('--recursive', help=\"Search for files recursively.\", action=\"store_true\")\n\tparser.add_argument('--dry-run', help=\"Do not rename the files.\", action=\"store_true\")\n\tparser.add_argument('--verbose', help=\"Print more information.\", action=\"store_true\")\n\t\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Take a list of files and a list of output paths and copy each file to its corresponding output path\")\n\tparser.add_argument('--files', '-f', type=str, required=True, nargs='+', help='The input files')\n\tparser.add_argument('--output-paths', '-o', type=str, required=True, nargs='+', help='The output paths')\n\tparser.add_argument('--overwrite', '-w', action='store_true', help='Overwrite existing files')\n\tparser.add_argument('--verbose', '-v', action='store_true', help='Print information about each file as it is processed')\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tThis script simulates a set of models using the specified simulator program, and saves the results.\n\t\"\"\")\n\n\tparser.add_argument(\"--simulator\", \"-s\", metavar=\"simulator\", required=True, help=\"The simulator program\")\n\tparser.add_argument(\"--input-models\", \"-i\", metavar=\"input_models\", required=True, help=\"The input models\")\n\tparser.add_argument(\"--output-results\", \"-o\", metavar=\"output_results\", required=True, help=\"The output results\")\n\tparser.add_argument(\"--print-progress\", \"-p\", action=\"store_true\", default=False, help=\"Print simulation progress\")\n\tparser.add_argument(\"--max-threads\", \"-m\", type=int, metavar=\"max_threads\", default=1, help=\"The maximum number of threads to use (default: 1)\")\n\tparser.add_argument(\"--max-trials\", \"-t\", type=int, metavar=\"max_trials\", default=1, help=\"The maximum\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\t# Create an ArgumentParser instance \n\tparser = ArgumentParser(description=\"Determine the number of times a particular word occurs in a file\")\n\t# Add the arguments to the parser\n\tparser.add_argument(\"--file\", required=True, help=\"File to be analyzed\", type=str)\n\tparser.add_argument(\"--word\", required=True, help=\"Word to be analyzed\", type=str)\n\tparser.add_argument(\"--ignore_case\", required=False, help=\"Ignore case in the file\", action=\"store_true\")\n\tparser.add_argument(\"--ignore_punctuation\", required=False, help=\"Ignore punctuation in the file\", action=\"store_true\")\n\tparser.add_argument(\"--ignore_numbers\", required=False, help=\"Ignore numbers in the file\", action=\"store_true\")\n\t# Parse the arguments\n\targs = parser.parse_args()\n\t# Return the arguments\n\treturn args\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"A program for creating a distributed control plane for a set of mininet hosts\")\n\n\tparser.add_argument(\"--hosts\", \"-n\", action=\"store\", type=int, dest=\"hosts\",\n\t\thelp=\"The number of hosts to create in the topology\", default=3)\n\n\tparser.add_argument(\"--controller\", \"-c\", action=\"store\", type=str, dest=\"controller\",\n\t\thelp=\"The type of controller to use.  May be 'remote' or 'local'\", default=\"local\")\n\n\tparser.add_argument(\"--debug\", \"-d\", action=\"store_true\", dest=\"debug\",\n\t\thelp=\"Turn on debugging messages\", default=False)\n\n\tparser.add_argument(\"--switch\", \"-s\", action=\"store\", type=str, dest=\"switch\",\n\t\thelp=\"The type of software switch to use.  May be 'user' or 'kernel'\", default=\"user\")\n\n\tparser.add_argument(\"--topo\", \"-t\", action=\"store\", type=str, dest=\"topo\",\n\t\thelp=\"The type of\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"\"\"\n\tGiven a list of samples and a list of genomes, check if the genomes are in the samples.\n\t\"\"\")\n\n\tparser.add_argument(\"-s\", \"--samples\", dest=\"samples\",\n\t\t\t\t\t\thelp=\"A list of sample names\",\n\t\t\t\t\t\trequired=True)\n\tparser.add_argument(\"-g\", \"--genomes\", dest=\"genomes\",\n\t\t\t\t\t\thelp=\"A list of genome names\",\n\t\t\t\t\t\trequired=True)\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\",\n\t\t\t\t\t\thelp=\"The output file to write the result to\",\n\t\t\t\t\t\trequired=True)\n\treturn parser.parse_args()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Convert from the output of the TIGER/LDC parser to the CoNLL-U format\",\n\t\tepilog=\"Example usage: python3 tiger_to_conllu.py --input-file=../trees/sample.tiger.xml --output-file=../trees/sample.conllu\")\n\tparser.add_argument(\"--input-file\", type=str, required=True, help=\"The TIGER/LDC XML file to convert.\")\n\tparser.add_argument(\"--output-file\", type=str, required=True, help=\"The path to the file to which the CoNLL-U format representation of the TIGER/LDC XML file should be written.\")\n\tparser.add_argument(\"--verbose\", action=\"store_true\", help=\"If specified, print verbose output.\")\n\treturn parser.parse_args(arguments)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"A script to plot the Ntuple data in a ROOT file. Currently only works with the data produced by the 'run_printer' program in the ntuple_tools directory.\")\n\tparser.add_argument(\"-i\", \"--infile\", \n\t\t\t\t\t\thelp=\"The name of the ROOT file to be parsed. Default: 'test.root'\", \n\t\t\t\t\t\tdefault=\"test.root\")\n\tparser.add_argument(\"-o\", \"--outfile\", \n\t\t\t\t\t\thelp=\"The name of the output file. Default: 'test.png'\", \n\t\t\t\t\t\tdefault=\"test.png\")\n\tparser.add_argument(\"-n\", \"--nentries\", \n\t\t\t\t\t\thelp=\"The number of entries to plot. Default: 100\", \n\t\t\t\t\t\tdefault=100, \n\t\t\t\t\t\ttype=int)\n\tparser.add_argument(\"-b\", \"--binning\", \n\t\t\t\t\t\thelp=\"The number of\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description='This script is used to generate a report for the number of times each unique word occurs in the text')\n\tparser.add_argument(\"-i\", \"--input\", dest=\"input_file\", help=\"The file to process\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output_file\", help=\"The file to write the output to\", required=True)\n\tparser.add_argument(\"-n\", \"--top_n\", dest=\"top_n\", help=\"The number of unique words to include in the report\", required=True)\n\tparser.add_argument(\"-l\", \"--log\", dest=\"log_file\", help=\"The file to write the log to\", required=True)\n\targs = parser.parse_args()\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*arguments):\n\tparser = argparse.ArgumentParser(description=\"Downloads a file and splits it into multiple files\")\n\tparser.add_argument(\"-u\", \"--url\", dest=\"url\", help=\"the URL of the file to be downloaded and split\", required=True)\n\tparser.add_argument(\"-o\", \"--output\", dest=\"output\", help=\"the base name of the output files\", required=True)\n\tparser.add_argument(\"-s\", \"--size\", dest=\"size\", default=DEFAULT_SPLIT_SIZE, type=int, help=\"the size of the split files (in bytes)\")\n\tparser.add_argument(\"-c\", \"--chunk-size\", dest=\"chunk_size\", default=DEFAULT_CHUNK_SIZE, type=int, help=\"the size of the chunk that is read from the file at a time\")\n\tparser.add_argument(\"-d\", \"--decompress\", dest=\"decompress\", action=\"store_true\", default=False, help=\"whether the download file should be decompressed\")\n\tparser.add_argument(\"-n\", \"--num-processes\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62b438b666fea644fe22cc72", "code": ["def parser_flags(parser):\n\treturn \" \".join(f for f in parser._get_kwargs() if f[0] == \"flags\")\n\n\n", "def parser_flags(parser):\n\treturn \" \".join(list(set(list(map(lambda x: x.split(\" \")[1], parser.format_help().split(\"\\n\")))[2:-2])))\n\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(' '.join(action.option_strings))\n\treturn ' '.join(flags)\n\n", "def parser_flags(parser):\n\treturn \" \".join(sorted(parser._option_string_actions.keys()))\n\n\n", "def parser_flags(parser):\n\tflags = parser.format_help().split('\\n')[1].strip()\n\tflags = re.sub(r'\\[.*?\\]', '', flags)\n\tflags = re.sub(r'\\s+', ' ', flags)\n\treturn flags\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(\" \".join(action.option_strings))\n\treturn \" \".join(flags)\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(action.option_strings for action in parser._actions)\n\n", "def parser_flags(parser):\n\tflags = []\n\tparser_actions = parser._actions\n\tfor action in parser_actions:\n\t\tif isinstance(action, argparse._StoreAction):\n\t\t\tflags.append('--' + action.dest)\n\t\telif isinstance(action, argparse._HelpAction):\n\t\t\tpass\n\t\telif isinstance(action, argparse._AppendAction):\n\t\t\tflags.append('--' + action.dest)\n\t\telse:\n\t\t\traise ValueError('Unknown parser action')\n\treturn ' '.join(flags)\n\n", "def parser_flags(parser):\n\treturn \" \".join(flag for flag in parser._option_string_actions.keys() if flag.startswith(\"-\"))\n\n", "def parser_flags(parser):\n\treturn \" \".join(parser.format_usage().strip().split(\" \", 1)[1].split()[:-1])\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\treturn \" \".join(f for f in parser._get_kwargs() if f[0] == \"flags\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(list(set(list(map(lambda x: x.split(\" \")[1], parser.format_help().split(\"\\n\")))[2:-2])))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(' '.join(action.option_strings))\n\treturn ' '.join(flags)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(sorted(parser._option_string_actions.keys()))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = parser.format_help().split('\\n')[1].strip()\n\tflags = re.sub(r'\\[.*?\\]', '', flags)\n\tflags = re.sub(r'\\s+', ' ', flags)\n\treturn flags\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(\" \".join(action.option_strings))\n\treturn \" \".join(flags)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(action.option_strings for action in parser._actions)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tparser_actions = parser._actions\n\tfor action in parser_actions:\n\t\tif isinstance(action, argparse._StoreAction):\n\t\t\tflags.append('--' + action.dest)\n\t\telif isinstance(action, argparse._HelpAction):\n\t\t\tpass\n\t\telif isinstance(action, argparse._AppendAction):\n\t\t\tflags.append('--' + action.dest)\n\t\telse:\n\t\t\traise ValueError('Unknown parser action')\n\treturn ' '.join(flags)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(flag for flag in parser._option_string_actions.keys() if flag.startswith(\"-\"))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def parser_flags(parser):\n\treturn \" \".join(parser.format_usage().strip().split(\" \", 1)[1].split()[:-1])\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/completion.py", "project": "witten/borgmatic", "name": "bash_completion", "docstring": "Return a bash completion script for the borgmatic command. Produce this by introspecting\nborgmatic's command-line argument parsers.", "ques_id": "62b438b566fea644fe22cc70", "code": ["def bash_completion():\n\tparser = get_root_parser()\n\tactions = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tfor option_string in action.option_strings:\n\t\t\t\tactions.append((option_string, action))\n\t\telse:\n\t\t\tactions.append((action.dest, action))\n\tcommand_line = ' '.join([action[0] for action in actions])\n\tprint('%s %s' % (sys.argv[1], command_line))\n\n\n", "def bash_completion():\n\t# Use the main parser to produce a list of commands.\n\tparser = ArgumentParser(description=__doc__)  # pylint: disable=unused-variable\n\tsubparsers = parser.add_subparsers()\n\tadd_args_to_parser(subparsers, 'main')\n\n\t# Generate the list of commands.\n\tcommands = []\n\tcommands.append('help')\n\tfor command in main_parser.choices.keys():\n\t\tcommands.append(command)\n\tcommands.sort()\n\n\t# Produce the output.\n\treturn ' '.join(commands)\n\n\n", "def bash_completion():\n\tparser = create_parser()\n\tsubparsers = parser.subparsers\n\tsubparser_choices = subparsers.choices\n\tchoices = list(subparser_choices.keys())\n\n\tdef bash_completion_helper(subparsers, subparser_choices, choices):\n\t\t\"\"\"\n\t\tHelper function for bash_completion() to recursively iterate through subparsers.\n\t\t\"\"\"\n\t\tsubparser_list = list(subparsers.choices.keys())\n\t\tfor subparser in subparser_list:\n\t\t\tif subparser in subparser_choices:\n\t\t\t\tchoices.append(subparser)\n\t\t\t\tbash_completion_helper(subparsers.choices[subparser], subparser_choices, choices)\n\n\tbash_completion_helper(subparsers, subparser_choices, choices)\n\treturn choices\n\n\n", "def bash_completion():\n\tfrom borgmatic.borg import get_parser as borg_parser\n\tfrom borgmatic.cli import get_parser as cli_parser\n\tfrom borgmatic.config import get_parser as config_parser\n\n\tall_parsers = (borg_parser(), cli_parser(), config_parser())\n\tall_commands = set()\n\tall_options = set()\n\n\tfor parser in all_parsers:\n\t\tfor command in parser._actions[0].choices.keys():\n\t\t\tall_commands.add(command)\n\n\t\tfor option in parser._option_string_actions.keys():\n\t\t\tall_options.add(option)\n\n\tprint(' '.join(all_commands))\n\tprint(' '.join(all_options))\n\n\n", "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\tborgmatic.cli.add_arguments(subparsers, [])\n\tprint(parser.format_help())\n\n\n", "def bash_completion():\n\t# pylint: disable=import-outside-toplevel\n\timport borgmatic.borg\n\n\t# Borgmatic's command-line argument parser.\n\tparser = borgmatic.borg.get_argument_parser()\n\n\t# Borgmatic's sub-command argument parsers.\n\tsubparsers = parser.add_subparsers(dest='borgmatic_command')\n\n\t# Parse the borgmatic command-line arguments, but stop after the command.\n\targuments = parser.parse_args(sys.argv[1:2])\n\n\t# Find the sub-command parser.\n\tsubparser = None\n\tif arguments.borgmatic_command:\n\t\tsubparser = subparsers.choices[arguments.borgmatic_command]\n\n\t# If this is a sub-command, use the sub-command's argument parser.\n\tif subparser:\n\t\tparser = subparser\n\n\t# Add an argument specifying the name of the command being completed.\n\tcommand_argument = parser.add_argument('command')\n\n\t# Parse the command-line arguments, stopping after the command.\n\targuments =\n", "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\t# Add the \"backup\" command.\n\tbackup_parser = subparsers.add_parser('backup')\n\tbackup_parser.add_argument('--dry-run', action='store_true')\n\n\t# Add the \"check\" command.\n\tcheck_parser = subparsers.add_parser('check')\n\tcheck_parser.add_argument('--dry-run', action='store_true')\n\n\t# Add the \"prune\" command.\n\tprune_parser = subparsers.add_parser('prune')\n\tprune_parser.add_argument('--dry-run', action='store_true')\n\tprune_parser.add_argument('--keep-daily', type=int)\n\tprune_parser.add_argument('--keep-weekly', type=int)\n\tprune_parser.add_argument('--keep-monthly', type=int)\n\tprune_parser.add_argument('--keep-yearly',\n", "def bash_completion():\n\tdescription = 'borgmatic, a simple, configuration-driven backup tool for Borg'\n\tparser = argparse.ArgumentParser(description=description)\n\tparser.add_argument('-v', '--verbosity', type=int, choices=[0, 1, 2],\n\t\thelp='increase output verbosity')\n\tparser.add_argument('-d', '--dry-run', action='store_true',\n\t\thelp='show what would have been done, but do not do it')\n\tparser.add_argument('--lock-wait-time', type=int, default=5,\n\t\thelp='time to wait for lock, in seconds')\n\tparser.add_argument('--lock-wait-time-warning-threshold', type=int, default=10,\n\t\thelp='warning threshold for lock wait time, in seconds')\n\tparser.add_argument('--info', action='store_true',\n\t\thelp='show repository information')\n\tparser.add_argument('--list', action='store_true',\n\t\thelp='list archives in repository')\n\tparser.add_argument('--check',\n", "def bash_completion():\n\timport argcomplete\n\timport argparse\n\timport importlib.util\n\timport inspect\n\timport pkgutil\n\timport sys\n\n\tparser = argparse.ArgumentParser(description='borgmatic')\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tfor _, name, _ in pkgutil.iter_modules(path=['borgmatic']):\n\t\tif name.startswith('_'):\n\t\t\tcontinue\n\t\tspec = importlib.util.find_spec('borgmatic.commands.{}'.format(name))\n\t\tmodule = importlib.util.module_from_spec(spec)\n\t\tspec.loader.exec_module(module)\n\t\tsubparser = subparsers.add_parser(name)\n\t\tmodule.build_parser(subparser)\n\n\targcomplete.autocomplete(parser)\n\n\n", "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\tadd_arguments(subparsers)\n\tactions = subparsers.choices.values()  # pylint: disable=no-member\n\tcommands = ('{} '.format(action.prog) for action in actions)\n\tprint(' '.join(commands))\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def bash_completion():\n\tparser = get_root_parser()\n\tactions = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tfor option_string in action.option_strings:\n\t\t\t\tactions.append((option_string, action))\n\t\telse:\n\t\t\tactions.append((action.dest, action))\n\tcommand_line = ' '.join([action[0] for action in actions])\n\tprint('%s %s' % (sys.argv[1], command_line))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t# Use the main parser to produce a list of commands.\n\tparser = ArgumentParser(description=__doc__)  # pylint: disable=unused-variable\n\tsubparsers = parser.add_subparsers()\n\tadd_args_to_parser(subparsers, 'main')\n\n\t# Generate the list of commands.\n\tcommands = []\n\tcommands.append('help')\n\tfor command in main_parser.choices.keys():\n\t\tcommands.append(command)\n\tcommands.sort()\n\n\t# Produce the output.\n\treturn ' '.join(commands)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = create_parser()\n\tsubparsers = parser.subparsers\n\tsubparser_choices = subparsers.choices\n\tchoices = list(subparser_choices.keys())\n\n\tdef bash_completion_helper(subparsers, subparser_choices, choices):\n\t\t\"\"\"\n\t\tHelper function for bash_completion() to recursively iterate through subparsers.\n\t\t\"\"\"\n\t\tsubparser_list = list(subparsers.choices.keys())\n\t\tfor subparser in subparser_list:\n\t\t\tif subparser in subparser_choices:\n\t\t\t\tchoices.append(subparser)\n\t\t\t\tbash_completion_helper(subparsers.choices[subparser], subparser_choices, choices)\n\n\tbash_completion_helper(subparsers, subparser_choices, choices)\n\treturn choices\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tfrom borgmatic.borg import get_parser as borg_parser\n\tfrom borgmatic.cli import get_parser as cli_parser\n\tfrom borgmatic.config import get_parser as config_parser\n\n\tall_parsers = (borg_parser(), cli_parser(), config_parser())\n\tall_commands = set()\n\tall_options = set()\n\n\tfor parser in all_parsers:\n\t\tfor command in parser._actions[0].choices.keys():\n\t\t\tall_commands.add(command)\n\n\t\tfor option in parser._option_string_actions.keys():\n\t\t\tall_options.add(option)\n\n\tprint(' '.join(all_commands))\n\tprint(' '.join(all_options))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\tborgmatic.cli.add_arguments(subparsers, [])\n\tprint(parser.format_help())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\t# pylint: disable=import-outside-toplevel\n\timport borgmatic.borg\n\n\t# Borgmatic's command-line argument parser.\n\tparser = borgmatic.borg.get_argument_parser()\n\n\t# Borgmatic's sub-command argument parsers.\n\tsubparsers = parser.add_subparsers(dest='borgmatic_command')\n\n\t# Parse the borgmatic command-line arguments, but stop after the command.\n\targuments = parser.parse_args(sys.argv[1:2])\n\n\t# Find the sub-command parser.\n\tsubparser = None\n\tif arguments.borgmatic_command:\n\t\tsubparser = subparsers.choices[arguments.borgmatic_command]\n\n\t# If this is a sub-command, use the sub-command's argument parser.\n\tif subparser:\n\t\tparser = subparser\n\n\t# Add an argument specifying the name of the command being completed.\n\tcommand_argument = parser.add_argument('command')\n\n\t# Parse the command-line arguments, stopping after the command.\n\targuments =\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\t# Add the \"backup\" command.\n\tbackup_parser = subparsers.add_parser('backup')\n\tbackup_parser.add_argument('--dry-run', action='store_true')\n\n\t# Add the \"check\" command.\n\tcheck_parser = subparsers.add_parser('check')\n\tcheck_parser.add_argument('--dry-run', action='store_true')\n\n\t# Add the \"prune\" command.\n\tprune_parser = subparsers.add_parser('prune')\n\tprune_parser.add_argument('--dry-run', action='store_true')\n\tprune_parser.add_argument('--keep-daily', type=int)\n\tprune_parser.add_argument('--keep-weekly', type=int)\n\tprune_parser.add_argument('--keep-monthly', type=int)\n\tprune_parser.add_argument('--keep-yearly',\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tdescription = 'borgmatic, a simple, configuration-driven backup tool for Borg'\n\tparser = argparse.ArgumentParser(description=description)\n\tparser.add_argument('-v', '--verbosity', type=int, choices=[0, 1, 2],\n\t\thelp='increase output verbosity')\n\tparser.add_argument('-d', '--dry-run', action='store_true',\n\t\thelp='show what would have been done, but do not do it')\n\tparser.add_argument('--lock-wait-time', type=int, default=5,\n\t\thelp='time to wait for lock, in seconds')\n\tparser.add_argument('--lock-wait-time-warning-threshold', type=int, default=10,\n\t\thelp='warning threshold for lock wait time, in seconds')\n\tparser.add_argument('--info', action='store_true',\n\t\thelp='show repository information')\n\tparser.add_argument('--list', action='store_true',\n\t\thelp='list archives in repository')\n\tparser.add_argument('--check',\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\timport argcomplete\n\timport argparse\n\timport importlib.util\n\timport inspect\n\timport pkgutil\n\timport sys\n\n\tparser = argparse.ArgumentParser(description='borgmatic')\n\tsubparsers = parser.add_subparsers(dest='command')\n\n\tfor _, name, _ in pkgutil.iter_modules(path=['borgmatic']):\n\t\tif name.startswith('_'):\n\t\t\tcontinue\n\t\tspec = importlib.util.find_spec('borgmatic.commands.{}'.format(name))\n\t\tmodule = importlib.util.module_from_spec(spec)\n\t\tspec.loader.exec_module(module)\n\t\tsubparser = subparsers.add_parser(name)\n\t\tmodule.build_parser(subparser)\n\n\targcomplete.autocomplete(parser)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def bash_completion():\n\tparser = argparse.ArgumentParser()\n\tsubparsers = parser.add_subparsers()\n\tadd_arguments(subparsers)\n\tactions = subparsers.choices.values()  # pylint: disable=no-member\n\tcommands = ('{} '.format(action.prog) for action in actions)\n\tprint(' '.join(commands))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_arguments", "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.", "ques_id": "62b438a266fea644fe22cc2e", "code": ["def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tadd_arguments(parser)\n\targs = parser.parse_args(unparsed_arguments)\n\treturn args\n\n\n", "def parse_arguments(*unparsed_arguments):\n\timport argparse\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a command and notify the user when it is done.\",\n\t\tepilog=\"\"\"\n\t\t\t\tIf --command is not specified, the command will be read from stdin. The command may\n\t\t\tbe a sequence of commands separated by semicolons. The first command will be treated as\n\t\t\tthe command to be run, and the rest will be treated as commands to be run when the first\n\t\t\tcommand finishes.\n\t\t\t\tThe first command may contain placeholders which will be replaced when the command is\n\t\t\trun. The placeholders are:\n\t\t\t\t\t%(start_time)s - the time at which the command was started\n\t\t\t\t\t%(end_time)s - the time at which the command was finished\n\t\t\t\t\t%(exit_status)s - the exit status of the command\n\t\t\t\t\t%(output)s - the output of the command\n\t\t\t\tThe commands to be run when the first command finishes may contain placeholders which\n\t\t\twill be replaced when\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A script for manipulating an index of files in a directory tree.\",\n\t\tformatter_class=argparse.RawTextHelpFormatter,\n\t\tadd_help=False,\n\t)\n\t\n\tparser.add_argument(\n\t\t\"-h\",\n\t\t\"--help\",\n\t\taction=\"help\",\n\t\thelp=\"Show this help message and exit.\",\n\t)\n\t\n\tparser.add_argument(\n\t\t\"--strftime-format\",\n\t\ttype=str,\n\t\tdefault=\"%Y/%m/%d %H:%M:%S\",\n\t\thelp=\"The format string to pass to strftime() when displaying dates and times. \"\n\t\t\"(default: %(default)s)\",\n\t)\n\t\n\tparser.add_argument(\n\t\t\"--display-datetimes-in-local-time\",\n\t\taction=\"store_true\",\n\t\thelp=\"If set, then display all dates and times in local time rather than UTC. \"\n\t\t\"(default: %(default)s)\",\n\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A program to perform various analyses of molecular dynamics trajectories.\"\n\t)\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser\")\n\n\tsubparser_create_average_structure = subparsers.add_parser(\n\t\t\"create-average-structure\",\n\t\thelp=\"\"\"\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Manage a DendroPy library (e.g. a collection of trees in a single directory).\")\n\t#parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", default=False)\n\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=\"%(prog)s {}\".format(__version__))\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\n\t# GLOBAL ARGUMENTS\n\t#parser.add_argument(\"-V\", \"--version\", action=\"version\", version=\"%(prog)s {}\".format(__version__))\n\n\t# HELP\n\thelp_parser = subparsers.add_parser(\"help\", help=\"Show help\")\n\thelp_parser.add_argument(\"help_args\", metavar=\"help-args\", nargs=argparse.REMAINDER, help=\"help on help\")\n\n\t# LIST\n\tlist_parser = subparsers.add_parser(\"list\", help\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Run a single- or multi-threaded pipeline on a single machine.\")\n\tparser.add_argument(\"-p\", \"--pipeline\", action=\"store\", dest=\"pipeline_description_pathname\",\n\t\t\thelp=\"Pathname of the pipeline description file that this script should execute.\", required=True)\n\tparser.add_argument(\"-d\", \"--debug-level\", action=\"store\", dest=\"debug_level\", type=int,\n\t\t\thelp=\"Debug level to use for logging by the pipeline.  Higher numbers indicate higher verbosity.\",\n\t\t\tdefault=1)\n\tparser.add_argument(\"-f\", \"--force\", action=\"store_true\", dest=\"force_overwrite\",\n\t\t\thelp=\"Whether to overwrite the output file if it already exists.\")\n\tparser.add_argument(\"-t\", \"--threads\", action=\"store\", dest=\"threads\", type=int,\n\t\t\thelp=\"Number of threads to use when running the pipeline.\")\n\tparser.add_argument(\"-r\", \"--reference-filename\", action=\"store\", dest\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Wrapper script for the CNVkit pipeline.\")\n\t# Parse any global options first, since they have to be passed to the subparsers.\n\tglobal_options, remaining_arguments = parser.parse_known_args(unparsed_arguments)\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\", dest=\"subparser_name\")\n\tsubparser_name_to_class = {\"coverage\": CoverageSubparser, \"fix\": FixSubparser, \"call\": CallSubparser}\n\tfor subparser_name, subparser_class in subparser_name_to_class.items():\n\t\tsubparser = subparser_class(subparsers, global_options)\n\t# Parse the remaining arguments using the appropriate subparser.\n\tsubparser_name_to_namespace = {}\n\tfor subparser_name in subparser_name_to_class:\n\t\tsubparser = subparsers.choices[subparser_name]\n\t\tsubparser_name_to_namespace[subparser_name\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a script to generate the necessary files for a new release of a Java library.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--dry-run\", action=\"store_true\", help=\"do not actually modify any files\")\n\tparser.add_argument(\"--verbose\", action=\"store_true\", help=\"print progress information during the run\")\n\tparser.add_argument(\"--no-commit\", action=\"store_true\", help=\"do not commit after the run\")\n\tparser.add_argument(\"--no-push\", action=\"store_true\", help=\"do not push after the run\")\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\")\n\n\tparser_bump_version = subparsers.add_parser(\"bump-version\", help=\"bump the version of the library\")\n\tparser_bump_version.add_argument(\"version\", metavar=\"VERSION\", help=\"the new version\")\n\tparser_bump_version.add_argument(\"--\n", "def parse_arguments(*unparsed_arguments):\n\tglobal argparsers\n\tglobal args\n\tglobal parser\n\tglobal subparsers\n\n\targparsers = {}\n\tsubparsers = None\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"This is a script to perform various tasks related to my PhD research.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\", metavar=\"subparser-name\")\n\tsubparsers.required = True\n\tsubparsers.dest = \"subparser_name\"\n\tsubparsers.metavar = \"subparser-name\"\n\targparsers[\"global\"] = parser\n\n\tadd_argument(\"task\",\n\t\taction=\"store\",\n\t\thelp=\"The task to perform. Run %(prog)s task --help for more information.\")\n\tadd_argument(\"--configuration-file\",\n\t\taction=\"store\",\n\t\thelp=\"The configuration file to load. Use --help for more info.\")\n\n\tadd_subparser(\"task\")\n\n\t\n", "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tCreate a new version of a .po file from a .pot file. The .po file is created from a template\n\t\t\t.pot file. The .pot file is assumed to be a template for a .po file, with each entry in the\n\t\t\t.pot file corresponding to an entry in the .po file. If the .pot file contains an entry that\n\t\t\tis not in the .po file, a new entry is created in the .po file. If the .pot file contains an\n\t\t\tentry that is also in the .po file, the entry in the .po file is updated.\n\t\t\t\"\"\",\n\t\tepilog=\"\"\"\n\t\t\tThis script is intended to be used to translate a .pot file into a .po file that contains\n\t\t\ttranslations from a previous version of the .po file. If the .po file does not exist, or if\n\t\t\tthe .pot file is not a template for the .po file, the result of using this script is\n\t\t\tunpredictable.\n\t\t\t\"\"\"\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tadd_arguments(parser)\n\targs = parser.parse_args(unparsed_arguments)\n\treturn args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\timport argparse\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a command and notify the user when it is done.\",\n\t\tepilog=\"\"\"\n\t\t\t\tIf --command is not specified, the command will be read from stdin. The command may\n\t\t\tbe a sequence of commands separated by semicolons. The first command will be treated as\n\t\t\tthe command to be run, and the rest will be treated as commands to be run when the first\n\t\t\tcommand finishes.\n\t\t\t\tThe first command may contain placeholders which will be replaced when the command is\n\t\t\trun. The placeholders are:\n\t\t\t\t\t%(start_time)s - the time at which the command was started\n\t\t\t\t\t%(end_time)s - the time at which the command was finished\n\t\t\t\t\t%(exit_status)s - the exit status of the command\n\t\t\t\t\t%(output)s - the output of the command\n\t\t\t\tThe commands to be run when the first command finishes may contain placeholders which\n\t\t\twill be replaced when\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A script for manipulating an index of files in a directory tree.\",\n\t\tformatter_class=argparse.RawTextHelpFormatter,\n\t\tadd_help=False,\n\t)\n\t\n\tparser.add_argument(\n\t\t\"-h\",\n\t\t\"--help\",\n\t\taction=\"help\",\n\t\thelp=\"Show this help message and exit.\",\n\t)\n\t\n\tparser.add_argument(\n\t\t\"--strftime-format\",\n\t\ttype=str,\n\t\tdefault=\"%Y/%m/%d %H:%M:%S\",\n\t\thelp=\"The format string to pass to strftime() when displaying dates and times. \"\n\t\t\"(default: %(default)s)\",\n\t)\n\t\n\tparser.add_argument(\n\t\t\"--display-datetimes-in-local-time\",\n\t\taction=\"store_true\",\n\t\thelp=\"If set, then display all dates and times in local time rather than UTC. \"\n\t\t\"(default: %(default)s)\",\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A program to perform various analyses of molecular dynamics trajectories.\"\n\t)\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser\")\n\n\tsubparser_create_average_structure = subparsers.add_parser(\n\t\t\"create-average-structure\",\n\t\thelp=\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Manage a DendroPy library (e.g. a collection of trees in a single directory).\")\n\t#parser.add_argument(\"-v\", \"--verbose\", action=\"store_true\", default=False)\n\tparser.add_argument(\"-V\", \"--version\", action=\"version\", version=\"%(prog)s {}\".format(__version__))\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\n\t# GLOBAL ARGUMENTS\n\t#parser.add_argument(\"-V\", \"--version\", action=\"version\", version=\"%(prog)s {}\".format(__version__))\n\n\t# HELP\n\thelp_parser = subparsers.add_parser(\"help\", help=\"Show help\")\n\thelp_parser.add_argument(\"help_args\", metavar=\"help-args\", nargs=argparse.REMAINDER, help=\"help on help\")\n\n\t# LIST\n\tlist_parser = subparsers.add_parser(\"list\", help\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Run a single- or multi-threaded pipeline on a single machine.\")\n\tparser.add_argument(\"-p\", \"--pipeline\", action=\"store\", dest=\"pipeline_description_pathname\",\n\t\t\thelp=\"Pathname of the pipeline description file that this script should execute.\", required=True)\n\tparser.add_argument(\"-d\", \"--debug-level\", action=\"store\", dest=\"debug_level\", type=int,\n\t\t\thelp=\"Debug level to use for logging by the pipeline.  Higher numbers indicate higher verbosity.\",\n\t\t\tdefault=1)\n\tparser.add_argument(\"-f\", \"--force\", action=\"store_true\", dest=\"force_overwrite\",\n\t\t\thelp=\"Whether to overwrite the output file if it already exists.\")\n\tparser.add_argument(\"-t\", \"--threads\", action=\"store\", dest=\"threads\", type=int,\n\t\t\thelp=\"Number of threads to use when running the pipeline.\")\n\tparser.add_argument(\"-r\", \"--reference-filename\", action=\"store\", dest\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(description=\"Wrapper script for the CNVkit pipeline.\")\n\t# Parse any global options first, since they have to be passed to the subparsers.\n\tglobal_options, remaining_arguments = parser.parse_known_args(unparsed_arguments)\n\tsubparsers = parser.add_subparsers(help=\"sub-command help\", dest=\"subparser_name\")\n\tsubparser_name_to_class = {\"coverage\": CoverageSubparser, \"fix\": FixSubparser, \"call\": CallSubparser}\n\tfor subparser_name, subparser_class in subparser_name_to_class.items():\n\t\tsubparser = subparser_class(subparsers, global_options)\n\t# Parse the remaining arguments using the appropriate subparser.\n\tsubparser_name_to_namespace = {}\n\tfor subparser_name in subparser_name_to_class:\n\t\tsubparser = subparsers.choices[subparser_name]\n\t\tsubparser_name_to_namespace[subparser_name\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"Run a script to generate the necessary files for a new release of a Java library.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\"--dry-run\", action=\"store_true\", help=\"do not actually modify any files\")\n\tparser.add_argument(\"--verbose\", action=\"store_true\", help=\"print progress information during the run\")\n\tparser.add_argument(\"--no-commit\", action=\"store_true\", help=\"do not commit after the run\")\n\tparser.add_argument(\"--no-push\", action=\"store_true\", help=\"do not push after the run\")\n\n\tsubparsers = parser.add_subparsers(title=\"subcommands\")\n\n\tparser_bump_version = subparsers.add_parser(\"bump-version\", help=\"bump the version of the library\")\n\tparser_bump_version.add_argument(\"version\", metavar=\"VERSION\", help=\"the new version\")\n\tparser_bump_version.add_argument(\"--\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tglobal argparsers\n\tglobal args\n\tglobal parser\n\tglobal subparsers\n\n\targparsers = {}\n\tsubparsers = None\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"This is a script to perform various tasks related to my PhD research.\",\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tsubparsers = parser.add_subparsers(dest=\"subparser_name\", metavar=\"subparser-name\")\n\tsubparsers.required = True\n\tsubparsers.dest = \"subparser_name\"\n\tsubparsers.metavar = \"subparser-name\"\n\targparsers[\"global\"] = parser\n\n\tadd_argument(\"task\",\n\t\taction=\"store\",\n\t\thelp=\"The task to perform. Run %(prog)s task --help for more information.\")\n\tadd_argument(\"--configuration-file\",\n\t\taction=\"store\",\n\t\thelp=\"The configuration file to load. Use --help for more info.\")\n\n\tadd_subparser(\"task\")\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_arguments(*unparsed_arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tCreate a new version of a .po file from a .pot file. The .po file is created from a template\n\t\t\t.pot file. The .pot file is assumed to be a template for a .po file, with each entry in the\n\t\t\t.pot file corresponding to an entry in the .po file. If the .pot file contains an entry that\n\t\t\tis not in the .po file, a new entry is created in the .po file. If the .pot file contains an\n\t\t\tentry that is also in the .po file, the entry in the .po file is updated.\n\t\t\t\"\"\",\n\t\tepilog=\"\"\"\n\t\t\tThis script is intended to be used to translate a .pot file into a .po file that contains\n\t\t\ttranslations from a previous version of the .po file. If the .po file does not exist, or if\n\t\t\tthe .pot file is not a template for the .po file, the result of using this script is\n\t\t\tunpredictable.\n\t\t\t\"\"\"\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "parse_subparser_arguments", "docstring": "Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\ninstance, give each requested action's subparser a shot at parsing all arguments. This allows\ncommon arguments like \"--repository\" to be shared across multiple subparsers.\n\nReturn the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\narguments, a list of remaining arguments not claimed by any subparser).", "ques_id": "62b438a266fea644fe22cc2d", "code": ["def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# First, split the sequence of arguments into two parts: the first part will contain all\n\t# arguments up to and including the first non-option argument, and the second part will\n\t# contain all remaining arguments.\n\tpre_action_args = []\n\tpost_action_args = []\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith('-'):\n\t\t\tpre_action_args.append(arg)\n\t\telse:\n\t\t\tpost_action_args.append(arg)\n\t\t\tbreak\n\tpost_action_args.extend(unparsed_arguments[len(pre_action_args):])\n\n\t# Create a dict mapping from subparser name to a dict mapping from argument name to\n\t# argument value.\n\taction_args = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\taction_args[subparser_name] = {}\n\n\t# Now, walk through every argument in pre_action_args, and give each subparser a shot\n\t# at claiming it. If the argument is claimed, remove it from\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor subparser_name in subparsers:\n\t\tparsed_arguments[subparser_name] = subparsers[subparser_name].parse_args(unparsed_arguments)\n\t\tremaining_arguments = [arg for arg in unparsed_arguments if arg not in parsed_arguments[subparser_name]]\n\treturn (parsed_arguments, remaining_arguments)\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = subparsers.keys()\n\tsubparser_name = None\n\tsubparsed_arguments = []\n\tsubparser_arguments = []\n\tsubparser_namespaces = {}\n\tsubparser_positionals = []\n\tsubparser_optionals = []\n\tsubparser_argument_names = []\n\tsubparser_argument_defaults = {}\n\tsubparser_argument_choices = {}\n\tsubparser_argument_required = set()\n\tsubparser_argument_help = {}\n\n\tfor argument in unparsed_arguments:\n\t\tif argument == \"--\":\n\t\t\tsubparser_arguments.append(argument)\n\t\telif argument.startswith(\"--\") and \"=\" in argument:\n\t\t\tname, value = argument[2:].split(\"=\", 1)\n\t\t\tsubparser_arguments.append(\"--\" + name)\n\t\t\tsubparser_arguments.append(value)\n\t\telif argument.startswith(\"--\") and \"=\" not in argument:\n\t\t\tsubparser_arguments.append(argument\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tdef should_parse_with_subparser(argument):\n\t\treturn any(argument.startswith(\"--\" + subparser_name) for subparser_name in subparsers.keys())\n\n\t# Split arguments into those that should be parsed by a subparser and those that should not.\n\tsubparser_arguments = [argument for argument in unparsed_arguments if should_parse_with_subparser(argument)]\n\tother_arguments = [argument for argument in unparsed_arguments if not should_parse_with_subparser(argument)]\n\t# Parse the subparser arguments with each subparser.\n\tsubparser_argument_dict = {\n\t\tsubparser_name: subparser.parse_args(subparser_arguments)\n\t\tfor subparser_name, subparser in subparsers.iteritems()\n\t}\n\treturn subparser_argument_dict, other_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tresult = {}\n\tremaining_arguments = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tresult[subparser_name] = subparser.parse_known_args(unparsed_arguments)[0]\n\t\texcept SystemExit:\n\t\t\t# If this subparser fails to parse the arguments, that's ok: we just want to\n\t\t\t# continue to the next subparser.\n\t\t\tpass\n\t\telse:\n\t\t\t# If this subparser was able to parse the arguments, then we're done with this\n\t\t\t# subparser.\n\t\t\tremaining_arguments = unparsed_arguments\n\n\treturn result, remaining_arguments\n\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# TODO: This should probably be factored out into a separate module.\n\targuments = {}\n\tremaining = []\n\tfor argument in unparsed_arguments:\n\t\tif argument == \"--\":\n\t\t\tremaining.extend(unparsed_arguments[unparsed_arguments.index(argument) + 1 :])\n\t\t\tbreak\n\t\telif not argument.startswith(\"-\"):\n\t\t\tfor parser_name, parser in subparsers.items():\n\t\t\t\tif argument in parser._optionals._actions:\n\t\t\t\t\targuments[parser_name] = parser.parse_args(unparsed_arguments)\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tremaining.append(argument)\n\t\telse:\n\t\t\tremaining.append(argument)\n\treturn arguments, remaining\n\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_arguments = dict()\n\tunclaimed_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_known_args([argument])[0]\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\telse:\n\t\t\tunclaimed_arguments.append(argument)\n\treturn parsed_arguments, unclaimed_arguments\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t#\tCopy the list because it gets popped from.\n\targs = list(unparsed_arguments)\n\t#\tTry each subparser in turn.\n\tfor subparser_name, subparser in subparsers.items():\n\t\t#\tAdd the subparser name to argparse's help output, so we can tell which\n\t\t#\tsubparser a given argument belongs to.\n\t\tsubparser.description = (\n\t\t\t\"%s\\n\\n%s\" % (subparser_name, subparser.description or '')\n\t\t\t)\n\t\t#\tTry to parse the arguments.\n\t\ttry:\n\t\t\targs = subparser.parse_args(args)\n\t\texcept:\n\t\t\t#\tIf this subparser can't parse the arguments, skip it.\n\t\t\tcontinue\n\t\t#\tIf this subparser can parse the arguments, return the result.\n\t\treturn {subparser_name: args}, []\n\t#\tIf no subparser can parse the arguments, return an empty result.\n\treturn {}, args\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tremaining_args = list(unparsed_arguments)\n\tresults = {}\n\twhile remaining_args:\n\t\t# Try to parse the current argument with each subparser in turn.\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser_results, remaining_args = subparser.parse_known_args(remaining_args)\n\t\t\t\tresults[subparser_name] = subparser_results\n\t\t\t\tbreak\n\t\t\texcept ArgumentError:\n\t\t\t\t# If it doesn't work, try the next one.\n\t\t\t\tpass\n\t\telse:\n\t\t\t# If we didn't make any progress, then we're done.\n\t\t\tbreak\n\treturn results, remaining_args\n\n", "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = subparsers.keys()\n\tfor arg in unparsed_arguments:\n\t\tif arg in subparser_names:\n\t\t\tbreak\n\telse:\n\t\traise ValueError(\"Subparser not specified\")\n\tsubparsers[arg].add_argument('remaining_args', nargs=argparse.REMAINDER)\n\tresult = {}\n\tfor name, subparser in subparsers.items():\n\t\tresult[name] = subparser.parse_args(unparsed_arguments)\n\treturn result\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# First, split the sequence of arguments into two parts: the first part will contain all\n\t# arguments up to and including the first non-option argument, and the second part will\n\t# contain all remaining arguments.\n\tpre_action_args = []\n\tpost_action_args = []\n\tfor arg in unparsed_arguments:\n\t\tif arg.startswith('-'):\n\t\t\tpre_action_args.append(arg)\n\t\telse:\n\t\t\tpost_action_args.append(arg)\n\t\t\tbreak\n\tpost_action_args.extend(unparsed_arguments[len(pre_action_args):])\n\n\t# Create a dict mapping from subparser name to a dict mapping from argument name to\n\t# argument value.\n\taction_args = {}\n\tfor subparser_name, subparser in subparsers.items():\n\t\taction_args[subparser_name] = {}\n\n\t# Now, walk through every argument in pre_action_args, and give each subparser a shot\n\t# at claiming it. If the argument is claimed, remove it from\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_arguments = {}\n\tremaining_arguments = []\n\tfor subparser_name in subparsers:\n\t\tparsed_arguments[subparser_name] = subparsers[subparser_name].parse_args(unparsed_arguments)\n\t\tremaining_arguments = [arg for arg in unparsed_arguments if arg not in parsed_arguments[subparser_name]]\n\treturn (parsed_arguments, remaining_arguments)\n\n", "is_pass": false, "return_code": 2}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = subparsers.keys()\n\tsubparser_name = None\n\tsubparsed_arguments = []\n\tsubparser_arguments = []\n\tsubparser_namespaces = {}\n\tsubparser_positionals = []\n\tsubparser_optionals = []\n\tsubparser_argument_names = []\n\tsubparser_argument_defaults = {}\n\tsubparser_argument_choices = {}\n\tsubparser_argument_required = set()\n\tsubparser_argument_help = {}\n\n\tfor argument in unparsed_arguments:\n\t\tif argument == \"--\":\n\t\t\tsubparser_arguments.append(argument)\n\t\telif argument.startswith(\"--\") and \"=\" in argument:\n\t\t\tname, value = argument[2:].split(\"=\", 1)\n\t\t\tsubparser_arguments.append(\"--\" + name)\n\t\t\tsubparser_arguments.append(value)\n\t\telif argument.startswith(\"--\") and \"=\" not in argument:\n\t\t\tsubparser_arguments.append(argument\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tdef should_parse_with_subparser(argument):\n\t\treturn any(argument.startswith(\"--\" + subparser_name) for subparser_name in subparsers.keys())\n\n\t# Split arguments into those that should be parsed by a subparser and those that should not.\n\tsubparser_arguments = [argument for argument in unparsed_arguments if should_parse_with_subparser(argument)]\n\tother_arguments = [argument for argument in unparsed_arguments if not should_parse_with_subparser(argument)]\n\t# Parse the subparser arguments with each subparser.\n\tsubparser_argument_dict = {\n\t\tsubparser_name: subparser.parse_args(subparser_arguments)\n\t\tfor subparser_name, subparser in subparsers.iteritems()\n\t}\n\treturn subparser_argument_dict, other_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tresult = {}\n\tremaining_arguments = []\n\n\tfor subparser_name, subparser in subparsers.items():\n\t\ttry:\n\t\t\tresult[subparser_name] = subparser.parse_known_args(unparsed_arguments)[0]\n\t\texcept SystemExit:\n\t\t\t# If this subparser fails to parse the arguments, that's ok: we just want to\n\t\t\t# continue to the next subparser.\n\t\t\tpass\n\t\telse:\n\t\t\t# If this subparser was able to parse the arguments, then we're done with this\n\t\t\t# subparser.\n\t\t\tremaining_arguments = unparsed_arguments\n\n\treturn result, remaining_arguments\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t# TODO: This should probably be factored out into a separate module.\n\targuments = {}\n\tremaining = []\n\tfor argument in unparsed_arguments:\n\t\tif argument == \"--\":\n\t\t\tremaining.extend(unparsed_arguments[unparsed_arguments.index(argument) + 1 :])\n\t\t\tbreak\n\t\telif not argument.startswith(\"-\"):\n\t\t\tfor parser_name, parser in subparsers.items():\n\t\t\t\tif argument in parser._optionals._actions:\n\t\t\t\t\targuments[parser_name] = parser.parse_args(unparsed_arguments)\n\t\t\t\t\tbreak\n\t\t\telse:\n\t\t\t\tremaining.append(argument)\n\t\telse:\n\t\t\tremaining.append(argument)\n\treturn arguments, remaining\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tparsed_arguments = dict()\n\tunclaimed_arguments = []\n\tfor argument in unparsed_arguments:\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tparsed_arguments[subparser_name] = subparser.parse_known_args([argument])[0]\n\t\t\t\tbreak\n\t\t\texcept SystemExit:\n\t\t\t\tpass\n\t\telse:\n\t\t\tunclaimed_arguments.append(argument)\n\treturn parsed_arguments, unclaimed_arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\t#\tCopy the list because it gets popped from.\n\targs = list(unparsed_arguments)\n\t#\tTry each subparser in turn.\n\tfor subparser_name, subparser in subparsers.items():\n\t\t#\tAdd the subparser name to argparse's help output, so we can tell which\n\t\t#\tsubparser a given argument belongs to.\n\t\tsubparser.description = (\n\t\t\t\"%s\\n\\n%s\" % (subparser_name, subparser.description or '')\n\t\t\t)\n\t\t#\tTry to parse the arguments.\n\t\ttry:\n\t\t\targs = subparser.parse_args(args)\n\t\texcept:\n\t\t\t#\tIf this subparser can't parse the arguments, skip it.\n\t\t\tcontinue\n\t\t#\tIf this subparser can parse the arguments, return the result.\n\t\treturn {subparser_name: args}, []\n\t#\tIf no subparser can parse the arguments, return an empty result.\n\treturn {}, args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tremaining_args = list(unparsed_arguments)\n\tresults = {}\n\twhile remaining_args:\n\t\t# Try to parse the current argument with each subparser in turn.\n\t\tfor subparser_name, subparser in subparsers.items():\n\t\t\ttry:\n\t\t\t\tsubparser_results, remaining_args = subparser.parse_known_args(remaining_args)\n\t\t\t\tresults[subparser_name] = subparser_results\n\t\t\t\tbreak\n\t\t\texcept ArgumentError:\n\t\t\t\t# If it doesn't work, try the next one.\n\t\t\t\tpass\n\t\telse:\n\t\t\t# If we didn't make any progress, then we're done.\n\t\t\tbreak\n\treturn results, remaining_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_subparser_arguments(unparsed_arguments, subparsers):\n\tsubparser_names = subparsers.keys()\n\tfor arg in unparsed_arguments:\n\t\tif arg in subparser_names:\n\t\t\tbreak\n\telse:\n\t\traise ValueError(\"Subparser not specified\")\n\tsubparsers[arg].add_argument('remaining_args', nargs=argparse.REMAINDER)\n\tresult = {}\n\tfor name, subparser in subparsers.items():\n\t\tresult[name] = subparser.parse_args(unparsed_arguments)\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "borgmatic/commands/arguments.py", "project": "witten/borgmatic", "name": "make_parsers", "docstring": "Build a top-level parser and its subparsers and return them as a tuple.", "ques_id": "62b438a266fea644fe22cc2c", "code": ["def make_parsers():\n\t# Build the top-level parser.\n\tparent_parser = argparse.ArgumentParser(description='Run a program using the specified configuration.')\n\tparent_parser.add_argument('-v', '--verbose', help='print verbose output', action='store_true')\n\t# Build the subparsers.\n\tsubparsers = parent_parser.add_subparsers(help='sub-command help', dest='subparser_name')\n\t# Build the subparser for the 'run' command.\n\trun_parser = subparsers.add_parser('run', help='run a program using the specified configuration')\n\trun_parser.add_argument('config_file', help='the configuration file to use', type=argparse.FileType('r'))\n\trun_parser.add_argument('-s', '--stop-at-error', help='stop at the first error', action='store_true')\n\t# Build the subparser for the 'validate' command.\n\tvalidate_parser = subparsers.add_parser('validate', help='validate a configuration file')\n\tvalid\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(prog=\"toxmail\", description=__doc__)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"0.1\")\n\tparser.add_argument(\"--verbose\", action=\"store_true\", help=\"show verbose output\")\n\n\tsubparsers = parser.add_subparsers(help=\"commands\", dest=\"command\")\n\tsubparsers.required = True\n\n\t#\n\t# Subcommand: create\n\t#\n\tparser_create = subparsers.add_parser(\"create\", help=\"create a new toxmail account\")\n\tparser_create.add_argument(\"name\", metavar=\"NAME\", help=\"the name of the toxmail account\")\n\tparser_create.add_argument(\"--salt\", metavar=\"SALT\", help=\"the salt\")\n\tparser_create.add_argument(\"--passphrase\", metavar=\"PASSPHRASE\", help=\"the passphrase\")\n\tparser_create.add_argument(\"--keyfile\", metavar=\"KEYFILE\", help=\"the keyfile\")\n\n\t#\n\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool for controlling fenced regions in code.\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n\t\thelp=\"Enable verbose output.\")\n\tparser.add_argument(\"-c\", \"--comment\", default=\"//\",\n\t\thelp=\"Set the comment character to use.\")\n\tparser.add_argument(\"-C\", \"--comment-on\", default=\"//\",\n\t\thelp=\"Set the comment character to use when enabling.\")\n\tparser.add_argument(\"-C\", \"--comment-off\", default=\"//\",\n\t\thelp=\"Set the comment character to use when disabling.\")\n\n\tsubparsers = parser.add_subparsers()\n\tbuild_enable_parser(subparsers)\n\tbuild_disable_parser(subparsers)\n\tbuild_list_parser(subparsers)\n\treturn parser, subparsers\n\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tA program for quickly finding the best-matching subsequence in a\n\t\t\tsequence database.\n\t\t\"\"\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t)\n\tsubparser = parser.add_subparsers()\n\tblast_parser = subparser.add_parser(\n\t\t\"blast\",\n\t\thelp=\"Use the BLAST algorithm to find the best-matching subsequence\",\n\t)\n\tsubparser.add_parser(\n\t\t\"perfect-match\",\n\t\thelp=\"Find the perfect match of the subsequence in the database\",\n\t)\n\tsubparser.add_parser(\n\t\t\"trivial\",\n\t\thelp=\"Find the best-matching subsequence in the database by brute force\",\n\t)\n\tsubparser.add_parser(\"trivial-parallel\")\n\tsubparser.add_parser(\"trivial-parallel-local\")\n\tsubparser.add_parser(\"trivial-parallel-global\")\n\tsubparser.add\n", "def make_parsers():\n\t# Build the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool to query the UMLS and generate a hypernym graph\",\n\t\tformatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s \" + __version__)\n\tparser.add_argument(\n\t\t\"--umls-user\",\n\t\ttype=str,\n\t\trequired=True,\n\t\thelp=\"UMLS username\")\n\tparser.add_argument(\n\t\t\"--umls-password\",\n\t\ttype=str,\n\t\trequired=True,\n\t\thelp=\"UMLS password\")\n\tparser.add_argument(\n\t\t\"--umls-host\",\n\t\ttype=str,\n\t\tdefault=\"https://uts-ws.nlm.nih.gov\",\n\t\thelp=\"UMLS host name (default: %(default)s)\")\n\n\t# Build the subparsers\n\tsubparsers\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description=\"A tool for diffing two JSON files.\")\n\tsubparsers = top_parser.add_subparsers(title=\"actions\", dest=\"action\")\n\n\tparser_diff = subparsers.add_parser(\"diff\", help=\"diff two JSON files\")\n\tparser_diff.add_argument(\"left\", type=str, help=\"the left JSON file\")\n\tparser_diff.add_argument(\"right\", type=str, help=\"the right JSON file\")\n\tparser_diff.add_argument(\"--key\", type=str, help=\"only diff the given key, if any\")\n\n\tparser_patch = subparsers.add_parser(\"patch\", help=\"patch a JSON file\")\n\tparser_patch.add_argument(\"file\", type=str, help=\"the JSON file to patch\")\n\tparser_patch.add_argument(\"patch\", type=str, help=\"the diff file\")\n\n\treturn top_parser, subparsers\n\n\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(prog=\"pyhashcat\")\n\tsubparsers = top_parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\n\t# Add the `hashcat` subcommand parser.\n\thashcat_parser = subparsers.add_parser(\"hashcat\", help=\"Run hashcat.\")\n\thashcat_parser.set_defaults(func=run_hashcat)\n\thashcat_parser.add_argument(\"hash\", type=str, help=\"Path to the file containing the hashes to crack.\")\n\thashcat_parser.add_argument(\"wordlist\", type=str, help=\"Path to the file containing the list of words to try.\")\n\thashcat_parser.add_argument(\"--rules\", type=str, help=\"Path to the file containing the rules to apply.\")\n\thashcat_parser.add_argument(\"--output\", type=str, help=\"Path to the file to write the cracked hashes to.\")\n\thashcat_parser.add_argument(\"--format\", type=str, help=\"Hash format. See hashcat documentation for details\n", "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tA tool for managing and deploying software-defined networks in OpenStack\n\t\t\"\"\"\n\t)\n\tparser.add_argument(\n\t\t\"--debug\",\n\t\taction=\"store_true\",\n\t\thelp=\"\"\"\n\t\tPrint debugging information.\n\t\t\"\"\"\n\t)\n\tparser.add_argument(\n\t\t\"--verbose\",\n\t\taction=\"store_true\",\n\t\thelp=\"\"\"\n\t\tPrint verbose information.\n\t\t\"\"\"\n\t)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s {}\".format(__version__),\n\t\thelp=\"Print tool version.\"\n\t)\n\tparser.add_argument(\n\t\t\"--config-file\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"\"\"\n\t\tPath to the configuration file.\n\t\t\"\"\"\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\thelp=\"\"\"\n", "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tdescription='A tool for managing your python projects.')\n\ttop_parser.add_argument(\n\t\t'-v', '--verbose', action='store_true', help='increase verbosity')\n\ttop_parser.add_argument(\n\t\t'-q', '--quiet', action='store_true', help='decrease verbosity')\n\ttop_parser.add_argument(\n\t\t'-V', '--version', action='version', version=__version__,\n\t\thelp='show program\\'s version number and exit')\n\n\tsubparsers = top_parser.add_subparsers()\n\n\t# create the parser for the \"init\" command\n\tparser_init = subparsers.add_parser(\n\t\t'init', help='initialize project')\n\tparser_init.add_argument(\n\t\t'-t', '--template', default='default',\n\t\thelp='name of template to use (default: default)')\n\tparser_init.add_argument(\n\t\t'-f', '--force', action='store\n", "def make_parsers():\n\t# Top-level parser\n\tparser = argparse.ArgumentParser(description=\"Evaluate a model's performance with respect to a specified dataset.\")\n\tparser.add_argument(\"-m\", \"--model\", help=\"Path to the model file to evaluate.\", required=True)\n\tparser.add_argument(\"-d\", \"--dataset\", help=\"Path to the dataset file to evaluate.\", required=True)\n\tparser.add_argument(\"-e\", \"--embedding\", help=\"Path to the embedding file to use.\", required=True)\n\tparser.add_argument(\"-t\", \"--task\", help=\"Task to evaluate on.\", choices=[\"pos\", \"ner\"], default=\"pos\")\n\tparser.add_argument(\"-r\", \"--report\", help=\"Path to the report file that will be generated.\", required=True)\n\t# Subparsers\n\tsubparsers = parser.add_subparsers(dest=\"task\")\n\t# POS tagging subparser\n\tpos_parser = subparsers.add_parser(\"pos\", help=\"Evaluate POS tagging performance.\")\n\tpos_parser.add_argument\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def make_parsers():\n\t# Build the top-level parser.\n\tparent_parser = argparse.ArgumentParser(description='Run a program using the specified configuration.')\n\tparent_parser.add_argument('-v', '--verbose', help='print verbose output', action='store_true')\n\t# Build the subparsers.\n\tsubparsers = parent_parser.add_subparsers(help='sub-command help', dest='subparser_name')\n\t# Build the subparser for the 'run' command.\n\trun_parser = subparsers.add_parser('run', help='run a program using the specified configuration')\n\trun_parser.add_argument('config_file', help='the configuration file to use', type=argparse.FileType('r'))\n\trun_parser.add_argument('-s', '--stop-at-error', help='stop at the first error', action='store_true')\n\t# Build the subparser for the 'validate' command.\n\tvalidate_parser = subparsers.add_parser('validate', help='validate a configuration file')\n\tvalid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(prog=\"toxmail\", description=__doc__)\n\tparser.add_argument(\"--version\", action=\"version\", version=\"0.1\")\n\tparser.add_argument(\"--verbose\", action=\"store_true\", help=\"show verbose output\")\n\n\tsubparsers = parser.add_subparsers(help=\"commands\", dest=\"command\")\n\tsubparsers.required = True\n\n\t#\n\t# Subcommand: create\n\t#\n\tparser_create = subparsers.add_parser(\"create\", help=\"create a new toxmail account\")\n\tparser_create.add_argument(\"name\", metavar=\"NAME\", help=\"the name of the toxmail account\")\n\tparser_create.add_argument(\"--salt\", metavar=\"SALT\", help=\"the salt\")\n\tparser_create.add_argument(\"--passphrase\", metavar=\"PASSPHRASE\", help=\"the passphrase\")\n\tparser_create.add_argument(\"--keyfile\", metavar=\"KEYFILE\", help=\"the keyfile\")\n\n\t#\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool for controlling fenced regions in code.\")\n\tparser.add_argument(\"-v\", \"--verbose\", action=\"store_true\",\n\t\thelp=\"Enable verbose output.\")\n\tparser.add_argument(\"-c\", \"--comment\", default=\"//\",\n\t\thelp=\"Set the comment character to use.\")\n\tparser.add_argument(\"-C\", \"--comment-on\", default=\"//\",\n\t\thelp=\"Set the comment character to use when enabling.\")\n\tparser.add_argument(\"-C\", \"--comment-off\", default=\"//\",\n\t\thelp=\"Set the comment character to use when disabling.\")\n\n\tsubparsers = parser.add_subparsers()\n\tbuild_enable_parser(subparsers)\n\tbuild_disable_parser(subparsers)\n\tbuild_list_parser(subparsers)\n\treturn parser, subparsers\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\t\tA program for quickly finding the best-matching subsequence in a\n\t\t\tsequence database.\n\t\t\"\"\",\n\t\tformatter_class=argparse.RawDescriptionHelpFormatter,\n\t)\n\tsubparser = parser.add_subparsers()\n\tblast_parser = subparser.add_parser(\n\t\t\"blast\",\n\t\thelp=\"Use the BLAST algorithm to find the best-matching subsequence\",\n\t)\n\tsubparser.add_parser(\n\t\t\"perfect-match\",\n\t\thelp=\"Find the perfect match of the subsequence in the database\",\n\t)\n\tsubparser.add_parser(\n\t\t\"trivial\",\n\t\thelp=\"Find the best-matching subsequence in the database by brute force\",\n\t)\n\tsubparser.add_parser(\"trivial-parallel\")\n\tsubparser.add_parser(\"trivial-parallel-local\")\n\tsubparser.add_parser(\"trivial-parallel-global\")\n\tsubparser.add\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t# Build the top-level parser\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"A tool to query the UMLS and generate a hypernym graph\",\n\t\tformatter_class=argparse.RawTextHelpFormatter)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s \" + __version__)\n\tparser.add_argument(\n\t\t\"--umls-user\",\n\t\ttype=str,\n\t\trequired=True,\n\t\thelp=\"UMLS username\")\n\tparser.add_argument(\n\t\t\"--umls-password\",\n\t\ttype=str,\n\t\trequired=True,\n\t\thelp=\"UMLS password\")\n\tparser.add_argument(\n\t\t\"--umls-host\",\n\t\ttype=str,\n\t\tdefault=\"https://uts-ws.nlm.nih.gov\",\n\t\thelp=\"UMLS host name (default: %(default)s)\")\n\n\t# Build the subparsers\n\tsubparsers\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(description=\"A tool for diffing two JSON files.\")\n\tsubparsers = top_parser.add_subparsers(title=\"actions\", dest=\"action\")\n\n\tparser_diff = subparsers.add_parser(\"diff\", help=\"diff two JSON files\")\n\tparser_diff.add_argument(\"left\", type=str, help=\"the left JSON file\")\n\tparser_diff.add_argument(\"right\", type=str, help=\"the right JSON file\")\n\tparser_diff.add_argument(\"--key\", type=str, help=\"only diff the given key, if any\")\n\n\tparser_patch = subparsers.add_parser(\"patch\", help=\"patch a JSON file\")\n\tparser_patch.add_argument(\"file\", type=str, help=\"the JSON file to patch\")\n\tparser_patch.add_argument(\"patch\", type=str, help=\"the diff file\")\n\n\treturn top_parser, subparsers\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(prog=\"pyhashcat\")\n\tsubparsers = top_parser.add_subparsers(title=\"subcommands\", dest=\"subparser_name\")\n\n\t# Add the `hashcat` subcommand parser.\n\thashcat_parser = subparsers.add_parser(\"hashcat\", help=\"Run hashcat.\")\n\thashcat_parser.set_defaults(func=run_hashcat)\n\thashcat_parser.add_argument(\"hash\", type=str, help=\"Path to the file containing the hashes to crack.\")\n\thashcat_parser.add_argument(\"wordlist\", type=str, help=\"Path to the file containing the list of words to try.\")\n\thashcat_parser.add_argument(\"--rules\", type=str, help=\"Path to the file containing the rules to apply.\")\n\thashcat_parser.add_argument(\"--output\", type=str, help=\"Path to the file to write the cracked hashes to.\")\n\thashcat_parser.add_argument(\"--format\", type=str, help=\"Hash format. See hashcat documentation for details\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\tparser = argparse.ArgumentParser(\n\t\tdescription=\"\"\"\n\t\tA tool for managing and deploying software-defined networks in OpenStack\n\t\t\"\"\"\n\t)\n\tparser.add_argument(\n\t\t\"--debug\",\n\t\taction=\"store_true\",\n\t\thelp=\"\"\"\n\t\tPrint debugging information.\n\t\t\"\"\"\n\t)\n\tparser.add_argument(\n\t\t\"--verbose\",\n\t\taction=\"store_true\",\n\t\thelp=\"\"\"\n\t\tPrint verbose information.\n\t\t\"\"\"\n\t)\n\tparser.add_argument(\n\t\t\"--version\",\n\t\taction=\"version\",\n\t\tversion=\"%(prog)s {}\".format(__version__),\n\t\thelp=\"Print tool version.\"\n\t)\n\tparser.add_argument(\n\t\t\"--config-file\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"\"\"\n\t\tPath to the configuration file.\n\t\t\"\"\"\n\t)\n\tsubparsers = parser.add_subparsers(\n\t\thelp=\"\"\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\ttop_parser = argparse.ArgumentParser(\n\t\tdescription='A tool for managing your python projects.')\n\ttop_parser.add_argument(\n\t\t'-v', '--verbose', action='store_true', help='increase verbosity')\n\ttop_parser.add_argument(\n\t\t'-q', '--quiet', action='store_true', help='decrease verbosity')\n\ttop_parser.add_argument(\n\t\t'-V', '--version', action='version', version=__version__,\n\t\thelp='show program\\'s version number and exit')\n\n\tsubparsers = top_parser.add_subparsers()\n\n\t# create the parser for the \"init\" command\n\tparser_init = subparsers.add_parser(\n\t\t'init', help='initialize project')\n\tparser_init.add_argument(\n\t\t'-t', '--template', default='default',\n\t\thelp='name of template to use (default: default)')\n\tparser_init.add_argument(\n\t\t'-f', '--force', action='store\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_parsers():\n\t# Top-level parser\n\tparser = argparse.ArgumentParser(description=\"Evaluate a model's performance with respect to a specified dataset.\")\n\tparser.add_argument(\"-m\", \"--model\", help=\"Path to the model file to evaluate.\", required=True)\n\tparser.add_argument(\"-d\", \"--dataset\", help=\"Path to the dataset file to evaluate.\", required=True)\n\tparser.add_argument(\"-e\", \"--embedding\", help=\"Path to the embedding file to use.\", required=True)\n\tparser.add_argument(\"-t\", \"--task\", help=\"Task to evaluate on.\", choices=[\"pos\", \"ner\"], default=\"pos\")\n\tparser.add_argument(\"-r\", \"--report\", help=\"Path to the report file that will be generated.\", required=True)\n\t# Subparsers\n\tsubparsers = parser.add_subparsers(dest=\"task\")\n\t# POS tagging subparser\n\tpos_parser = subparsers.add_parser(\"pos\", help=\"Evaluate POS tagging performance.\")\n\tpos_parser.add_argument\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_10.py", "name": "paging", "docstring": "Returns WAPI response page by page\n\nArgs:\n    response (list): WAPI response.\n    max_results (int): Maximum number of objects to be returned in one page.\nReturns:\n    Generator object with WAPI response split page by page.", "ques_id": "62ece4982e6aefcf4aabbd5f", "code": ["def paging(response, max_results):\n\tif response:\n\t\ttotal_objects = len(response)\n\t\tremaining_objects = total_objects\n\t\tnext_start = 0\n\t\twhile remaining_objects > 0:\n\t\t\tif remaining_objects >= max_results:\n\t\t\t\tyield response[next_start:next_start+max_results]\n\t\t\telse:\n\t\t\t\tyield response[next_start:]\n\t\t\tnext_start += max_results\n\t\t\tremaining_objects -= max_results\n\n\n", "def paging(response, max_results):\n\t# Extract the total number of objects returned\n\ttotal_objects = response.total_objects\n\t# Extract the number of objects returned in the current page\n\tcurrent_objects = len(response)\n\t# Calculate the number of pages needed to display all objects\n\tpages = total_objects // max_results\n\t# Calculate the number of objects in the last page\n\tremaining_objects = total_objects % max_results\n\n\t# If the number of objects returned in the current page is\n\t# less than the max_results, there is no need to page\n\tif current_objects < max_results:\n\t\tyield response\n\telse:\n\t\t# Generate a list of page numbers\n\t\tpages_list = range(pages + 1)\n\t\t# Loop through the pages\n\t\tfor page in pages_list:\n\t\t\t# If it's the last page and there is a remainder\n\t\t\t# return the remaining number of objects\n\t\t\tif page == pages and remaining_objects > 0:\n\t\t\t\tmax_results = remaining_objects\n\n\t\t\t# If the number of objects in the current page is\n", "def paging(response, max_results):\n\tif 'total_pages' in response[0]:\n\t\ttotal_pages = int(response[0]['total_pages'])\n\t\tfor page in range(1, total_pages + 1):\n\t\t\tresponse = wapi_request(\"GET\", object=object_type, params={\"_max_results\": max_results, \"_page\": page})\n\t\t\tyield response\n\telse:\n\t\tyield response\n\n", "def paging(response, max_results):\n\tnext_start = 0\n\twhile next_start < len(response):\n\t\tyield response[next_start:next_start+max_results]\n\t\tnext_start += max_results\n\n\n", "def paging(response, max_results):\n\tpage_number = 0\n\twhile True:\n\t\tpage = response[page_number:page_number + max_results]\n\t\tpage_number += max_results\n\t\tif not page:\n\t\t\tbreak\n\t\tyield page\n\n\n", "def paging(response, max_results):\n\twhile True:\n\t\tpage = response.json()\n\t\tif 'next_page' in page:\n\t\t\tresponse = requests.get(page['next_page'], verify=False)\n\t\t\tfor obj in page['objects']:\n\t\t\t\tyield obj\n\t\telse:\n\t\t\tfor obj in page['objects']:\n\t\t\t\tyield obj\n\t\t\tbreak\n\n", "def paging(response, max_results):\n\tpage = 0\n\n\twhile len(response) > 0:\n\t\tpage += 1\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\n\n", "def paging(response, max_results):\n\t# TODO: Add test for this\n\t# TODO: Add documentation for this\n\t# TODO: Add pagination for other methods\n\tresults = 0\n\twhile results < len(response):\n\t\tif len(response) - results < max_results:\n\t\t\tyield response[results:]\n\t\telse:\n\t\t\tyield response[results:results + max_results]\n\t\tresults += max_results\n\n\n", "def paging(response, max_results):\n\tif int(max_results) < 1:\n\t\traise Exception('max_results must be positive integer')\n\tpage = 1\n\twhile len(response) > 0:\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\t\tpage += 1\n\n", "def paging(response, max_results):\n\ttotal_pages = int(response.get('total_pages'))\n\tfor page in range(1, total_pages + 1):\n\t\tlog.debug('Paging to page %s of %s', page, total_pages)\n\t\tresponse = client.call('/wapi/v2.7/' + response.get('_ref') + '?_return_fields%2B=total_pages&_return_as_object=1&_max_results=' + str(max_results) + '&_page=' + str(page))\n\t\tyield response\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def paging(response, max_results):\n\tif response:\n\t\ttotal_objects = len(response)\n\t\tremaining_objects = total_objects\n\t\tnext_start = 0\n\t\twhile remaining_objects > 0:\n\t\t\tif remaining_objects >= max_results:\n\t\t\t\tyield response[next_start:next_start+max_results]\n\t\t\telse:\n\t\t\t\tyield response[next_start:]\n\t\t\tnext_start += max_results\n\t\t\tremaining_objects -= max_results\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t# Extract the total number of objects returned\n\ttotal_objects = response.total_objects\n\t# Extract the number of objects returned in the current page\n\tcurrent_objects = len(response)\n\t# Calculate the number of pages needed to display all objects\n\tpages = total_objects // max_results\n\t# Calculate the number of objects in the last page\n\tremaining_objects = total_objects % max_results\n\n\t# If the number of objects returned in the current page is\n\t# less than the max_results, there is no need to page\n\tif current_objects < max_results:\n\t\tyield response\n\telse:\n\t\t# Generate a list of page numbers\n\t\tpages_list = range(pages + 1)\n\t\t# Loop through the pages\n\t\tfor page in pages_list:\n\t\t\t# If it's the last page and there is a remainder\n\t\t\t# return the remaining number of objects\n\t\t\tif page == pages and remaining_objects > 0:\n\t\t\t\tmax_results = remaining_objects\n\n\t\t\t# If the number of objects in the current page is\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\tif 'total_pages' in response[0]:\n\t\ttotal_pages = int(response[0]['total_pages'])\n\t\tfor page in range(1, total_pages + 1):\n\t\t\tresponse = wapi_request(\"GET\", object=object_type, params={\"_max_results\": max_results, \"_page\": page})\n\t\t\tyield response\n\telse:\n\t\tyield response\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\tnext_start = 0\n\twhile next_start < len(response):\n\t\tyield response[next_start:next_start+max_results]\n\t\tnext_start += max_results\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\tpage_number = 0\n\twhile True:\n\t\tpage = response[page_number:page_number + max_results]\n\t\tpage_number += max_results\n\t\tif not page:\n\t\t\tbreak\n\t\tyield page\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\twhile True:\n\t\tpage = response.json()\n\t\tif 'next_page' in page:\n\t\t\tresponse = requests.get(page['next_page'], verify=False)\n\t\t\tfor obj in page['objects']:\n\t\t\t\tyield obj\n\t\telse:\n\t\t\tfor obj in page['objects']:\n\t\t\t\tyield obj\n\t\t\tbreak\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def paging(response, max_results):\n\tpage = 0\n\n\twhile len(response) > 0:\n\t\tpage += 1\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\t# TODO: Add test for this\n\t# TODO: Add documentation for this\n\t# TODO: Add pagination for other methods\n\tresults = 0\n\twhile results < len(response):\n\t\tif len(response) - results < max_results:\n\t\t\tyield response[results:]\n\t\telse:\n\t\t\tyield response[results:results + max_results]\n\t\tresults += max_results\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\tif int(max_results) < 1:\n\t\traise Exception('max_results must be positive integer')\n\tpage = 1\n\twhile len(response) > 0:\n\t\tyield response[:max_results]\n\t\tresponse = response[max_results:]\n\t\tpage += 1\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def paging(response, max_results):\n\ttotal_pages = int(response.get('total_pages'))\n\tfor page in range(1, total_pages + 1):\n\t\tlog.debug('Paging to page %s of %s', page, total_pages)\n\t\tresponse = client.call('/wapi/v2.7/' + response.get('_ref') + '?_return_fields%2B=total_pages&_return_as_object=1&_max_results=' + str(max_results) + '&_page=' + str(page))\n\t\tyield response\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_20.py", "name": "size_to_bytes", "docstring": "Convert human readable file size to bytes.\n\nResulting value is an approximation as input value is in most case rounded.\n\nArgs:\n    size: A string representing a human readable file size (eg: '500K')\n\nReturns:\n    A decimal representation of file size\n\n    Examples::\n\n        >>> size_to_bytes(\"500\")\n        500\n        >>> size_to_bytes(\"1K\")\n        1000", "ques_id": "62ece4982e6aefcf4aabbd60", "code": ["def size_to_bytes(size: str) -> int:\n\tif not size:\n\t\treturn 0\n\n\tif size.isdigit():\n\t\treturn int(size)\n\n\tmatches = re.match(r'^(?P<num>\\d+)(?P<unit>[BKMG]{0,1})$', size.upper())\n\tif matches:\n\t\tunit = matches.group('unit')\n\t\tnum = int(matches.group('num'))\n\t\tif unit == 'G':\n\t\t\treturn num * 1024 * 1024 * 1024\n\t\telif unit == 'M':\n\t\t\treturn num * 1024 * 1024\n\t\telif unit == 'K':\n\t\t\treturn num * 1024\n\t\telif unit == 'B':\n\t\t\treturn num\n\t\telse:\n\t\t\treturn num\n\telse:\n\t\treturn 0\n\n\n", "def size_to_bytes(size: str) -> int:\n\t# check if size is a number\n\tif size.isdigit():\n\t\treturn int(size)\n\n\t# get all possible suffixes\n\tsuffixes = {\n\t\t'K': 2**10,\n\t\t'M': 2**20,\n\t\t'G': 2**30,\n\t\t'T': 2**40\n\t}\n\n\t# get the last character of the string which will be the suffix\n\tsuffix = size[-1]\n\n\t# check if suffix is valid\n\tif suffix not in suffixes.keys():\n\t\traise ValueError('Invalid input `size`.')\n\n\t# remove the suffix from the size string\n\tsize = size[:-1]\n\n\t# check if size is a number\n\tif not size.isdigit():\n\t\traise ValueError('Invalid input `size`.')\n\n\t# convert size to integer\n\tsize = int(size)\n\n\t# return the size in bytes\n\treturn size * suffixes[suffix]\n\n", "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif size.endswith('B'):\n\t\tsize = size[:-1]\n\tif size.isdigit():\n\t\treturn int(size)\n\telif size.endswith('K'):\n\t\treturn int(size[:-1]) * 1024\n\telif size.endswith('M'):\n\t\treturn int(size[:-1]) * 1024 * 1024\n\telif size.endswith('G'):\n\t\treturn int(size[:-1]) * 1024 * 1024 * 1024\n\telif size.endswith('T'):\n\t\treturn int(size[:-1]) * 1024 * 1024 * 1024 * 1024\n\telse:\n\t\traise InvalidArgument('Invalid file size: %s' % size)\n\n\n", "def size_to_bytes(size: str) -> int:\n\tif size.isdigit():\n\t\treturn int(size)\n\tunits = ['K', 'M', 'G', 'T']\n\tunit_base = 1024\n\tfor unit in units:\n\t\tif size.endswith(unit):\n\t\t\treturn int(size[:-1]) * unit_base\n\t\tunit_base *= 1024\n\traise ValueError('Invalid size: %s' % size)\n\n\n", "def size_to_bytes(size: str) -> int:\n\tunits = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024**2,\n\t\t'G': 1024**3,\n\t\t'T': 1024**4,\n\t\t'P': 1024**5,\n\t\t'E': 1024**6,\n\t\t'Z': 1024**7,\n\t\t'Y': 1024**8\n\t}\n\tif size[-1] not in units.keys():\n\t\traise ValueError('Invalid size format: {}'.format(size))\n\treturn int(float(size[:-1]) * units[size[-1]])\n\n\n", "def size_to_bytes(size: str) -> int:\n\tmatch = re.match(r'^(\\d+)(\\w)$', size)\n\tif match is None:\n\t\traise ValueError('Invalid size: {}'.format(size))\n\tnumber, unit = match.groups()\n\tnumber = int(number)\n\tif unit == 'K':\n\t\treturn number * 1024\n\telif unit == 'M':\n\t\treturn number * 1024 * 1024\n\telse:\n\t\traise ValueError('Invalid size unit: {}'.format(unit))\n\n\n", "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif not re.match(r' ', size):\n\t\tsize = re.sub(r'([KMGT])', r' \\1B', size)\n\tnumber, unit = [string.strip() for string in size.split()]\n\tmagnitude = {'B': 1, 'K': 1024, 'M': 1024**2, 'G': 1024**3, 'T': 1024**4}\n\treturn int(float(number) * magnitude[unit])\n\n", "def size_to_bytes(size: str) -> int:\n\treturn int(size_to_bytes_str(size))\n\n\n", "def size_to_bytes(size: str) -> int:\n\tsize_prefixes = [\"K\", \"M\", \"G\", \"T\"]\n\tsize_multipliers = [1024, 1024 * 1024, 1024 * 1024 * 1024, 1024 * 1024 * 1024 * 1024]\n\tprefix = size[-1]\n\tsize_multiplier = 1\n\tsize_number = size[:-1]\n\tif prefix in size_prefixes:\n\t\tsize_multiplier = size_multipliers[size_prefixes.index(prefix)]\n\t\tsize_number = int(size_number)\n\t\treturn size_number * size_multiplier\n\telse:\n\t\treturn int(size)\n\n\n", "def size_to_bytes(size: str) -> int:\n\tif size.isdigit():\n\t\treturn int(size)\n\tsize_bytes = 0\n\tsize_name = size[-1].upper()\n\tif size_name == \"K\":\n\t\tsize_bytes = float(size[:-1]) * 1024  # Kilobytes\n\telif size_name == \"M\":\n\t\tsize_bytes = float(size[:-1]) * 1024 ** 2  # Megabytes\n\telif size_name == \"G\":\n\t\tsize_bytes = float(size[:-1]) * 1024 ** 3  # Gigabytes\n\telif size_name == \"T\":\n\t\tsize_bytes = float(size[:-1]) * 1024 ** 4  # Terabytes\n\treturn int(size_bytes)\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def size_to_bytes(size: str) -> int:\n\tif not size:\n\t\treturn 0\n\n\tif size.isdigit():\n\t\treturn int(size)\n\n\tmatches = re.match(r'^(?P<num>\\d+)(?P<unit>[BKMG]{0,1})$', size.upper())\n\tif matches:\n\t\tunit = matches.group('unit')\n\t\tnum = int(matches.group('num'))\n\t\tif unit == 'G':\n\t\t\treturn num * 1024 * 1024 * 1024\n\t\telif unit == 'M':\n\t\t\treturn num * 1024 * 1024\n\t\telif unit == 'K':\n\t\t\treturn num * 1024\n\t\telif unit == 'B':\n\t\t\treturn num\n\t\telse:\n\t\t\treturn num\n\telse:\n\t\treturn 0\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\t# check if size is a number\n\tif size.isdigit():\n\t\treturn int(size)\n\n\t# get all possible suffixes\n\tsuffixes = {\n\t\t'K': 2**10,\n\t\t'M': 2**20,\n\t\t'G': 2**30,\n\t\t'T': 2**40\n\t}\n\n\t# get the last character of the string which will be the suffix\n\tsuffix = size[-1]\n\n\t# check if suffix is valid\n\tif suffix not in suffixes.keys():\n\t\traise ValueError('Invalid input `size`.')\n\n\t# remove the suffix from the size string\n\tsize = size[:-1]\n\n\t# check if size is a number\n\tif not size.isdigit():\n\t\traise ValueError('Invalid input `size`.')\n\n\t# convert size to integer\n\tsize = int(size)\n\n\t# return the size in bytes\n\treturn size * suffixes[suffix]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif size.endswith('B'):\n\t\tsize = size[:-1]\n\tif size.isdigit():\n\t\treturn int(size)\n\telif size.endswith('K'):\n\t\treturn int(size[:-1]) * 1024\n\telif size.endswith('M'):\n\t\treturn int(size[:-1]) * 1024 * 1024\n\telif size.endswith('G'):\n\t\treturn int(size[:-1]) * 1024 * 1024 * 1024\n\telif size.endswith('T'):\n\t\treturn int(size[:-1]) * 1024 * 1024 * 1024 * 1024\n\telse:\n\t\traise InvalidArgument('Invalid file size: %s' % size)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tif size.isdigit():\n\t\treturn int(size)\n\tunits = ['K', 'M', 'G', 'T']\n\tunit_base = 1024\n\tfor unit in units:\n\t\tif size.endswith(unit):\n\t\t\treturn int(size[:-1]) * unit_base\n\t\tunit_base *= 1024\n\traise ValueError('Invalid size: %s' % size)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tunits = {\n\t\t'B': 1,\n\t\t'K': 1024,\n\t\t'M': 1024**2,\n\t\t'G': 1024**3,\n\t\t'T': 1024**4,\n\t\t'P': 1024**5,\n\t\t'E': 1024**6,\n\t\t'Z': 1024**7,\n\t\t'Y': 1024**8\n\t}\n\tif size[-1] not in units.keys():\n\t\traise ValueError('Invalid size format: {}'.format(size))\n\treturn int(float(size[:-1]) * units[size[-1]])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tmatch = re.match(r'^(\\d+)(\\w)$', size)\n\tif match is None:\n\t\traise ValueError('Invalid size: {}'.format(size))\n\tnumber, unit = match.groups()\n\tnumber = int(number)\n\tif unit == 'K':\n\t\treturn number * 1024\n\telif unit == 'M':\n\t\treturn number * 1024 * 1024\n\telse:\n\t\traise ValueError('Invalid size unit: {}'.format(unit))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize = size.upper()\n\tif not re.match(r' ', size):\n\t\tsize = re.sub(r'([KMGT])', r' \\1B', size)\n\tnumber, unit = [string.strip() for string in size.split()]\n\tmagnitude = {'B': 1, 'K': 1024, 'M': 1024**2, 'G': 1024**3, 'T': 1024**4}\n\treturn int(float(number) * magnitude[unit])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\treturn int(size_to_bytes_str(size))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tsize_prefixes = [\"K\", \"M\", \"G\", \"T\"]\n\tsize_multipliers = [1024, 1024 * 1024, 1024 * 1024 * 1024, 1024 * 1024 * 1024 * 1024]\n\tprefix = size[-1]\n\tsize_multiplier = 1\n\tsize_number = size[:-1]\n\tif prefix in size_prefixes:\n\t\tsize_multiplier = size_multipliers[size_prefixes.index(prefix)]\n\t\tsize_number = int(size_number)\n\t\treturn size_number * size_multiplier\n\telse:\n\t\treturn int(size)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def size_to_bytes(size: str) -> int:\n\tif size.isdigit():\n\t\treturn int(size)\n\tsize_bytes = 0\n\tsize_name = size[-1].upper()\n\tif size_name == \"K\":\n\t\tsize_bytes = float(size[:-1]) * 1024  # Kilobytes\n\telif size_name == \"M\":\n\t\tsize_bytes = float(size[:-1]) * 1024 ** 2  # Megabytes\n\telif size_name == \"G\":\n\t\tsize_bytes = float(size[:-1]) * 1024 ** 3  # Gigabytes\n\telif size_name == \"T\":\n\t\tsize_bytes = float(size[:-1]) * 1024 ** 4  # Terabytes\n\treturn int(size_bytes)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_14.py", "name": "_dictsum", "docstring": "Combine values of the dictionaries supplied by iterable dicts.\n\n>>> _dictsum([{'a': 1, 'b': 2}, {'a': 5, 'b': 0}])\n{'a': 6, 'b': 2}", "ques_id": "62ece4982e6aefcf4aabbd61", "code": ["def _dictsum(dicts):\n\tif not dicts:\n\t\treturn {}\n\t# convert all dicts to Counter\n\tdicts = [Counter(d) for d in dicts]\n\t# Sum the items in the dictionary\n\tdicts = sum(dicts, Counter())\n\treturn dicts\n\n\n", "def _dictsum(dicts):\n\tdicts = [d for d in dicts if d]\n\tresult = dicts[0]\n\n\tfor d in dicts[1:]:\n\t\tfor key, value in d.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] += value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\n\treturn result\n\n", "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k in result:\n\t\t\t\tresult[k] += d[k]\n\t\t\telse:\n\t\t\t\tresult[k] = d[k]\n\treturn result\n\n\n", "def _dictsum(dicts):\n\treturn reduce(lambda x, y: dict(Counter(x) + Counter(y)), dicts)\n\n\n", "def _dictsum(dicts):\n\tdict_res = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dict_res:\n\t\t\t\tdict_res[k] += v\n\t\t\telse:\n\t\t\t\tdict_res[k] = v\n\treturn dict_res\n\n\n", "def _dictsum(dicts):\n\tnewDict = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in newDict:\n\t\t\t\tnewDict[k] += v\n\t\t\telse:\n\t\t\t\tnewDict[k] = v\n\treturn newDict\n\n", "def _dictsum(dicts):\n\tnew_dict = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tnew_dict[k] = new_dict.get(k, 0) + d[k]\n\treturn new_dict\n\n", "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] += value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n\n", "def _dictsum(dicts):\n\tif len(dicts) == 1:\n\t\treturn dicts[0]\n\telse:\n\t\tdic = {}\n\t\tfor d in dicts:\n\t\t\tfor k,v in d.items():\n\t\t\t\tif k in dic:\n\t\t\t\t\tdic[k] = dic[k] + v\n\t\t\t\telse:\n\t\t\t\t\tdic[k] = v\n\t\treturn dic\n\n", "def _dictsum(dicts):\n\tret = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k not in ret:\n\t\t\t\tret[k] = v\n\t\t\telse:\n\t\t\t\tret[k] += v\n\treturn ret\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _dictsum(dicts):\n\tif not dicts:\n\t\treturn {}\n\t# convert all dicts to Counter\n\tdicts = [Counter(d) for d in dicts]\n\t# Sum the items in the dictionary\n\tdicts = sum(dicts, Counter())\n\treturn dicts\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\tdicts = [d for d in dicts if d]\n\tresult = dicts[0]\n\n\tfor d in dicts[1:]:\n\t\tfor key, value in d.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] += value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tif k in result:\n\t\t\t\tresult[k] += d[k]\n\t\t\telse:\n\t\t\t\tresult[k] = d[k]\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\treturn reduce(lambda x, y: dict(Counter(x) + Counter(y)), dicts)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dictsum(dicts):\n\tdict_res = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in dict_res:\n\t\t\t\tdict_res[k] += v\n\t\t\telse:\n\t\t\t\tdict_res[k] = v\n\treturn dict_res\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tnewDict = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k in newDict:\n\t\t\t\tnewDict[k] += v\n\t\t\telse:\n\t\t\t\tnewDict[k] = v\n\treturn newDict\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tnew_dict = {}\n\tfor d in dicts:\n\t\tfor k in d:\n\t\t\tnew_dict[k] = new_dict.get(k, 0) + d[k]\n\treturn new_dict\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tresult = {}\n\tfor d in dicts:\n\t\tfor key, value in d.items():\n\t\t\tif key in result:\n\t\t\t\tresult[key] += value\n\t\t\telse:\n\t\t\t\tresult[key] = value\n\treturn result\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tif len(dicts) == 1:\n\t\treturn dicts[0]\n\telse:\n\t\tdic = {}\n\t\tfor d in dicts:\n\t\t\tfor k,v in d.items():\n\t\t\t\tif k in dic:\n\t\t\t\t\tdic[k] = dic[k] + v\n\t\t\t\telse:\n\t\t\t\t\tdic[k] = v\n\t\treturn dic\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dictsum(dicts):\n\tret = {}\n\tfor d in dicts:\n\t\tfor k, v in d.items():\n\t\t\tif k not in ret:\n\t\t\t\tret[k] = v\n\t\t\telse:\n\t\t\t\tret[k] += v\n\treturn ret\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_15.py", "name": "_replace_url_args", "docstring": "Replace any custom string URL items with values in args", "ques_id": "62ece4982e6aefcf4aabbd62", "code": ["def _replace_url_args(url, url_args):\n\tif url_args is not None:\n\t\tfor key, val in url_args.iteritems():\n\t\t\tif val is not None:\n\t\t\t\turl = url.replace('{' + key + '}', val)\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace('{' + key + '}', str(value))\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\tif value:\n\t\t\t\turl = url.replace(key, value)\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, str(value))\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tfor key, value in url_args.items():\n\t\turl = url.replace('{' + key + '}', str(value))\n\n\treturn url\n\n", "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, val in url_args.items():\n\t\t\tif val:\n\t\t\t\turl = url.replace('{' + key + '}', val)\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif url_args is None:\n\t\treturn url\n\tfor key, value in url_args.items():\n\t\tif value is not None:\n\t\t\turl = url.replace('{%s}' % key, str(value))\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace('{' + key + '}', value)\n\treturn url\n\n\n", "def _replace_url_args(url, url_args):\n\tif not url_args:\n\t\treturn url\n\tfor k, v in url_args.items():\n\t\turl = url.replace('<{}>'.format(k), str(v))\n\treturn url\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args is not None:\n\t\tfor key, val in url_args.iteritems():\n\t\t\tif val is not None:\n\t\t\t\turl = url.replace('{' + key + '}', val)\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace('{' + key + '}', str(value))\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, value)\n\treturn url\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\tif value:\n\t\t\t\turl = url.replace(key, value)\n\treturn url\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace(key, str(value))\n\treturn url\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _replace_url_args(url, url_args):\n\tfor key, value in url_args.items():\n\t\turl = url.replace('{' + key + '}', str(value))\n\n\treturn url\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, val in url_args.items():\n\t\t\tif val:\n\t\t\t\turl = url.replace('{' + key + '}', val)\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args is None:\n\t\treturn url\n\tfor key, value in url_args.items():\n\t\tif value is not None:\n\t\t\turl = url.replace('{%s}' % key, str(value))\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif url_args:\n\t\tfor key, value in url_args.items():\n\t\t\turl = url.replace('{' + key + '}', value)\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_url_args(url, url_args):\n\tif not url_args:\n\t\treturn url\n\tfor k, v in url_args.items():\n\t\turl = url.replace('<{}>'.format(k), str(v))\n\treturn url\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_5.py", "name": "is_none_string", "docstring": "Check if a string represents a None value.", "ques_id": "62ece4982e6aefcf4aabbd63", "code": ["def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\t# if the type of val is string and the lowercase of val is equal to 'none', return True\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn type(val) == str and val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn is_string(val) and val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\tif isinstance(val, str) and val.lower() == 'none':\n\t\treturn True\n\treturn False\n\n", "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\t# if the type of val is string and the lowercase of val is equal to 'none', return True\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn type(val) == str and val.lower() == 'none'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn is_string(val) and val.lower() == 'none'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_none_string(val: any) -> bool:\n\tif isinstance(val, str) and val.lower() == 'none':\n\t\treturn True\n\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_none_string(val: any) -> bool:\n\treturn isinstance(val, str) and val.lower() == 'none'\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_1.py", "name": "strip_root", "docstring": "Remove root from path, throw exception on failure.", "ques_id": "62ece4982e6aefcf4aabbd64", "code": ["def strip_root(path, root):\n\tif root == '':\n\t\treturn path\n\tif not path.startswith(root):\n\t\traise Exception(\"Invalid path: %s does not start with %s\" % (path, root))\n\treturn path[len(root):]\n\n\n", "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise ValueError(\"Path does not start with root\")\n\n\n", "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise Exception(\n\t\t\"Root %r is not in path %r\" % (root, path))\n", "def strip_root(path, root):\n\tif path.startswith(root):\n\t\tpath = path[len(root):]\n\t\tif path.startswith(os.path.sep):\n\t\t\treturn path[1:]\n\t\telse:\n\t\t\treturn path\n\telse:\n\t\traise Exception(\"Path '{}' does not start with root '{}'\".format(path, root))\n\n", "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise Exception(\"Path %s does not start with root %s\" % (path, root))\n\treturn path[len(root):]\n\n\n", "def strip_root(path, root):\n\treturn os.path.relpath(path, root)\n\n", "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise exceptions.PathNotInRoot(\"{} is not in {}\".format(path, root))\n\treturn path[len(root):]\n\n", "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise RuntimeError(\"Path %s does not start with root %s\" % (path, root))\n\n\n", "def strip_root(path, root):\n\ttry:\n\t\treturn path.split(root, 1)[1]\n\texcept:\n\t\traise Exception(\"Path %s does not start with %s\" % (path, root))\n\n", "def strip_root(path, root):\n\tif path == root:\n\t\treturn ''\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise Exception(\"Path %s is not in root %s\" % (path, root))\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def strip_root(path, root):\n\tif root == '':\n\t\treturn path\n\tif not path.startswith(root):\n\t\traise Exception(\"Invalid path: %s does not start with %s\" % (path, root))\n\treturn path[len(root):]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\telse:\n\t\traise ValueError(\"Path does not start with root\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise Exception(\n\t\t\"Root %r is not in path %r\" % (root, path))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path.startswith(root):\n\t\tpath = path[len(root):]\n\t\tif path.startswith(os.path.sep):\n\t\t\treturn path[1:]\n\t\telse:\n\t\t\treturn path\n\telse:\n\t\traise Exception(\"Path '{}' does not start with root '{}'\".format(path, root))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise Exception(\"Path %s does not start with root %s\" % (path, root))\n\treturn path[len(root):]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\treturn os.path.relpath(path, root)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def strip_root(path, root):\n\tif not path.startswith(root):\n\t\traise exceptions.PathNotInRoot(\"{} is not in {}\".format(path, root))\n\treturn path[len(root):]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise RuntimeError(\"Path %s does not start with root %s\" % (path, root))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\ttry:\n\t\treturn path.split(root, 1)[1]\n\texcept:\n\t\traise Exception(\"Path %s does not start with %s\" % (path, root))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def strip_root(path, root):\n\tif path == root:\n\t\treturn ''\n\tif path.startswith(root):\n\t\treturn path[len(root):]\n\traise Exception(\"Path %s is not in root %s\" % (path, root))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_8.py", "name": "parser_flags", "docstring": "Given an argparse.ArgumentParser instance, return its argument flags in a space-separated\nstring.", "ques_id": "62ece4982e6aefcf4aabbd65", "code": ["def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\tflags.append(action.dest)\n\treturn \" \".join(flags)\n\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest == 'help':\n\t\t\tcontinue\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join([f for f in flags if f.startswith('-')])\n\n", "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split(' ')[1:])\n\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(action.option_strings[0])\n\t\telse:\n\t\t\tflags.append(action.dest)\n\n\treturn ' '.join(flags)\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(map(lambda x: x.lstrip('-'), parser._get_optional_actions()))\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split(' ')[1:])\n\n", "def parser_flags(parser):\n\treturn ' '.join(['-%s' % flag for flag in parser._option_string_actions.keys()])\n\n", "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split()[1:-1])\n\n\n", "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split(' ')[1:])\n\n\n", "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\tflags.append(action.dest)\n\treturn \" \".join(flags)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.dest == 'help':\n\t\t\tcontinue\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join([f for f in flags if f.startswith('-')])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split(' ')[1:])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.append(action.option_strings[0])\n\t\telse:\n\t\t\tflags.append(action.dest)\n\n\treturn ' '.join(flags)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(map(lambda x: x.lstrip('-'), parser._get_optional_actions()))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split(' ')[1:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(['-%s' % flag for flag in parser._option_string_actions.keys()])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split()[1:-1])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\treturn ' '.join(parser.format_usage().split(' ')[1:])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parser_flags(parser):\n\tflags = []\n\tfor action in parser._actions:\n\t\tif action.option_strings:\n\t\t\tflags.extend(action.option_strings)\n\t\telse:\n\t\t\tflags.append(action.dest)\n\treturn ' '.join(flags)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_19.py", "name": "was_processed", "docstring": "Check if a file or directory has already been processed.\n\nTo prevent recursion, expand the path name to an absolution path\ncall this function with a set that will store all the entries and\nthe entry to test. If the entry is already in the set, report the issue\nand return ``True``. Otherwise, add the entry to the set and return\n``False`` to allow the path to be processed.\n\nArgs:\n    processed: Set to store processed pathnames\n    path_name: Path to a directory or file\n    verbose: True if verbose output is requested\n\nReturns:\n    True if it's already in the set. False if not.", "ques_id": "62ece4982e6aefcf4aabbd66", "code": ["def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Skipping {0} - already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name) # Add the pathname to the set\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tif os.path.abspath(path_name) in processed:\n\t\tprint(\"{} was already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(os.path.abspath(path_name))\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\t# Check if the pathname has been processed already\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\t# It has been processed already\n\t\tif verbose:\n\t\t\tprint('Already processed %s' % path_name)\n\t\treturn True\n\telse:\n\t\t# It has not been processed yet\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n\n", "def was_processed(processed, path_name, verbose):\n\tabs_path_name = os.path.abspath(path_name)\n\tif abs_path_name in processed:\n\t\tprint(\"{} already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"{} added to processed\".format(path_name))\n\t\tprocessed.add(abs_path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed:\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('already processed', path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('{} was already processed.'.format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n\n", "def was_processed(processed, path_name, verbose):\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint('Skipping {}: already processed'.format(abs_path))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tprint(\"Warning: recursion detected. Skipping\", path_name, file=sys.stderr)\n\t\treturn True\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"Processing\", path_name, file=sys.stderr)\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n\n", "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tprint('{} already processed'.format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\tif verbose:\n\t\t\tprint('processing {}'.format(path_name))\n\t\treturn False\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Skipping {0} - already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name) # Add the pathname to the set\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tif os.path.abspath(path_name) in processed:\n\t\tprint(\"{} was already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(os.path.abspath(path_name))\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\t# Check if the pathname has been processed already\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\t# It has been processed already\n\t\tif verbose:\n\t\t\tprint('Already processed %s' % path_name)\n\t\treturn True\n\telse:\n\t\t# It has not been processed yet\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tabs_path_name = os.path.abspath(path_name)\n\tif abs_path_name in processed:\n\t\tprint(\"{} already processed\".format(path_name))\n\t\treturn True\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"{} added to processed\".format(path_name))\n\t\tprocessed.add(abs_path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint(\"Already processed:\", path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('already processed', path_name)\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tif verbose:\n\t\t\tprint('{} was already processed.'.format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tabs_path = os.path.abspath(path_name)\n\tif abs_path in processed:\n\t\tif verbose:\n\t\t\tprint('Skipping {}: already processed'.format(abs_path))\n\t\treturn True\n\telse:\n\t\tprocessed.add(abs_path)\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tprint(\"Warning: recursion detected. Skipping\", path_name, file=sys.stderr)\n\t\treturn True\n\telse:\n\t\tif verbose:\n\t\t\tprint(\"Processing\", path_name, file=sys.stderr)\n\t\tprocessed.add(path_name)\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def was_processed(processed, path_name, verbose):\n\tpath_name = os.path.abspath(path_name)\n\tif path_name in processed:\n\t\tprint('{} already processed'.format(path_name))\n\t\treturn True\n\telse:\n\t\tprocessed.add(path_name)\n\t\tif verbose:\n\t\t\tprint('processing {}'.format(path_name))\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_9.py", "name": "vertex3tuple", "docstring": "return 3 points for each vertex of the polygon. This will include the vertex and the 2 points on both sides of the vertex::\n\npolygon with vertices ABCD\nWill return\nDAB, ABC, BCD, CDA -> returns 3tuples\n#A    B    C    D  -> of vertices", "ques_id": "62ece4982e6aefcf4aabbd67", "code": ["def vertex3tuple(vertices):\n\ttuple_list = []\n\tfor index in range(len(vertices)):\n\t\t# if the subscript is out of bounds, take the value of index as 0\n\t\tif index == 0:\n\t\t\ttuple_list.append((vertices[index], vertices[index], vertices[index+1]))\n\t\telif index == len(vertices)-1:\n\t\t\ttuple_list.append((vertices[index], vertices[index-1], vertices[0]))\n\t\telse:\n\t\t\ttuple_list.append((vertices[index], vertices[index-1], vertices[index+1]))\n\treturn tuple_list\n\n", "def vertex3tuple(vertices):\n\treturn [(vertices[i], vertices[(i + 1) % len(vertices)], vertices[(i + 2) % len(vertices)]) for i in range(len(vertices))]\n\n\n", "def vertex3tuple(vertices):\n\tresult = []\n\tfor i in range(len(vertices)):\n\t\tresult.append((vertices[i-2], vertices[i-1], vertices[i]))\n\treturn result\n\n", "def vertex3tuple(vertices):\n\tvertex3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertex3tuple.append((vertices[i],vertices[i-1],vertices[i-2]))\n\treturn vertex3tuple\n\n", "def vertex3tuple(vertices):\n\tl = []\n\tfor i in range(len(vertices)):\n\t\tif (i > 1):\n\t\t\tl.append((vertices[i-2], vertices[i-1], vertices[i]))\n\t\telse:\n\t\t\tl.append(vertices[i])\n\treturn l\n\n\n", "def vertex3tuple(vertices):\n\tvertices = copy.deepcopy(vertices)\n\tvertices.append(vertices[0])\n\tvertices.append(vertices[1])\n\treturn zip(vertices, vertices[1:], vertices[2:])\n\n", "def vertex3tuple(vertices):\n\tvertex3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertex3 = []\n\t\tfor j in range(3):\n\t\t\tvertex3.append(vertices[i-1+j])\n\t\tvertex3tuple.append(tuple(vertex3))\n\treturn vertex3tuple\n\n\n", "def vertex3tuple(vertices):\n\tvertices_list = []\n\tvertices_list.append((vertices[-1], vertices[0], vertices[1]))\n\tfor i in range(1, len(vertices)-1):\n\t\tvertices_list.append((vertices[i-1], vertices[i], vertices[i+1]))\n\tvertices_list.append((vertices[-2], vertices[-1], vertices[0]))\n\treturn vertices_list\n\n\n", "def vertex3tuple(vertices):\n\tp = []\n\tfor i in range(len(vertices)):\n\t\tp.append((vertices[i - 1], vertices[i], vertices[(i + 1) % len(vertices)]))\n\treturn p\n\n\n", "def vertex3tuple(vertices):\n\tvertices3 = []\n\tfor i in range(len(vertices)):\n\t\tvertices3.append((vertices[i], vertices[(i - 1) % len(vertices)], vertices[(i + 1) % len(vertices)]))\n\treturn vertices3\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def vertex3tuple(vertices):\n\ttuple_list = []\n\tfor index in range(len(vertices)):\n\t\t# if the subscript is out of bounds, take the value of index as 0\n\t\tif index == 0:\n\t\t\ttuple_list.append((vertices[index], vertices[index], vertices[index+1]))\n\t\telif index == len(vertices)-1:\n\t\t\ttuple_list.append((vertices[index], vertices[index-1], vertices[0]))\n\t\telse:\n\t\t\ttuple_list.append((vertices[index], vertices[index-1], vertices[index+1]))\n\treturn tuple_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\treturn [(vertices[i], vertices[(i + 1) % len(vertices)], vertices[(i + 2) % len(vertices)]) for i in range(len(vertices))]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\tresult = []\n\tfor i in range(len(vertices)):\n\t\tresult.append((vertices[i-2], vertices[i-1], vertices[i]))\n\treturn result\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\tvertex3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertex3tuple.append((vertices[i],vertices[i-1],vertices[i-2]))\n\treturn vertex3tuple\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\tl = []\n\tfor i in range(len(vertices)):\n\t\tif (i > 1):\n\t\t\tl.append((vertices[i-2], vertices[i-1], vertices[i]))\n\t\telse:\n\t\t\tl.append(vertices[i])\n\treturn l\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\tvertices = copy.deepcopy(vertices)\n\tvertices.append(vertices[0])\n\tvertices.append(vertices[1])\n\treturn zip(vertices, vertices[1:], vertices[2:])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\tvertex3tuple = []\n\tfor i in range(len(vertices)):\n\t\tvertex3 = []\n\t\tfor j in range(3):\n\t\t\tvertex3.append(vertices[i-1+j])\n\t\tvertex3tuple.append(tuple(vertex3))\n\treturn vertex3tuple\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def vertex3tuple(vertices):\n\tvertices_list = []\n\tvertices_list.append((vertices[-1], vertices[0], vertices[1]))\n\tfor i in range(1, len(vertices)-1):\n\t\tvertices_list.append((vertices[i-1], vertices[i], vertices[i+1]))\n\tvertices_list.append((vertices[-2], vertices[-1], vertices[0]))\n\treturn vertices_list\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\tp = []\n\tfor i in range(len(vertices)):\n\t\tp.append((vertices[i - 1], vertices[i], vertices[(i + 1) % len(vertices)]))\n\treturn p\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def vertex3tuple(vertices):\n\tvertices3 = []\n\tfor i in range(len(vertices)):\n\t\tvertices3.append((vertices[i], vertices[(i - 1) % len(vertices)], vertices[(i + 1) % len(vertices)]))\n\treturn vertices3\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_6.py", "name": "int_to_string", "docstring": "Convert a number to a string, using the given alphabet.\n\nThe output has the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd68", "code": ["def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tassert number >= 0, \"Number must be positive\"\n\tassert len(alphabet) > 1, \"Alphabet must contain at least 2 characters\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\toutput = alphabet[0] * padding if padding else ''\n\twhile number > 0:\n\t\toutput += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn output[::-1]\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tif number < base:\n\t\tif padding is not None:\n\t\t\treturn alphabet[0] * (padding - 1) + alphabet[number]\n\t\treturn alphabet[number]\n\telse:\n\t\treturn int_to_string(number // base, alphabet, padding - 1 if padding else None) + alphabet[number % base]\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tresult = ''\n\twhile number:\n\t\tresult = alphabet[number % base] + result\n\t\tnumber //= base\n\tif padding is not None:\n\t\tresult = result.rjust(padding, alphabet[0])\n\treturn result\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tassert len(alphabet) > 0\n\tassert number >= 0\n\tassert isinstance(number, int)\n\tassert isinstance(padding, int) or padding is None\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\tif padding is not None:\n\t\tassert number < pow(len(alphabet), padding)\n\tbase = len(alphabet)\n\toutput = \"\"\n\twhile number != 0:\n\t\toutput += alphabet[number % base]\n\t\tnumber = number // base\n\tif padding is not None:\n\t\twhile len(output) < padding:\n\t\t\toutput += alphabet[0]\n\treturn output[::-1]\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif not alphabet:\n\t\traise ValueError('alphabet must not be empty')\n\tif number < 0:\n\t\traise ValueError('number must be non-negative')\n\tif padding is None:\n\t\tpadding = 0\n\telif padding < 0:\n\t\traise ValueError('padding must not be negative')\n\n\tbase = len(alphabet)\n\tresult = ''\n\twhile number:\n\t\tresult = alphabet[number % base] + result\n\t\tnumber //= base\n\n\tif padding:\n\t\tresult = result.rjust(padding, alphabet[0])\n\n\treturn result\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif (number == 0):\n\t\treturn alphabet[0]\n\tarr = []\n\tbase = len(alphabet)\n\twhile number:\n\t\trem = number % base\n\t\tnumber = number // base\n\t\tarr.append(alphabet[rem])\n\tarr.reverse()\n\tout = ''.join(arr)\n\tif (padding != None):\n\t\tout = out.zfill(padding)\n\treturn out\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\tarr = []\n\tbase = len(alphabet)\n\twhile number:\n\t\trem = number % base\n\t\tnumber = number // base\n\t\tarr.append(alphabet[rem])\n\tarr.reverse()\n\treturn ''.join(arr).rjust(padding, alphabet[0])\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\tarr = []\n\tbase = len(alphabet)\n\twhile number:\n\t\tremainder = number % base\n\t\tnumber = number // base\n\t\tarr.append(alphabet[remainder])\n\tarr.reverse()\n\treturn ''.join(arr).ljust(padding, alphabet[0])\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tif padding is None:\n\t\tpadding = 0\n\tresult = \"\"\n\twhile number > 0:\n\t\tdigit = number % base\n\t\tresult = alphabet[digit] + result\n\t\tnumber //= base\n\treturn alphabet[0] * (padding - len(result)) + result\n\n\n", "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\tif padding:\n\t\t\treturn alphabet[0] * padding\n\t\telse:\n\t\t\treturn alphabet[0]\n\n\tbase = len(alphabet)\n\toutput = []\n\twhile number != 0:\n\t\tnumber, digit = divmod(number, base)\n\t\toutput.append(alphabet[digit])\n\n\tif padding:\n\t\toutput.append(alphabet[0] * (padding - len(output)))\n\n\treturn ''.join(reversed(output))\n"], "level": "self_contained", "generate_results": [{"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tassert number >= 0, \"Number must be positive\"\n\tassert len(alphabet) > 1, \"Alphabet must contain at least 2 characters\"\n\tif number == 0:\n\t\treturn alphabet[0]\n\toutput = alphabet[0] * padding if padding else ''\n\twhile number > 0:\n\t\toutput += alphabet[number % len(alphabet)]\n\t\tnumber //= len(alphabet)\n\treturn output[::-1]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tif number < base:\n\t\tif padding is not None:\n\t\t\treturn alphabet[0] * (padding - 1) + alphabet[number]\n\t\treturn alphabet[number]\n\telse:\n\t\treturn int_to_string(number // base, alphabet, padding - 1 if padding else None) + alphabet[number % base]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tresult = ''\n\twhile number:\n\t\tresult = alphabet[number % base] + result\n\t\tnumber //= base\n\tif padding is not None:\n\t\tresult = result.rjust(padding, alphabet[0])\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tassert len(alphabet) > 0\n\tassert number >= 0\n\tassert isinstance(number, int)\n\tassert isinstance(padding, int) or padding is None\n\n\tif number == 0:\n\t\treturn alphabet[0]\n\tif padding is not None:\n\t\tassert number < pow(len(alphabet), padding)\n\tbase = len(alphabet)\n\toutput = \"\"\n\twhile number != 0:\n\t\toutput += alphabet[number % base]\n\t\tnumber = number // base\n\tif padding is not None:\n\t\twhile len(output) < padding:\n\t\t\toutput += alphabet[0]\n\treturn output[::-1]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif not alphabet:\n\t\traise ValueError('alphabet must not be empty')\n\tif number < 0:\n\t\traise ValueError('number must be non-negative')\n\tif padding is None:\n\t\tpadding = 0\n\telif padding < 0:\n\t\traise ValueError('padding must not be negative')\n\n\tbase = len(alphabet)\n\tresult = ''\n\twhile number:\n\t\tresult = alphabet[number % base] + result\n\t\tnumber //= base\n\n\tif padding:\n\t\tresult = result.rjust(padding, alphabet[0])\n\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif (number == 0):\n\t\treturn alphabet[0]\n\tarr = []\n\tbase = len(alphabet)\n\twhile number:\n\t\trem = number % base\n\t\tnumber = number // base\n\t\tarr.append(alphabet[rem])\n\tarr.reverse()\n\tout = ''.join(arr)\n\tif (padding != None):\n\t\tout = out.zfill(padding)\n\treturn out\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\tarr = []\n\tbase = len(alphabet)\n\twhile number:\n\t\trem = number % base\n\t\tnumber = number // base\n\t\tarr.append(alphabet[rem])\n\tarr.reverse()\n\treturn ''.join(arr).rjust(padding, alphabet[0])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\treturn alphabet[0]\n\tarr = []\n\tbase = len(alphabet)\n\twhile number:\n\t\tremainder = number % base\n\t\tnumber = number // base\n\t\tarr.append(alphabet[remainder])\n\tarr.reverse()\n\treturn ''.join(arr).ljust(padding, alphabet[0])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tbase = len(alphabet)\n\tif padding is None:\n\t\tpadding = 0\n\tresult = \"\"\n\twhile number > 0:\n\t\tdigit = number % base\n\t\tresult = alphabet[digit] + result\n\t\tnumber //= base\n\treturn alphabet[0] * (padding - len(result)) + result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def int_to_string(number: int, alphabet: List[str], padding: Optional[int] = None) -> str:\n\tif number == 0:\n\t\tif padding:\n\t\t\treturn alphabet[0] * padding\n\t\telse:\n\t\t\treturn alphabet[0]\n\n\tbase = len(alphabet)\n\toutput = []\n\twhile number != 0:\n\t\tnumber, digit = divmod(number, base)\n\t\toutput.append(alphabet[digit])\n\n\tif padding:\n\t\toutput.append(alphabet[0] * (padding - len(output)))\n\n\treturn ''.join(reversed(output))\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_12.py", "name": "_replace_register", "docstring": "Replace value from flows to given register number\n\n'register_value' key in dictionary will be replaced by register number\ngiven by 'register_number'\n\n:param flow_params: Dictionary containing defined flows\n:param register_number: The number of register where value will be stored\n:param register_value: Key to be replaced by register number", "ques_id": "62ece4982e6aefcf4aabbd69", "code": ["def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tflow_params.pop(register_value, None)\n\treturn flow_params, register_number\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tregister_value = int(register_value)\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tif register_number in flow_params and register_value in flow_params:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\telse:\n\t\traise Exception(\"Register number or value do not exist\")\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tif register_number in flow_params:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\telse:\n\t\tflow_params[register_number] = None\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tif register_number in flow_params:\n\t\tflow_params[register_number] += flow_params[register_value]\n\t\tdel flow_params[register_value]\n\telse:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\n\n", "def _replace_register(flow_params, register_number, register_value):\n\tif type(register_value) == int:\n\t\tflow_params[register_number] = register_value\n\telif type(register_value) == str:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\treturn flow_params\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tflow_params.pop(register_value, None)\n\treturn flow_params, register_number\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tregister_value = int(register_value)\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\treturn flow_params\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tif register_number in flow_params and register_value in flow_params:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\telse:\n\t\traise Exception(\"Register number or value do not exist\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tif register_number in flow_params:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\telse:\n\t\tflow_params[register_number] = None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tflow_params[register_number] = flow_params[register_value]\n\tdel flow_params[register_value]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tif register_number in flow_params:\n\t\tflow_params[register_number] += flow_params[register_value]\n\t\tdel flow_params[register_value]\n\telse:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _replace_register(flow_params, register_number, register_value):\n\tif type(register_value) == int:\n\t\tflow_params[register_number] = register_value\n\telif type(register_value) == str:\n\t\tflow_params[register_number] = flow_params[register_value]\n\t\tdel flow_params[register_value]\n\treturn flow_params\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_2.py", "name": "replace_dots", "docstring": "Replaces all values of '.' to arg from the given string", "ques_id": "62ece4982e6aefcf4aabbd6a", "code": ["def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace(\".\", arg)\n\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace(\".\", arg)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def replace_dots(value, arg):\n\treturn value.replace('.', arg)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "self_contained/test_3.py", "name": "subclasses", "docstring": "Return all subclasses of a class, recursively", "ques_id": "62ece4982e6aefcf4aabbd6b", "code": ["def subclasses(cls):\n\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n\n\n", "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tyield subclass\n\t\tfor subclass in subclasses(subclass):\n\t\t\tyield subclass\n\n", "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tyield subclass\n\t\tfor subclass in subclasses(subclass):\n\t\t\tyield subclass\n\n", "def subclasses(cls):\n\treturn set(cls.__subclasses__()).union(\n\t\t[s for c in cls.__subclasses__() for s in subclasses(c)]\n\t)\n\n", "def subclasses(cls):\n\tall = cls.__subclasses__()\n\tfor sub in list(all):\n\t\tall.extend(subclasses(sub))\n\treturn all\n\n", "def subclasses(cls):\n\tsubclasses = cls.__subclasses__()\n\tfor sub in subclasses:\n\t\tsubclasses.extend(sub.__subclasses__())\n\treturn subclasses\n\n", "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tfor sub in subclasses(subclass):\n\t\t\tyield sub\n\t\tyield subclass\n\n", "def subclasses(cls):\n\tyield cls\n\tfor child in cls.__subclasses__():\n\t\tfor sub in subclasses(child):\n\t\t\tyield sub\n\n\n", "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\n\treturn subs\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def subclasses(cls):\n\treturn cls.__subclasses__() + [g for s in cls.__subclasses__() for g in subclasses(s)]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tyield subclass\n\t\tfor subclass in subclasses(subclass):\n\t\t\tyield subclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tyield subclass\n\t\tfor subclass in subclasses(subclass):\n\t\t\tyield subclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\treturn set(cls.__subclasses__()).union(\n\t\t[s for c in cls.__subclasses__() for s in subclasses(c)]\n\t)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subclasses(cls):\n\tall = cls.__subclasses__()\n\tfor sub in list(all):\n\t\tall.extend(subclasses(sub))\n\treturn all\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tsubclasses = cls.__subclasses__()\n\tfor sub in subclasses:\n\t\tsubclasses.extend(sub.__subclasses__())\n\treturn subclasses\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\treturn subs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tfor subclass in cls.__subclasses__():\n\t\tfor sub in subclasses(subclass):\n\t\t\tyield sub\n\t\tyield subclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tyield cls\n\tfor child in cls.__subclasses__():\n\t\tfor sub in subclasses(child):\n\t\t\tyield sub\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def subclasses(cls):\n\tsubs = cls.__subclasses__()\n\tfor sub in subs:\n\t\tsubs.extend(subclasses(sub))\n\n\treturn subs\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "self_contained/test_7.py", "name": "string_to_int", "docstring": "Convert a string to a number, using the given alphabet.\n\nThe input is assumed to have the most significant digit first.", "ques_id": "62ece4982e6aefcf4aabbd6d", "code": ["def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif string == \"\":\n\t\treturn 0\n\treturn (string_to_int(string[:-1], alphabet) + 1) * len(alphabet) + alphabet.index(string[-1])\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\treturn sum([(alphabet.index(string[i]) + 1) * len(alphabet) ** (len(string) - i - 1) for i in range(len(string))])\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif not string:\n\t\treturn 0\n\tlast_char = string[-1]\n\trest_string = string[:-1]\n\treturn string_to_int(rest_string, alphabet) * len(alphabet) + alphabet.index(last_char)\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\treturn sum([(len(alphabet)**i)*alphabet.index(s) for i, s in enumerate(string[::-1])])\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tnum = 0\n\tfor i, letter in enumerate(string):\n\t\tnum += alphabet.index(letter) * len(alphabet) ** (len(string) - 1 - i)\n\treturn num\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tassert len(string) > 0, \"string is empty\"\n\tassert string.isalpha(), \"string is not a word\"\n\tassert string.islower(), \"string is not lowercase\"\n\tassert len(string) == len(set(string)), \"string is not a permutation\"\n\tassert all(letter in alphabet for letter in string), \"string contains letters not in alphabet\"\n\talphabet_len = len(alphabet)\n\tstring_len = len(string)\n\tstring_list = [alphabet.index(letter) for letter in string]\n\treturn sum(string_list[i] * alphabet_len ** (string_len - i - 1) for i in range(string_len))\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t# covert the string to a list of characters\n\tchar_list = list(string)\n\t# get the frequency of the letters in the string\n\tfreq_list = [char_list.count(x) for x in alphabet]\n\t# initialize the number\n\tnumber = 0\n\t# loop through the frequency list\n\tfor i in range(len(freq_list)):\n\t\t# calculate the number\n\t\tnumber += freq_list[i] * len(alphabet) ** i\n\t# return the number\n\treturn number\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tstring = string[::-1]\n\tres = 0\n\tfor i in range(len(string)):\n\t\tres += alphabet.index(string[i]) * (len(alphabet) ** i)\n\treturn res\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t# Your code here\n\tif len(string) == 0:\n\t\treturn 0\n\tif len(string) == 1:\n\t\treturn alphabet.index(string) + 1\n\treturn (alphabet.index(string[0]) + 1) * len(alphabet) ** (len(string) - 1) + string_to_int(string[1:], alphabet)\n\n\n", "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif string == '':\n\t\treturn 0\n\telse:\n\t\tlast_char = string[-1]\n\t\tprefix = string[0:-1]\n\t\treturn alphabet.index(last_char) + len(alphabet) * string_to_int(prefix, alphabet)\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif string == \"\":\n\t\treturn 0\n\treturn (string_to_int(string[:-1], alphabet) + 1) * len(alphabet) + alphabet.index(string[-1])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\treturn sum([(alphabet.index(string[i]) + 1) * len(alphabet) ** (len(string) - i - 1) for i in range(len(string))])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif not string:\n\t\treturn 0\n\tlast_char = string[-1]\n\trest_string = string[:-1]\n\treturn string_to_int(rest_string, alphabet) * len(alphabet) + alphabet.index(last_char)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\treturn sum([(len(alphabet)**i)*alphabet.index(s) for i, s in enumerate(string[::-1])])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tnum = 0\n\tfor i, letter in enumerate(string):\n\t\tnum += alphabet.index(letter) * len(alphabet) ** (len(string) - 1 - i)\n\treturn num\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tassert len(string) > 0, \"string is empty\"\n\tassert string.isalpha(), \"string is not a word\"\n\tassert string.islower(), \"string is not lowercase\"\n\tassert len(string) == len(set(string)), \"string is not a permutation\"\n\tassert all(letter in alphabet for letter in string), \"string contains letters not in alphabet\"\n\talphabet_len = len(alphabet)\n\tstring_len = len(string)\n\tstring_list = [alphabet.index(letter) for letter in string]\n\treturn sum(string_list[i] * alphabet_len ** (string_len - i - 1) for i in range(string_len))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t# covert the string to a list of characters\n\tchar_list = list(string)\n\t# get the frequency of the letters in the string\n\tfreq_list = [char_list.count(x) for x in alphabet]\n\t# initialize the number\n\tnumber = 0\n\t# loop through the frequency list\n\tfor i in range(len(freq_list)):\n\t\t# calculate the number\n\t\tnumber += freq_list[i] * len(alphabet) ** i\n\t# return the number\n\treturn number\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tstring = string[::-1]\n\tres = 0\n\tfor i in range(len(string)):\n\t\tres += alphabet.index(string[i]) * (len(alphabet) ** i)\n\treturn res\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\t# Your code here\n\tif len(string) == 0:\n\t\treturn 0\n\tif len(string) == 1:\n\t\treturn alphabet.index(string) + 1\n\treturn (alphabet.index(string[0]) + 1) * len(alphabet) ** (len(string) - 1) + string_to_int(string[1:], alphabet)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def string_to_int(string: str, alphabet: List[str]) -> int:\n\tif string == '':\n\t\treturn 0\n\telse:\n\t\tlast_char = string[-1]\n\t\tprefix = string[0:-1]\n\t\treturn alphabet.index(last_char) + len(alphabet) * string_to_int(prefix, alphabet)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_24.py", "name": "get_repo_archive", "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.", "ques_id": "62ece4982e6aefcf4aabbd6f", "code": ["def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = get_archive(url=url, destination_path=destination_path)\n\textract_archive(archive_path=archive_path, destination_path=destination_path)\n\treturn destination_path.joinpath(archive_path.name.replace(\".tar.gz\", \"\"))\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_name = url.split('/')[-1]\n\tlogger.info(f\"Downloading {url}\")\n\twith tarfile.open(str(destination_path / archive_name), 'w:gz') as tar:\n\t\ttar.add(url, arcname=archive_name)\n\tlogger.info(f\"Extracting {archive_name}\")\n\twith tarfile.open(str(destination_path / archive_name)) as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path / archive_name.split('.')[0]\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.info(f\"Downloading archive from {url}\")\n\tfilename = url.split('/')[-1]\n\tarchive_path = destination_path / filename\n\n\tif archive_path.exists():\n\t\tlogger.info(f\"File {archive_path} already exists, skipping download\")\n\telse:\n\t\tlogger.info(f\"Downloading file {archive_path}\")\n\t\twith open(archive_path, 'wb') as f:\n\t\t\tf.write(requests.get(url).content)\n\n\tif archive_path.suffix == '.tar.gz':\n\t\tlogger.info(f\"Extracting {archive_path}\")\n\t\twith tarfile.open(archive_path, 'r:gz') as tar:\n\t\t\ttar.extractall(destination_path)\n\telif archive_path.suffix == '.zip':\n\t\tlogger.info(f\"Extracting {archive_path}\")\n\t\twith zipfile.ZipFile(archive_path, 'r') as zip_ref:\n\t\t\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tdestination_path = Path(destination_path)\n\tdestination_path.mkdir(parents=True, exist_ok=True)\n\tarchive_name = url.split(\"/\")[-1]\n\tarchive_path = destination_path.joinpath(archive_name)\n\tif not archive_path.exists():\n\t\tprint(\"Downloading: \" + url)\n\t\tarchive_path = download_file(url, destination_path=destination_path)\n\n\tif not archive_path.exists():\n\t\tprint(\"Error: failed to download archive: \" + archive_name)\n\t\treturn None\n\telse:\n\t\tprint(\"Extracting: \" + archive_name)\n\t\textracted_path = extract_archive(archive_path=archive_path, destination_path=destination_path)\n\t\treturn extracted_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_file_name = url.split('/')[-1]\n\tarchive_file_path = destination_path / archive_file_name\n\tif not archive_file_path.exists():\n\t\tprint(f\"Retrieving {archive_file_name}\")\n\t\tr = requests.get(url, stream=True)\n\t\tarchive_file_path.write_bytes(r.content)\n\telse:\n\t\tprint(f\"{archive_file_name} already retrieved\")\n\tdestination_path = destination_path / archive_file_name.split('.')[0]\n\tif not destination_path.exists():\n\t\tprint(f\"Extracting {archive_file_name}\")\n\t\twith tarfile.open(archive_file_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(destination_path)\n\telse:\n\t\tprint(f\"{archive_file_name} already extracted\")\n\treturn destination_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t# download tar.gz archive to the destination path\n\tarchive_path = destination_path / f\"{url.split('/')[-1]}\"\n\tdownload_file(url, archive_path)\n\n\t# extract the archive\n\tif not destination_path.is_dir():\n\t\tdestination_path.mkdir()\n\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(destination_path)\n\t\t# archive is extracted to the destination path\n\t\textracted_archive_path = destination_path / f\"{archive_path.stem}\"\n\n\treturn extracted_archive_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\ttry:\n\t\tresponse = requests.get(url, stream=True)\n\t\tresponse.raise_for_status()\n\t\tfile_name = url.split('/')[-1]\n\t\tfile_path = destination_path / file_name\n\t\twith open(file_path, 'wb') as f:\n\t\t\tfor chunk in response.iter_content(chunk_size=1024):\n\t\t\t\tf.write(chunk)\n\t\twith tarfile.open(file_path, 'r:gz') as tar:\n\t\t\ttar.extractall(destination_path)\n\t\treturn destination_path\n\texcept requests.HTTPError as err:\n\t\traise err\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tos.makedirs(destination_path, exist_ok=True)\n\tarchive_path = destination_path / url.split('/')[-1]\n\tif not archive_path.exists():\n\t\tprint(\"Downloading archive: {}\".format(url))\n\t\tarchive_path = download_file(url, destination_path)\n\t\tprint(\"Done\")\n\t\tprint(\"Extracting archive: {}\".format(archive_path))\n\t\textract_archive(archive_path, destination_path)\n\t\tprint(\"Done\")\n\telse:\n\t\tprint(\"Archive already exist: {}\".format(archive_path))\n\treturn archive_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_name = url.split('/')[-1]\n\tarchive_path = destination_path / archive_name\n\tlogging.info(f'Downloading {archive_name} from {url}')\n\twith open(archive_path, 'wb') as file:\n\t\tresponse = requests.get(url)\n\t\tfile.write(response.content)\n\tlogging.info(f'Extracting {archive_name} to {destination_path}')\n\twith tarfile.open(archive_path, 'r:gz') as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n\n", "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = destination_path / url.split(\"/\")[-1]\n\tif not destination_path.exists():\n\t\tdestination_path.mkdir(parents=True)\n\tif not archive_path.exists():\n\t\tlogger.info(\"Downloading %s\", archive_path)\n\t\tarchive_path.write_bytes(urllib.request.urlopen(url).read())\n\n\tlogger.info(\"Extracting %s\", archive_path)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(destination_path)\n\n\treturn destination_path\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = get_archive(url=url, destination_path=destination_path)\n\textract_archive(archive_path=archive_path, destination_path=destination_path)\n\treturn destination_path.joinpath(archive_path.name.replace(\".tar.gz\", \"\"))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_name = url.split('/')[-1]\n\tlogger.info(f\"Downloading {url}\")\n\twith tarfile.open(str(destination_path / archive_name), 'w:gz') as tar:\n\t\ttar.add(url, arcname=archive_name)\n\tlogger.info(f\"Extracting {archive_name}\")\n\twith tarfile.open(str(destination_path / archive_name)) as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path / archive_name.split('.')[0]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tlogger.info(f\"Downloading archive from {url}\")\n\tfilename = url.split('/')[-1]\n\tarchive_path = destination_path / filename\n\n\tif archive_path.exists():\n\t\tlogger.info(f\"File {archive_path} already exists, skipping download\")\n\telse:\n\t\tlogger.info(f\"Downloading file {archive_path}\")\n\t\twith open(archive_path, 'wb') as f:\n\t\t\tf.write(requests.get(url).content)\n\n\tif archive_path.suffix == '.tar.gz':\n\t\tlogger.info(f\"Extracting {archive_path}\")\n\t\twith tarfile.open(archive_path, 'r:gz') as tar:\n\t\t\ttar.extractall(destination_path)\n\telif archive_path.suffix == '.zip':\n\t\tlogger.info(f\"Extracting {archive_path}\")\n\t\twith zipfile.ZipFile(archive_path, 'r') as zip_ref:\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tdestination_path = Path(destination_path)\n\tdestination_path.mkdir(parents=True, exist_ok=True)\n\tarchive_name = url.split(\"/\")[-1]\n\tarchive_path = destination_path.joinpath(archive_name)\n\tif not archive_path.exists():\n\t\tprint(\"Downloading: \" + url)\n\t\tarchive_path = download_file(url, destination_path=destination_path)\n\n\tif not archive_path.exists():\n\t\tprint(\"Error: failed to download archive: \" + archive_name)\n\t\treturn None\n\telse:\n\t\tprint(\"Extracting: \" + archive_name)\n\t\textracted_path = extract_archive(archive_path=archive_path, destination_path=destination_path)\n\t\treturn extracted_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_file_name = url.split('/')[-1]\n\tarchive_file_path = destination_path / archive_file_name\n\tif not archive_file_path.exists():\n\t\tprint(f\"Retrieving {archive_file_name}\")\n\t\tr = requests.get(url, stream=True)\n\t\tarchive_file_path.write_bytes(r.content)\n\telse:\n\t\tprint(f\"{archive_file_name} already retrieved\")\n\tdestination_path = destination_path / archive_file_name.split('.')[0]\n\tif not destination_path.exists():\n\t\tprint(f\"Extracting {archive_file_name}\")\n\t\twith tarfile.open(archive_file_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(destination_path)\n\telse:\n\t\tprint(f\"{archive_file_name} already extracted\")\n\treturn destination_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\t# download tar.gz archive to the destination path\n\tarchive_path = destination_path / f\"{url.split('/')[-1]}\"\n\tdownload_file(url, archive_path)\n\n\t# extract the archive\n\tif not destination_path.is_dir():\n\t\tdestination_path.mkdir()\n\n\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\ttar.extractall(destination_path)\n\t\t# archive is extracted to the destination path\n\t\textracted_archive_path = destination_path / f\"{archive_path.stem}\"\n\n\treturn extracted_archive_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\ttry:\n\t\tresponse = requests.get(url, stream=True)\n\t\tresponse.raise_for_status()\n\t\tfile_name = url.split('/')[-1]\n\t\tfile_path = destination_path / file_name\n\t\twith open(file_path, 'wb') as f:\n\t\t\tfor chunk in response.iter_content(chunk_size=1024):\n\t\t\t\tf.write(chunk)\n\t\twith tarfile.open(file_path, 'r:gz') as tar:\n\t\t\ttar.extractall(destination_path)\n\t\treturn destination_path\n\texcept requests.HTTPError as err:\n\t\traise err\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tos.makedirs(destination_path, exist_ok=True)\n\tarchive_path = destination_path / url.split('/')[-1]\n\tif not archive_path.exists():\n\t\tprint(\"Downloading archive: {}\".format(url))\n\t\tarchive_path = download_file(url, destination_path)\n\t\tprint(\"Done\")\n\t\tprint(\"Extracting archive: {}\".format(archive_path))\n\t\textract_archive(archive_path, destination_path)\n\t\tprint(\"Done\")\n\telse:\n\t\tprint(\"Archive already exist: {}\".format(archive_path))\n\treturn archive_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_name = url.split('/')[-1]\n\tarchive_path = destination_path / archive_name\n\tlogging.info(f'Downloading {archive_name} from {url}')\n\twith open(archive_path, 'wb') as file:\n\t\tresponse = requests.get(url)\n\t\tfile.write(response.content)\n\tlogging.info(f'Extracting {archive_name} to {destination_path}')\n\twith tarfile.open(archive_path, 'r:gz') as tar:\n\t\ttar.extractall(destination_path)\n\treturn destination_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_repo_archive(url: str, destination_path: Path) -> Path:\n\tarchive_path = destination_path / url.split(\"/\")[-1]\n\tif not destination_path.exists():\n\t\tdestination_path.mkdir(parents=True)\n\tif not archive_path.exists():\n\t\tlogger.info(\"Downloading %s\", archive_path)\n\t\tarchive_path.write_bytes(urllib.request.urlopen(url).read())\n\n\tlogger.info(\"Extracting %s\", archive_path)\n\twith tarfile.open(archive_path) as tar:\n\t\ttar.extractall(destination_path)\n\n\treturn destination_path\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_20.py", "name": "os_is_mac", "docstring": "Checks if the os is macOS\n\n:return: True is macOS\n:rtype: bool", "ques_id": "62ece4982e6aefcf4aabbd70", "code": ["def os_is_mac():\n\treturn platform.system() == 'Darwin'\n\n\n", "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n\n", "def os_is_mac():\n\tif platform.system() == \"Darwin\":\n\t\treturn True\n\treturn False\n\n\n", "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n", "def os_is_mac():\n\treturn os.name == 'mac'\n\n\n", "def os_is_mac():\n\treturn True if platform.system() == \"Darwin\" else False\n\n\n", "def os_is_mac():\n\treturn os_platform() == 'darwin'\n\n\n", "def os_is_mac():\n\treturn os.name == \"posix\" and platform.system() == \"Darwin\"\n\n\n", "def os_is_mac():\n\treturn platform.system().lower() == 'darwin'\n\n\n", "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def os_is_mac():\n\treturn platform.system() == 'Darwin'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\tif platform.system() == \"Darwin\":\n\t\treturn True\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn os.name == 'mac'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn True if platform.system() == \"Darwin\" else False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn os_platform() == 'darwin'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn os.name == \"posix\" and platform.system() == \"Darwin\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def os_is_mac():\n\treturn platform.system().lower() == 'darwin'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def os_is_mac():\n\treturn platform.system() == \"Darwin\"\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_14.py", "name": "regex_dict", "docstring": "Convert *.cpp keys to regex keys\n\nGiven a dict where the keys are all filenames with wildcards, convert only\nthe keys into equivalent regexes and leave the values intact.\n\nExample:\n\nrules = {\n    '*.cpp':\n        {'a': 'arf', 'b': 'bark', 'c': 'coo'},\n    '*.h':\n        {'h': 'help'}\n}\nregex_keys = regex_dict(rules)\n\nArgs:\n    item: dict to convert\nReturns:\n    dict with keys converted to regexes", "ques_id": "62ece4982e6aefcf4aabbd71", "code": ["def regex_dict(item):\n\tif not isinstance(item, dict): raise TypeError(\"item must be a dict\")\n\treturn {wildcard_to_regex(k):v for k,v in item.iteritems()}\n\n", "def regex_dict(item):\n\treturn {re.compile(p): v for p, v in item.items()}\n\n\n", "def regex_dict(item):\n\treturn {re.compile(fnmatch.translate(k)): v for k, v in item.items()}\n\n\n", "def regex_dict(item):\n\treturn {fnmatch.translate(key): value for key, value in item.items()}\n\n\n", "def regex_dict(item):\n\tfor key in item:\n\t\titem[re.compile(fnmatch.translate(key))] = item[key]\n\t\tdel item[key]\n\treturn item\n\n\n", "def regex_dict(item):\n\tregex_dict = {}\n\tfor key, value in item.items():\n\t\tregex_dict[re.compile(fnmatch.translate(key))] = value\n\treturn regex_dict\n\n", "def regex_dict(item):\n\tresult = {}\n\tfor key in item:\n\t\tresult[re.compile(fnmatch.translate(key))] = item[key]\n\treturn result\n\n", "def regex_dict(item):\n\treturn {re.compile(key):value for key,value in item.items()}\n\n", "def regex_dict(item):\n\treturn {re.compile(key): value for key, value in item.items()}\n\n\n", "def regex_dict(item):\n\treturn {re.compile(fnmatch.translate(key)): value for key, value in item.items()}\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def regex_dict(item):\n\tif not isinstance(item, dict): raise TypeError(\"item must be a dict\")\n\treturn {wildcard_to_regex(k):v for k,v in item.iteritems()}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\treturn {re.compile(p): v for p, v in item.items()}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\treturn {re.compile(fnmatch.translate(k)): v for k, v in item.items()}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\treturn {fnmatch.translate(key): value for key, value in item.items()}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tfor key in item:\n\t\titem[re.compile(fnmatch.translate(key))] = item[key]\n\t\tdel item[key]\n\treturn item\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tregex_dict = {}\n\tfor key, value in item.items():\n\t\tregex_dict[re.compile(fnmatch.translate(key))] = value\n\treturn regex_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\tresult = {}\n\tfor key in item:\n\t\tresult[re.compile(fnmatch.translate(key))] = item[key]\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\treturn {re.compile(key):value for key,value in item.items()}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\treturn {re.compile(key): value for key, value in item.items()}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def regex_dict(item):\n\treturn {re.compile(fnmatch.translate(key)): value for key, value in item.items()}\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_4.py", "name": "unquote", "docstring": "Remove quote from the given name.", "ques_id": "62ece4982e6aefcf4aabbd72", "code": ["def unquote(name):\n\treturn re.sub(r'(^\")|(\"$)', '', name)\n\t\n", "def unquote(name):\n\treturn re.sub('\"', '', name)\n\n\n", "def unquote(name):\n\tif name:\n\t\treturn re.sub(r'^\"|\"$', '', name)  # remove quote\n\telse:\n\t\treturn name\n\n\n", "def unquote(name):\n\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n\n\n", "def unquote(name):\n\treturn re.sub(r'[\\'\\\"]+', '', name)\n\n", "def unquote(name):\n\treturn re.sub(r'^\"|\"$', '', name)\n\n", "def unquote(name):\n\treturn re.sub('[\"]', '', name)\n\n\n", "def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\treturn name[1:-1]\n\telse:\n\t\treturn name\n\n\n", "def unquote(name):\n\treturn re.sub('[\\'\\\"]', '', name)\n\n", "def unquote(name):\n\treturn re.sub(r'^\"|\"$', '', name)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def unquote(name):\n\treturn re.sub(r'(^\")|(\"$)', '', name)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub('\"', '', name)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name:\n\t\treturn re.sub(r'^\"|\"$', '', name)  # remove quote\n\telse:\n\t\treturn name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub(r'^\"(.*)\"$', r'\\1', name)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub(r'[\\'\\\"]+', '', name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub(r'^\"|\"$', '', name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub('[\"]', '', name)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\tif name[0] == '\"' and name[-1] == '\"':\n\t\treturn name[1:-1]\n\telse:\n\t\treturn name\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub('[\\'\\\"]', '', name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def unquote(name):\n\treturn re.sub(r'^\"|\"$', '', name)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_21.py", "name": "split", "docstring": "Multi-platform variant of shlex.split() for command-line splitting.\nFor use with subprocess, for argv injection etc. Using fast REGEX.\n\nplatform: 'this' = auto from current platform;\n          1 = POSIX;\n          0 = Windows/CMD\n          (other values reserved)", "ques_id": "62ece4982e6aefcf4aabbd73", "code": ["def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = sys.platform.startswith('win')\n\tif platform:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tif sys.platform.startswith('win'):\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 0:\n\t\treturn s.split(';')\n\telse:\n\t\treturn s.split(':')\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this()\n\tif platform == 1:\n\t\treturn s.split()\n\telse:\n\t\treturn s.split(' ')\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this_platform_is_windows()\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise ValueError('Unknown platform.')\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception(\"Invalid platform given: %s\" % platform)\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this_platform\n\tif platform == 'posix':\n\t\tplatform = 1\n\telif platform == 'windows':\n\t\tplatform = 0\n\tif platform == 1:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = int(_is_posix())\n\tif platform:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')\n\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = detect_platform()\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise ValueError('Unknown platform: {0}'.format(platform))\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tif os.name == 'nt': # Windows/CMD\n\t\t\tplatform = 0\n\t\telse: # POSIX\n\t\t\tplatform = 1\n\tif platform == 1:\n\t\treturn _split1(s)\n\telif platform == 0:\n\t\treturn _split0(s)\n\telse:\n\t\traise Exception('Unknown platform')\n\n", "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = _this_platform\n\tif platform == 1:\n\t\treturn s.split('/')\n\treturn s.split('\\\\')\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = sys.platform.startswith('win')\n\tif platform:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tif sys.platform.startswith('win'):\n\t\t\tplatform = 0\n\t\telse:\n\t\t\tplatform = 1\n\tif platform == 0:\n\t\treturn s.split(';')\n\telse:\n\t\treturn s.split(':')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this()\n\tif platform == 1:\n\t\treturn s.split()\n\telse:\n\t\treturn s.split(' ')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this_platform_is_windows()\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise ValueError('Unknown platform.')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = os.name\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise Exception(\"Invalid platform given: %s\" % platform)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = this_platform\n\tif platform == 'posix':\n\t\tplatform = 1\n\telif platform == 'windows':\n\t\tplatform = 0\n\tif platform == 1:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = int(_is_posix())\n\tif platform:\n\t\treturn s.split('/')\n\telse:\n\t\treturn s.split('\\\\')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = detect_platform()\n\tif platform == 1:\n\t\treturn s.split('/')\n\telif platform == 0:\n\t\treturn s.split('\\\\')\n\telse:\n\t\traise ValueError('Unknown platform: {0}'.format(platform))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tif os.name == 'nt': # Windows/CMD\n\t\t\tplatform = 0\n\t\telse: # POSIX\n\t\t\tplatform = 1\n\tif platform == 1:\n\t\treturn _split1(s)\n\telif platform == 0:\n\t\treturn _split0(s)\n\telse:\n\t\traise Exception('Unknown platform')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def split(s, platform='this'):\n\tif platform == 'this':\n\t\tplatform = _this_platform\n\tif platform == 1:\n\t\treturn s.split('/')\n\treturn s.split('\\\\')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_15.py", "name": "prepare_repository_from_archive", "docstring": "Given an existing archive_path, uncompress it.\nReturns a file repo url which can be used as origin url.\n\nThis does not deal with the case where the archive passed along does not exist.", "ques_id": "62ece4982e6aefcf4aabbd74", "code": ["def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\n\tif not filename:\n\t\tfilename = Path(archive_path).name\n\n\ttar_file_path = tmp_path / filename\n\n\tshutil.copy(archive_path, tar_file_path)\n\n\ttar_file = tarfile.open(tar_file_path, \"r:gz\")\n\ttar_file.extractall(tmp_path)\n\ttar_file.close()\n\n\trepo_path = tmp_path / os.listdir(tmp_path)[0]\n\n\treturn str(repo_path)\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\tname, ext = os.path.splitext(filename)\n\ttmp_dir = os.path.join(tmp_path, name)\n\tos.makedirs(tmp_dir, exist_ok=True)\n\tarchive_file, ext = os.path.splitext(archive_path)\n\tif ext == \".zip\":\n\t\twith ZipFile(archive_file) as zip_file:\n\t\t\tzip_file.extractall(tmp_dir)\n\telse:\n\t\twith tarfile.open(archive_file) as tar_file:\n\t\t\ttar_file.extractall(tmp_dir)\n\treturn tmp_dir\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tfile_repo_path = get_repo_name_from_archive_path(archive_path, filename)\n\tfile_repo_url = get_file_repo_url_from_file_path(file_repo_path)\n\n\tif not is_archive_file(archive_path):\n\t\traise Exception(\"Path {} is not an archive file\".format(archive_path))\n\n\tif not Path(tmp_path).exists():\n\t\traise Exception(\"Path {} does not exist\".format(tmp_path))\n\tif not os.access(tmp_path, os.W_OK):\n\t\traise Exception(\"Path {} is not writable\".format(tmp_path))\n\n\tif not Path(file_repo_path).exists():\n\t\tuncompress_file(archive_path, file_repo_path)\n\n\treturn file_repo_url\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not archive_path:\n\t\traise ValueError(\"archive_path must be set\")\n\n\tarchive_file_path = get_archive_path(archive_path, filename, tmp_path)\n\tuncompressed_repo_path = uncompress_archive(archive_file_path)\n\trepo_url = prepare_repository(uncompressed_repo_path)\n\treturn repo_url\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\ttmp_file = tmp_path / filename\n\twith open(archive_path, \"rb\") as archive_file:\n\t\twith open(tmp_file, \"wb\") as tmp_file_handle:\n\t\t\ttmp_file_handle.write(archive_file.read())\n\tif filename.endswith(\"tgz\"):\n\t\tarchive_path = tarfile.open(tmp_file)\n\telif filename.endswith(\"zip\"):\n\t\tarchive_path = zipfile.ZipFile(tmp_file)\n\telse:\n\t\traise RuntimeError(\"Unsupported extension\")\n\textracted_path = tmp_path / \"repo\"\n\textracted_path.mkdir(exist_ok=True)\n\tarchive_path.extractall(extracted_path)\n\tarchive_path.close()\n\ttmp_file.unlink()\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\trepo_path = tmp_path / filename.split(\".\")[0]\n\trepo_path.mkdir(parents=True, exist_ok=True)\n\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(repo_path)\n\n\treturn str(repo_path)\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\tfile_repo_url = f\"file://{tmp_path}/{filename}\"\n\tlogging.debug(f\"Preparing {archive_path} to {file_repo_url}...\")\n\tPath(tmp_path).mkdir(exist_ok=True, parents=True)\n\tif archive_path.endswith(\".tar.gz\"):\n\t\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(path=tmp_path)\n\telif archive_path.endswith(\".zip\"):\n\t\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(path=tmp_path)\n\telse:\n\t\traise ValueError(f\"Unsupported archive {archive_path}\")\n\treturn file_repo_url\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tassert archive_path\n\tassert tmp_path, \"Please provide a tmp_path\"\n\n\tif not filename:\n\t\tfilename = get_filename_from_url(archive_path)\n\n\t# Get a temp path, create a folder with the file name.\n\ttmp_path = Path(tmp_path)\n\trepo_path = tmp_path / filename\n\trepo_path.mkdir(parents=True, exist_ok=True)\n\n\t# Uncompress the file.\n\tif archive_path.endswith(\".gz\"):\n\t\twith gzip.open(archive_path, \"rb\") as f_in:\n\t\t\twith open(repo_path / \"repo.git\", \"wb\") as f_out:\n\t\t\t\tshutil.copyfileobj(f_in, f_out)\n\telse:\n\t\twith ZipFile(archive_path) as zip_file:\n\t\t\tzip_file.extractall(repo_path)\n\n\t# Return the repo URL\n\treturn str(repo_path / \"repo.git\")\n\n\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tarchive_path = Path(archive_path)\n\ttmp_path = Path(tmp_path)\n\tarchive_extension = archive_path.suffix\n\tfilename = str(archive_path.stem) if filename is None else filename\n\tif archive_extension not in ARCHIVE_EXTENSIONS:\n\t\traise NotSupportedArchiveExtension(f\"{archive_extension} is not supported.\")\n\tarchive_path.parent.mkdir(exist_ok=True)\n\ttmp_repo_path = tmp_path / filename\n\tif archive_path.exists():\n\t\ttmp_repo_path.mkdir(exist_ok=True)\n\t\tif archive_extension == \".zip\":\n\t\t\twith zipfile.ZipFile(archive_path, \"r\") as z:\n\t\t\t\tz.extractall(tmp_repo_path)\n\t\telse:\n\t\t\twith tarfile.open(archive_path, \"r\") as tar:\n\t\t\t\ttar.extractall(tmp_repo_path)\n\t\treturn str(tmp_re\n", "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\ttmp_dir = os.path.join(tmp_path, \"repository\")\n\ttmp_extract = os.path.join(tmp_dir, filename)\n\tif not os.path.exists(tmp_dir):\n\t\tos.makedirs(tmp_dir)\n\n\tif os.path.exists(tmp_extract):\n\t\tos.remove(tmp_extract)\n\n\t# Move file\n\tshutil.move(archive_path, tmp_extract)\n\n\tif tmp_extract.endswith(\".zip\"):\n\t\tzip_ref = zipfile.ZipFile(tmp_extract, \"r\")\n\t\tzip_ref.extractall(tmp_dir)\n\t\tzip_ref.close()\n\telif tmp_extract.endswith(\".tar.gz\"):\n\t\ttar = tarfile.open(tmp_extract, \"r:gz\")\n\t\ttar.extractall(path=tmp_dir)\n\t\ttar\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\n\tif not filename:\n\t\tfilename = Path(archive_path).name\n\n\ttar_file_path = tmp_path / filename\n\n\tshutil.copy(archive_path, tar_file_path)\n\n\ttar_file = tarfile.open(tar_file_path, \"r:gz\")\n\ttar_file.extractall(tmp_path)\n\ttar_file.close()\n\n\trepo_path = tmp_path / os.listdir(tmp_path)[0]\n\n\treturn str(repo_path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\tname, ext = os.path.splitext(filename)\n\ttmp_dir = os.path.join(tmp_path, name)\n\tos.makedirs(tmp_dir, exist_ok=True)\n\tarchive_file, ext = os.path.splitext(archive_path)\n\tif ext == \".zip\":\n\t\twith ZipFile(archive_file) as zip_file:\n\t\t\tzip_file.extractall(tmp_dir)\n\telse:\n\t\twith tarfile.open(archive_file) as tar_file:\n\t\t\ttar_file.extractall(tmp_dir)\n\treturn tmp_dir\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tfile_repo_path = get_repo_name_from_archive_path(archive_path, filename)\n\tfile_repo_url = get_file_repo_url_from_file_path(file_repo_path)\n\n\tif not is_archive_file(archive_path):\n\t\traise Exception(\"Path {} is not an archive file\".format(archive_path))\n\n\tif not Path(tmp_path).exists():\n\t\traise Exception(\"Path {} does not exist\".format(tmp_path))\n\tif not os.access(tmp_path, os.W_OK):\n\t\traise Exception(\"Path {} is not writable\".format(tmp_path))\n\n\tif not Path(file_repo_path).exists():\n\t\tuncompress_file(archive_path, file_repo_path)\n\n\treturn file_repo_url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not archive_path:\n\t\traise ValueError(\"archive_path must be set\")\n\n\tarchive_file_path = get_archive_path(archive_path, filename, tmp_path)\n\tuncompressed_repo_path = uncompress_archive(archive_file_path)\n\trepo_url = prepare_repository(uncompressed_repo_path)\n\treturn repo_url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\ttmp_file = tmp_path / filename\n\twith open(archive_path, \"rb\") as archive_file:\n\t\twith open(tmp_file, \"wb\") as tmp_file_handle:\n\t\t\ttmp_file_handle.write(archive_file.read())\n\tif filename.endswith(\"tgz\"):\n\t\tarchive_path = tarfile.open(tmp_file)\n\telif filename.endswith(\"zip\"):\n\t\tarchive_path = zipfile.ZipFile(tmp_file)\n\telse:\n\t\traise RuntimeError(\"Unsupported extension\")\n\textracted_path = tmp_path / \"repo\"\n\textracted_path.mkdir(exist_ok=True)\n\tarchive_path.extractall(extracted_path)\n\tarchive_path.close()\n\ttmp_file.unlink()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = os.path.basename(archive_path)\n\n\ttmp_path = Path(tmp_path)\n\ttmp_path.mkdir(parents=True, exist_ok=True)\n\trepo_path = tmp_path / filename.split(\".\")[0]\n\trepo_path.mkdir(parents=True, exist_ok=True)\n\n\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\tzip_ref.extractall(repo_path)\n\n\treturn str(repo_path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif filename is None:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\tfile_repo_url = f\"file://{tmp_path}/{filename}\"\n\tlogging.debug(f\"Preparing {archive_path} to {file_repo_url}...\")\n\tPath(tmp_path).mkdir(exist_ok=True, parents=True)\n\tif archive_path.endswith(\".tar.gz\"):\n\t\twith tarfile.open(archive_path, \"r:gz\") as tar:\n\t\t\ttar.extractall(path=tmp_path)\n\telif archive_path.endswith(\".zip\"):\n\t\twith zipfile.ZipFile(archive_path, \"r\") as zip_ref:\n\t\t\tzip_ref.extractall(path=tmp_path)\n\telse:\n\t\traise ValueError(f\"Unsupported archive {archive_path}\")\n\treturn file_repo_url\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tassert archive_path\n\tassert tmp_path, \"Please provide a tmp_path\"\n\n\tif not filename:\n\t\tfilename = get_filename_from_url(archive_path)\n\n\t# Get a temp path, create a folder with the file name.\n\ttmp_path = Path(tmp_path)\n\trepo_path = tmp_path / filename\n\trepo_path.mkdir(parents=True, exist_ok=True)\n\n\t# Uncompress the file.\n\tif archive_path.endswith(\".gz\"):\n\t\twith gzip.open(archive_path, \"rb\") as f_in:\n\t\t\twith open(repo_path / \"repo.git\", \"wb\") as f_out:\n\t\t\t\tshutil.copyfileobj(f_in, f_out)\n\telse:\n\t\twith ZipFile(archive_path) as zip_file:\n\t\t\tzip_file.extractall(repo_path)\n\n\t# Return the repo URL\n\treturn str(repo_path / \"repo.git\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tarchive_path = Path(archive_path)\n\ttmp_path = Path(tmp_path)\n\tarchive_extension = archive_path.suffix\n\tfilename = str(archive_path.stem) if filename is None else filename\n\tif archive_extension not in ARCHIVE_EXTENSIONS:\n\t\traise NotSupportedArchiveExtension(f\"{archive_extension} is not supported.\")\n\tarchive_path.parent.mkdir(exist_ok=True)\n\ttmp_repo_path = tmp_path / filename\n\tif archive_path.exists():\n\t\ttmp_repo_path.mkdir(exist_ok=True)\n\t\tif archive_extension == \".zip\":\n\t\t\twith zipfile.ZipFile(archive_path, \"r\") as z:\n\t\t\t\tz.extractall(tmp_repo_path)\n\t\telse:\n\t\t\twith tarfile.open(archive_path, \"r\") as tar:\n\t\t\t\ttar.extractall(tmp_repo_path)\n\t\treturn str(tmp_re\n", "is_pass": false, "return_code": 1}, {"generate_code": "def prepare_repository_from_archive(\n    archive_path: str,\n    filename: Optional[str] = None,\n    tmp_path: Union[PosixPath, str] = \"/tmp\",\n) -> str:\n\tif not filename:\n\t\tfilename = archive_path.split(\"/\")[-1]\n\n\ttmp_dir = os.path.join(tmp_path, \"repository\")\n\ttmp_extract = os.path.join(tmp_dir, filename)\n\tif not os.path.exists(tmp_dir):\n\t\tos.makedirs(tmp_dir)\n\n\tif os.path.exists(tmp_extract):\n\t\tos.remove(tmp_extract)\n\n\t# Move file\n\tshutil.move(archive_path, tmp_extract)\n\n\tif tmp_extract.endswith(\".zip\"):\n\t\tzip_ref = zipfile.ZipFile(tmp_extract, \"r\")\n\t\tzip_ref.extractall(tmp_dir)\n\t\tzip_ref.close()\n\telif tmp_extract.endswith(\".tar.gz\"):\n\t\ttar = tarfile.open(tmp_extract, \"r:gz\")\n\t\ttar.extractall(path=tmp_dir)\n\t\ttar\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_5.py", "name": "addignored", "docstring": "Use the git command to obtain the file names, turn it into a list, sort the list for only ignored files, return those files as a single string with each filename separated by a comma.", "ques_id": "62ece4982e6aefcf4aabbd75", "code": ["def addignored(ignored):\n\tignored = subprocess.Popen([\"git\", \"ls-files\", \"--others\", \"--ignored\", \"--exclude-standard\"], stdout=subprocess.PIPE)\n\tignoredlist = ignored.communicate()[0].decode('utf-8').split('\\n')\n\tignoredlist.sort()\n\tignoredlist = [x for x in ignoredlist if x]\n\treturn ','.join(ignoredlist)\n\n\n", "def addignored(ignored):\n\tfile_list = subprocess.check_output(['git', 'ls-files', '-i', '--exclude-standard']).splitlines()\n\tfile_list.sort()\n\tignored.extend(file_list)\n\n", "def addignored(ignored):\n\tfilename = \"\"\n\tfiles = gitcmd(\"git ls-files -i --exclude-standard\").strip().split()\n\tfiles.sort()\n\tfor f in files:\n\t\tfilename += f + \", \"\n\treturn filename[:-2]\n\n", "def addignored(ignored):\n\tgit_cmd = \"git ls-files -i --exclude-standard\"\n\toutput = subprocess.check_output(git_cmd, shell=True).strip().splitlines()\n\toutput = sorted(output)\n\treturn \",\".join(output)\n\n", "def addignored(ignored):\n\tgit_command = \"git ls-files --others --exclude-standard\"\n\ttry:\n\t\toutput = subprocess.check_output(git_command.split()).strip().decode()\n\t\tif len(output) > 0:\n\t\t\treturn ignored + \", \" + output\n\texcept:\n\t\tpass\n\n\treturn ignored\n\n\n", "def addignored(ignored):\n\t# If ignored is NULL then return NULL\n\tif ignored == \"\":\n\t\treturn \"\"\n\t# Otherwise get the list of ignored files\n\tlistofignored = subprocess.check_output([\"git\", \"ls-files\", \"--ignored\", \"--exclude-standard\"])\n\t# Split the list into a list of files\n\tlistofignored = listofignored.split('\\n')\n\t# Filter out the empty files\n\tlistofignored = filter(None, listofignored)\n\t# Sort the list of files\n\tlistofignored.sort()\n\t# Join the files into a comma separated string\n\tlistofignored = ','.join(listofignored)\n\t# Return the string of files\n\treturn listofignored\n\n", "def addignored(ignored):\n\tignored = subprocess.check_output(\"git ls-files --others --exclude-standard\".split(\" \"))\n\tignored = ignored.decode(\"utf-8\")\n\tignored = ignored.split(\"\\n\")\n\tignored = str(ignored)[1:-1]\n\tignored = ignored.replace(\"'\", \"\")\n\tignored = ignored.replace(\" \", \"\")\n\treturn ignored\n\n\n", "def addignored(ignored):\n\tif ignored:\n\t\tignored_str = ''\n\t\tfor line in ignored.split('\\n'):\n\t\t\tif line: ignored_str += '--exclude=\"' + line + '\" '\n\t\treturn ignored_str\n\telse:\n\t\treturn ''\n\t# print(ignored_str)\n\t# return ignored_str\n\n", "def addignored(ignored):\n\tignored_files = []\n\tfor i in ignored:\n\t\tignored_files.append(i.strip())\n\tignored_files.sort()\n\tignored = \"\"\n\tfor i in ignored_files:\n\t\tignored = ignored + i + \",\"\n\treturn ignored\n\n", "def addignored(ignored):\n\tfilelist = subprocess.check_output(['git','ls-files','-o','-i','-X',ignored])\n\tfilelist = str(filelist, 'utf-8')\n\tfilelist = filelist.split('\\n')\n\tfilelist = [f for f in filelist if f != '']\n\tfilelist.sort()\n\treturn ', '.join(filelist)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def addignored(ignored):\n\tignored = subprocess.Popen([\"git\", \"ls-files\", \"--others\", \"--ignored\", \"--exclude-standard\"], stdout=subprocess.PIPE)\n\tignoredlist = ignored.communicate()[0].decode('utf-8').split('\\n')\n\tignoredlist.sort()\n\tignoredlist = [x for x in ignoredlist if x]\n\treturn ','.join(ignoredlist)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tfile_list = subprocess.check_output(['git', 'ls-files', '-i', '--exclude-standard']).splitlines()\n\tfile_list.sort()\n\tignored.extend(file_list)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tfilename = \"\"\n\tfiles = gitcmd(\"git ls-files -i --exclude-standard\").strip().split()\n\tfiles.sort()\n\tfor f in files:\n\t\tfilename += f + \", \"\n\treturn filename[:-2]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tgit_cmd = \"git ls-files -i --exclude-standard\"\n\toutput = subprocess.check_output(git_cmd, shell=True).strip().splitlines()\n\toutput = sorted(output)\n\treturn \",\".join(output)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tgit_command = \"git ls-files --others --exclude-standard\"\n\ttry:\n\t\toutput = subprocess.check_output(git_command.split()).strip().decode()\n\t\tif len(output) > 0:\n\t\t\treturn ignored + \", \" + output\n\texcept:\n\t\tpass\n\n\treturn ignored\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\t# If ignored is NULL then return NULL\n\tif ignored == \"\":\n\t\treturn \"\"\n\t# Otherwise get the list of ignored files\n\tlistofignored = subprocess.check_output([\"git\", \"ls-files\", \"--ignored\", \"--exclude-standard\"])\n\t# Split the list into a list of files\n\tlistofignored = listofignored.split('\\n')\n\t# Filter out the empty files\n\tlistofignored = filter(None, listofignored)\n\t# Sort the list of files\n\tlistofignored.sort()\n\t# Join the files into a comma separated string\n\tlistofignored = ','.join(listofignored)\n\t# Return the string of files\n\treturn listofignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tignored = subprocess.check_output(\"git ls-files --others --exclude-standard\".split(\" \"))\n\tignored = ignored.decode(\"utf-8\")\n\tignored = ignored.split(\"\\n\")\n\tignored = str(ignored)[1:-1]\n\tignored = ignored.replace(\"'\", \"\")\n\tignored = ignored.replace(\" \", \"\")\n\treturn ignored\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tif ignored:\n\t\tignored_str = ''\n\t\tfor line in ignored.split('\\n'):\n\t\t\tif line: ignored_str += '--exclude=\"' + line + '\" '\n\t\treturn ignored_str\n\telse:\n\t\treturn ''\n\t# print(ignored_str)\n\t# return ignored_str\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tignored_files = []\n\tfor i in ignored:\n\t\tignored_files.append(i.strip())\n\tignored_files.sort()\n\tignored = \"\"\n\tfor i in ignored_files:\n\t\tignored = ignored + i + \",\"\n\treturn ignored\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def addignored(ignored):\n\tfilelist = subprocess.check_output(['git','ls-files','-o','-i','-X',ignored])\n\tfilelist = str(filelist, 'utf-8')\n\tfilelist = filelist.split('\\n')\n\tfilelist = [f for f in filelist if f != '']\n\tfilelist.sort()\n\treturn ', '.join(filelist)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_11.py", "name": "match", "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported", "ques_id": "62ece4982e6aefcf4aabbd76", "code": ["def match(filename):\n\treturn filename.lower() == 'doxyfile'\n\n", "def match(filename):\n\treturn filename.lower() == 'doxyfile'\n\n", "def match(filename):\n\treturn filename.lower().endswith(\"doxyfile\")\n\n\n", "def match(filename):\n\treturn filename.lower().endswith('.doxyfile')\n\n\n", "def match(filename):\n\treturn filename.lower() == \"doxyfile\"\n\n", "def match(filename):\n\treturn filename.lower().endswith('doxyfile')\n\n\n", "def match(filename):\n\treturn filename.lower().endswith('doxyfile')\n\n\n", "def match(filename):\n\treturn 'doxyfile' in filename\n\n\n", "def match(filename):\n\treturn filename.lower() == 'doxyfile'\n\n", "def match(filename):\n\treturn os.path.split(filename)[1].lower() == \"doxyfile\"\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def match(filename):\n\treturn filename.lower() == 'doxyfile'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\treturn filename.lower() == 'doxyfile'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\treturn filename.lower().endswith(\"doxyfile\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\treturn filename.lower().endswith('.doxyfile')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.lower() == \"doxyfile\"\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\treturn filename.lower().endswith('doxyfile')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\treturn filename.lower().endswith('doxyfile')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\treturn 'doxyfile' in filename\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def match(filename):\n\treturn filename.lower() == 'doxyfile'\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def match(filename):\n\treturn os.path.split(filename)[1].lower() == \"doxyfile\"\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_1.py", "name": "parse_frequency", "docstring": "Given a frequency string with a number and a unit of time, return a corresponding\ndatetime.timedelta instance or None if the frequency is None or \"always\".\n\nFor instance, given \"3 weeks\", return datetime.timedelta(weeks=3)\n\nRaise ValueError if the given frequency cannot be parsed.", "ques_id": "62ece4982e6aefcf4aabbd77", "code": ["def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, timeunit = frequency.split()\n\texcept ValueError:\n\t\traise ValueError(\"Frequency should be of the form 'number timeunit'\")\n\n\ttry:\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Frequency should have an integer number\")\n\n\tif timeunit not in [\"hours\", \"days\", \"weeks\"]:\n\t\traise ValueError(\"Frequency should have a timeunit of hours, days or weeks\")\n\n\treturn datetime.timedelta(**{timeunit: number})\n\n\n", "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tfreq_re = re.compile(r'(\\d+)\\s+([a-zA-Z]+)')\n\tfreq_match = freq_re.match(frequency)\n\tif not freq_match:\n\t\traise ValueError(\"Frequency '%s' is not valid\" % frequency)\n\tnumber = int(freq_match.group(1))\n\ttimeunit = freq_match.group(2)\n\tif timeunit in (\"seconds\", \"second\"):\n\t\ttimeunit = \"seconds\"\n\t\ttimeunit_delta = datetime.timedelta(seconds=number)\n\telif timeunit in (\"minutes\", \"minute\"):\n\t\ttimeunit = \"minutes\"\n\t\ttimeunit_delta = datetime.timedelta(minutes=number)\n\telif timeunit in (\"hours\", \"hour\"):\n\t\ttimeunit = \"hours\"\n\t\ttimeunit_delta = datetime.timedelta(hours=\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == 'always':\n\t\treturn None\n\tm = re.match(r'(\\d+)\\s*(\\w+)', frequency)\n\tif m is None:\n\t\traise ValueError(\"Invalid frequency: '%s'\" % frequency)\n\tnumber = int(m.group(1))\n\tunit = m.group(2)\n\tif unit not in ('days', 'weeks', 'hours', 'minutes'):\n\t\traise ValueError(\"Invalid frequency unit: '%s'\" % unit)\n\treturn datetime.timedelta(**{unit: number})\n\n\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\ttry:\n\t\tnumber, timeunit = frequency.split(\" \")\n\t\treturn datetime.timedelta(**{timeunit: int(number)})\n\texcept (ValueError, KeyError):\n\t\traise ValueError(\"Invalid frequency description %s\" % frequency)\n\n\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\telse:\n\t\ttry:\n\t\t\tfrequency_list = frequency.split()\n\t\t\treturn timedelta(**{frequency_list[1]: int(frequency_list[0])})\n\t\texcept (IndexError, KeyError, ValueError):\n\t\t\traise ValueError(\"Cannot parse frequency string '%s'\" % frequency)\n\n\n", "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\n\t# extract number and unit string\n\t# if frequency is \"always\"\n\tif frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, unit = frequency.split(\" \")\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency '{}'\".format(frequency))\n\n\treturn datetime.timedelta(**{unit: number})\n\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, timeunit = frequency.split()\n\texcept ValueError:\n\t\traise ValueError(\"Cannot parse frequency %s\" % frequency)\n\ttry:\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"The number of timeunits must be an integer\")\n\ttimeunit = timeunit.lower()\n\tif timeunit == \"seconds\" or timeunit == \"second\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit == \"minutes\" or timeunit == \"minute\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"hours\" or timeunit == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"days\" or timeunit == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit == \"weeks\" or timeunit == \"week\":\n\t\treturn datetime.timed\n", "def parse_frequency(frequency):\n\t# Check for \"always\" or None\n\tif frequency is None or frequency.strip().lower() == 'always':\n\t\treturn None\n\n\t# Parse out the time unit and factor\n\tpattern = '(?P<factor>\\d+) (?P<unit>\\w+)'\n\tmatch = re.search(pattern, frequency)\n\tif match:\n\t\tfactor = int(match.group('factor'))\n\t\tunit = match.group('unit')\n\t\tif unit in ('seconds', 'second'):\n\t\t\treturn datetime.timedelta(seconds=factor)\n\t\telif unit in ('minutes', 'minute'):\n\t\t\treturn datetime.timedelta(minutes=factor)\n\t\telif unit in ('hours', 'hour'):\n\t\t\treturn datetime.timedelta(hours=factor)\n\t\telif unit in ('days', 'day'):\n\t\t\treturn datetime.timedelta(days=factor)\n\t\telse:\n\t\t\traise ValueError(\"Unable to parse frequency %s\" % frequency)\n\telse:\n\n", "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\tif frequency == 'always':\n\t\treturn None\n\tmatch = re.match(r'^(\\d+) (\\w+)$', frequency)\n\tif match is None:\n\t\traise ValueError('Bad frequency format: ' + frequency)\n\ttime_unit = match.group(2)\n\ttime_unit_plural = time_unit + 's'\n\tif time_unit not in ['year', 'month', 'week', 'day', 'hour', 'minute', 'second'] and time_unit_plural not in ['years', 'months', 'weeks', 'days', 'hours', 'minutes', 'seconds']:\n\t\traise ValueError('Bad frequency time unit: ' + time_unit)\n\treturn datetime.timedelta(**{time_unit: int(match.group(1))})\n\n", "def parse_frequency(frequency):\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\n\tmatch = re.match(r\"(\\d+) ?([a-z]+)\", frequency, re.IGNORECASE)\n\tif match is None:\n\t\traise ValueError(\"Invalid frequency format: %s\" % frequency)\n\tnumber = int(match.group(1))\n\ttimeunit = match.group(2)\n\tif timeunit == \"s\" or timeunit == \"sec\" or timeunit == \"second\" or timeunit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit == \"m\" or timeunit == \"min\" or timeunit == \"minute\" or timeunit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"h\" or timeunit == \"hour\" or timeunit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"d\" or timeunit == \"day\" or timeunit == \"days\":\n\t\treturn dat\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, timeunit = frequency.split()\n\texcept ValueError:\n\t\traise ValueError(\"Frequency should be of the form 'number timeunit'\")\n\n\ttry:\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Frequency should have an integer number\")\n\n\tif timeunit not in [\"hours\", \"days\", \"weeks\"]:\n\t\traise ValueError(\"Frequency should have a timeunit of hours, days or weeks\")\n\n\treturn datetime.timedelta(**{timeunit: number})\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\tif frequency == \"always\":\n\t\treturn None\n\tfreq_re = re.compile(r'(\\d+)\\s+([a-zA-Z]+)')\n\tfreq_match = freq_re.match(frequency)\n\tif not freq_match:\n\t\traise ValueError(\"Frequency '%s' is not valid\" % frequency)\n\tnumber = int(freq_match.group(1))\n\ttimeunit = freq_match.group(2)\n\tif timeunit in (\"seconds\", \"second\"):\n\t\ttimeunit = \"seconds\"\n\t\ttimeunit_delta = datetime.timedelta(seconds=number)\n\telif timeunit in (\"minutes\", \"minute\"):\n\t\ttimeunit = \"minutes\"\n\t\ttimeunit_delta = datetime.timedelta(minutes=number)\n\telif timeunit in (\"hours\", \"hour\"):\n\t\ttimeunit = \"hours\"\n\t\ttimeunit_delta = datetime.timedelta(hours=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == 'always':\n\t\treturn None\n\tm = re.match(r'(\\d+)\\s*(\\w+)', frequency)\n\tif m is None:\n\t\traise ValueError(\"Invalid frequency: '%s'\" % frequency)\n\tnumber = int(m.group(1))\n\tunit = m.group(2)\n\tif unit not in ('days', 'weeks', 'hours', 'minutes'):\n\t\traise ValueError(\"Invalid frequency unit: '%s'\" % unit)\n\treturn datetime.timedelta(**{unit: number})\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\n\ttry:\n\t\tnumber, timeunit = frequency.split(\" \")\n\t\treturn datetime.timedelta(**{timeunit: int(number)})\n\texcept (ValueError, KeyError):\n\t\traise ValueError(\"Invalid frequency description %s\" % frequency)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\telse:\n\t\ttry:\n\t\t\tfrequency_list = frequency.split()\n\t\t\treturn timedelta(**{frequency_list[1]: int(frequency_list[0])})\n\t\texcept (IndexError, KeyError, ValueError):\n\t\t\traise ValueError(\"Cannot parse frequency string '%s'\" % frequency)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\n\t# extract number and unit string\n\t# if frequency is \"always\"\n\tif frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, unit = frequency.split(\" \")\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"Invalid frequency '{}'\".format(frequency))\n\n\treturn datetime.timedelta(**{unit: number})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency == \"always\":\n\t\treturn None\n\ttry:\n\t\tnumber, timeunit = frequency.split()\n\texcept ValueError:\n\t\traise ValueError(\"Cannot parse frequency %s\" % frequency)\n\ttry:\n\t\tnumber = int(number)\n\texcept ValueError:\n\t\traise ValueError(\"The number of timeunits must be an integer\")\n\ttimeunit = timeunit.lower()\n\tif timeunit == \"seconds\" or timeunit == \"second\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit == \"minutes\" or timeunit == \"minute\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"hours\" or timeunit == \"hour\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"days\" or timeunit == \"day\":\n\t\treturn datetime.timedelta(days=number)\n\telif timeunit == \"weeks\" or timeunit == \"week\":\n\t\treturn datetime.timed\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\t# Check for \"always\" or None\n\tif frequency is None or frequency.strip().lower() == 'always':\n\t\treturn None\n\n\t# Parse out the time unit and factor\n\tpattern = '(?P<factor>\\d+) (?P<unit>\\w+)'\n\tmatch = re.search(pattern, frequency)\n\tif match:\n\t\tfactor = int(match.group('factor'))\n\t\tunit = match.group('unit')\n\t\tif unit in ('seconds', 'second'):\n\t\t\treturn datetime.timedelta(seconds=factor)\n\t\telif unit in ('minutes', 'minute'):\n\t\t\treturn datetime.timedelta(minutes=factor)\n\t\telif unit in ('hours', 'hour'):\n\t\t\treturn datetime.timedelta(hours=factor)\n\t\telif unit in ('days', 'day'):\n\t\t\treturn datetime.timedelta(days=factor)\n\t\telse:\n\t\t\traise ValueError(\"Unable to parse frequency %s\" % frequency)\n\telse:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None:\n\t\treturn None\n\tif frequency == 'always':\n\t\treturn None\n\tmatch = re.match(r'^(\\d+) (\\w+)$', frequency)\n\tif match is None:\n\t\traise ValueError('Bad frequency format: ' + frequency)\n\ttime_unit = match.group(2)\n\ttime_unit_plural = time_unit + 's'\n\tif time_unit not in ['year', 'month', 'week', 'day', 'hour', 'minute', 'second'] and time_unit_plural not in ['years', 'months', 'weeks', 'days', 'hours', 'minutes', 'seconds']:\n\t\traise ValueError('Bad frequency time unit: ' + time_unit)\n\treturn datetime.timedelta(**{time_unit: int(match.group(1))})\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_frequency(frequency):\n\tif frequency is None or frequency.lower() == \"always\":\n\t\treturn None\n\n\tmatch = re.match(r\"(\\d+) ?([a-z]+)\", frequency, re.IGNORECASE)\n\tif match is None:\n\t\traise ValueError(\"Invalid frequency format: %s\" % frequency)\n\tnumber = int(match.group(1))\n\ttimeunit = match.group(2)\n\tif timeunit == \"s\" or timeunit == \"sec\" or timeunit == \"second\" or timeunit == \"seconds\":\n\t\treturn datetime.timedelta(seconds=number)\n\telif timeunit == \"m\" or timeunit == \"min\" or timeunit == \"minute\" or timeunit == \"minutes\":\n\t\treturn datetime.timedelta(minutes=number)\n\telif timeunit == \"h\" or timeunit == \"hour\" or timeunit == \"hours\":\n\t\treturn datetime.timedelta(hours=number)\n\telif timeunit == \"d\" or timeunit == \"day\" or timeunit == \"days\":\n\t\treturn dat\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_18.py", "name": "is_local", "docstring": "Checks if the host is the localhost\n\n:param host: The hostname or ip\n:return: True if the host is the localhost", "ques_id": "62ece4992e6aefcf4aabbd78", "code": ["def is_local(host):\n\treturn host in local_host_list\n\n\n", "def is_local(host):\n\treturn host in local_hosts\n\n", "def is_local(host):\n\tif host in [\"localhost\", \"127.0.0.1\"]:\n\t\treturn True\n\treturn is_local_ip(host) or is_local_name(host)\n\n\n", "def is_local(host):\n\tif host in [\"localhost\", \"127.0.0.1\", socket.gethostname(), socket.getfqdn(), socket.gethostbyname(socket.gethostname())]:\n\t\treturn True\n\treturn False\n\n\n", "def is_local(host):\n\tif host.startswith(\"127.\"):\n\t\treturn True\n\n\tif host == \"localhost\":\n\t\treturn True\n\n\tif host in [\"localhost.localdomain\", \"localhost6.localdomain6\", \"ip6-localhost\"]:\n\t\treturn True\n\n\tif host in [\"0.0.0.0\", \"::\", \"::1\"]:\n\t\treturn True\n\n\tif host == socket.gethostname():\n\t\treturn True\n\n\tif host in socket.gethostbyname_ex(socket.gethostname())[2]:\n\t\treturn True\n\n\treturn False\n\n\n", "def is_local(host):\n\treturn is_local_ip(host) or is_local_domain_name(host) or host == \"localhost\"\n\n\n", "def is_local(host):\n\tif host in local_hosts:\n\t\treturn True\n\tif host == socket.gethostname():\n\t\treturn True\n\n\ttry:\n\t\tlocal_ip = socket.gethostbyname(socket.gethostname())\n\t\tif host == local_ip:\n\t\t\treturn True\n\texcept socket.gaierror:\n\t\tpass\n\n\ttry:\n\t\tlocal_ip = socket.gethostbyname(socket.getfqdn())\n\t\tif host == local_ip:\n\t\t\treturn True\n\texcept socket.gaierror:\n\t\tpass\n\n\treturn False\n", "def is_local(host):\n\treturn host in [socket.gethostname(), socket.getfqdn(), socket.gethostbyname(socket.gethostname()), socket.gethostbyname(socket.getfqdn()), \"localhost\", \"127.0.0.1\"]\n\n", "def is_local(host):\n\treturn is_empty(host) or host == \"localhost\" or host == \"127.0.0.1\" or host in local_hostname() or is_local_ip(host)\n\n\n", "def is_local(host):\n\thost = host.lower()\n\tif host in [\"localhost\", \"127.0.0.1\", socket.gethostname(), socket.gethostbyname(socket.gethostname()), socket.getfqdn()]:\n\t\treturn True\n\treturn False\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_local(host):\n\treturn host in local_host_list\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\treturn host in local_hosts\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\tif host in [\"localhost\", \"127.0.0.1\"]:\n\t\treturn True\n\treturn is_local_ip(host) or is_local_name(host)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\tif host in [\"localhost\", \"127.0.0.1\", socket.gethostname(), socket.getfqdn(), socket.gethostbyname(socket.gethostname())]:\n\t\treturn True\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\tif host.startswith(\"127.\"):\n\t\treturn True\n\n\tif host == \"localhost\":\n\t\treturn True\n\n\tif host in [\"localhost.localdomain\", \"localhost6.localdomain6\", \"ip6-localhost\"]:\n\t\treturn True\n\n\tif host in [\"0.0.0.0\", \"::\", \"::1\"]:\n\t\treturn True\n\n\tif host == socket.gethostname():\n\t\treturn True\n\n\tif host in socket.gethostbyname_ex(socket.gethostname())[2]:\n\t\treturn True\n\n\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\treturn is_local_ip(host) or is_local_domain_name(host) or host == \"localhost\"\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\tif host in local_hosts:\n\t\treturn True\n\tif host == socket.gethostname():\n\t\treturn True\n\n\ttry:\n\t\tlocal_ip = socket.gethostbyname(socket.gethostname())\n\t\tif host == local_ip:\n\t\t\treturn True\n\texcept socket.gaierror:\n\t\tpass\n\n\ttry:\n\t\tlocal_ip = socket.gethostbyname(socket.getfqdn())\n\t\tif host == local_ip:\n\t\t\treturn True\n\texcept socket.gaierror:\n\t\tpass\n\n\treturn False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\treturn host in [socket.gethostname(), socket.getfqdn(), socket.gethostbyname(socket.gethostname()), socket.gethostbyname(socket.getfqdn()), \"localhost\", \"127.0.0.1\"]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_local(host):\n\treturn is_empty(host) or host == \"localhost\" or host == \"127.0.0.1\" or host in local_hostname() or is_local_ip(host)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_local(host):\n\thost = host.lower()\n\tif host in [\"localhost\", \"127.0.0.1\", socket.gethostname(), socket.gethostbyname(socket.gethostname()), socket.getfqdn()]:\n\t\treturn True\n\treturn False\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_8.py", "name": "make_find_paths", "docstring": "Given a sequence of path fragments or patterns as passed to `--find`, transform all path\nfragments into glob patterns. Pass through existing patterns untouched.\n\nFor example, given find_paths of:\n\n  ['foo.txt', 'pp:root/somedir']\n\n... transform that into:\n\n  ['sh:**/*foo.txt*/**', 'pp:root/somedir']", "ques_id": "62ece4992e6aefcf4aabbd79", "code": ["def make_find_paths(find_paths):\n\treturn tuple(glob.glob(os.path.expanduser(path)) if \"*\" not in path else path for path in find_paths)\n\n", "def make_find_paths(find_paths):\n\treturn tuple(path if isinstance(path, str) else ' '.join(path) for path in find_paths)\n\n\n", "def make_find_paths(find_paths):\n\tif find_paths:\n\t\treturn tuple(make_glob_pattern(p) for p in find_paths)\n\telse:\n\t\treturn tuple()\n\n", "def make_find_paths(find_paths):\n\tdef transform_path(p):\n\t\treturn os.path.normpath(p) if os.path.isdir(p) else p\n\treturn tuple(map(transform_path, find_paths))\n\n", "def make_find_paths(find_paths):\n\treturn tuple(path if is_glob_pattern(path) else f\"{path}/**/*\" for path in find_paths)\n\n", "def make_find_paths(find_paths):\n\tfind_paths = list(find_paths)\n\ti = 0\n\twhile i < len(find_paths):\n\t\tpath = find_paths[i]\n\t\tif '*' in path:\n\t\t\ti += 1\n\t\t\tcontinue\n\t\tif os.path.isdir(path):\n\t\t\tfind_paths[i] = os.path.join(path, '*')\n\t\telse:\n\t\t\tfind_paths[i] = path + '*'\n\t\ti += 1\n\treturn tuple(find_paths)\n\n\n", "def make_find_paths(find_paths):\n\treturn tuple(map(lambda x: x if is_glob(x) else '{}/**/*'.format(x), find_paths))\n\n\n", "def make_find_paths(find_paths):\n\tnew_find_paths = []\n\tfor find_path in find_paths:\n\t\tif os.path.isdir(find_path):\n\t\t\tnew_find_paths.append(os.path.join(find_path, \"**/*\"))\n\t\telse:\n\t\t\tnew_find_paths.append(find_path)\n\treturn tuple(new_find_paths)\n\n\n", "def make_find_paths(find_paths):\n\tif not find_paths:\n\t\treturn tuple()\n\n\treturn tuple(glob.glob(path) if not is_pattern(path) else path for path in find_paths)\n\n\n", "def make_find_paths(find_paths):\n\treturn tuple(\n\t\tpath if path.startswith('.') or ('*' in path) else '**/{}'.format(path)\n\t\tfor path in find_paths\n\t)\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def make_find_paths(find_paths):\n\treturn tuple(glob.glob(os.path.expanduser(path)) if \"*\" not in path else path for path in find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\treturn tuple(path if isinstance(path, str) else ' '.join(path) for path in find_paths)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tif find_paths:\n\t\treturn tuple(make_glob_pattern(p) for p in find_paths)\n\telse:\n\t\treturn tuple()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tdef transform_path(p):\n\t\treturn os.path.normpath(p) if os.path.isdir(p) else p\n\treturn tuple(map(transform_path, find_paths))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\treturn tuple(path if is_glob_pattern(path) else f\"{path}/**/*\" for path in find_paths)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tfind_paths = list(find_paths)\n\ti = 0\n\twhile i < len(find_paths):\n\t\tpath = find_paths[i]\n\t\tif '*' in path:\n\t\t\ti += 1\n\t\t\tcontinue\n\t\tif os.path.isdir(path):\n\t\t\tfind_paths[i] = os.path.join(path, '*')\n\t\telse:\n\t\t\tfind_paths[i] = path + '*'\n\t\ti += 1\n\treturn tuple(find_paths)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\treturn tuple(map(lambda x: x if is_glob(x) else '{}/**/*'.format(x), find_paths))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tnew_find_paths = []\n\tfor find_path in find_paths:\n\t\tif os.path.isdir(find_path):\n\t\t\tnew_find_paths.append(os.path.join(find_path, \"**/*\"))\n\t\telse:\n\t\t\tnew_find_paths.append(find_path)\n\treturn tuple(new_find_paths)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\tif not find_paths:\n\t\treturn tuple()\n\n\treturn tuple(glob.glob(path) if not is_pattern(path) else path for path in find_paths)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def make_find_paths(find_paths):\n\treturn tuple(\n\t\tpath if path.startswith('.') or ('*' in path) else '**/{}'.format(path)\n\t\tfor path in find_paths\n\t)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_19.py", "name": "is_gitbash", "docstring": "returns True if you run in a Windows gitbash\n\n:return: True if gitbash", "ques_id": "62ece4992e6aefcf4aabbd7a", "code": ["def is_gitbash():\n\tif \"git-bash.exe\" in sys.executable:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_gitbash():\n\tif os.name == 'nt':\n\t\ttry:\n\t\t\treturn 'MINGW' in os.environ['TERM']\n\t\texcept KeyError:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "def is_gitbash():\n\tif sys.platform == 'win32':\n\t\tif os.getenv('TERM') == 'cygwin':\n\t\t\treturn True\n\treturn False\n\n", "def is_gitbash():\n\treturn os.environ['TERM'] == 'cygwin'\n\n\n", "def is_gitbash():\n\treturn platform.system() == \"Windows\" and \"git\" in os.environ.get(\"TERM\")\n\n\n", "def is_gitbash():\n\treturn os.getenv(\"GIT_PYTHON_GIT_EXECUTABLE\") is not None\n\n\n", "def is_gitbash():\n\treturn (os.path.basename(os.environ['SHELL'])=='bash.exe')\n\n", "def is_gitbash():\n\treturn 'git-bash' in platform.system().lower()\n\n\n", "def is_gitbash():\n\treturn 'git-bash' in os.environ['TERM']\n\n", "def is_gitbash():\n\treturn platform.system() == \"Windows\" and \"GIT_PREFIX\" in os.environ\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def is_gitbash():\n\tif \"git-bash.exe\" in sys.executable:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\tif os.name == 'nt':\n\t\ttry:\n\t\t\treturn 'MINGW' in os.environ['TERM']\n\t\texcept KeyError:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\tif sys.platform == 'win32':\n\t\tif os.getenv('TERM') == 'cygwin':\n\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\treturn os.environ['TERM'] == 'cygwin'\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\treturn platform.system() == \"Windows\" and \"git\" in os.environ.get(\"TERM\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\treturn os.getenv(\"GIT_PYTHON_GIT_EXECUTABLE\") is not None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\treturn (os.path.basename(os.environ['SHELL'])=='bash.exe')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\treturn 'git-bash' in platform.system().lower()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_gitbash():\n\treturn 'git-bash' in os.environ['TERM']\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_gitbash():\n\treturn platform.system() == \"Windows\" and \"GIT_PREFIX\" in os.environ\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_9.py", "name": "write_configuration", "docstring": "Given a target config filename and rendered config YAML, write it out to file. Create any\ncontaining directories as needed. But if the file already exists and overwrite is False,\nabort before writing anything.", "ques_id": "62ece4992e6aefcf4aabbd7b", "code": ["def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\t# print(\"%s already exists and overwrite is False. Aborting.\" % config_filename)\n\t\treturn False\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tf.close()\n\t\tos.chmod(config_filename, mode)\n\t\treturn True\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not os.path.exists(config_filename) or overwrite:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\t\tos.chmod(config_filename, mode)\n\telse:\n\t\tprint(\"File %s already exists, not overwriting.\" % (config_filename,))\n\t\tsys.exit(1)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tprint(\"%s already exists, aborting without writing.\" % config_filename)\n\t\tsys.exit(1)\n\telse:\n\t\twith open(config_filename, 'w') as outfile:\n\t\t\toutfile.write(rendered_config)\n\t\t\tos.chmod(config_filename, mode)\n\t\tprint(\"Wrote %s\" % config_filename)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise FileExistsError(\"File already exists: {}\".format(config_filename))\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise IOError('Config file {} already exists, and overwrite is False'.format(config_filename))\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\n\t# Set permissions for the file\n\tos.chmod(config_filename, mode)\n\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# os.path.exists() is True if the path is a file or a directory.\n\t# os.path.isfile() is True only if the path is a file.\n\tif not overwrite and os.path.isfile(config_filename):\n\t\traise IOError(\"Configuration file %s already exists.\" % config_filename)\n\t\t\n\twith open(config_filename, \"w\") as f:\n\t\t# yaml.dump() writes out a YAML representation of the given data.\n\t\tf.write(yaml.dump(rendered_config, default_flow_style=False))\n\t\t\n\t# Change the file mode to the specified value.\n\tos.chmod(config_filename, mode)\n\t\n\t# Set the owner and group ID of path to the numeric uid and gid.\n\t# If either of the ID is negative, then that ID is not changed.\n\tos.chown(config_filename, -1, -1)\n\t\n\treturn None\n\t\n\t\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tprint(\"File {} already exists. Aborting write.\".format(config_filename))\n\t\treturn\n\telif not os.path.exists(config_filename):\n\t\tprint(\"Creating {}.\".format(config_filename))\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\telse:\n\t\tprint(\"Writing to {}.\".format(config_filename))\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# print(\"config_filename: %s\" % config_filename)\n\t# print(\"rendered_config: %s\" % rendered_config)\n\t# print(\"overwrite: %s\" % overwrite)\n\t# print(\"os.path.exists(config_filename): %s\" % os.path.exists(config_filename))\n\t# print(\"overwrite or not os.path.exists(config_filename): %s\" % (overwrite or not os.path.exists(config_filename)))\n\tif overwrite or not os.path.exists(config_filename):\n\t\t# print(\"Attempting to write to file: %s\" % config_filename)\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\t\tf.close()\n\t\tos.chmod(config_filename, mode)\n\t\t# print(\"Wrote to file: %s\" % config_filename)\n\telse:\n\t\traise Exception(\"File %s already exists. Remove it before proceeding.\" % config_filename)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tlogging.info(\"Configuration file %s already exists, aborting.\" % config_filename)\n\t\treturn\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tf.close()\n\n\tos.chmod(config_filename, mode)\n\n", "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif path.exists(config_filename) and not overwrite:\n\t\tprint(\"{} already exists, aborting\".format(config_filename))\n\t\texit(1)\n\telse:\n\t\twith open(config_filename, 'w+') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\t\tconfig_file.close()\n\t\t\tprint(\"{} written\".format(config_filename))\n\t\t\tchmod(config_filename, mode)\n\t\treturn None\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\t# print(\"%s already exists and overwrite is False. Aborting.\" % config_filename)\n\t\treturn False\n\telse:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\tf.close()\n\t\tos.chmod(config_filename, mode)\n\t\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not os.path.exists(config_filename) or overwrite:\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\t\tos.chmod(config_filename, mode)\n\telse:\n\t\tprint(\"File %s already exists, not overwriting.\" % (config_filename,))\n\t\tsys.exit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tprint(\"%s already exists, aborting without writing.\" % config_filename)\n\t\tsys.exit(1)\n\telse:\n\t\twith open(config_filename, 'w') as outfile:\n\t\t\toutfile.write(rendered_config)\n\t\t\tos.chmod(config_filename, mode)\n\t\tprint(\"Wrote %s\" % config_filename)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise FileExistsError(\"File already exists: {}\".format(config_filename))\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif not overwrite and os.path.exists(config_filename):\n\t\traise IOError('Config file {} already exists, and overwrite is False'.format(config_filename))\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\n\t# Set permissions for the file\n\tos.chmod(config_filename, mode)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# os.path.exists() is True if the path is a file or a directory.\n\t# os.path.isfile() is True only if the path is a file.\n\tif not overwrite and os.path.isfile(config_filename):\n\t\traise IOError(\"Configuration file %s already exists.\" % config_filename)\n\t\t\n\twith open(config_filename, \"w\") as f:\n\t\t# yaml.dump() writes out a YAML representation of the given data.\n\t\tf.write(yaml.dump(rendered_config, default_flow_style=False))\n\t\t\n\t# Change the file mode to the specified value.\n\tos.chmod(config_filename, mode)\n\t\n\t# Set the owner and group ID of path to the numeric uid and gid.\n\t# If either of the ID is negative, then that ID is not changed.\n\tos.chown(config_filename, -1, -1)\n\t\n\treturn None\n\t\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tprint(\"File {} already exists. Aborting write.\".format(config_filename))\n\t\treturn\n\telif not os.path.exists(config_filename):\n\t\tprint(\"Creating {}.\".format(config_filename))\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n\telse:\n\t\tprint(\"Writing to {}.\".format(config_filename))\n\t\twith open(config_filename, \"w\") as f:\n\t\t\tf.write(rendered_config)\n\t\tos.chmod(config_filename, mode)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\t# print(\"config_filename: %s\" % config_filename)\n\t# print(\"rendered_config: %s\" % rendered_config)\n\t# print(\"overwrite: %s\" % overwrite)\n\t# print(\"os.path.exists(config_filename): %s\" % os.path.exists(config_filename))\n\t# print(\"overwrite or not os.path.exists(config_filename): %s\" % (overwrite or not os.path.exists(config_filename)))\n\tif overwrite or not os.path.exists(config_filename):\n\t\t# print(\"Attempting to write to file: %s\" % config_filename)\n\t\twith open(config_filename, 'w') as f:\n\t\t\tf.write(rendered_config)\n\t\t\tf.close()\n\t\tos.chmod(config_filename, mode)\n\t\t# print(\"Wrote to file: %s\" % config_filename)\n\telse:\n\t\traise Exception(\"File %s already exists. Remove it before proceeding.\" % config_filename)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif os.path.exists(config_filename) and not overwrite:\n\t\tlogging.info(\"Configuration file %s already exists, aborting.\" % config_filename)\n\t\treturn\n\n\twith open(config_filename, 'w') as f:\n\t\tf.write(rendered_config)\n\t\tf.close()\n\n\tos.chmod(config_filename, mode)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def write_configuration(config_filename, rendered_config, mode=0o600, overwrite=False):\n\tif path.exists(config_filename) and not overwrite:\n\t\tprint(\"{} already exists, aborting\".format(config_filename))\n\t\texit(1)\n\telse:\n\t\twith open(config_filename, 'w+') as config_file:\n\t\t\tconfig_file.write(rendered_config)\n\t\t\tconfig_file.close()\n\t\t\tprint(\"{} written\".format(config_filename))\n\t\t\tchmod(config_filename, mode)\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_22.py", "name": "oneline", "docstring": "converts a script to one line command.\nTHis is useful to run a single ssh command and pass a one line script.\n\n:param script:\n:return:", "ques_id": "62ece4992e6aefcf4aabbd7c", "code": ["def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join([s.strip() for s in script.splitlines() if s.strip()])\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join([line.strip() for line in script.split('\\n') if line.strip()])\n\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.split(\"\\n\"))\n\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join([c.strip() for c in script.splitlines() if c.strip()])\n\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script)\n\n\n", "def oneline(script, seperator=\" && \"):\n\tlines = script.split(\"\\n\")\n\tlines = [line.strip() for line in lines]\n\tlines = [line for line in lines if line != '']\n\treturn seperator.join(lines)\n\n\n", "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.strip().split(\"\\n\"))\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join([s.strip() for s in script.splitlines() if s.strip()])\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join([line.strip() for line in script.split('\\n') if line.strip()])\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.split(\"\\n\"))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join([c.strip() for c in script.splitlines() if c.strip()])\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.splitlines())\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\tlines = script.split(\"\\n\")\n\tlines = [line.strip() for line in lines]\n\tlines = [line for line in lines if line != '']\n\treturn seperator.join(lines)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def oneline(script, seperator=\" && \"):\n\treturn seperator.join(script.strip().split(\"\\n\"))\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_16.py", "name": "subprocess_run_helper", "docstring": "Run a function in a sub-process.\n\nParameters\n----------\nfunc : function\n    The function to be run.  It must be in a module that is importable.\n*args : str\n    Any additional command line arguments to be passed in\n    the first argument to ``subprocess.run``.\nextra_env : dict[str, str]\n    Any additional environment variables to be set for the subprocess.", "ques_id": "62ece4992e6aefcf4aabbd7d", "code": ["def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# use the input timeout value, if it exists\n\tsubprocess_timeout = timeout\n\n\t# if the user didn't specify a timeout, use the default\n\tif not subprocess_timeout:\n\t\tsubprocess_timeout = config.DEFAULT_SUBPROCESS_TIMEOUT\n\n\t# Get the module name and function name\n\t# both must be importable\n\t# e.g. from module1 import function1\n\tmodule_name = func.__module__\n\tfunction_name = func.__name__\n\n\t# get the absolute path of the module\n\t# we need to pass this to the subprocess\n\t# so that the subprocess can import it\n\tmodule_path = os.path.abspath(inspect.getfile(func))\n\n\t# get the path to the python executable\n\t# we need this so that we can run the module\n\t# using the same python interpreter as us\n\tpython_executable = sys.executable\n\n\t# set up the arguments to the subprocess\n\t# the first argument is the path to the python executable\n\t# the second argument is the path to the module\n\t# the third\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tcmd = [sys.executable, '-m', func.__module__, func.__name__]\n\tcmd.extend(args)\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\treturn subprocess.run(cmd, timeout=timeout, env=env)\n\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# Store the current environment variables\n\tcur_env = dict(os.environ)\n\n\t# If any extra environment variables were passed in, add them to the current environment variables\n\tif extra_env is not None:\n\t\tcur_env.update(extra_env)\n\n\t# Set the current environment variables as the environment variables for the sub-process\n\tenv = cur_env\n\n\t# If a timeout was passed in, set it as the timeout for the sub-process\n\tif timeout is not None:\n\t\ttimeout = timeout\n\n\t# Run the function in a sub-process\n\treturn subprocess.run(func, timeout=timeout, env=env, *args)\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tmodule_name = func.__module__\n\tfunc_name = func.__name__\n\tnew_env = os.environ.copy()\n\tnew_env.update(extra_env)\n\tcommand = [sys.executable, \"-m\", module_name, func_name] + list(args)\n\treturn subprocess.run(command, env=new_env, timeout=timeout)\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# The module name of the function to be run\n\tmodulename = func.__module__\n\tif modulename == '__main__':\n\t\t# We are running a script. We need to find the actual path\n\t\tmodulename = os.path.splitext(os.path.basename(sys.argv[0]))[0]\n\t\tpath = os.path.dirname(os.path.abspath(sys.argv[0]))\n\t\tsys.path.insert(0, path)\n\n\t# We need to import the module\n\timportlib.import_module(modulename)\n\n\t# Create a temporary file to store the arguments\n\tfd, filename = tempfile.mkstemp()\n\twith os.fdopen(fd, 'wb') as f:\n\t\tpickle.dump(args, f)\n\n\t# Create a temporary file to store the function\n\tfd, funcname = tempfile.mkstemp()\n\twith os.fdopen(fd, 'wb') as f:\n\t\tpickle.dump(func.__name__, f)\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# Get the function name and module name\n\tfunc_name = func.__name__\n\tmodule_name = inspect.getmodule(func).__name__\n\n\t# Create the command string\n\tcmd = [sys.executable, '-m', module_name, '--func', func_name]\n\tif args:\n\t\tcmd.extend(args)\n\n\t# Prepare the environment\n\tenv_vars = os.environ.copy()\n\tif extra_env:\n\t\tenv_vars.update(extra_env)\n\n\t# Run the subprocess\n\treturn subprocess.run(cmd, env=env_vars, timeout=timeout)\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif timeout < 0:\n\t\traise ValueError('timeout must be a positive int')\n\tif extra_env is None:\n\t\textra_env = {}\n\t# get the function's module name\n\tfunc_module = get_module_name(func)\n\t# get the name of the function\n\tfunc_name = get_function_name(func)\n\t# create the command to invoke the function\n\tcmd = [func_module + '.' + func_name]\n\tfor arg in args:\n\t\tcmd.append(str(arg))\n\t# run the process\n\tp = subprocess.run(cmd, timeout=timeout, env={**os.environ, **extra_env})\n\treturn p\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tcmd = [sys.executable, '-m', func.__module__, func.__name__] + list(args)\n\twith tempfile.TemporaryDirectory() as tmpdir:\n\t\twith open(os.path.join(tmpdir, 'stdout.log'), 'w') as stdout_file, open(os.path.join(tmpdir, 'stderr.log'), 'w') as stderr_file:\n\t\t\tenv = os.environ.copy()\n\t\t\tif extra_env:\n\t\t\t\tenv.update(extra_env)\n\t\t\tsubprocess.run(cmd, stdout=stdout_file, stderr=stderr_file, env=env, check=True, timeout=timeout)\n\treturn CompletedProcess(cmd, 0, None, None)\n\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\t# Set PATH to ensure that the subprocess uses the same binary that we use\n\tenv = os.environ.copy()\n\tenv[\"PATH\"] = os.pathsep.join([os.path.dirname(sys.executable)] + env[\"PATH\"].split(os.pathsep))\n\tfor k, v in extra_env.items():\n\t\tenv[k] = v\n\tp = subprocess.Popen([sys.executable, \"-c\", \"import sys; sys.path.append(sys.argv[1]); import \" + func.__module__ + \"; \" + func.__name__ + \".main(sys.argv[2:])\", os.getcwd(), *args], env=env)\n\ttry:\n\t\treturn p.wait(timeout)\n\texcept subprocess.TimeoutExpired:\n\t\tp.kill()\n\t\tp.wait()\n\t\traise\n\n", "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tdef subprocess_run_func(func, *args):\n\t\t# The function to be run in the subprocess\n\t\timport sys\n\t\timport os\n\t\tif extra_env:\n\t\t\tfor key, value in extra_env.items():\n\t\t\t\tos.environ[key] = value\n\t\tsys.argv = [sys.argv[0]] + list(args)\n\t\tfunc()\n\n\timport subprocess\n\timport multiprocessing\n\timport signal\n\n\t# Create a separate process to run the function\n\tp = multiprocessing.Process(target=subprocess_run_func, args=(func, *args))\n\n\t# Start the process\n\tp.start()\n\n\t# Wait for the process to complete\n\tp.join(timeout)\n\n\t# If the process is still running, terminate it.\n\tif p.is_alive():\n\t\tp.terminate()\n\t\tp.join()\n\n\t\t# If the process is still alive, kill it.\n\t\tif p.is_alive():\n\t\t\tp.kill()\n\t\t\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# use the input timeout value, if it exists\n\tsubprocess_timeout = timeout\n\n\t# if the user didn't specify a timeout, use the default\n\tif not subprocess_timeout:\n\t\tsubprocess_timeout = config.DEFAULT_SUBPROCESS_TIMEOUT\n\n\t# Get the module name and function name\n\t# both must be importable\n\t# e.g. from module1 import function1\n\tmodule_name = func.__module__\n\tfunction_name = func.__name__\n\n\t# get the absolute path of the module\n\t# we need to pass this to the subprocess\n\t# so that the subprocess can import it\n\tmodule_path = os.path.abspath(inspect.getfile(func))\n\n\t# get the path to the python executable\n\t# we need this so that we can run the module\n\t# using the same python interpreter as us\n\tpython_executable = sys.executable\n\n\t# set up the arguments to the subprocess\n\t# the first argument is the path to the python executable\n\t# the second argument is the path to the module\n\t# the third\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tcmd = [sys.executable, '-m', func.__module__, func.__name__]\n\tcmd.extend(args)\n\tenv = os.environ.copy()\n\tenv.update(extra_env)\n\treturn subprocess.run(cmd, timeout=timeout, env=env)\n\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# Store the current environment variables\n\tcur_env = dict(os.environ)\n\n\t# If any extra environment variables were passed in, add them to the current environment variables\n\tif extra_env is not None:\n\t\tcur_env.update(extra_env)\n\n\t# Set the current environment variables as the environment variables for the sub-process\n\tenv = cur_env\n\n\t# If a timeout was passed in, set it as the timeout for the sub-process\n\tif timeout is not None:\n\t\ttimeout = timeout\n\n\t# Run the function in a sub-process\n\treturn subprocess.run(func, timeout=timeout, env=env, *args)\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\tmodule_name = func.__module__\n\tfunc_name = func.__name__\n\tnew_env = os.environ.copy()\n\tnew_env.update(extra_env)\n\tcommand = [sys.executable, \"-m\", module_name, func_name] + list(args)\n\treturn subprocess.run(command, env=new_env, timeout=timeout)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# The module name of the function to be run\n\tmodulename = func.__module__\n\tif modulename == '__main__':\n\t\t# We are running a script. We need to find the actual path\n\t\tmodulename = os.path.splitext(os.path.basename(sys.argv[0]))[0]\n\t\tpath = os.path.dirname(os.path.abspath(sys.argv[0]))\n\t\tsys.path.insert(0, path)\n\n\t# We need to import the module\n\timportlib.import_module(modulename)\n\n\t# Create a temporary file to store the arguments\n\tfd, filename = tempfile.mkstemp()\n\twith os.fdopen(fd, 'wb') as f:\n\t\tpickle.dump(args, f)\n\n\t# Create a temporary file to store the function\n\tfd, funcname = tempfile.mkstemp()\n\twith os.fdopen(fd, 'wb') as f:\n\t\tpickle.dump(func.__name__, f)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\t# Get the function name and module name\n\tfunc_name = func.__name__\n\tmodule_name = inspect.getmodule(func).__name__\n\n\t# Create the command string\n\tcmd = [sys.executable, '-m', module_name, '--func', func_name]\n\tif args:\n\t\tcmd.extend(args)\n\n\t# Prepare the environment\n\tenv_vars = os.environ.copy()\n\tif extra_env:\n\t\tenv_vars.update(extra_env)\n\n\t# Run the subprocess\n\treturn subprocess.run(cmd, env=env_vars, timeout=timeout)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif timeout < 0:\n\t\traise ValueError('timeout must be a positive int')\n\tif extra_env is None:\n\t\textra_env = {}\n\t# get the function's module name\n\tfunc_module = get_module_name(func)\n\t# get the name of the function\n\tfunc_name = get_function_name(func)\n\t# create the command to invoke the function\n\tcmd = [func_module + '.' + func_name]\n\tfor arg in args:\n\t\tcmd.append(str(arg))\n\t# run the process\n\tp = subprocess.run(cmd, timeout=timeout, env={**os.environ, **extra_env})\n\treturn p\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tcmd = [sys.executable, '-m', func.__module__, func.__name__] + list(args)\n\twith tempfile.TemporaryDirectory() as tmpdir:\n\t\twith open(os.path.join(tmpdir, 'stdout.log'), 'w') as stdout_file, open(os.path.join(tmpdir, 'stderr.log'), 'w') as stderr_file:\n\t\t\tenv = os.environ.copy()\n\t\t\tif extra_env:\n\t\t\t\tenv.update(extra_env)\n\t\t\tsubprocess.run(cmd, stdout=stdout_file, stderr=stderr_file, env=env, check=True, timeout=timeout)\n\treturn CompletedProcess(cmd, 0, None, None)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tif extra_env is None:\n\t\textra_env = {}\n\t# Set PATH to ensure that the subprocess uses the same binary that we use\n\tenv = os.environ.copy()\n\tenv[\"PATH\"] = os.pathsep.join([os.path.dirname(sys.executable)] + env[\"PATH\"].split(os.pathsep))\n\tfor k, v in extra_env.items():\n\t\tenv[k] = v\n\tp = subprocess.Popen([sys.executable, \"-c\", \"import sys; sys.path.append(sys.argv[1]); import \" + func.__module__ + \"; \" + func.__name__ + \".main(sys.argv[2:])\", os.getcwd(), *args], env=env)\n\ttry:\n\t\treturn p.wait(timeout)\n\texcept subprocess.TimeoutExpired:\n\t\tp.kill()\n\t\tp.wait()\n\t\traise\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def subprocess_run_helper(func, *args, timeout, extra_env=None):\n\tdef subprocess_run_func(func, *args):\n\t\t# The function to be run in the subprocess\n\t\timport sys\n\t\timport os\n\t\tif extra_env:\n\t\t\tfor key, value in extra_env.items():\n\t\t\t\tos.environ[key] = value\n\t\tsys.argv = [sys.argv[0]] + list(args)\n\t\tfunc()\n\n\timport subprocess\n\timport multiprocessing\n\timport signal\n\n\t# Create a separate process to run the function\n\tp = multiprocessing.Process(target=subprocess_run_func, args=(func, *args))\n\n\t# Start the process\n\tp.start()\n\n\t# Wait for the process to complete\n\tp.join(timeout)\n\n\t# If the process is still running, terminate it.\n\tif p.is_alive():\n\t\tp.terminate()\n\t\tp.join()\n\n\t\t# If the process is still alive, kill it.\n\t\tif p.is_alive():\n\t\t\tp.kill()\n\t\t\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_6.py", "name": "_resolve_string", "docstring": "Get the value from environment given a matcher containing a name and an optional default value.\nIf the variable is not defined in environment and no default value is provided, an Error is raised.", "ques_id": "62ece4992e6aefcf4aabbd7e", "code": ["def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tdefault_value = matcher.group('default_value')\n\tvalue = os.environ.get(name, default_value)\n\tif value == None:\n\t\traise Error(\"Environment variable %s is not defined and no default value was provided\" % name)\n\treturn value\n\n\n", "def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\ttry:\n\t\treturn os.environ[name]\n\texcept KeyError:\n\t\tif default is not None:\n\t\t\treturn default\n\t\telse:\n\t\t\traise Error('Environment variable {0} not defined'.format(name))\n\n\n", "def _resolve_string(matcher):\n\tname = matcher.group(1)\n\tvalue = _get_env_value(name)\n\tif value is None:\n\t\tif matcher.group(3) is None:\n\t\t\traise Error(\"environment variable '%s' is not defined\" % name)\n\t\treturn matcher.group(3)\n\treturn value\n\n\n", "def _resolve_string(matcher):\n\tvariable = matcher.group('name')\n\tdefault = matcher.group('default')\n\tvalue = os.environ.get(variable, None)\n\tif value == None and default == None:\n\t\traise Error('Undefined variable: ' + variable)\n\treturn value if value != None else default\n\n\n", "def _resolve_string(matcher):\n\tname = matcher.group(\"name\")\n\tdefault = matcher.group(\"default\")\n\tif default:\n\t\treturn os.getenv(name, default)\n\telse:\n\t\ttry:\n\t\t\treturn os.environ[name]\n\t\texcept KeyError:\n\t\t\traise Error(\"Environment variable '{0}' is not defined.\".format(name))\n\n", "def _resolve_string(matcher):\n\tname = matcher.groupdict()['name']\n\tdefault = matcher.groupdict()['default']\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Error(\"Failed to resolve string: '{}' is not defined in the environment and no default value is provided\".format(name))\n\n", "def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tvalue = os.environ.get(name)\n\tif not value:\n\t\tif matcher.group('default') is not None:\n\t\t\tvalue = matcher.group('default')\n\t\telse:\n\t\t\traise ValueError('Environment variable {0} is not defined and no default value was provided'.format(name))\n\treturn value\n\n\n", "def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\treturn os.environ.get(name, default) if default else os.environ[name]\n\n\n", "def _resolve_string(matcher):\n\tname = matcher.groupdict()['name']\n\tdefault = matcher.groupdict()['default']\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Error('Undefined variable %s' % name)\n\n\n", "def _resolve_string(matcher):\n\tif matcher.group(\"name\") in os.environ:\n\t\treturn os.environ[matcher.group(\"name\")]\n\telif matcher.group(\"default\"):\n\t\treturn matcher.group(\"default\")\n\telse:\n\t\traise Error(\"Undefined environment variable: {0}\".format(matcher.group(\"name\")))\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tdefault_value = matcher.group('default_value')\n\tvalue = os.environ.get(name, default_value)\n\tif value == None:\n\t\traise Error(\"Environment variable %s is not defined and no default value was provided\" % name)\n\treturn value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\ttry:\n\t\treturn os.environ[name]\n\texcept KeyError:\n\t\tif default is not None:\n\t\t\treturn default\n\t\telse:\n\t\t\traise Error('Environment variable {0} not defined'.format(name))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group(1)\n\tvalue = _get_env_value(name)\n\tif value is None:\n\t\tif matcher.group(3) is None:\n\t\t\traise Error(\"environment variable '%s' is not defined\" % name)\n\t\treturn matcher.group(3)\n\treturn value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tvariable = matcher.group('name')\n\tdefault = matcher.group('default')\n\tvalue = os.environ.get(variable, None)\n\tif value == None and default == None:\n\t\traise Error('Undefined variable: ' + variable)\n\treturn value if value != None else default\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group(\"name\")\n\tdefault = matcher.group(\"default\")\n\tif default:\n\t\treturn os.getenv(name, default)\n\telse:\n\t\ttry:\n\t\t\treturn os.environ[name]\n\t\texcept KeyError:\n\t\t\traise Error(\"Environment variable '{0}' is not defined.\".format(name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.groupdict()['name']\n\tdefault = matcher.groupdict()['default']\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Error(\"Failed to resolve string: '{}' is not defined in the environment and no default value is provided\".format(name))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tvalue = os.environ.get(name)\n\tif not value:\n\t\tif matcher.group('default') is not None:\n\t\t\tvalue = matcher.group('default')\n\t\telse:\n\t\t\traise ValueError('Environment variable {0} is not defined and no default value was provided'.format(name))\n\treturn value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.group('name')\n\tdefault = matcher.group('default')\n\treturn os.environ.get(name, default) if default else os.environ[name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tname = matcher.groupdict()['name']\n\tdefault = matcher.groupdict()['default']\n\tif name in os.environ:\n\t\treturn os.environ[name]\n\telif default is not None:\n\t\treturn default\n\telse:\n\t\traise Error('Undefined variable %s' % name)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _resolve_string(matcher):\n\tif matcher.group(\"name\") in os.environ:\n\t\treturn os.environ[matcher.group(\"name\")]\n\telif matcher.group(\"default\"):\n\t\treturn matcher.group(\"default\")\n\telse:\n\t\traise Error(\"Undefined environment variable: {0}\".format(matcher.group(\"name\")))\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "slib_runnable/test_2.py", "name": "_parse_image_ref", "docstring": "Parse an image href into composite parts.\n\n:param image_href: href of an image\n:returns: a tuple of the form (image_id, netloc, use_ssl)\n:raises ValueError:", "ques_id": "62ece4992e6aefcf4aabbd7f", "code": ["def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\n\tuse_ssl = url.scheme == 'https' or url.scheme == 'glance+https'\n\treturn image_id, netloc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, url.netloc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, netloc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\ttry:\n\t\tparsed = urlparse(image_href)\n\t\tloc = parsed.netloc or parsed.path\n\t\tuse_ssl = parsed.scheme == 'https'\n\t\treturn parsed.path.split('/')[-1], loc, use_ssl\n\texcept Exception as e:\n\t\traise ValueError(_('Invalid image_href: %s') % e)\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, netloc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed = urlparse(image_href)\n\n\t# if url is not valid\n\tif parsed.scheme not in ('http', 'https'):\n\t\traise ValueError(_('Unsupported scheme: %(scheme)s') % {'scheme': parsed.scheme})\n\n\treturn (parsed.path.split('/')[-1], parsed.netloc, parsed.scheme == 'https')\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\tloc = parsed_url.netloc\n\timage_id = parsed_url.path.split('/')[-1]\n\n\tif image_id == '':\n\t\traise ValueError(\"Invalid image_href provided.\")\n\n\tuse_ssl = parsed_url.scheme == 'https'\n\treturn image_id, loc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlparse(image_href)\n\tassert components.scheme in ('http', 'https')\n\tassert components.netloc\n\timage_id = components.path.split('/')[-1]\n\tassert image_id, 'URL does not contain an image id: %s' % image_href\n\treturn image_id, components.netloc, components.scheme == 'https'\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t# TODO(bcwaldon): this function should be refactored to use urlparse\n\turl = urlparse(image_href)\n\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\n\treturn image_id, netloc, use_ssl\n\n\n", "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urllib.parse.urlparse(image_href)\n\timage_id = url.path.split('/')[-1]\n\tnetloc = url.netloc\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, netloc, use_ssl\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\n\tuse_ssl = url.scheme == 'https' or url.scheme == 'glance+https'\n\treturn image_id, netloc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, url.netloc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, netloc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\ttry:\n\t\tparsed = urlparse(image_href)\n\t\tloc = parsed.netloc or parsed.path\n\t\tuse_ssl = parsed.scheme == 'https'\n\t\treturn parsed.path.split('/')[-1], loc, use_ssl\n\texcept Exception as e:\n\t\traise ValueError(_('Invalid image_href: %s') % e)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urlparse(image_href)\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, netloc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed = urlparse(image_href)\n\n\t# if url is not valid\n\tif parsed.scheme not in ('http', 'https'):\n\t\traise ValueError(_('Unsupported scheme: %(scheme)s') % {'scheme': parsed.scheme})\n\n\treturn (parsed.path.split('/')[-1], parsed.netloc, parsed.scheme == 'https')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tparsed_url = urlparse(image_href)\n\tloc = parsed_url.netloc\n\timage_id = parsed_url.path.split('/')[-1]\n\n\tif image_id == '':\n\t\traise ValueError(\"Invalid image_href provided.\")\n\n\tuse_ssl = parsed_url.scheme == 'https'\n\treturn image_id, loc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\tcomponents = urlparse(image_href)\n\tassert components.scheme in ('http', 'https')\n\tassert components.netloc\n\timage_id = components.path.split('/')[-1]\n\tassert image_id, 'URL does not contain an image id: %s' % image_href\n\treturn image_id, components.netloc, components.scheme == 'https'\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\t# TODO(bcwaldon): this function should be refactored to use urlparse\n\turl = urlparse(image_href)\n\n\tnetloc = url.netloc or url.path\n\timage_id = url.path.split('/')[-1]\n\tuse_ssl = url.scheme == 'https'\n\n\treturn image_id, netloc, use_ssl\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _parse_image_ref(image_href: str) -> Tuple[str, str, bool]:\n\turl = urllib.parse.urlparse(image_href)\n\timage_id = url.path.split('/')[-1]\n\tnetloc = url.netloc\n\tuse_ssl = url.scheme == 'https'\n\treturn image_id, netloc, use_ssl\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_13.py", "name": "remove_ending_os_sep", "docstring": "Iterate over a string list and remove trailing os seperator characters.\n\nEach string is tested if its length is greater than one and if the last\ncharacter is the pathname seperator. If so, the pathname seperator character\nis removed.\n\nArgs:\n    input_list: list of strings\n\nReturns:\n    Processed list of strings\n\nRaises:\n    TypeError", "ques_id": "62ece4992e6aefcf4aabbd80", "code": ["def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\t\n\treturn [s.rstrip(os.sep) for s in input_list if len(s) > 1 and s.endswith(os.sep)]\n\n", "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\n\treturn [x[:-1] if len(x) > 1 and x.endswith(os.sep) else x for x in input_list]\n\n", "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\treturn [p.rstrip(os.sep) for p in input_list if len(p) > 1 and p.endswith(os.sep)]\n\n\n", "def remove_ending_os_sep(input_list):\n\t\n\tif not input_list:\n\t\treturn []\n\t\n\treturn [x.rstrip(os.sep) if x and len(x) > 1 and x.endswith(os.sep) else x\n\t        for x in input_list]\n\n\n", "def remove_ending_os_sep(input_list):\n\treturn_list = []\n\tif input_list is not None:\n\t\tfor l in input_list:\n\t\t\tif len(l) > 1 and l[-1] == os.sep:\n\t\t\t\treturn_list.append(l[:-1])\n\t\t\telse:\n\t\t\t\treturn_list.append(l)\n\treturn return_list\n\n", "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\toutput_list = []\n\tfor each_path in input_list:\n\t\tif len(each_path) > 1 and each_path.endswith(os.sep):\n\t\t\toutput_list.append(each_path[:-1])\n\t\telse:\n\t\t\toutput_list.append(each_path)\n\treturn output_list\n\n\n", "def remove_ending_os_sep(input_list):\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == os.path.sep:\n\t\t\toutput_list.append(item[:-1])\n\t\telse:\n\t\t\toutput_list.append(item)\n\treturn output_list\n\n\n", "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\treturn [x[:-1] if len(x) > 1 and x[-1] == os.sep else x for x in input_list]\n\n\n", "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\treturn_list = []\n\tfor s in input_list:\n\t\tif len(s) > 1 and s[-1] == os.sep:\n\t\t\treturn_list.append(s[:-1])\n\t\telse:\n\t\t\treturn_list.append(s)\n\treturn return_list\n\n", "def remove_ending_os_sep(input_list):\n\tnew_list = []\n\tif input_list is None:\n\t\treturn []\n\tfor i in input_list:\n\t\tif len(i) > 1 and i.endswith(os.sep):\n\t\t\tnew_list.append(i[:-1])\n\t\telse:\n\t\t\tnew_list.append(i)\n\treturn new_list\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\t\n\treturn [s.rstrip(os.sep) for s in input_list if len(s) > 1 and s.endswith(os.sep)]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\n\treturn [x[:-1] if len(x) > 1 and x.endswith(os.sep) else x for x in input_list]\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\treturn [p.rstrip(os.sep) for p in input_list if len(p) > 1 and p.endswith(os.sep)]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\t\n\tif not input_list:\n\t\treturn []\n\t\n\treturn [x.rstrip(os.sep) if x and len(x) > 1 and x.endswith(os.sep) else x\n\t        for x in input_list]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def remove_ending_os_sep(input_list):\n\treturn_list = []\n\tif input_list is not None:\n\t\tfor l in input_list:\n\t\t\tif len(l) > 1 and l[-1] == os.sep:\n\t\t\t\treturn_list.append(l[:-1])\n\t\t\telse:\n\t\t\t\treturn_list.append(l)\n\treturn return_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\toutput_list = []\n\tfor each_path in input_list:\n\t\tif len(each_path) > 1 and each_path.endswith(os.sep):\n\t\t\toutput_list.append(each_path[:-1])\n\t\telse:\n\t\t\toutput_list.append(each_path)\n\treturn output_list\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\toutput_list = []\n\tif input_list is None:\n\t\treturn output_list\n\tfor item in input_list:\n\t\tif len(item) > 1 and item[-1] == os.path.sep:\n\t\t\toutput_list.append(item[:-1])\n\t\telse:\n\t\t\toutput_list.append(item)\n\treturn output_list\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\treturn [x[:-1] if len(x) > 1 and x[-1] == os.sep else x for x in input_list]\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tif input_list is None:\n\t\treturn []\n\treturn_list = []\n\tfor s in input_list:\n\t\tif len(s) > 1 and s[-1] == os.sep:\n\t\t\treturn_list.append(s[:-1])\n\t\telse:\n\t\t\treturn_list.append(s)\n\treturn return_list\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def remove_ending_os_sep(input_list):\n\tnew_list = []\n\tif input_list is None:\n\t\treturn []\n\tfor i in input_list:\n\t\tif len(i) > 1 and i.endswith(os.sep):\n\t\t\tnew_list.append(i[:-1])\n\t\telse:\n\t\t\tnew_list.append(i)\n\treturn new_list\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_17.py", "name": "get_pattern", "docstring": "This method converts the given string to regex pattern", "ques_id": "62ece4992e6aefcf4aabbd82", "code": ["def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = pattern.strip() if strip else pattern\n\t\tpattern = re.compile(pattern, flags=re.I)\n\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\treturn re.compile(pattern.strip())\n\t\telse:\n\t\t\treturn re.compile(pattern)\n\telif isinstance(pattern, re._pattern_type):\n\t\treturn pattern\n\telse:\n\t\traise TypeError(\"pattern must be a string or regex pattern object\")\n\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.IGNORECASE | re.UNICODE)\n\tif strip:\n\t\tpattern = re.compile('^' + pattern.pattern.strip() + '$', re.IGNORECASE | re.UNICODE)\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.MULTILINE)\n\telif not isinstance(pattern, re.Pattern):\n\t\traise ValueError('{} is not a valid pattern'.format(pattern))\n\n\tif strip:\n\t\tpattern = re.compile(pattern.pattern.strip())\n\n\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tif isinstance(pattern, str):\n\t\treturn re.compile(pattern)\n\telse:\n\t\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, basestring) and strip:\n\t\tpattern = pattern.strip()\n\tif isinstance(pattern, basestring):\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.MULTILINE)\n\tif strip:\n\t\tpattern.strip()\n\treturn pattern\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, re._pattern_type):\n\t\tif strip:\n\t\t\tpattern = re.compile(pattern.pattern.strip())\n\n\t\treturn pattern\n\telse:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\n\t\treturn re.compile(pattern)\n\n", "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, re.Pattern):\n\t\treturn pattern\n\tif strip:\n\t\tpattern = pattern.strip()\n\treturn re.compile(pattern)\n\n\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = pattern.strip() if strip else pattern\n\t\tpattern = re.compile(pattern, flags=re.I)\n\treturn pattern\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tif strip:\n\t\t\treturn re.compile(pattern.strip())\n\t\telse:\n\t\t\treturn re.compile(pattern)\n\telif isinstance(pattern, re._pattern_type):\n\t\treturn pattern\n\telse:\n\t\traise TypeError(\"pattern must be a string or regex pattern object\")\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.IGNORECASE | re.UNICODE)\n\tif strip:\n\t\tpattern = re.compile('^' + pattern.pattern.strip() + '$', re.IGNORECASE | re.UNICODE)\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.MULTILINE)\n\telif not isinstance(pattern, re.Pattern):\n\t\traise ValueError('{} is not a valid pattern'.format(pattern))\n\n\tif strip:\n\t\tpattern = re.compile(pattern.pattern.strip())\n\n\treturn pattern\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif strip:\n\t\tpattern = pattern.strip()\n\tif isinstance(pattern, str):\n\t\treturn re.compile(pattern)\n\telse:\n\t\treturn pattern\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, basestring) and strip:\n\t\tpattern = pattern.strip()\n\tif isinstance(pattern, basestring):\n\t\tpattern = re.compile(pattern)\n\treturn pattern\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, str):\n\t\tpattern = re.compile(pattern, re.MULTILINE)\n\tif strip:\n\t\tpattern.strip()\n\treturn pattern\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, re._pattern_type):\n\t\tif strip:\n\t\t\tpattern = re.compile(pattern.pattern.strip())\n\n\t\treturn pattern\n\telse:\n\t\tif strip:\n\t\t\tpattern = pattern.strip()\n\n\t\treturn re.compile(pattern)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_pattern(pattern, strip=True):\n\tif isinstance(pattern, re.Pattern):\n\t\treturn pattern\n\tif strip:\n\t\tpattern = pattern.strip()\n\treturn re.compile(pattern)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "slib_runnable/test_7.py", "name": "run_command", "docstring": "Call the given command(s).", "ques_id": "62ece4992e6aefcf4aabbd83", "code": ["def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\t#print(commands)\n\tfor cmd in commands:\n\t\ttry:\n\t\t\tif cwd is None:\n\t\t\t\tcwd = os.getcwd()\n\t\t\tif verbose:\n\t\t\t\tprint('Running command:', cmd, args, 'in directory:', cwd)\n\t\t\tif isinstance(args, list):\n\t\t\t\tcmd_line = [cmd] + args\n\t\t\telse:\n\t\t\t\tcmd_line = [cmd] + [args]\n\t\t\tif hide_stderr:\n\t\t\t\twith open(os.devnull, \"w\") as fnull:\n\t\t\t\t\tresult = subprocess.run(cmd_line, stdout=subprocess.PIPE, stderr=fnull, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tresult = subprocess.run(cmd_line, stdout=subprocess.PIPE, cwd=cwd,\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tfor cmd in commands:\n\t\ttry:\n\t\t\t# RETURN a subprocess.Popen object\n\t\t\tif verbose:\n\t\t\t\tprint('Running (in %s): %s' % (cwd, ' '.join([cmd] + args)))\n\t\t\tif hide_stderr:\n\t\t\t\t#with open(os.devnull, \"w\") as fnull:\n\t\t\t\t#\tp = subprocess.Popen([cmd] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=fnull, env=env)\n\t\t\t\tp = subprocess.Popen([cmd] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\t\telse:\n\t\t\t\tp = subprocess.Popen([cmd] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\tsub = subprocess.Popen(commands, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, env=env, bufsize=1)\n\t\tstdout, stderr = sub.communicate(args)\n\t\tstdout = stdout.decode('utf-8')\n\t\tstderr = stderr.decode('utf-8')\n\t\tif sub.returncode != 0:\n\t\t\tif verbose and not hide_stderr:\n\t\t\t\tprint(stderr)\n\t\t\traise Exception('Error running command. Return code: {}'.format(sub.returncode))\n\t\tif verbose:\n\t\t\tprint(stdout)\n\t\t\tif not hide_stderr:\n\t\t\t\tprint(stderr)\n\t\treturn stdout, sub.returncode\n\texcept OSError as e:\n\t\tif e.errno == os.errno.ENOENT:\n\t\t\traise\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif type(commands) is not list:\n\t\tcommands = [commands]\n\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\tstdout = subprocess.PIPE\n\tstderr = subprocess.PIPE\n\tif verbose:\n\t\tstdout = None\n\t\tif hide_stderr:\n\t\t\tstderr = subprocess.STDOUT\n\t\telse:\n\t\t\tstderr = None\n\n\tfor command in commands:\n\t\ttry:\n\t\t\tproc = subprocess.Popen([command] + args, stdout=stdout, stderr=stderr, cwd=cwd, env=env)\n\t\t\tstdout, stderr = proc.communicate()\n\t\t\tif proc.returncode == 0:\n\t\t\t\treturn stdout\n\t\texcept OSError:\n\t\t\tpass\n\treturn None\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\n\tif not env:\n\t\tenv = os.environ.copy()\n\n\tif not isinstance(args, (list, tuple)):\n\t\targs = [args]\n\n\t# Filter out empty arguments\n\targs = [a for a in args if a]\n\n\t# Hide stderr?\n\tstderr = subprocess.STDOUT if hide_stderr else None\n\n\tfor cmd in commands:\n\t\t# If the command is a list, then run it directly\n\t\tif isinstance(cmd, (list, tuple)):\n\t\t\ttry:\n\t\t\t\tproc = subprocess.Popen(cmd, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=stderr)\n\t\t\t\toutput = proc.communicate()[0]\n\t\t\t\treturn output, proc.returncode\n\t\t\texcept OSError as e:\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t# Commands is a list of command.\n\tfor cmd in commands:\n\t\ttry:\n\t\t\t# Check if the command exists.\n\t\t\tsubprocess.check_call(['which', cmd], stdout=subprocess.PIPE)\n\t\t\t# Command exists, run it\n\t\t\tbreak\n\t\texcept subprocess.CalledProcessError:\n\t\t\t# Command does not exist, check another command.\n\t\t\tif cmd == commands[-1]:\n\t\t\t\t# No more commands to check, raise exception.\n\t\t\t\tif len(commands) == 1:\n\t\t\t\t\t# Only one command, so use the command name for the error message.\n\t\t\t\t\traise Exception('{0} does not exist'.format(cmd))\n\t\t\t\telse:\n\t\t\t\t\t# More than one command, so use the list of commands for the error message.\n\t\t\t\t\traise Exception('None of {0} exist'.format(', '.join(commands)))\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tresult = {}\n\tresult['stdout'] = None\n\tresult['stderr'] = None\n\tresult['returncode'] = None\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running command: \" + command + \" \" + args)\n\n\t\ttry:\n\t\t\tif env is not None:\n\t\t\t\tproc = subprocess.Popen(command + \" \" + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env, shell=True)\n\t\t\telse:\n\t\t\t\tproc = subprocess.Popen(command + \" \" + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, shell=True)\n\t\t\tresult['stdout'], result['stderr'] = proc.communicate()\n\t\t\tresult['returncode'] = proc.returncode\n\t\texcept OSError as\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\ttry:\n\t\t\tproc = subprocess.Popen(\n\t\t\t\t[command] + args,\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env\n\t\t\t)\n\t\t\toutput = proc.communicate()[0].decode('utf-8')\n\t\t\tif proc.returncode != 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint('Command failed: %s' % command)\n\t\t\t\treturn None\n\t\t\tif verbose:\n\t\t\t\tprint('Command succeeded: %s' % command)\n\t\t\treturn output\n\t\texcept OSError:\n\t\t\tif verbose:\n\t\t\t\tprint('Command not found: %s' % command)\n\treturn None\n\n\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif type(commands) == str:\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running '%s' with arguments '%s'\" % (command, args))\n\t\ttry:\n\t\t\tif hide_stderr:\n\t\t\t\tprocess = subprocess.Popen([command] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tprocess = subprocess.Popen([command] + args, cwd=cwd, env=env)\n\t\t\toutput = process.communicate()\n\t\t\tif process.returncode == 0:\n\t\t\t\treturn output, process.returncode\n\t\texcept OSError:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Command '%s' not found\" % command)\n\t\t\tcontinue\n\n\tif verbose:\n\t\tprint(\"Command '%s' failed\" % commands[0])\n\n\treturn\n", "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\t# prepare command\n\t\tcommand = commands[0]\n\t\tif not command.startswith(\"http://\") and not command.startswith(\"https://\"):\n\t\t\tcommand = os.path.join(os.path.dirname(os.path.abspath(__file__)), command)\n\t\tfor i in range(1, len(commands)):\n\t\t\tcommand = command + \" && \" + os.path.join(os.path.dirname(os.path.abspath(__file__)), commands[i])\n\t\t# run command\n\t\tif verbose:\n\t\t\tprint(\"Running command: \" + command + \" \" + \" \".join(args))\n\t\tif sys.version_info[0] < 3:\n\t\t\tp = subprocess.Popen(command + \" \" + \" \".join(args), stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, cwd=cwd, env=env)\n\t\telse:\n\t\t\tp = subprocess.P\n"], "level": "slib_runnable", "generate_results": [{"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\t#print(commands)\n\tfor cmd in commands:\n\t\ttry:\n\t\t\tif cwd is None:\n\t\t\t\tcwd = os.getcwd()\n\t\t\tif verbose:\n\t\t\t\tprint('Running command:', cmd, args, 'in directory:', cwd)\n\t\t\tif isinstance(args, list):\n\t\t\t\tcmd_line = [cmd] + args\n\t\t\telse:\n\t\t\t\tcmd_line = [cmd] + [args]\n\t\t\tif hide_stderr:\n\t\t\t\twith open(os.devnull, \"w\") as fnull:\n\t\t\t\t\tresult = subprocess.run(cmd_line, stdout=subprocess.PIPE, stderr=fnull, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tresult = subprocess.run(cmd_line, stdout=subprocess.PIPE, cwd=cwd,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, list):\n\t\tcommands = [commands]\n\tfor cmd in commands:\n\t\ttry:\n\t\t\t# RETURN a subprocess.Popen object\n\t\t\tif verbose:\n\t\t\t\tprint('Running (in %s): %s' % (cwd, ' '.join([cmd] + args)))\n\t\t\tif hide_stderr:\n\t\t\t\t#with open(os.devnull, \"w\") as fnull:\n\t\t\t\t#\tp = subprocess.Popen([cmd] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=fnull, env=env)\n\t\t\t\tp = subprocess.Popen([cmd] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)\n\t\t\telse:\n\t\t\t\tp = subprocess.Popen([cmd] + args, cwd=cwd, stdout=subprocess.PIPE, stderr=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\tsub = subprocess.Popen(commands, cwd=cwd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE, env=env, bufsize=1)\n\t\tstdout, stderr = sub.communicate(args)\n\t\tstdout = stdout.decode('utf-8')\n\t\tstderr = stderr.decode('utf-8')\n\t\tif sub.returncode != 0:\n\t\t\tif verbose and not hide_stderr:\n\t\t\t\tprint(stderr)\n\t\t\traise Exception('Error running command. Return code: {}'.format(sub.returncode))\n\t\tif verbose:\n\t\t\tprint(stdout)\n\t\t\tif not hide_stderr:\n\t\t\t\tprint(stderr)\n\t\treturn stdout, sub.returncode\n\texcept OSError as e:\n\t\tif e.errno == os.errno.ENOENT:\n\t\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif type(commands) is not list:\n\t\tcommands = [commands]\n\n\tif env is None:\n\t\tenv = os.environ.copy()\n\n\tstdout = subprocess.PIPE\n\tstderr = subprocess.PIPE\n\tif verbose:\n\t\tstdout = None\n\t\tif hide_stderr:\n\t\t\tstderr = subprocess.STDOUT\n\t\telse:\n\t\t\tstderr = None\n\n\tfor command in commands:\n\t\ttry:\n\t\t\tproc = subprocess.Popen([command] + args, stdout=stdout, stderr=stderr, cwd=cwd, env=env)\n\t\t\tstdout, stderr = proc.communicate()\n\t\t\tif proc.returncode == 0:\n\t\t\t\treturn stdout\n\t\texcept OSError:\n\t\t\tpass\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif not isinstance(commands, (list, tuple)):\n\t\tcommands = [commands]\n\n\tif not cwd:\n\t\tcwd = os.getcwd()\n\n\tif not env:\n\t\tenv = os.environ.copy()\n\n\tif not isinstance(args, (list, tuple)):\n\t\targs = [args]\n\n\t# Filter out empty arguments\n\targs = [a for a in args if a]\n\n\t# Hide stderr?\n\tstderr = subprocess.STDOUT if hide_stderr else None\n\n\tfor cmd in commands:\n\t\t# If the command is a list, then run it directly\n\t\tif isinstance(cmd, (list, tuple)):\n\t\t\ttry:\n\t\t\t\tproc = subprocess.Popen(cmd, cwd=cwd, env=env, stdout=subprocess.PIPE, stderr=stderr)\n\t\t\t\toutput = proc.communicate()[0]\n\t\t\t\treturn output, proc.returncode\n\t\t\texcept OSError as e:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\t# Commands is a list of command.\n\tfor cmd in commands:\n\t\ttry:\n\t\t\t# Check if the command exists.\n\t\t\tsubprocess.check_call(['which', cmd], stdout=subprocess.PIPE)\n\t\t\t# Command exists, run it\n\t\t\tbreak\n\t\texcept subprocess.CalledProcessError:\n\t\t\t# Command does not exist, check another command.\n\t\t\tif cmd == commands[-1]:\n\t\t\t\t# No more commands to check, raise exception.\n\t\t\t\tif len(commands) == 1:\n\t\t\t\t\t# Only one command, so use the command name for the error message.\n\t\t\t\t\traise Exception('{0} does not exist'.format(cmd))\n\t\t\t\telse:\n\t\t\t\t\t# More than one command, so use the list of commands for the error message.\n\t\t\t\t\traise Exception('None of {0} exist'.format(', '.join(commands)))\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\n\tresult = {}\n\tresult['stdout'] = None\n\tresult['stderr'] = None\n\tresult['returncode'] = None\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running command: \" + command + \" \" + args)\n\n\t\ttry:\n\t\t\tif env is not None:\n\t\t\t\tproc = subprocess.Popen(command + \" \" + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env, shell=True)\n\t\t\telse:\n\t\t\t\tproc = subprocess.Popen(command + \" \" + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, shell=True)\n\t\t\tresult['stdout'], result['stderr'] = proc.communicate()\n\t\t\tresult['returncode'] = proc.returncode\n\t\texcept OSError as\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif isinstance(commands, str):\n\t\tcommands = [commands]\n\tfor command in commands:\n\t\ttry:\n\t\t\tproc = subprocess.Popen(\n\t\t\t\t[command] + args,\n\t\t\t\tstdout=subprocess.PIPE,\n\t\t\t\tstderr=(subprocess.PIPE if hide_stderr else None),\n\t\t\t\tcwd=cwd,\n\t\t\t\tenv=env\n\t\t\t)\n\t\t\toutput = proc.communicate()[0].decode('utf-8')\n\t\t\tif proc.returncode != 0:\n\t\t\t\tif verbose:\n\t\t\t\t\tprint('Command failed: %s' % command)\n\t\t\t\treturn None\n\t\t\tif verbose:\n\t\t\t\tprint('Command succeeded: %s' % command)\n\t\t\treturn output\n\t\texcept OSError:\n\t\t\tif verbose:\n\t\t\t\tprint('Command not found: %s' % command)\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\tif type(commands) == str:\n\t\tcommands = [commands]\n\n\tfor command in commands:\n\t\tif verbose:\n\t\t\tprint(\"Running '%s' with arguments '%s'\" % (command, args))\n\t\ttry:\n\t\t\tif hide_stderr:\n\t\t\t\tprocess = subprocess.Popen([command] + args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=cwd, env=env)\n\t\t\telse:\n\t\t\t\tprocess = subprocess.Popen([command] + args, cwd=cwd, env=env)\n\t\t\toutput = process.communicate()\n\t\t\tif process.returncode == 0:\n\t\t\t\treturn output, process.returncode\n\t\texcept OSError:\n\t\t\tif verbose:\n\t\t\t\tprint(\"Command '%s' not found\" % command)\n\t\t\tcontinue\n\n\tif verbose:\n\t\tprint(\"Command '%s' failed\" % commands[0])\n\n\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n\ttry:\n\t\t# prepare command\n\t\tcommand = commands[0]\n\t\tif not command.startswith(\"http://\") and not command.startswith(\"https://\"):\n\t\t\tcommand = os.path.join(os.path.dirname(os.path.abspath(__file__)), command)\n\t\tfor i in range(1, len(commands)):\n\t\t\tcommand = command + \" && \" + os.path.join(os.path.dirname(os.path.abspath(__file__)), commands[i])\n\t\t# run command\n\t\tif verbose:\n\t\t\tprint(\"Running command: \" + command + \" \" + \" \".join(args))\n\t\tif sys.version_info[0] < 3:\n\t\t\tp = subprocess.Popen(command + \" \" + \" \".join(args), stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True, cwd=cwd, env=env)\n\t\telse:\n\t\t\tp = subprocess.P\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_4.py", "name": "is_ipv4", "docstring": "Test if IPv4 address or not\n    ", "ques_id": "62ece4992e6aefcf4aabbd84", "code": ["def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\tif re.match(PATTERN_IPV4, target):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def is_ipv4(target):\n\ttry:\n\t\tipaddress.ip_address(target)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n", "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\texcept ipaddress.AddressValueError:\n\t\treturn False\n\treturn True\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\ttry:\n\t\ttest = socket.inet_aton(target)\n\texcept socket.error:\n\t\treturn False\n\telse:\n\t\treturn True\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n\n", "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\t\treturn True\n\texcept ipaddress.AddressValueError:\n\t\treturn False\n\n\n", "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\texcept ValueError:\n\t\treturn False\n\treturn True\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\tif re.match(PATTERN_IPV4, target):\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tipaddress.ip_address(target)\n\t\treturn True\n\texcept ValueError:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\texcept ipaddress.AddressValueError:\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_pton(socket.AF_INET, target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\ttest = socket.inet_aton(target)\n\texcept socket.error:\n\t\treturn False\n\telse:\n\t\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\t\treturn True\n\texcept ipaddress.AddressValueError:\n\t\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tsocket.inet_aton(target)\n\t\treturn True\n\texcept socket.error:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def is_ipv4(target):\n\ttry:\n\t\tipaddress.IPv4Address(target)\n\texcept ValueError:\n\t\treturn False\n\treturn True\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_11.py", "name": "find_roots", "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader", "ques_id": "62ece4992e6aefcf4aabbd85", "code": ["def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif not roots:\n\t\troots = set()\n\tfor s, _, o in graph.triples((None, prop, None)):\n\t\troots.discard(s)\n\t\troots.discard(o)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()  # type: Set[\"Node\"]\n\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.discard(o)\n\t\tfind_roots(graph, prop, roots)\n\t\troots.discard(s)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\troots = roots or set()\n\tchildren = set(graph.objects(None, prop))\n\tparents = set(graph.subjects(prop, None))\n\troots.update(parents - children)\n\tif len(roots) < len(parents):\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, _, _ in graph.triples((None, prop, None)):\n\t\troots.add(s)\n\tfor s, _, _ in graph.triples((None, prop, None)):\n\t\troots.discard(s)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.add(o)\n\t\tgraph.remove((s, p, o))\n\t\troots = find_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif not roots:\n\t\troots = set()\n\troots.update(set(graph.subjects(prop, None)))\n\tfor r in set(graph.subjects(prop, None)):\n\t\tgraph.remove((r, prop, None))\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\n\troots.update(set(graph.subjects(prop, None)))\n\n\tfor s in set(graph.subjects(prop, None)):\n\t\tfind_roots(graph, prop, roots)\n\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor _, _, parent in graph.triples((None, prop, None)):\n\t\tif parent not in roots:\n\t\t\troots.add(parent)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.add(s)\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.discard(o)\n\treturn roots\n\n\n", "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, o in graph.subject_objects(prop):\n\t\tif o in roots:\n\t\t\troots.remove(o)\n\t\tif s not in roots:\n\t\t\troots.add(s)\n\treturn roots\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif not roots:\n\t\troots = set()\n\tfor s, _, o in graph.triples((None, prop, None)):\n\t\troots.discard(s)\n\t\troots.discard(o)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()  # type: Set[\"Node\"]\n\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.discard(o)\n\t\tfind_roots(graph, prop, roots)\n\t\troots.discard(s)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\troots = roots or set()\n\tchildren = set(graph.objects(None, prop))\n\tparents = set(graph.subjects(prop, None))\n\troots.update(parents - children)\n\tif len(roots) < len(parents):\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, _, _ in graph.triples((None, prop, None)):\n\t\troots.add(s)\n\tfor s, _, _ in graph.triples((None, prop, None)):\n\t\troots.discard(s)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.add(o)\n\t\tgraph.remove((s, p, o))\n\t\troots = find_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif not roots:\n\t\troots = set()\n\troots.update(set(graph.subjects(prop, None)))\n\tfor r in set(graph.subjects(prop, None)):\n\t\tgraph.remove((r, prop, None))\n\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\n\troots.update(set(graph.subjects(prop, None)))\n\n\tfor s in set(graph.subjects(prop, None)):\n\t\tfind_roots(graph, prop, roots)\n\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor _, _, parent in graph.triples((None, prop, None)):\n\t\tif parent not in roots:\n\t\t\troots.add(parent)\n\t\t\tfind_roots(graph, prop, roots)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.add(s)\n\tfor s, p, o in graph.triples((None, prop, None)):\n\t\troots.discard(o)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n\tif roots is None:\n\t\troots = set()\n\tfor s, o in graph.subject_objects(prop):\n\t\tif o in roots:\n\t\t\troots.remove(o)\n\t\tif s not in roots:\n\t\t\troots.add(s)\n\treturn roots\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_1.py", "name": "_dump_string", "docstring": "Dump to a py2-unicode or py3-string", "ques_id": "62ece4992e6aefcf4aabbd86", "code": ["def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, dumper)\n\n\n", "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, dumper=dumper)\n\n\n", "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, default_flow_style=False, default_style='')\n\n\n", "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, default_flow_style=False, sort_keys=False, Dumper=dumper)\n\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = Dumper()\n\t\n\treturn dumper.dump(obj)\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\tdumper.ignore_aliases = lambda self, data: True\n\tdumper.Dumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n", "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n", "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, dumper)\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml_dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n", "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, dumper)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, dumper=dumper)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, default_flow_style=False, default_style='')\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, default_flow_style=False, sort_keys=False, Dumper=dumper)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = Dumper()\n\t\n\treturn dumper.dump(obj)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\tdumper.ignore_aliases = lambda self, data: True\n\tdumper.Dumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\treturn yaml.dump(obj, dumper)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml_dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _dump_string(obj, dumper=None):\n\tif dumper is None:\n\t\tdumper = yaml.Dumper\n\treturn yaml.dump(obj, Dumper=dumper)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_8.py", "name": "build_app_logger", "docstring": "General purpose application logger. Useful mainly for debugging", "ques_id": "62ece4992e6aefcf4aabbd87", "code": ["def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tlogging.getLogger('requests').setLevel(logging.WARNING)\n\tlogging.getLogger('urllib3').setLevel(logging.WARNING)\n\tlogging.getLogger('py.warnings').setLevel(logging.WARNING)\n\n\tif not logger.handlers:\n\t\t# create file handler which logs even debug messages\n\t\tfh = logging.FileHandler(logfile)\n\t\tfh.setLevel(logging.DEBUG)\n\t\tfh.setFormatter(formatter)\n\t\tlogger.addHandler(fh)\n\t\t# create console handler with a higher log level\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\t\tch.setFormatter(formatter)\n\t\tlogger.\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfileHandler = logging.FileHandler(logfile)\n\tfileHandler.setFormatter(formatter)\n\tlogger.addHandler(fileHandler)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.WARNING)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.propagate = False\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\n\tconsole_handler = logging.StreamHandler()\n\tif debug:\n\t\tconsole_handler.setLevel(logging.DEBUG)\n\telse:\n\t\tconsole_handler.setLevel(logging.INFO)\n\n\tformatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n\tfile_handler.setFormatter(formatter)\n\tconsole_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif not logger.handlers:\n\t\tif debug:\n\t\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\telse:\n\t\t\tformatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n\t\tfh = logging.FileHandler(logfile)\n\t\tfh.setLevel(logging.DEBUG)\n\t\tfh.setFormatter(formatter)\n\t\tlogger.addHandler(fh)\n\t\tch = logging.StreamHandler()\n\t\tch.setFormatter(formatter)\n\t\tch.setLevel(logging.DEBUG)\n\t\tlogger.addHandler(ch)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\tlogger.addHandler(fh)\n\tif debug:\n\t\tlogger.addHandler(ch)\n\n\treturn logger\n\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# Set up console logging\n\tconsole = logging.StreamHandler()\n\tconsole.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.addHandler(console)\n\n\t# Set up file logging\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(logging.Formatter(\n\t\t'%(asctime)s %(levelname)s %(message)s'\n\t))\n\tlogger.addHandler(file_handler)\n\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t# create logger with 'spam_application'\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\t# add the handlers to logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\treturn logger\n\n", "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tif debug:\n\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\telse:\n\t\tformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.ERROR)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tlogging.getLogger('requests').setLevel(logging.WARNING)\n\tlogging.getLogger('urllib3').setLevel(logging.WARNING)\n\tlogging.getLogger('py.warnings').setLevel(logging.WARNING)\n\n\tif not logger.handlers:\n\t\t# create file handler which logs even debug messages\n\t\tfh = logging.FileHandler(logfile)\n\t\tfh.setLevel(logging.DEBUG)\n\t\tfh.setFormatter(formatter)\n\t\tlogger.addHandler(fh)\n\t\t# create console handler with a higher log level\n\t\tch = logging.StreamHandler()\n\t\tch.setLevel(logging.DEBUG)\n\t\tch.setFormatter(formatter)\n\t\tlogger.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfileHandler = logging.FileHandler(logfile)\n\tfileHandler.setFormatter(formatter)\n\tlogger.addHandler(fileHandler)\n\tif debug:\n\t\tlogger.setLevel(logging.DEBUG)\n\telse:\n\t\tlogger.setLevel(logging.WARNING)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tch.setFormatter(formatter)\n\tlogger.addHandler(ch)\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tlogger.addHandler(fh)\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\n\tlogger.setLevel(logging.DEBUG)\n\tlogger.propagate = False\n\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\n\tconsole_handler = logging.StreamHandler()\n\tif debug:\n\t\tconsole_handler.setLevel(logging.DEBUG)\n\telse:\n\t\tconsole_handler.setLevel(logging.INFO)\n\n\tformatter = logging.Formatter('%(asctime)s %(name)-12s %(levelname)-8s %(message)s')\n\tfile_handler.setFormatter(formatter)\n\tconsole_handler.setFormatter(formatter)\n\n\tlogger.addHandler(file_handler)\n\tlogger.addHandler(console_handler)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\tif not logger.handlers:\n\t\tif debug:\n\t\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\t\telse:\n\t\t\tformatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n\t\tfh = logging.FileHandler(logfile)\n\t\tfh.setLevel(logging.DEBUG)\n\t\tfh.setFormatter(formatter)\n\t\tlogger.addHandler(fh)\n\t\tch = logging.StreamHandler()\n\t\tch.setFormatter(formatter)\n\t\tch.setLevel(logging.DEBUG)\n\t\tlogger.addHandler(ch)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\n\tformatter = logging.Formatter(\n\t\t'%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\n\tlogger.addHandler(fh)\n\tif debug:\n\t\tlogger.addHandler(ch)\n\n\treturn logger\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\t# Set up console logging\n\tconsole = logging.StreamHandler()\n\tconsole.setLevel(logging.DEBUG if debug else logging.INFO)\n\tlogger.addHandler(console)\n\n\t# Set up file logging\n\tfile_handler = logging.FileHandler(logfile)\n\tfile_handler.setLevel(logging.DEBUG)\n\tfile_handler.setFormatter(logging.Formatter(\n\t\t'%(asctime)s %(levelname)s %(message)s'\n\t))\n\tlogger.addHandler(file_handler)\n\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\t# create logger with 'spam_application'\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.DEBUG)\n\t# create formatter and add it to the handlers\n\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\tfh.setFormatter(formatter)\n\tch.setFormatter(formatter)\n\t# add the handlers to logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\treturn logger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def build_app_logger(name='app', logfile='app.log', debug=True):\n\tlogger = logging.getLogger(name)\n\tlogger.setLevel(logging.DEBUG)\n\n\tif debug:\n\t\tformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\telse:\n\t\tformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n\n\t# create file handler which logs even debug messages\n\tfh = logging.FileHandler(logfile)\n\tfh.setLevel(logging.DEBUG)\n\tfh.setFormatter(formatter)\n\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n\tch.setLevel(logging.ERROR)\n\tch.setFormatter(formatter)\n\n\t# add the handlers to the logger\n\tlogger.addHandler(fh)\n\tlogger.addHandler(ch)\n\n\treturn logger\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_6.py", "name": "make_array", "docstring": "Function to create an array with shape and dtype.\n\nParameters\n----------\nshape : tuple\n    shape of the array to create\ndtype : `numpy.dtype`\n    data-type of the array to create", "ques_id": "62ece4992e6aefcf4aabbd88", "code": ["def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def make_array(shape, dtype=np.dtype(\"float32\")):\n\treturn np.zeros(shape, dtype=dtype)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_2.py", "name": "gaussian", "docstring": "Gaussian centered around 0.2 with a sigma of 0.1.", "ques_id": "62ece4992e6aefcf4aabbd89", "code": ["def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-0.5*(x-u)**2/sigma**2)\n\n", "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\n\treturn exp(-((x-u)**2)/(2*(sigma**2))) / (sigma*sqrt(2*pi))\n\n", "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn ((1/(sigma*np.sqrt(2*np.pi)))*np.exp(-(x-u)**2/(2*sigma**2)))\n\n", "def gaussian(x):\n\treturn np.exp(-((x - 0.2)**2)/(2*(0.1**2)))/np.sqrt(2*np.pi*(0.1**2))\n\n", "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-0.5 * np.power((x - u) / sigma, 2))\n\n", "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-((x - u)**2)/(2*sigma))\n\n", "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-1 * (x - u) ** 2 / (2 * sigma ** 2))\n\n", "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-(x-u)**2/(2*sigma**2))\n\n", "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-((x-u)**2)/(sigma*2))\n\n", "def gaussian(x):\n\treturn np.exp(-(x-0.2)**2/(2*0.1**2))\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-0.5*(x-u)**2/sigma**2)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\n\treturn exp(-((x-u)**2)/(2*(sigma**2))) / (sigma*sqrt(2*pi))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn ((1/(sigma*np.sqrt(2*np.pi)))*np.exp(-(x-u)**2/(2*sigma**2)))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\treturn np.exp(-((x - 0.2)**2)/(2*(0.1**2)))/np.sqrt(2*np.pi*(0.1**2))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-0.5 * np.power((x - u) / sigma, 2))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-((x - u)**2)/(2*sigma))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-1 * (x - u) ** 2 / (2 * sigma ** 2))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-(x-u)**2/(2*sigma**2))\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def gaussian(x):\n\tu = 0.2\n\tsigma = 0.1\n\treturn np.exp(-((x-u)**2)/(sigma*2))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def gaussian(x):\n\treturn np.exp(-(x-0.2)**2/(2*0.1**2))\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "plib_runnable/test_13.py", "name": "load_configurations", "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.", "ques_id": "62ece4992e6aefcf4aabbd8a", "code": ["def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfig_filenames = set(config_filenames)\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(config_filename, overrides, resolve_env)\n\t\t\tconfigs[config_filename] = config\n\t\texcept ConfigurationError as e:\n\t\t\terrors.append(e.record)\n\treturn configs, errors\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename) as config_file:\n\t\t\t\tconfig = yaml.load(config_file, Loader=yaml.FullLoader)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = resolve_environment_variables(config)\n\t\t\t\tif overrides:\n\t\t\t\t\tconfig = deep_merge_dicts(config, overrides)\n\t\t\t\tconfig = validate_configuration(config, config_filename)\n\t\t\t\tconfigs[config_filename] = config\n\t\texcept (FileNotFoundError, PermissionError) as e:\n\t\t\tlogger.error(error_template, type=e.__class__.__name__, filename=e.filename, error=e)\n\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t'levelname': 'ERROR',\n\t\t\t\t'message': error_template,\n\t\t\t\t'type':\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfig_data = {}\n\terror_log = []\n\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tconfig_data[filename] = yaml.safe_load(f)\n\t\texcept (IOError, OSError) as e:\n\t\t\terror_log.append(logging.LogRecord(filename, logging.ERROR, filename, 0, '{}: {}'.format(filename, e), (), None))\n\t\texcept yaml.YAMLError as e:\n\t\t\terror_log.append(logging.LogRecord(filename, logging.ERROR, filename, 0, '{}: {}'.format(filename, e), (), None))\n\n\tif overrides:\n\t\tfor filename, config in overrides.items():\n\t\t\tconfig_data[filename] = config\n\n\tif resolve_env:\n\t\t# resolve the environment variables in the config files\n\t\tresolved_config_data = {}\n\t\tfor filename, config in config_data.items():\n\t\t\tresolved\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as config_file:\n\t\t\t\tconfig = json.load(config_file)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = expand_environment_variables(config)\n\t\t\t\tif overrides:\n\t\t\t\t\tconfig = override_configuration(config, overrides)\n\t\t\t\tconfigurations[config_filename] = config\n\t\texcept OSError as e:\n\t\t\terrors.append(logging.LogRecord(__name__,\n\t\t\t\t\t\t\t\t\t\t\tlogging.ERROR,\n\t\t\t\t\t\t\t\t\t\t\tpathname=config_filename,\n\t\t\t\t\t\t\t\t\t\t\tmsg='Cannot read configuration file: {}'.format(config_filename),\n\t\t\t\t\t\t\t\t\t\t\texc_info=e))\n\t\texcept json.decoder.JSONDecodeError as e:\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\n\tfor config_filename in config_filenames:\n\t\tconfig_filename = os.path.realpath(config_filename)\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\tconfig_str = f.read()\n\t\texcept IOError as e:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=config_filename,\n\t\t\t\t\tlineno=0,\n\t\t\t\t\tmsg='Error opening configuration file: %s' % str(e),\n\t\t\t\t\targs=None,\n\t\t\t\t\texc_info=sys.exc_info(),\n\t\t\t\t)\n\t\t\t)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tconfig = yaml.load(config_str)\n\t\t\texcept yaml.YAMLError as e:\n\t\t\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tconfig = json.load(f)\n\t\t\tif resolve_env:\n\t\t\t\tconfig = resolve_environment_variables(config)\n\t\t\tif overrides:\n\t\t\t\tconfig.update(overrides)\n\t\t\tvalidate(config, config_schema)\n\t\t\tconfigs[filename] = config\n\t\texcept Exception as e:\n\t\t\trecord = logging.LogRecord(\"config\", logging.ERROR, \"\", 0, \"Failed to load configuration from %s: %s\" % (filename, str(e)), (), None)\n\t\t\terrors.append(record)\n\t\t\tcontinue\n\treturn configs, errors\n\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tif isinstance(config_filenames, (str, bytes)):\n\t\tconfig_filenames = (config_filenames,)\n\tconfig_filenames = tuple(config_filenames)\n\tresolve_env = bool(resolve_env)\n\timport logging\n\n\t# load configuration files\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename) as f:\n\t\t\t\tconfigs[config_filename] = yaml.safe_load(f)\n\t\texcept (IOError, OSError, yaml.YAMLError) as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname=__name__,\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=0,\n\t\t\t\tmsg=\"Error loading configuration file.\",\n\t\t\t\targs=(\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tconfig = {}\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\tconfig = json.load(f)\n\t\texcept (IOError, OSError) as e:\n\t\t\terror = logging.LogRecord(\n\t\t\t\tname='config',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=0,\n\t\t\t\tmsg='Error opening file: {}'.format(e),\n\t\t\t\targs=None,\n\t\t\t\texc_info=None\n\t\t\t)\n\t\t\terrors.append(error)\n\t\t\tcontinue\n\t\texcept (ValueError, JSONDecodeError) as e:\n\t\t\terror = logging.LogRecord(\n\t\t\t\tname='config',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tglobal _logger\n\tassert _logger is not None\n\tassert isinstance(config_filenames, (list, tuple))\n\tassert all(isinstance(config_filename, str) for config_filename in config_filenames)\n\n\tconfigurations = {}\n\tconfiguration_errors = []\n\n\t#\n\t# Load each configuration file.\n\t#\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename) as f:\n\t\t\t\tconfiguration = yaml.load(f, Loader=yaml.FullLoader)\n\t\t\t\tif not isinstance(configuration, dict):\n\t\t\t\t\traise Exception(\"Top level of configuration must be a dictionary\")\n\t\t\t\tconfigurations[config_filename] = configuration\n\t\texcept Exception as e:\n\t\t\t_logger.error(\"Cannot parse configuration file '%s' due to error: %s\" % (config_filename, e))\n\t\t\tconfiguration_errors.append(logging.LogRecord(\"root\", logging.ERROR, config_filename, 0\n", "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\timport os, logging, json, jsonschema, configparser\n\n\tif not isinstance(config_filenames, (list, tuple)):\n\t\tconfig_filenames = [config_filenames]\n\n\tif overrides is None:\n\t\toverrides = {}\n\n\tconfigs = {}\n\terrors = []\n\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\tconfig_data = f.read()\n\n\t\t\tif resolve_env:\n\t\t\t\tconfig_data = os.path.expandvars(config_data)\n\n\t\t\tconfig = json.loads(config_data)\n\n\t\t\tif config_filename in overrides:\n\t\t\t\tconfig = _merge_dicts(config, overrides[config_filename])\n\n\t\t\tjsonschema.validate(config, CONFIG_SCHEMA)\n\n\t\t\tconfigs[config_filename] = config\n\t\texcept json.JSONDecodeError as e:\n\t\t\terrors.\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfig_filenames = set(config_filenames)\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\tconfig = load_configuration(config_filename, overrides, resolve_env)\n\t\t\tconfigs[config_filename] = config\n\t\texcept ConfigurationError as e:\n\t\t\terrors.append(e.record)\n\treturn configs, errors\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename) as config_file:\n\t\t\t\tconfig = yaml.load(config_file, Loader=yaml.FullLoader)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = resolve_environment_variables(config)\n\t\t\t\tif overrides:\n\t\t\t\t\tconfig = deep_merge_dicts(config, overrides)\n\t\t\t\tconfig = validate_configuration(config, config_filename)\n\t\t\t\tconfigs[config_filename] = config\n\t\texcept (FileNotFoundError, PermissionError) as e:\n\t\t\tlogger.error(error_template, type=e.__class__.__name__, filename=e.filename, error=e)\n\t\t\terrors.append(logging.makeLogRecord({\n\t\t\t\t'levelname': 'ERROR',\n\t\t\t\t'message': error_template,\n\t\t\t\t'type':\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfig_data = {}\n\terror_log = []\n\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tconfig_data[filename] = yaml.safe_load(f)\n\t\texcept (IOError, OSError) as e:\n\t\t\terror_log.append(logging.LogRecord(filename, logging.ERROR, filename, 0, '{}: {}'.format(filename, e), (), None))\n\t\texcept yaml.YAMLError as e:\n\t\t\terror_log.append(logging.LogRecord(filename, logging.ERROR, filename, 0, '{}: {}'.format(filename, e), (), None))\n\n\tif overrides:\n\t\tfor filename, config in overrides.items():\n\t\t\tconfig_data[filename] = config\n\n\tif resolve_env:\n\t\t# resolve the environment variables in the config files\n\t\tresolved_config_data = {}\n\t\tfor filename, config in config_data.items():\n\t\t\tresolved\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as config_file:\n\t\t\t\tconfig = json.load(config_file)\n\t\t\t\tif resolve_env:\n\t\t\t\t\tconfig = expand_environment_variables(config)\n\t\t\t\tif overrides:\n\t\t\t\t\tconfig = override_configuration(config, overrides)\n\t\t\t\tconfigurations[config_filename] = config\n\t\texcept OSError as e:\n\t\t\terrors.append(logging.LogRecord(__name__,\n\t\t\t\t\t\t\t\t\t\t\tlogging.ERROR,\n\t\t\t\t\t\t\t\t\t\t\tpathname=config_filename,\n\t\t\t\t\t\t\t\t\t\t\tmsg='Cannot read configuration file: {}'.format(config_filename),\n\t\t\t\t\t\t\t\t\t\t\texc_info=e))\n\t\texcept json.decoder.JSONDecodeError as e:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\n\tfor config_filename in config_filenames:\n\t\tconfig_filename = os.path.realpath(config_filename)\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\tconfig_str = f.read()\n\t\texcept IOError as e:\n\t\t\terrors.append(\n\t\t\t\tlogging.LogRecord(\n\t\t\t\t\tname=__name__,\n\t\t\t\t\tlevel=logging.ERROR,\n\t\t\t\t\tpathname=config_filename,\n\t\t\t\t\tlineno=0,\n\t\t\t\t\tmsg='Error opening configuration file: %s' % str(e),\n\t\t\t\t\targs=None,\n\t\t\t\t\texc_info=sys.exc_info(),\n\t\t\t\t)\n\t\t\t)\n\t\telse:\n\t\t\ttry:\n\t\t\t\tconfig = yaml.load(config_str)\n\t\t\texcept yaml.YAMLError as e:\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigs = {}\n\terrors = []\n\tfor filename in config_filenames:\n\t\ttry:\n\t\t\twith open(filename, 'r') as f:\n\t\t\t\tconfig = json.load(f)\n\t\t\tif resolve_env:\n\t\t\t\tconfig = resolve_environment_variables(config)\n\t\t\tif overrides:\n\t\t\t\tconfig.update(overrides)\n\t\t\tvalidate(config, config_schema)\n\t\t\tconfigs[filename] = config\n\t\texcept Exception as e:\n\t\t\trecord = logging.LogRecord(\"config\", logging.ERROR, \"\", 0, \"Failed to load configuration from %s: %s\" % (filename, str(e)), (), None)\n\t\t\terrors.append(record)\n\t\t\tcontinue\n\treturn configs, errors\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tif overrides is None:\n\t\toverrides = {}\n\tif isinstance(config_filenames, (str, bytes)):\n\t\tconfig_filenames = (config_filenames,)\n\tconfig_filenames = tuple(config_filenames)\n\tresolve_env = bool(resolve_env)\n\timport logging\n\n\t# load configuration files\n\tconfigs = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename) as f:\n\t\t\t\tconfigs[config_filename] = yaml.safe_load(f)\n\t\texcept (IOError, OSError, yaml.YAMLError) as e:\n\t\t\terrors.append(logging.LogRecord(\n\t\t\t\tname=__name__,\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=0,\n\t\t\t\tmsg=\"Error loading configuration file.\",\n\t\t\t\targs=(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tconfigurations = {}\n\terrors = []\n\tfor config_filename in config_filenames:\n\t\tconfig = {}\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\tconfig = json.load(f)\n\t\texcept (IOError, OSError) as e:\n\t\t\terror = logging.LogRecord(\n\t\t\t\tname='config',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno=0,\n\t\t\t\tmsg='Error opening file: {}'.format(e),\n\t\t\t\targs=None,\n\t\t\t\texc_info=None\n\t\t\t)\n\t\t\terrors.append(error)\n\t\t\tcontinue\n\t\texcept (ValueError, JSONDecodeError) as e:\n\t\t\terror = logging.LogRecord(\n\t\t\t\tname='config',\n\t\t\t\tlevel=logging.ERROR,\n\t\t\t\tpathname=config_filename,\n\t\t\t\tlineno\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\tglobal _logger\n\tassert _logger is not None\n\tassert isinstance(config_filenames, (list, tuple))\n\tassert all(isinstance(config_filename, str) for config_filename in config_filenames)\n\n\tconfigurations = {}\n\tconfiguration_errors = []\n\n\t#\n\t# Load each configuration file.\n\t#\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename) as f:\n\t\t\t\tconfiguration = yaml.load(f, Loader=yaml.FullLoader)\n\t\t\t\tif not isinstance(configuration, dict):\n\t\t\t\t\traise Exception(\"Top level of configuration must be a dictionary\")\n\t\t\t\tconfigurations[config_filename] = configuration\n\t\texcept Exception as e:\n\t\t\t_logger.error(\"Cannot parse configuration file '%s' due to error: %s\" % (config_filename, e))\n\t\t\tconfiguration_errors.append(logging.LogRecord(\"root\", logging.ERROR, config_filename, 0\n", "is_pass": false, "return_code": 1}, {"generate_code": "def load_configurations(config_filenames, overrides=None, resolve_env=True):\n\timport os, logging, json, jsonschema, configparser\n\n\tif not isinstance(config_filenames, (list, tuple)):\n\t\tconfig_filenames = [config_filenames]\n\n\tif overrides is None:\n\t\toverrides = {}\n\n\tconfigs = {}\n\terrors = []\n\n\tfor config_filename in config_filenames:\n\t\ttry:\n\t\t\twith open(config_filename, 'r') as f:\n\t\t\t\tconfig_data = f.read()\n\n\t\t\tif resolve_env:\n\t\t\t\tconfig_data = os.path.expandvars(config_data)\n\n\t\t\tconfig = json.loads(config_data)\n\n\t\t\tif config_filename in overrides:\n\t\t\t\tconfig = _merge_dicts(config, overrides[config_filename])\n\n\t\t\tjsonschema.validate(config, CONFIG_SCHEMA)\n\n\t\t\tconfigs[config_filename] = config\n\t\texcept json.JSONDecodeError as e:\n\t\t\terrors.\n", "is_pass": false, "return_code": 1}]}
{"file_path": "plib_runnable/test_3.py", "name": "force_string", "docstring": "This function returns the bytes object corresponding to ``obj``\nin case it is a string using UTF-8. ", "ques_id": "62ece4992e6aefcf4aabbd8b", "code": ["def force_string(obj):\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n\n", "def force_string(obj):\n\treturn obj.decode('utf-8') if isinstance(obj, bytes) or isinstance(obj, np.bytes_) else obj\n\n", "def force_string(obj):\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n", "def force_string(obj):\n\tif isinstance(obj, (np.bytes, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n\n", "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\tobj = obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\tobj = obj.decode('utf-8')\n\treturn obj\n\n\n", "def force_string(obj):\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n\n", "def force_string(obj):\n\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "def force_string(obj):\n\treturn obj.decode('utf-8') if isinstance(obj, np.bytes_) or isinstance(obj, bytes) else obj\n\n", "def force_string(obj):\n\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "def force_string(obj):\n\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def force_string(obj):\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\treturn obj.decode('utf-8') if isinstance(obj, bytes) or isinstance(obj, np.bytes_) else obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, (np.bytes, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, bytes):\n\t\tobj = obj.decode('utf-8')\n\telif isinstance(obj, np.bytes_):\n\t\tobj = obj.decode('utf-8')\n\treturn obj\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, (np.bytes_, bytes)):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\treturn obj.decode('utf-8') if isinstance(obj, np.bytes_) or isinstance(obj, bytes) else obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\treturn obj\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def force_string(obj):\n\tif isinstance(obj, np.bytes_) or isinstance(obj, bytes):\n\t\treturn obj.decode('utf-8')\n\telse:\n\t\treturn obj\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "neo4j/time/__init__.py", "project": "neo4j/neo4j-python-driver", "name": "from_ticks", "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)", "ques_id": "62e60723d76274f8a4026b76", "code": ["def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks is out of bounds (0 <= ticks < 86400000000000)')\n\treturn cls(ticks=ticks, tz=tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\tpass\n\t\n", "def from_ticks(cls, ticks, tz=None):\n\treturn cls.from_ticks_since_midnight(ticks, tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\n\treturn cls(ticks, tz)\n\n\t@classmethod\n\tdef from_iso8601(cls, tstr, tz=None):\n\t\"\"\"\n\tCreate a Time from an ISO 8601 string.\n\n", "def from_ticks(cls, ticks, tz=None):\n\treturn cls(ticks, tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif tz is None:\n\t\ttz = get_localzone()\n\treturn cls(ticks, tz)\n\n\t@classmethod\n\tdef from_time(cls, time, tz=None):\n\t\t\"\"\"\n\t\tCreate a time from a time.time() value.\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks is out of bounds\")\n\treturn cls(0, 0, 0, ticks, tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= nanos_per_day:\n\t\traise ValueError(\"ticks out of bounds\")\n\n\treturn cls(int(ticks / nanos_per_second), ticks % nanos_per_second, tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\treturn cls(ticks=ticks, tz=tz)\n\n", "def from_ticks(cls, ticks, tz=None):\n\tif not isinstance(ticks, int):\n\t\traise TypeError(\"ticks must be an integer\")\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks must be in bounds 0 <= ticks < 86400000000000\")\n\n\thour = ticks // 3600000000000\n\tticks -= hour * 3600000000000\n\tminute = ticks // 60000000000\n\tticks -= minute * 60000000000\n\tsecond = ticks // 1000000000\n\tticks -= second * 1000000000\n\tmicrosecond = ticks // 1000\n\tnanosecond = ticks % 1000\n\n\treturn cls(hour, minute, second, microsecond, nanosecond, tz)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks is out of bounds (0 <= ticks < 86400000000000)')\n\treturn cls(ticks=ticks, tz=tz)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tpass\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\treturn cls.from_ticks_since_midnight(ticks, tz)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\n\treturn cls(ticks, tz)\n\n\t@classmethod\n\tdef from_iso8601(cls, tstr, tz=None):\n\t\"\"\"\n\tCreate a Time from an ISO 8601 string.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\treturn cls(ticks, tz)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif tz is None:\n\t\ttz = get_localzone()\n\treturn cls(ticks, tz)\n\n\t@classmethod\n\tdef from_time(cls, time, tz=None):\n\t\t\"\"\"\n\t\tCreate a time from a time.time() value.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks is out of bounds\")\n\treturn cls(0, 0, 0, ticks, tz)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= nanos_per_day:\n\t\traise ValueError(\"ticks out of bounds\")\n\n\treturn cls(int(ticks / nanos_per_second), ticks % nanos_per_second, tz)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError('ticks out of bounds')\n\treturn cls(ticks=ticks, tz=tz)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_ticks(cls, ticks, tz=None):\n\tif not isinstance(ticks, int):\n\t\traise TypeError(\"ticks must be an integer\")\n\tif ticks < 0 or ticks >= 86400000000000:\n\t\traise ValueError(\"ticks must be in bounds 0 <= ticks < 86400000000000\")\n\n\thour = ticks // 3600000000000\n\tticks -= hour * 3600000000000\n\tminute = ticks // 60000000000\n\tticks -= minute * 60000000000\n\tsecond = ticks // 1000000000\n\tticks -= second * 1000000000\n\tmicrosecond = ticks // 1000\n\tnanosecond = ticks % 1000\n\n\treturn cls(hour, minute, second, microsecond, nanosecond, tz)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/_async/io/_bolt.py", "project": "neo4j/neo4j-python-driver", "name": "protocol_handlers", "docstring": "Return a dictionary of available Bolt protocol handlers,\nkeyed by version tuple. If an explicit protocol version is\nprovided, the dictionary will contain either zero or one items,\ndepending on whether that version is supported. If no protocol\nversion is provided, all available versions will be returned.\n\n:param protocol_version: tuple identifying a specific protocol\n    version (e.g. (3, 5)) or None\n:return: dictionary of version tuple to handler class for all\n    relevant and supported protocol versions\n:raise TypeError: if protocol version is not passed in a tuple", "ques_id": "62e60873d76274f8a4026bd8", "code": ["def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be tuple\")\n\t\tif protocol_version not in cls._PROTOCOL_HANDLERS:\n\t\t\treturn {}\n\t\treturn {protocol_version: cls._PROTOCOL_HANDLERS[protocol_version]}\n\treturn cls._PROTOCOL_HANDLERS\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.__protocol_handlers\n\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple, not %r\" % protocol_version)\n\n\tif protocol_version not in cls.__protocol_handlers:\n\t\traise RuntimeError(\"unsupported protocol version %r\" % protocol_version)\n\n\treturn {protocol_version: cls.__protocol_handlers[protocol_version]}\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn {v: getattr(_handlers, 'V%d_%d' % v) for v in _handlers.SUPPORTED_VERSIONS}\n\telif isinstance(protocol_version, tuple):\n\t\tif protocol_version in _handlers.SUPPORTED_VERSIONS:\n\t\t\treturn {protocol_version: getattr(_handlers, 'V%d_%d' % protocol_version)}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\traise TypeError(\n\t\t\t\"protocol_version must be a tuple identifying a protocol version or None\")\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be passed in tuple form\")\n\t\tif protocol_version not in cls._PROTOCOL_HANDLERS:\n\t\t\treturn {}\n\t\treturn {protocol_version: cls._PROTOCOL_HANDLERS[protocol_version]}\n\telse:\n\t\treturn cls._PROTOCOL_HANDLERS\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be passed in a tuple\")\n\t\tif protocol_version in HANDLERS:\n\t\t\treturn {protocol_version: HANDLERS[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn HANDLERS.copy()\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be passed in a tuple\")\n\n\tif protocol_version is None:\n\t\treturn cls._version_handlers\n\telse:\n\t\treturn {protocol_version: cls._version_handlers[protocol_version]}\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\tif protocol_version not in cls._handlers:\n\t\t\treturn {}\n\t\treturn {protocol_version: cls._handlers[protocol_version]}\n\treturn cls._handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol_version must be a tuple\")\n\t\tif protocol_version in handlers:\n\t\t\treturn {protocol_version: handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn handlers.copy()\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be specified as tuple\")\n\thandlers = {}\n\tfor version in cls.PROTOCOL_VERSIONS:\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = cls.PROTOCOL_VERSIONS[version]\n\treturn handlers\n\n\n", "def protocol_handlers(cls, protocol_version=None):\n\ta = cls._protocol_handlers\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\ta = {protocol_version: a.get(protocol_version)}\n\treturn a\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be tuple\")\n\t\tif protocol_version not in cls._PROTOCOL_HANDLERS:\n\t\t\treturn {}\n\t\treturn {protocol_version: cls._PROTOCOL_HANDLERS[protocol_version]}\n\treturn cls._PROTOCOL_HANDLERS\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn cls.__protocol_handlers\n\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be a tuple, not %r\" % protocol_version)\n\n\tif protocol_version not in cls.__protocol_handlers:\n\t\traise RuntimeError(\"unsupported protocol version %r\" % protocol_version)\n\n\treturn {protocol_version: cls.__protocol_handlers[protocol_version]}\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is None:\n\t\treturn {v: getattr(_handlers, 'V%d_%d' % v) for v in _handlers.SUPPORTED_VERSIONS}\n\telif isinstance(protocol_version, tuple):\n\t\tif protocol_version in _handlers.SUPPORTED_VERSIONS:\n\t\t\treturn {protocol_version: getattr(_handlers, 'V%d_%d' % protocol_version)}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\traise TypeError(\n\t\t\t\"protocol_version must be a tuple identifying a protocol version or None\")\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be passed in tuple form\")\n\t\tif protocol_version not in cls._PROTOCOL_HANDLERS:\n\t\t\treturn {}\n\t\treturn {protocol_version: cls._PROTOCOL_HANDLERS[protocol_version]}\n\telse:\n\t\treturn cls._PROTOCOL_HANDLERS\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be passed in a tuple\")\n\t\tif protocol_version in HANDLERS:\n\t\t\treturn {protocol_version: HANDLERS[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn HANDLERS.copy()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be passed in a tuple\")\n\n\tif protocol_version is None:\n\t\treturn cls._version_handlers\n\telse:\n\t\treturn {protocol_version: cls._version_handlers[protocol_version]}\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\tif protocol_version not in cls._handlers:\n\t\t\treturn {}\n\t\treturn {protocol_version: cls._handlers[protocol_version]}\n\treturn cls._handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif protocol_version is not None:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol_version must be a tuple\")\n\t\tif protocol_version in handlers:\n\t\t\treturn {protocol_version: handlers[protocol_version]}\n\t\telse:\n\t\t\treturn {}\n\telse:\n\t\treturn handlers.copy()\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\tif not isinstance(protocol_version, tuple):\n\t\traise TypeError(\"protocol version must be specified as tuple\")\n\thandlers = {}\n\tfor version in cls.PROTOCOL_VERSIONS:\n\t\tif protocol_version is None or protocol_version == version:\n\t\t\thandlers[version] = cls.PROTOCOL_VERSIONS[version]\n\treturn handlers\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def protocol_handlers(cls, protocol_version=None):\n\ta = cls._protocol_handlers\n\tif protocol_version:\n\t\tif not isinstance(protocol_version, tuple):\n\t\t\traise TypeError(\"protocol version must be a tuple\")\n\t\ta = {protocol_version: a.get(protocol_version)}\n\treturn a\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "neo4j/api.py", "project": "neo4j/neo4j-python-driver", "name": "from_raw_values", "docstring": "Create a Bookmarks object from a list of raw bookmark string values.\n\nYou should not need to use this method unless you want to deserialize\nbookmarks.\n\n:param values: ASCII string values (raw bookmarks)\n:type values: Iterable[str]", "ques_id": "62e60e3bd76274f8a4026d1a", "code": ["def from_raw_values(cls, values):\n\treturn cls(map(cls.from_raw_value, values))\n\n\t\n\t\n\t\n", "def from_raw_values(cls, values):\n\tbm = cls()\n\tfor v in values:\n\t\tbm.add_bookmark_from_raw(v)\n\treturn bm\n\n", "def from_raw_values(cls, values):\n\turls = []\n\ttitles = []\n\tfor value in values:\n\t\turl, title = cls.parse(value)\n\t\turls.append(url)\n\t\ttitles.append(title)\n\treturn cls(urls, titles)\n\n\t\n", "def from_raw_values(cls, values):\n\tparsed_values = [cls._parse_bookmark(val) for val in values]\n\treturn cls(parsed_values)\n\n\n\t@classmethod\n\tdef from_file(cls, filepath):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a file.\n\t\t\"\"\"\n\t\twith open(filepath, \"r\") as f:\n\t\t\treturn cls.from_raw_values(f.readlines())\n\n\n\t@classmethod\n\tdef from_string(cls, string):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a string.\n\t\t\"\"\"\n\t\treturn cls.from_raw_values(string.split(\"\\n\"))\n\n\n\t@classmethod\n\tdef _parse_bookmark(cls, val):\n\t\t\"\"\"\n\t\tParse a bookmark value.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn cls.BOOKMARK_PATTERN.match(val).groups()\n\t\texcept AttributeError:\n\t\t\traise Exception(\"Invalid bookmark: %s\" % val)\n\n\n\n\t\n", "def from_raw_values(cls, values):\n\treturn cls(dict((k, v.split()) for k, v in values.items()))\n\n\n", "def from_raw_values(cls, values):\n\treturn cls(values)\n\n\n", "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\ttry:\n\t\t\tbookmark = Bookmark.from_raw_value(value)\n\t\texcept ValueError:\n\t\t\tcontinue\n\t\tbookmarks[bookmark.path] = bookmark\n\treturn bookmarks\n\n\n", "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmark = Bookmark.from_raw_value(value)\n\t\tbookmarks.append(bookmark)\n\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmark = cls.Bookmark.from_string(value)\n\t\tbookmarks.add_bookmark(bookmark)\n\treturn bookmarks\n\n", "def from_raw_values(cls, values):\n\tfrom . import Bookmark\n\treturn cls(map(Bookmark.from_raw_value, values))\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def from_raw_values(cls, values):\n\treturn cls(map(cls.from_raw_value, values))\n\n\t\n\t\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tbm = cls()\n\tfor v in values:\n\t\tbm.add_bookmark_from_raw(v)\n\treturn bm\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\turls = []\n\ttitles = []\n\tfor value in values:\n\t\turl, title = cls.parse(value)\n\t\turls.append(url)\n\t\ttitles.append(title)\n\treturn cls(urls, titles)\n\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tparsed_values = [cls._parse_bookmark(val) for val in values]\n\treturn cls(parsed_values)\n\n\n\t@classmethod\n\tdef from_file(cls, filepath):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a file.\n\t\t\"\"\"\n\t\twith open(filepath, \"r\") as f:\n\t\t\treturn cls.from_raw_values(f.readlines())\n\n\n\t@classmethod\n\tdef from_string(cls, string):\n\t\t\"\"\"\n\t\tCreate a Bookmarks object from a string.\n\t\t\"\"\"\n\t\treturn cls.from_raw_values(string.split(\"\\n\"))\n\n\n\t@classmethod\n\tdef _parse_bookmark(cls, val):\n\t\t\"\"\"\n\t\tParse a bookmark value.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn cls.BOOKMARK_PATTERN.match(val).groups()\n\t\texcept AttributeError:\n\t\t\traise Exception(\"Invalid bookmark: %s\" % val)\n\n\n\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\treturn cls(dict((k, v.split()) for k, v in values.items()))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\treturn cls(values)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\ttry:\n\t\t\tbookmark = Bookmark.from_raw_value(value)\n\t\texcept ValueError:\n\t\t\tcontinue\n\t\tbookmarks[bookmark.path] = bookmark\n\treturn bookmarks\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmark = Bookmark.from_raw_value(value)\n\t\tbookmarks.append(bookmark)\n\n\treturn bookmarks\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tbookmarks = cls()\n\tfor value in values:\n\t\tbookmark = cls.Bookmark.from_string(value)\n\t\tbookmarks.add_bookmark(bookmark)\n\treturn bookmarks\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def from_raw_values(cls, values):\n\tfrom . import Bookmark\n\treturn cls(map(Bookmark.from_raw_value, values))\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "lena/core/split.py", "project": "ynikitenko/lena", "name": "_get_seq_with_type", "docstring": "Return a (sequence, type) pair.\nSequence is derived from *seq*\n(or is *seq*, if that is of a sequence type).", "ques_id": "62b87b199a0c4fa8b80b354c", "code": ["def _get_seq_with_type(seq, bufsize=None):\n\tif not _is_seq(seq):\n\t\tif bufsize is None:\n\t\t\tbufsize = _DEFAULT_BUFSIZE\n\t\tseq = _iter_seq(seq, bufsize)\n\t\tseq_type = 'buffered'\n\telif isinstance(seq, _STRING_TYPES):\n\t\tseq_type = 'string'\n\telse:\n\t\tseq_type = 'list'\n\treturn seq, seq_type\n\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\ttry:\n\t\ts = list(seq)  # try to get a list\n\texcept TypeError:\n\t\ts = list(iter(seq))  # or an iterator\n\treturn s, type(s)\n\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple, xrange)):\n\t\tstype = type(seq)\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq)\n\t\tif stype is xrange:\n\t\t\tseq = list(seq)\n\t\t\tstype = list\n\telse:\n\t\tstype = None\n\t\tif bufsize is None:\n\t\t\tbufsize = 64\n\t\tseq = list(seq)\n\treturn (seq, stype)\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, _types.StringType):\n\t\treturn seq, _types.StringType\n\telse:\n\t\ttry:\n\t\t\tseq = _seq_type(seq)\n\t\texcept (TypeError, AttributeError):\n\t\t\traise TypeError, 'seq must be a sequence'\n\t\telse:\n\t\t\treturn seq, _seq_type\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif bufsize is None:\n\t\tbufsize = get_buffer_size()\n\tif isinstance(seq, collections.abc.Sequence):\n\t\tseq_type = type(seq)\n\telse:\n\t\tseq_type = _seq_type(bufsize)\n\t\ttry:\n\t\t\tseq = seq_type(seq)\n\t\texcept TypeError:\n\t\t\tseq = seq_type(iter(seq))\n\treturn seq, seq_type\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple, set)):\n\t\treturn seq, type(seq)\n\telse:\n\t\treturn list(iter(seq)), list\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple)) or hasattr(seq, \"__getitem__\"):\n\t\treturn seq, type(seq)\n\treturn list(iter(seq)), list\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, collections.Mapping):\n\t\treturn (seq, 'dict')\n\telif isinstance(seq, collections.Iterable):\n\t\treturn (seq, 'iter')\n\telse:\n\t\treturn (iter(seq), 'iter')\n\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, list):\n\t\t#list is ok\n\t\treturn (seq, 0)\n\telif isinstance(seq, tuple):\n\t\t#tuple is ok\n\t\treturn (seq, 1)\n\telif isinstance(seq, set):\n\t\t#make an ordered list\n\t\treturn (list(seq), 0)\n\telif isinstance(seq, frozenset):\n\t\t#make an ordered list\n\t\treturn (list(seq), 1)\n\telif hasattr(seq, '__getitem__') and hasattr(seq, '__iter__'):\n\t\t#make an ordered list\n\t\treturn (list(seq), 0)\n\telse:\n\t\t#try to make a list\n\t\ttry:\n\t\t\tseq = list(seq)\n\t\texcept:\n\t\t\t#try to make a list of one element\n\t\t\ttry:\n\t\t\t\tseq = [seq]\n\t\t\texcept:\n\t\t\t\traise TypeError('cannot convert sequence to list of one element')\n\t\treturn (seq, 0)\n", "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple, set)):\n\t\treturn seq, list\n\tif isinstance(seq, (str, bytes, bytearray)):\n\t\treturn seq, str\n\tif isinstance(seq, _Iter):\n\t\tif seq.type == list:\n\t\t\treturn seq.iter, list\n\t\tif seq.type == str:\n\t\t\treturn seq.iter, str\n\t\tif bufsize is None:\n\t\t\treturn seq.iter, list\n\t\tbuf = []\n\t\tcount = 0\n\t\tfor x in seq.iter:\n\t\t\tcount += 1\n\t\t\tbuf.append(x)\n\t\t\tif count >= bufsize:\n\t\t\t\tbreak\n\t\treturn buf, list\n\traise ValueError('`seq` must be a sequence or iterable')\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif not _is_seq(seq):\n\t\tif bufsize is None:\n\t\t\tbufsize = _DEFAULT_BUFSIZE\n\t\tseq = _iter_seq(seq, bufsize)\n\t\tseq_type = 'buffered'\n\telif isinstance(seq, _STRING_TYPES):\n\t\tseq_type = 'string'\n\telse:\n\t\tseq_type = 'list'\n\treturn seq, seq_type\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\ttry:\n\t\ts = list(seq)  # try to get a list\n\texcept TypeError:\n\t\ts = list(iter(seq))  # or an iterator\n\treturn s, type(s)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple, xrange)):\n\t\tstype = type(seq)\n\t\tif bufsize is None:\n\t\t\tbufsize = len(seq)\n\t\tif stype is xrange:\n\t\t\tseq = list(seq)\n\t\t\tstype = list\n\telse:\n\t\tstype = None\n\t\tif bufsize is None:\n\t\t\tbufsize = 64\n\t\tseq = list(seq)\n\treturn (seq, stype)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, _types.StringType):\n\t\treturn seq, _types.StringType\n\telse:\n\t\ttry:\n\t\t\tseq = _seq_type(seq)\n\t\texcept (TypeError, AttributeError):\n\t\t\traise TypeError, 'seq must be a sequence'\n\t\telse:\n\t\t\treturn seq, _seq_type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif bufsize is None:\n\t\tbufsize = get_buffer_size()\n\tif isinstance(seq, collections.abc.Sequence):\n\t\tseq_type = type(seq)\n\telse:\n\t\tseq_type = _seq_type(bufsize)\n\t\ttry:\n\t\t\tseq = seq_type(seq)\n\t\texcept TypeError:\n\t\t\tseq = seq_type(iter(seq))\n\treturn seq, seq_type\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple, set)):\n\t\treturn seq, type(seq)\n\telse:\n\t\treturn list(iter(seq)), list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple)) or hasattr(seq, \"__getitem__\"):\n\t\treturn seq, type(seq)\n\treturn list(iter(seq)), list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, collections.Mapping):\n\t\treturn (seq, 'dict')\n\telif isinstance(seq, collections.Iterable):\n\t\treturn (seq, 'iter')\n\telse:\n\t\treturn (iter(seq), 'iter')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, list):\n\t\t#list is ok\n\t\treturn (seq, 0)\n\telif isinstance(seq, tuple):\n\t\t#tuple is ok\n\t\treturn (seq, 1)\n\telif isinstance(seq, set):\n\t\t#make an ordered list\n\t\treturn (list(seq), 0)\n\telif isinstance(seq, frozenset):\n\t\t#make an ordered list\n\t\treturn (list(seq), 1)\n\telif hasattr(seq, '__getitem__') and hasattr(seq, '__iter__'):\n\t\t#make an ordered list\n\t\treturn (list(seq), 0)\n\telse:\n\t\t#try to make a list\n\t\ttry:\n\t\t\tseq = list(seq)\n\t\texcept:\n\t\t\t#try to make a list of one element\n\t\t\ttry:\n\t\t\t\tseq = [seq]\n\t\t\texcept:\n\t\t\t\traise TypeError('cannot convert sequence to list of one element')\n\t\treturn (seq, 0)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_seq_with_type(seq, bufsize=None):\n\tif isinstance(seq, (list, tuple, set)):\n\t\treturn seq, list\n\tif isinstance(seq, (str, bytes, bytearray)):\n\t\treturn seq, str\n\tif isinstance(seq, _Iter):\n\t\tif seq.type == list:\n\t\t\treturn seq.iter, list\n\t\tif seq.type == str:\n\t\t\treturn seq.iter, str\n\t\tif bufsize is None:\n\t\t\treturn seq.iter, list\n\t\tbuf = []\n\t\tcount = 0\n\t\tfor x in seq.iter:\n\t\t\tcount += 1\n\t\t\tbuf.append(x)\n\t\t\tif count >= bufsize:\n\t\t\t\tbreak\n\t\treturn buf, list\n\traise ValueError('`seq` must be a sequence or iterable')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/histogram.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.", "ques_id": "62b87b4f9a0c4fa8b80b3581", "code": ["def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute:\n\t\t\tself._scale = None\n\t\tif self._scale is None:\n\t\t\tself._scale = self.integral(recompute)\n\t\treturn self._scale\n\tif self._scale == 0.:\n\t\traise ValueError(\"Can't rescale histogram with zero scale\")\n\tself.divide(self._scale / other, inplace=True)\n\tself._scale = other\n\n", "def scale(self, other=None, recompute=False):\n\tif other is not None:\n\t\tif not self._scale:\n\t\t\traise LenaValueError(\"can't rescale zero-scaled histogram\")\n\t\tfactor = other / self._scale\n\t\tself._data *= factor\n\t\tself._scale = other\n\t\treturn\n\tif recompute or not self._scale:\n\t\tself._scale = sum(self._data)\n\treturn self._scale\n\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute or self._scale is None:\n\t\t\tself._scale = self.sum()\n\t\treturn self._scale\n\telse:\n\t\tif self._scale == 0.:\n\t\t\traise LenaValueError(\n\t\t\t\t\"can't rescale histogram with scale == 0.\"\n\t\t\t)\n\t\tself *= other / self._scale\n\t\tself._scale = other\n\n\tdef normalize(self, recompute=False):\n\t\"\"\"\n\tNormalize histogram to unit scale.\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute:\n\t\t\tself._scale = None\n\t\tif self._scale is None:\n\t\t\tself._scale = self.sum()\n\t\treturn self._scale\n\telif not isinstance(other, numbers.Real):\n\t\traise TypeError(\"other must be None or a real number\")\n\telse:\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"cannot rescale a histogram with zero scale\")\n\t\tself.__imul__(other / self._scale)\n\t\tself._scale = other\n\t\treturn self\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self._compute_scale()\n\t\treturn self._scale\n\telse:\n\t\tif self.scale != 0:\n\t\t\tself.data /= self.scale / other\n\t\t\tself._scale = other\n\t\telse:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Can't rescale histogram with zero scale\"\n\t\t\t)\n\n\tdef _compute_scale(self):\n\t\t\"\"\"\n\t\tCompute scale of this histogram.\n\t\t\"\"\"\n\t\tscale = self.data.sum()\n\t\treturn scale\n\n\tdef __mul__(self, other):\n\t\t\"\"\"\n\t\tMultiply two histograms.\n\t\t\"\"\"\n\t\tif isinstance(other, Histogram):\n\t\t\treturn self.multiply(other)\n\t\telif isinstance(other, (int, float)):\n\t\t\treturn self.multiply(other)\n\t\telse:\n\t\t\traise LenaTypeError(\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif not self._scale_valid or recompute:\n\t\t\tself._scale = sum(self._values)\n\t\t\tself._scale_valid = True\n\t\treturn self._scale\n\telse:\n\t\tif not self._scale_valid:\n\t\t\tself.scale()\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"zero scale, can't rescale\"\n\t\t\t)\n\t\tself._values = self._values * (other / self._scale)\n\t\tself._scale = other\n\t\tself._scale_valid = True\n\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute or not self.scale_computed:\n\t\t\tself.scale = sum(self.bins)\n\t\t\tself.scale_computed = True\n\t\treturn self.scale\n\telse:\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Scale is zero. Can't rescale a histogram\"\n\t\t\t)\n\t\tif self.scale == other:\n\t\t\treturn\n\t\tself.bins = [bin * (other / self.scale) for bin in self.bins]\n\t\tself.scale = other\n\t\tself.scale_computed = True\n\n\tdef __add__(self, other):\n\t\t\"\"\"\n\t\tAdd other to self.\n\t\t\"\"\"\n\t\tif isinstance(other, Histogram):\n\t\t\treturn self.__class__(\n\t\t\t\t[x + y for x, y in zip(self.bins, other.bins)],\n\t\t\t\tself.edges,\n\t\t\t\tscale=self.\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self.scale_ is None or recompute:\n\t\t\ttry:\n\t\t\t\tself.scale_ = self._compute_scale()\n\t\t\texcept ValueError:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"scale is zero and can't be computed\"\n\t\t\t\t)\n\t\treturn self.scale_\n\telse:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\n\t\t\t\t\"scale is not computed, can't rescale\"\n\t\t\t)\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"scale is zero, can't rescale\"\n\t\t\t)\n\t\tself.values_ = self.values_ * other / self.scale\n\t\tself.scale_ = other\n\n\tdef compute_entropy(self, recompute=False):\n\t\t\"\"\"\n\t\tCompute entropy of the histogram.\n\n", "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self.integral()\n\t\treturn self._scale\n\telse:\n\t\tother = float(other)\n\t\tif self.scale() == 0:\n\t\t\traise LenaValueError(\"scale is zero, can't rescale\")\n\t\tself.mul(other / self._scale)\n\t\tself._scale = other\n\t\treturn other\n\n", "def scale(self, other=None, recompute=False):\n\t# if no scale is provided, we compute it\n\tif other is None:\n\t\t# we compute the scale if it is not there\n\t\tif recompute or self.scale is None:\n\t\t\tself.scale = self.compute_scale()\n\t\t# and return it\n\t\treturn self.scale\n\t# if a scale is provided, we rescale\n\tif self.scale is None:\n\t\traise LenaValueError(\"Can't rescale a histogram without \"\n\t\t                     \"previously computed scale.\")\n\tif self.scale == 0:\n\t\traise LenaValueError(\"Can't rescale a histogram with zero scale.\")\n\t# compute the new scale\n\tnew_scale = self.scale * other / self.scale\n\t# rescale the histogram\n\tself.hist *= new_scale / self.scale\n\t# and store the new scale\n\tself.scale = new_scale\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute:\n\t\t\tself._scale = None\n\t\tif self._scale is None:\n\t\t\tself._scale = self.integral(recompute)\n\t\treturn self._scale\n\tif self._scale == 0.:\n\t\traise ValueError(\"Can't rescale histogram with zero scale\")\n\tself.divide(self._scale / other, inplace=True)\n\tself._scale = other\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is not None:\n\t\tif not self._scale:\n\t\t\traise LenaValueError(\"can't rescale zero-scaled histogram\")\n\t\tfactor = other / self._scale\n\t\tself._data *= factor\n\t\tself._scale = other\n\t\treturn\n\tif recompute or not self._scale:\n\t\tself._scale = sum(self._data)\n\treturn self._scale\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute or self._scale is None:\n\t\t\tself._scale = self.sum()\n\t\treturn self._scale\n\telse:\n\t\tif self._scale == 0.:\n\t\t\traise LenaValueError(\n\t\t\t\t\"can't rescale histogram with scale == 0.\"\n\t\t\t)\n\t\tself *= other / self._scale\n\t\tself._scale = other\n\n\tdef normalize(self, recompute=False):\n\t\"\"\"\n\tNormalize histogram to unit scale.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute:\n\t\t\tself._scale = None\n\t\tif self._scale is None:\n\t\t\tself._scale = self.sum()\n\t\treturn self._scale\n\telif not isinstance(other, numbers.Real):\n\t\traise TypeError(\"other must be None or a real number\")\n\telse:\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\"cannot rescale a histogram with zero scale\")\n\t\tself.__imul__(other / self._scale)\n\t\tself._scale = other\n\t\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self._compute_scale()\n\t\treturn self._scale\n\telse:\n\t\tif self.scale != 0:\n\t\t\tself.data /= self.scale / other\n\t\t\tself._scale = other\n\t\telse:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Can't rescale histogram with zero scale\"\n\t\t\t)\n\n\tdef _compute_scale(self):\n\t\t\"\"\"\n\t\tCompute scale of this histogram.\n\t\t\"\"\"\n\t\tscale = self.data.sum()\n\t\treturn scale\n\n\tdef __mul__(self, other):\n\t\t\"\"\"\n\t\tMultiply two histograms.\n\t\t\"\"\"\n\t\tif isinstance(other, Histogram):\n\t\t\treturn self.multiply(other)\n\t\telif isinstance(other, (int, float)):\n\t\t\treturn self.multiply(other)\n\t\telse:\n\t\t\traise LenaTypeError(\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif not self._scale_valid or recompute:\n\t\t\tself._scale = sum(self._values)\n\t\t\tself._scale_valid = True\n\t\treturn self._scale\n\telse:\n\t\tif not self._scale_valid:\n\t\t\tself.scale()\n\t\tif self._scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"zero scale, can't rescale\"\n\t\t\t)\n\t\tself._values = self._values * (other / self._scale)\n\t\tself._scale = other\n\t\tself._scale_valid = True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif recompute or not self.scale_computed:\n\t\t\tself.scale = sum(self.bins)\n\t\t\tself.scale_computed = True\n\t\treturn self.scale\n\telse:\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"Scale is zero. Can't rescale a histogram\"\n\t\t\t)\n\t\tif self.scale == other:\n\t\t\treturn\n\t\tself.bins = [bin * (other / self.scale) for bin in self.bins]\n\t\tself.scale = other\n\t\tself.scale_computed = True\n\n\tdef __add__(self, other):\n\t\t\"\"\"\n\t\tAdd other to self.\n\t\t\"\"\"\n\t\tif isinstance(other, Histogram):\n\t\t\treturn self.__class__(\n\t\t\t\t[x + y for x, y in zip(self.bins, other.bins)],\n\t\t\t\tself.edges,\n\t\t\t\tscale=self.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self.scale_ is None or recompute:\n\t\t\ttry:\n\t\t\t\tself.scale_ = self._compute_scale()\n\t\t\texcept ValueError:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"scale is zero and can't be computed\"\n\t\t\t\t)\n\t\treturn self.scale_\n\telse:\n\t\tif self.scale is None:\n\t\t\traise LenaValueError(\n\t\t\t\t\"scale is not computed, can't rescale\"\n\t\t\t)\n\t\tif self.scale == 0:\n\t\t\traise LenaValueError(\n\t\t\t\t\"scale is zero, can't rescale\"\n\t\t\t)\n\t\tself.values_ = self.values_ * other / self.scale\n\t\tself.scale_ = other\n\n\tdef compute_entropy(self, recompute=False):\n\t\t\"\"\"\n\t\tCompute entropy of the histogram.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\tif other is None:\n\t\tif self._scale is None or recompute:\n\t\t\tself._scale = self.integral()\n\t\treturn self._scale\n\telse:\n\t\tother = float(other)\n\t\tif self.scale() == 0:\n\t\t\traise LenaValueError(\"scale is zero, can't rescale\")\n\t\tself.mul(other / self._scale)\n\t\tself._scale = other\n\t\treturn other\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None, recompute=False):\n\t# if no scale is provided, we compute it\n\tif other is None:\n\t\t# we compute the scale if it is not there\n\t\tif recompute or self.scale is None:\n\t\t\tself.scale = self.compute_scale()\n\t\t# and return it\n\t\treturn self.scale\n\t# if a scale is provided, we rescale\n\tif self.scale is None:\n\t\traise LenaValueError(\"Can't rescale a histogram without \"\n\t\t                     \"previously computed scale.\")\n\tif self.scale == 0:\n\t\traise LenaValueError(\"Can't rescale a histogram with zero scale.\")\n\t# compute the new scale\n\tnew_scale = self.scale * other / self.scale\n\t# rescale the histogram\n\tself.hist *= new_scale / self.scale\n\t# and store the new scale\n\tself.scale = new_scale\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/graph.py", "project": "ynikitenko/lena", "name": "scale", "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.", "ques_id": "62b87b519a0c4fa8b80b3583", "code": ["def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale_\n\tif not isinstance(other, numbers.Number):\n\t\traise TypeError(\n\t\t\t\"Graph scale must be a number: {}\".format(other))\n\tif not self.scale_:\n\t\traise LenaValueError(\n\t\t\t\"Graph scale is unknown: {}\".format(self.scale_))\n\tscale = self.scale_\n\tif not scale[-1]:\n\t\traise LenaValueError(\n\t\t\t\"Last dimension of graph scale is zero: {}\".format(scale))\n\tscale = scale[:-1] + (other,)\n\tself.scale_ = scale\n\tself.errors_ = [\n\t\t_rescale_error(error, self.scale_)\n\t\tfor error in self.errors_\n\t]\n\treturn self\n\n\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif other == 0:\n\t\traise LenaValueError(\"cannot rescale to 0\")\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\tcoo_field = self.get_field(self.coordinate_field)\n\tcoo_field.values *= other / self._scale\n\tfor field in self.fields:\n\t\tif field.name == self.coordinate_field:\n\t\t\tcontinue\n\t\tif field.name.startswith(\"error_\"):\n\t\t\tfield.values *= other / self._scale\n\tself._scale = other\n\treturn self\n\n", "def scale(self, other=None):\n\tif self._scale is None:\n\t\t# scale is unknown\n\t\traise LenaValueError(\n\t\t\t\"Can't rescale graph with unknown scale\"\n\t\t)\n\tif other is None:\n\t\t# return current scale\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\t# scale is zero\n\t\traise LenaValueError(\n\t\t\t\"Can't rescale graph with zero scale\"\n\t\t)\n\t# convert to float to prevent integer division\n\t# (this may raise ValueError)\n\told_scale = float(self._scale)\n\tnew_scale = float(other)\n\t# convert to float to prevent integer division\n\t# (this may raise ValueError)\n\tscale_factor = new_scale / old_scale\n\t# rescale coordinates\n\tfor field in self.fields:\n\t\tfield.rescale(scale_factor)\n\t# rescale errors\n\tfor field in self.fields:\n\t\tif field.error is not None:\n\t\t\tfield.error.rescale(scale_factor)\n\t# rescale scale\n\tself._\n", "def scale(self, other=None):\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\telse:\n\t\t\tif self._scale == 0.0:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"can't rescale graph with unknown scale\"\n\t\t\t\t)\n\t\t\tif self._scale == other:\n\t\t\t\treturn\n\t\t\t# convert from um to um^-1\n\t\t\tother = 1.0 / other\n\t\t\t# find last coordinate\n\t\t\tlast_coord_field = self._graph.fields[-1]\n\t\t\t# rescale all fields\n\t\t\tfor field in self._graph.fields:\n\t\t\t\tcoord = self._graph[field]\n\t\t\t\tself._graph[field] = coord * other\n\t\t\t\tif field == last_coord_field:\n\t\t\t\t\tother = 1.0 / other\n\t\t\t\t\t# rescale errors\n\t\t\t\t\tfor err_field in self._graph.fields_errors:\n\t\t\t\t\t\terr = self._graph[err_field]\n\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale\n\tif self.scale is None:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\tif self.scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with zero scale\")\n\tscale_factor = other / self.scale\n\tfor fld in self.fields:\n\t\tfld.rescale(scale_factor)\n\tself.scale = other\n\n\n", "def scale(self, other=None):\n\t# get scale\n\tif other is None:\n\t\tif len(self._scale) == 0:\n\t\t\traise LenaValueError(\"scale is unknown\")\n\t\treturn self._scale[-1]\n\t# set scale\n\tif not isinstance(other, numbers.Real):\n\t\traise LenaTypeError(\n\t\t\t\"other must be a number, not {}\".format(type(other))\n\t\t)\n\tif self._scale[-1] == 0:\n\t\traise LenaValueError(\"scale is zero\")\n\t# rescale\n\tscale = other / self._scale[-1]\n\tself._scale[-1] = other\n\tfor field in self._fields:\n\t\tself._fields[field] *= scale\n\t\tif self._errors is not None:\n\t\t\tself._errors[field] *= scale\n\treturn other\n\n\n", "def scale(self, other=None):\n\treturn self._scale(other)\n\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale_\n\ttry:\n\t\tscale = float(other)\n\texcept ValueError:\n\t\traise LenaValueError(\n\t\t\t\"scale must be a number, got {!r}\".format(other)\n\t\t)\n\tif scale == 0:\n\t\traise LenaValueError(\"can't rescale to zero\")\n\tif self.scale_ == 0:\n\t\traise LenaValueError(\"can't rescale unknown scale\")\n\tself.scale_ *= scale\n\t# rescale fields\n\tfor field in self.fields:\n\t\t# use field.scale if it is not None\n\t\tscale = field.scale\n\t\tif scale is None:\n\t\t\tscale = self.scale_\n\t\tfield.values *= scale\n\tfor error in self.errors:\n\t\t# use error.scale if it is not None\n\t\tscale = error.scale\n\t\tif scale is None:\n\t\t\tscale = self.scale_\n\t\terror.values *= scale\n\treturn self\n\n\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\ttry:\n\t\tscale = other / self._scale\n\texcept ZeroDivisionError:\n\t\traise LenaValueError(\n\t\t\t\"graph scale is zero and cannot be rescaled\"\n\t\t)\n\tself._scale = other\n\tself._scale_field(scale, self._fields[-1])\n\n\tdef _scale_field(self, scale, field):\n\t\tfor v in field.values():\n\t\t\tv.value *= scale\n\t\t\tfor e in v.edges:\n\t\t\t\te.value *= scale\n\t\t\t\tfor b in e.bins:\n\t\t\t\t\tb.value *= scale\n\n\tdef __repr__(self):\n\t\treturn \"<{0} {1}>\".format(\n\t\t\tself.__class__.__name__, self.fields\n\t\t)\n\n\tdef __str__(self):\n\t\treturn \"{0} {1}\".format(\n\t\t\tself.__class__.__name__, self.fields\n\t\t)\n", "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif not isinstance(other, numbers.Real):\n\t\traise LenaValueError(\"scale must be a number\")\n\tif self._scale is None:\n\t\traise LenaValueError(\n\t\t\t\"cannot rescale a graph with unknown scale\"\n\t\t)\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale a graph with zero scale\")\n\t# resize all fields\n\tfor field in self._fields:\n\t\tfield.resize(scale=other / self._scale)\n\t# rescale errors\n\tfor err in self._errors:\n\t\terr.rescale(other / self._scale)\n\tself._scale = other\n\treturn None\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale_\n\tif not isinstance(other, numbers.Number):\n\t\traise TypeError(\n\t\t\t\"Graph scale must be a number: {}\".format(other))\n\tif not self.scale_:\n\t\traise LenaValueError(\n\t\t\t\"Graph scale is unknown: {}\".format(self.scale_))\n\tscale = self.scale_\n\tif not scale[-1]:\n\t\traise LenaValueError(\n\t\t\t\"Last dimension of graph scale is zero: {}\".format(scale))\n\tscale = scale[:-1] + (other,)\n\tself.scale_ = scale\n\tself.errors_ = [\n\t\t_rescale_error(error, self.scale_)\n\t\tfor error in self.errors_\n\t]\n\treturn self\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif other == 0:\n\t\traise LenaValueError(\"cannot rescale to 0\")\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\tcoo_field = self.get_field(self.coordinate_field)\n\tcoo_field.values *= other / self._scale\n\tfor field in self.fields:\n\t\tif field.name == self.coordinate_field:\n\t\t\tcontinue\n\t\tif field.name.startswith(\"error_\"):\n\t\t\tfield.values *= other / self._scale\n\tself._scale = other\n\treturn self\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif self._scale is None:\n\t\t# scale is unknown\n\t\traise LenaValueError(\n\t\t\t\"Can't rescale graph with unknown scale\"\n\t\t)\n\tif other is None:\n\t\t# return current scale\n\t\treturn self._scale\n\tif self._scale == 0:\n\t\t# scale is zero\n\t\traise LenaValueError(\n\t\t\t\"Can't rescale graph with zero scale\"\n\t\t)\n\t# convert to float to prevent integer division\n\t# (this may raise ValueError)\n\told_scale = float(self._scale)\n\tnew_scale = float(other)\n\t# convert to float to prevent integer division\n\t# (this may raise ValueError)\n\tscale_factor = new_scale / old_scale\n\t# rescale coordinates\n\tfor field in self.fields:\n\t\tfield.rescale(scale_factor)\n\t# rescale errors\n\tfor field in self.fields:\n\t\tif field.error is not None:\n\t\t\tfield.error.rescale(scale_factor)\n\t# rescale scale\n\tself._\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t\tif other is None:\n\t\t\treturn self._scale\n\t\telse:\n\t\t\tif self._scale == 0.0:\n\t\t\t\traise LenaValueError(\n\t\t\t\t\t\"can't rescale graph with unknown scale\"\n\t\t\t\t)\n\t\t\tif self._scale == other:\n\t\t\t\treturn\n\t\t\t# convert from um to um^-1\n\t\t\tother = 1.0 / other\n\t\t\t# find last coordinate\n\t\t\tlast_coord_field = self._graph.fields[-1]\n\t\t\t# rescale all fields\n\t\t\tfor field in self._graph.fields:\n\t\t\t\tcoord = self._graph[field]\n\t\t\t\tself._graph[field] = coord * other\n\t\t\t\tif field == last_coord_field:\n\t\t\t\t\tother = 1.0 / other\n\t\t\t\t\t# rescale errors\n\t\t\t\t\tfor err_field in self._graph.fields_errors:\n\t\t\t\t\t\terr = self._graph[err_field]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale\n\tif self.scale is None:\n\t\traise LenaValueError(\"cannot rescale graph with unknown scale\")\n\tif self.scale == 0:\n\t\traise LenaValueError(\"cannot rescale graph with zero scale\")\n\tscale_factor = other / self.scale\n\tfor fld in self.fields:\n\t\tfld.rescale(scale_factor)\n\tself.scale = other\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\t# get scale\n\tif other is None:\n\t\tif len(self._scale) == 0:\n\t\t\traise LenaValueError(\"scale is unknown\")\n\t\treturn self._scale[-1]\n\t# set scale\n\tif not isinstance(other, numbers.Real):\n\t\traise LenaTypeError(\n\t\t\t\"other must be a number, not {}\".format(type(other))\n\t\t)\n\tif self._scale[-1] == 0:\n\t\traise LenaValueError(\"scale is zero\")\n\t# rescale\n\tscale = other / self._scale[-1]\n\tself._scale[-1] = other\n\tfor field in self._fields:\n\t\tself._fields[field] *= scale\n\t\tif self._errors is not None:\n\t\t\tself._errors[field] *= scale\n\treturn other\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\treturn self._scale(other)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self.scale_\n\ttry:\n\t\tscale = float(other)\n\texcept ValueError:\n\t\traise LenaValueError(\n\t\t\t\"scale must be a number, got {!r}\".format(other)\n\t\t)\n\tif scale == 0:\n\t\traise LenaValueError(\"can't rescale to zero\")\n\tif self.scale_ == 0:\n\t\traise LenaValueError(\"can't rescale unknown scale\")\n\tself.scale_ *= scale\n\t# rescale fields\n\tfor field in self.fields:\n\t\t# use field.scale if it is not None\n\t\tscale = field.scale\n\t\tif scale is None:\n\t\t\tscale = self.scale_\n\t\tfield.values *= scale\n\tfor error in self.errors:\n\t\t# use error.scale if it is not None\n\t\tscale = error.scale\n\t\tif scale is None:\n\t\t\tscale = self.scale_\n\t\terror.values *= scale\n\treturn self\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\ttry:\n\t\tscale = other / self._scale\n\texcept ZeroDivisionError:\n\t\traise LenaValueError(\n\t\t\t\"graph scale is zero and cannot be rescaled\"\n\t\t)\n\tself._scale = other\n\tself._scale_field(scale, self._fields[-1])\n\n\tdef _scale_field(self, scale, field):\n\t\tfor v in field.values():\n\t\t\tv.value *= scale\n\t\t\tfor e in v.edges:\n\t\t\t\te.value *= scale\n\t\t\t\tfor b in e.bins:\n\t\t\t\t\tb.value *= scale\n\n\tdef __repr__(self):\n\t\treturn \"<{0} {1}>\".format(\n\t\t\tself.__class__.__name__, self.fields\n\t\t)\n\n\tdef __str__(self):\n\t\treturn \"{0} {1}\".format(\n\t\t\tself.__class__.__name__, self.fields\n\t\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def scale(self, other=None):\n\tif other is None:\n\t\treturn self._scale\n\tif not isinstance(other, numbers.Real):\n\t\traise LenaValueError(\"scale must be a number\")\n\tif self._scale is None:\n\t\traise LenaValueError(\n\t\t\t\"cannot rescale a graph with unknown scale\"\n\t\t)\n\tif self._scale == 0:\n\t\traise LenaValueError(\"cannot rescale a graph with zero scale\")\n\t# resize all fields\n\tfor field in self._fields:\n\t\tfield.resize(scale=other / self._scale)\n\t# rescale errors\n\tfor err in self._errors:\n\t\terr.rescale(other / self._scale)\n\tself._scale = other\n\treturn None\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "lena/structures/hist_functions.py", "project": "ynikitenko/lena", "name": "hist_to_graph", "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.", "ques_id": "62b87b869a0c4fa8b80b35e1", "code": ["def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2.\n\telse:\n\t\traise ValueError(\"invalid get_coordinate: %r\" % (get_coordinate,))\n\t# TODO: make sure that get_coordinate returns a numeric value\n\tif field_names is None:\n\t\tfield_names = (\"x\", \"y\")\n\tif not isinstance(field_names, tuple):\n\t\tfield_names = (field_names,)\n\tif len(field_names) != hist.dimension + 1:\n\t\traise ValueError(\"invalid number of field names\")\n\tif scale is True:\n\t\tscale = hist.scale\n\treturn graphs.Graph(\n\t\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif hist.dim != 1:\n\t\traise ValueError(\"histogram must be one-dimensional\")\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph_bin_factory = _graph_bin_factory(make_value, get_coordinate,\n\t                                       field_names, scale)\n\tgraph_bins = [graph_bin_factory(bin_) for bin_ in hist]\n\treturn graph(graph_bins, field_names=field_names, scale=scale)\n\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\telif get_coordinate != \"left\" and get_coordinate != \"right\" \\\n\t\t\tand get_coordinate != \"middle\":\n\t\traise ValueError(\"unsupported coordinate\")\n\tresult = graph.graph(field_names)\n\tfor bin_ in hist:\n\t\tcoordinate = get_coordinate(bin_)\n\t\tvalue = make_value(bin_)\n\t\tresult.append(coordinate, value)\n\tif scale is True:\n\t\tresult.scale = hist.scale\n\telif scale is not None:\n\t\tresult.scale = scale\n\treturn result\n\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tassert get_coordinate in (\"left\", \"right\", \"middle\")\n\tassert len(field_names) == hist.dimension + 1\n\tgraph = graph_t(hist.dimension)\n\tgraph.scale = scale if scale is not None else hist.scale\n\tfor bin_ in hist.bins:\n\t\tif make_value is not None:\n\t\t\tbin_ = make_value(bin_)\n\t\tgraph.append(bin_)\n\tgraph.fields = field_names\n\tif get_coordinate == \"middle\":\n\t\tgraph.shift_coordinate(hist.bin_width, 0)\n\telif get_coordinate == \"right\":\n\t\tgraph.shift_coordinate(hist.bin_width, -0.5)\n\telse:\n\t\tgraph.shift_coordinate(hist.bin_width, +0.5)\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\telse:\n\t\traise ValueError(\"unknown option %r\" % get_coordinate)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.value\n\telif not callable(make_value):\n\t\traise TypeError(\"make_value must be callable\")\n\tif scale is None:\n\t\tscale = UNKNOWN\n\telif scale is True:\n\t\tscale = hist.scale\n\tx = []\n\ty = []\n\tfor bin_ in hist:\n\t\tx.append(get_coordinate(bin_))\n\t\ty.append(make_value(bin_))\n\treturn graph(x, y, field_names=field_names, scale=scale)\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tassert get_coordinate in (\"left\", \"right\", \"middle\")\n\tassert len(field_names) == hist.dimension + 1\n\tassert (make_value is None) or (hist.is_numeric or callable(make_value))\n\tif scale is True:\n\t\tscale = hist.scale\n\telse:\n\t\tscale = None\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tdef make_point(bin_):\n\t\tcoordinate = bin_.coordinate[get_coordinate]\n\t\tvalue = make_value(bin_.value)\n\t\treturn coordinate, value\n\tpoints = map(make_point, hist.bins)\n\treturn graph(field_names, points, scale)\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t# TODO: implement make_value as a decorator\n\t# to allow make_value to accept context and a single value\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph = graph_type.new_graph(field_names, scale=scale)\n\tbins = hist.bins\n\tfor index in range(len(bins)):\n\t\tbin_ = bins[index]\n\t\tif make_value is None:\n\t\t\tgraph.add_point(index, bin_)\n\t\telse:\n\t\t\tgraph.add_point(index, make_value(bin_))\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph = graph_base.graph(scale=scale)\n\tgraph_points = graph.points\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\thist_bins = hist.bins\n\tfor bin_ in hist_bins:\n\t\tcoordinate = bin_.coordinate\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinate -= hist.bin_width / 2\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinate += hist.bin_width / 2\n\t\tgraph_points[coordinate] = make_value(bin_)\n\tgraph.set_field_names(field_names)\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t# TODO: Optimize\n\tgraph_bins = []\n\tfor bin in hist.iterbins():\n\t\tif make_value is None:\n\t\t\tvalue = bin.content\n\t\telse:\n\t\t\tvalue = make_value(bin.content)\n\t\tif value is None:\n\t\t\tcontinue\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinates = bin.left\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinates = bin.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tcoordinates = bin.middle\n\t\telse:\n\t\t\traise ValueError(get_coordinate)\n\t\tgraph_bins.append((coordinates, value))\n\tgraph = graph_type(graph_bins, hist.axis_names, field_names, scale)\n\treturn graph\n\n", "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True:\n\t\tscale = hist.scale\n\treturn graph(\n\t\tmap(lambda bin_: \n\t\t\t(bin_[0] if get_coordinate == \"left\"\n\t\t\t\telse (bin_[0] + bin_[1]) / 2 if get_coordinate == \"middle\"\n\t\t\t\telse bin_[1]),\n\t\t\thist.bins),\n\t\tmap(lambda bin_: make_value(bin_[2]) if make_value is not None\n\t\t\telse bin_[2], hist.bins),\n\t\tfield_names=field_names,\n\t\tscale=scale,\n\t\t)\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2.\n\telse:\n\t\traise ValueError(\"invalid get_coordinate: %r\" % (get_coordinate,))\n\t# TODO: make sure that get_coordinate returns a numeric value\n\tif field_names is None:\n\t\tfield_names = (\"x\", \"y\")\n\tif not isinstance(field_names, tuple):\n\t\tfield_names = (field_names,)\n\tif len(field_names) != hist.dimension + 1:\n\t\traise ValueError(\"invalid number of field names\")\n\tif scale is True:\n\t\tscale = hist.scale\n\treturn graphs.Graph(\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif hist.dim != 1:\n\t\traise ValueError(\"histogram must be one-dimensional\")\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph_bin_factory = _graph_bin_factory(make_value, get_coordinate,\n\t                                       field_names, scale)\n\tgraph_bins = [graph_bin_factory(bin_) for bin_ in hist]\n\treturn graph(graph_bins, field_names=field_names, scale=scale)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\telif get_coordinate != \"left\" and get_coordinate != \"right\" \\\n\t\t\tand get_coordinate != \"middle\":\n\t\traise ValueError(\"unsupported coordinate\")\n\tresult = graph.graph(field_names)\n\tfor bin_ in hist:\n\t\tcoordinate = get_coordinate(bin_)\n\t\tvalue = make_value(bin_)\n\t\tresult.append(coordinate, value)\n\tif scale is True:\n\t\tresult.scale = hist.scale\n\telif scale is not None:\n\t\tresult.scale = scale\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tassert get_coordinate in (\"left\", \"right\", \"middle\")\n\tassert len(field_names) == hist.dimension + 1\n\tgraph = graph_t(hist.dimension)\n\tgraph.scale = scale if scale is not None else hist.scale\n\tfor bin_ in hist.bins:\n\t\tif make_value is not None:\n\t\t\tbin_ = make_value(bin_)\n\t\tgraph.append(bin_)\n\tgraph.fields = field_names\n\tif get_coordinate == \"middle\":\n\t\tgraph.shift_coordinate(hist.bin_width, 0)\n\telif get_coordinate == \"right\":\n\t\tgraph.shift_coordinate(hist.bin_width, -0.5)\n\telse:\n\t\tgraph.shift_coordinate(hist.bin_width, +0.5)\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif get_coordinate == \"left\":\n\t\tget_coordinate = lambda bin_: bin_.left\n\telif get_coordinate == \"right\":\n\t\tget_coordinate = lambda bin_: bin_.right\n\telif get_coordinate == \"middle\":\n\t\tget_coordinate = lambda bin_: (bin_.left + bin_.right) / 2\n\telse:\n\t\traise ValueError(\"unknown option %r\" % get_coordinate)\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_.value\n\telif not callable(make_value):\n\t\traise TypeError(\"make_value must be callable\")\n\tif scale is None:\n\t\tscale = UNKNOWN\n\telif scale is True:\n\t\tscale = hist.scale\n\tx = []\n\ty = []\n\tfor bin_ in hist:\n\t\tx.append(get_coordinate(bin_))\n\t\ty.append(make_value(bin_))\n\treturn graph(x, y, field_names=field_names, scale=scale)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tassert get_coordinate in (\"left\", \"right\", \"middle\")\n\tassert len(field_names) == hist.dimension + 1\n\tassert (make_value is None) or (hist.is_numeric or callable(make_value))\n\tif scale is True:\n\t\tscale = hist.scale\n\telse:\n\t\tscale = None\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\tdef make_point(bin_):\n\t\tcoordinate = bin_.coordinate[get_coordinate]\n\t\tvalue = make_value(bin_.value)\n\t\treturn coordinate, value\n\tpoints = map(make_point, hist.bins)\n\treturn graph(field_names, points, scale)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t# TODO: implement make_value as a decorator\n\t# to allow make_value to accept context and a single value\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph = graph_type.new_graph(field_names, scale=scale)\n\tbins = hist.bins\n\tfor index in range(len(bins)):\n\t\tbin_ = bins[index]\n\t\tif make_value is None:\n\t\t\tgraph.add_point(index, bin_)\n\t\telse:\n\t\t\tgraph.add_point(index, make_value(bin_))\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True:\n\t\tscale = hist.scale\n\tgraph = graph_base.graph(scale=scale)\n\tgraph_points = graph.points\n\tif make_value is None:\n\t\tmake_value = lambda bin_: bin_\n\thist_bins = hist.bins\n\tfor bin_ in hist_bins:\n\t\tcoordinate = bin_.coordinate\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinate -= hist.bin_width / 2\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinate += hist.bin_width / 2\n\t\tgraph_points[coordinate] = make_value(bin_)\n\tgraph.set_field_names(field_names)\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\t# TODO: Optimize\n\tgraph_bins = []\n\tfor bin in hist.iterbins():\n\t\tif make_value is None:\n\t\t\tvalue = bin.content\n\t\telse:\n\t\t\tvalue = make_value(bin.content)\n\t\tif value is None:\n\t\t\tcontinue\n\t\tif get_coordinate == \"left\":\n\t\t\tcoordinates = bin.left\n\t\telif get_coordinate == \"right\":\n\t\t\tcoordinates = bin.right\n\t\telif get_coordinate == \"middle\":\n\t\t\tcoordinates = bin.middle\n\t\telse:\n\t\t\traise ValueError(get_coordinate)\n\t\tgraph_bins.append((coordinates, value))\n\tgraph = graph_type(graph_bins, hist.axis_names, field_names, scale)\n\treturn graph\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n\tif scale is True:\n\t\tscale = hist.scale\n\treturn graph(\n\t\tmap(lambda bin_: \n\t\t\t(bin_[0] if get_coordinate == \"left\"\n\t\t\t\telse (bin_[0] + bin_[1]) / 2 if get_coordinate == \"middle\"\n\t\t\t\telse bin_[1]),\n\t\t\thist.bins),\n\t\tmap(lambda bin_: make_value(bin_[2]) if make_value is not None\n\t\t\telse bin_[2], hist.bins),\n\t\tfield_names=field_names,\n\t\tscale=scale,\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "_verify", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1136", "code": ["def _verify(iface, candidate, tentative=False, vtype=None):\n\tif vtype is None:\n\t\tvtype = _verify_class\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(candidate, \"%r does not provide %s\" % (candidate, iface.__identifier__))\n\terrs = []\n\terrs.extend(vtype(iface, candidate))\n\treturn errs\n\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif (not tentative\n\t\tand not iface.providedBy(candidate)\n\t\tand not isinstance(candidate, (iface, _class_types))):\n\t\treturn False\n\tif vtype is None:\n\t\tvtype = iface\n\tinvalid = Invalid(candidate, vtype)\n\tfor desc in vtype.names(all=True):\n\t\tif not _check_desc(candidate, iface, desc):\n\t\t\tinvalid.append(desc)\n\tif invalid:\n\t\tif len(invalid) == 1:\n\t\t\traise invalid[0]\n\t\traise invalid\n\treturn True\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not vtype:\n\t\tvtype = iface\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not claim to provide %r\" % (candidate, iface)\n\t\t)\n\terrors = []\n\tfor name, spec in iface.namesAndDescriptions(True):\n\t\tif isinstance(spec, Method):\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(\n\t\t\t\t\tInvalid(\n\t\t\t\t\t\t\"%r has no %s method\" % (candidate, name)\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\t\t\tmember = getattr(candidate, name)\n\t\t\tif spec.getSignatureInfo(candidate) != (\n\t\t\t\tmember.__name__, member.__doc__, None\n\t\t\t):\n\t\t\t\terrors.append(\n\t\t\t\t\tInvalid(\n\t\t\t\t\t\t\"%r's %s method does not\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide %s\" % (candidate, iface.__identifier__))\n\n\tif vtype is None:\n\t\tvtype = _verify_descriptor\n\t# Verify all the methods.\n\tfor method_name in iface:\n\t\tif method_name.startswith('_v_'):\n\t\t\tcontinue\n\t\tdescriptor = getattr(iface, method_name, None)\n\t\tif not descriptor:\n\t\t\tcontinue\n\t\tvtype(candidate, iface, method_name, descriptor)\n\n\treturn True\n\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not vtype:\n\t\tvtype = iface\n\t\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t'%r does not provide %s' % (candidate, vtype.__name__))\n\t\n\tif vtype is not iface:\n\t\t# a subclass of iface.  We need to verify that the\n\t\t# candidate provides iface as well.\n\t\t_verify(\n\t\t\tiface=iface,\n\t\t\tcandidate=candidate,\n\t\t\ttentative=tentative,\n\t\t\tvtype=vtype,\n\t\t)\n\t\n\t# Now we need to check that the candidate provides all the\n\t# attributes and methods necessary to support the interface.\n\t\n\t# Verify methods\n\terrors = []\n\tfor name, spec in vtype.namesAndDescriptions(1):\n\t\ttry:\n\t\t\tmethod = getattr(candidate, name, _marker)\n\t\texcept Exception:\n\t\t\tmethod = _mark\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\"{0} does not provide {1}\".format(candidate, iface))\n\terrors = _verifyObject(iface, candidate, vtype)\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise Invalid(\"\\n\".join(str(e) for e in errors))\n\treturn True\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t'%r does not implement %s' % (candidate, iface.__identifier__),\n\t\t\tcandidate)\n\n\tif vtype is None:\n\t\tvtype = iface.validateInvariants\n\n\terrors = []\n\n\tdef report(msg):\n\t\terrors.append(Invalid(msg, candidate))\n\n\tdef checkObject(name, value, required):\n\t\tif value is None:\n\t\t\tif required:\n\t\t\t\treport('%r lacks %s %r' % (candidate, vtype.__name__, name))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tif vtype is iface.validateInvariants:\n\t\t\t\t\tvtype(name, value)\n\t\t\t\telse:\n\t\t\t\t\tvtype(value)\n\t\t\texcept Exception as e:\n\t\t\t\treport('%r has invalid %s %r: %s'\n\t\t\t\t\t   % (candidate,\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not vtype:\n\t\tvtype = _verify_class\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t'%r does not provide %r' % (candidate, iface))\n\treturn vtype(iface, candidate)\n\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif vtype is None:\n\t\tvtype = _verify\n\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"{} does not implement `{}`\".format(candidate, iface.__name__),\n\t\t\tcandidate, iface)\n\n\t# Collect all errors.\n\terrors = []\n\tfor name, descriptor in iface.namesAndDescriptions():\n\t\tif isinstance(descriptor, (Method, Attribute)):\n\t\t\ttry:\n\t\t\t\t# This will raise an exception if the attribute is missing\n\t\t\t\tvalue = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\terrors.append(\n\t\t\t\t\t\"{} does not have an attribute {}\".format(candidate, name))\n\t\t\t\tcontinue\n\n\t\t\t# This will raise an exception if the value is not correct\n\t\t\tdescriptor.bind(candidate).validate(value)\n\n\t\telif isinstance(descriptor, ClassProvides):\n\t\n", "def _verify(iface, candidate, tentative=False, vtype=None):\n\t# We're in the middle of testing.  We don't want to try to get\n\t# the interface of the interface.\n\tif iface is Interface:\n\t\treturn True\n\n\t# Make sure the candidate provides the interface:\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t\t)\n\n\tdef _get_type(ob):\n\t\tif inspect.isclass(ob):\n\t\t\treturn ob.__name__\n\t\tif inspect.isfunction(ob):\n\t\t\treturn \"method\"\n\t\treturn type(ob).__name__\n\n\tdef _get_name(ob):\n\t\tif inspect.isclass(ob):\n\t\t\treturn ob.__name__\n\t\tif inspect.isfunction(ob):\n\t\t\treturn ob.__name__\n\t\treturn ob.__class__.__name__\n\n\tdef get_type(ob):\n\t\tif vtype is not None:\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif vtype is None:\n\t\tvtype = _verify_class\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(candidate, \"%r does not provide %s\" % (candidate, iface.__identifier__))\n\terrs = []\n\terrs.extend(vtype(iface, candidate))\n\treturn errs\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif (not tentative\n\t\tand not iface.providedBy(candidate)\n\t\tand not isinstance(candidate, (iface, _class_types))):\n\t\treturn False\n\tif vtype is None:\n\t\tvtype = iface\n\tinvalid = Invalid(candidate, vtype)\n\tfor desc in vtype.names(all=True):\n\t\tif not _check_desc(candidate, iface, desc):\n\t\t\tinvalid.append(desc)\n\tif invalid:\n\t\tif len(invalid) == 1:\n\t\t\traise invalid[0]\n\t\traise invalid\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not vtype:\n\t\tvtype = iface\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not claim to provide %r\" % (candidate, iface)\n\t\t)\n\terrors = []\n\tfor name, spec in iface.namesAndDescriptions(True):\n\t\tif isinstance(spec, Method):\n\t\t\tif not hasattr(candidate, name):\n\t\t\t\terrors.append(\n\t\t\t\t\tInvalid(\n\t\t\t\t\t\t\"%r has no %s method\" % (candidate, name)\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t\tcontinue\n\t\t\tmember = getattr(candidate, name)\n\t\t\tif spec.getSignatureInfo(candidate) != (\n\t\t\t\tmember.__name__, member.__doc__, None\n\t\t\t):\n\t\t\t\terrors.append(\n\t\t\t\t\tInvalid(\n\t\t\t\t\t\t\"%r's %s method does not\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not provide %s\" % (candidate, iface.__identifier__))\n\n\tif vtype is None:\n\t\tvtype = _verify_descriptor\n\t# Verify all the methods.\n\tfor method_name in iface:\n\t\tif method_name.startswith('_v_'):\n\t\t\tcontinue\n\t\tdescriptor = getattr(iface, method_name, None)\n\t\tif not descriptor:\n\t\t\tcontinue\n\t\tvtype(candidate, iface, method_name, descriptor)\n\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not vtype:\n\t\tvtype = iface\n\t\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t'%r does not provide %s' % (candidate, vtype.__name__))\n\t\n\tif vtype is not iface:\n\t\t# a subclass of iface.  We need to verify that the\n\t\t# candidate provides iface as well.\n\t\t_verify(\n\t\t\tiface=iface,\n\t\t\tcandidate=candidate,\n\t\t\ttentative=tentative,\n\t\t\tvtype=vtype,\n\t\t)\n\t\n\t# Now we need to check that the candidate provides all the\n\t# attributes and methods necessary to support the interface.\n\t\n\t# Verify methods\n\terrors = []\n\tfor name, spec in vtype.namesAndDescriptions(1):\n\t\ttry:\n\t\t\tmethod = getattr(candidate, name, _marker)\n\t\texcept Exception:\n\t\t\tmethod = _mark\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\"{0} does not provide {1}\".format(candidate, iface))\n\terrors = _verifyObject(iface, candidate, vtype)\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise Invalid(\"\\n\".join(str(e) for e in errors))\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t'%r does not implement %s' % (candidate, iface.__identifier__),\n\t\t\tcandidate)\n\n\tif vtype is None:\n\t\tvtype = iface.validateInvariants\n\n\terrors = []\n\n\tdef report(msg):\n\t\terrors.append(Invalid(msg, candidate))\n\n\tdef checkObject(name, value, required):\n\t\tif value is None:\n\t\t\tif required:\n\t\t\t\treport('%r lacks %s %r' % (candidate, vtype.__name__, name))\n\t\telse:\n\t\t\ttry:\n\t\t\t\tif vtype is iface.validateInvariants:\n\t\t\t\t\tvtype(name, value)\n\t\t\t\telse:\n\t\t\t\t\tvtype(value)\n\t\t\texcept Exception as e:\n\t\t\t\treport('%r has invalid %s %r: %s'\n\t\t\t\t\t   % (candidate,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif not vtype:\n\t\tvtype = _verify_class\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t'%r does not provide %r' % (candidate, iface))\n\treturn vtype(iface, candidate)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\tif vtype is None:\n\t\tvtype = _verify\n\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"{} does not implement `{}`\".format(candidate, iface.__name__),\n\t\t\tcandidate, iface)\n\n\t# Collect all errors.\n\terrors = []\n\tfor name, descriptor in iface.namesAndDescriptions():\n\t\tif isinstance(descriptor, (Method, Attribute)):\n\t\t\ttry:\n\t\t\t\t# This will raise an exception if the attribute is missing\n\t\t\t\tvalue = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\terrors.append(\n\t\t\t\t\t\"{} does not have an attribute {}\".format(candidate, name))\n\t\t\t\tcontinue\n\n\t\t\t# This will raise an exception if the value is not correct\n\t\t\tdescriptor.bind(candidate).validate(value)\n\n\t\telif isinstance(descriptor, ClassProvides):\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _verify(iface, candidate, tentative=False, vtype=None):\n\t# We're in the middle of testing.  We don't want to try to get\n\t# the interface of the interface.\n\tif iface is Interface:\n\t\treturn True\n\n\t# Make sure the candidate provides the interface:\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t\t)\n\n\tdef _get_type(ob):\n\t\tif inspect.isclass(ob):\n\t\t\treturn ob.__name__\n\t\tif inspect.isfunction(ob):\n\t\t\treturn \"method\"\n\t\treturn type(ob).__name__\n\n\tdef _get_name(ob):\n\t\tif inspect.isclass(ob):\n\t\t\treturn ob.__name__\n\t\tif inspect.isfunction(ob):\n\t\t\treturn ob.__name__\n\t\treturn ob.__class__.__name__\n\n\tdef get_type(ob):\n\t\tif vtype is not None:\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyObject", "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.", "ques_id": "62b8b4baeb7e40a82d2d1137", "code": ["def verifyObject(iface, candidate, tentative=False):\n\terrors = []\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(Invalid(\n\t\t\t\t'Object %r does not provide %r '\n\t\t\t\t'(but does provide %s)' % (\n\t\t\t\t\tcandidate, iface, [\n\t\t\t\t\t\tprovided.__name__ for provided\n\t\t\t\t\t\tin providedBy(candidate)\n\t\t\t\t\t])))\n\tfor method_name in iface:\n\t\tmethod = getattr(candidate, method_name, None)\n\t\tif method is None:\n\t\t\terrors.append(Invalid(\n\t\t\t\t'Object %r does not provide %s' % (\n\t\t\t\t\tcandidate, method_name)))\n\t\t\tcontinue\n\n\t\tmethod_iface = iface[method_name]\n\t\tif method_iface.getTaggedValue('_callable'):\n\t\t\ttry:\n\t\t\t\tverifyObject(method_iface, method, tentative)\n\t\t\texcept Invalid\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not provide %s\" % (candidate, iface.getName()))\n\terrors = []\n\tfor name, d in getMembers(iface, getter=True).items():\n\t\ttry:\n\t\t\td.bind(candidate)\n\t\texcept Exception as e:\n\t\t\terrors.append(e)\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise Invalid(\"\\n\".join([str(e) for e in errors]))\n\treturn True\n\n\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t\t)\n\n\terrors = []\n\n\t# check methods\n\tfor name, method in iface.namesAndDescriptions(False):\n\t\tif isinstance(method, Method) and method.getSignatureInfo():\n\t\t\ttry:\n\t\t\t\tm = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\terrors.append(\n\t\t\t\t\t\"%r does not define %s\" % (candidate, name)\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tif hasattr(m, 'im_func'):\n\t\t\t\t\tm = m.im_func\n\t\t\t\tif isinstance(m, FunctionType):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tverifyClass(m, method)\n\t\t\t\t\texcept Invalid as e:\n\n", "def verifyObject(iface, candidate, tentative=False):\n\tif isinstance(iface, interface.InterfaceClass):\n\t\tif not iface.isOrExtends(IComponent):\n\t\t\traise ValueError('%r is not a valid IComponent interface' % (iface,))\n\telse:\n\t\tiface = interface.providedBy(iface)\n\t\tif not iface.isOrExtends(IComponent):\n\t\t\traise ValueError('%r does not implement a valid IComponent interface' % (iface,))\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid('%r does not provide %s' % (candidate, iface.getName()))\n\treturn _verifyObject(iface, candidate)\n\n\n", "def verifyObject(iface, candidate, tentative=False):\n\tifaces = [iface]\n\tif not tentative:\n\t\tif iface.extends:\n\t\t\tifaces.extend(iface.extends)\n\t\tfor iface in ifaces:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid(\"{} does not provide {}\".format(\n\t\t\t\t\tsafe_repr(candidate),\n\t\t\t\t\tiface.__identifier__,\n\t\t\t\t\t))\n\terrors = []\n\tfor iface in ifaces:\n\t\tfor name, descriptor in iface.namesAndDescriptions(1):\n\t\t\tif descriptor.getSignatureInfo():\n\t\t\t\ttry:\n\t\t\t\t\tdescriptor.bind(candidate)\n\t\t\t\texcept (AttributeError, TypeError) as e:\n\t\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\t\"{} is missing or has an incorrect signature ({})\"\n\t\t\t\t\t\t.format(name, e)\n\t\t\t\t\t\t))\n\t\t\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not iface.isOrExtends(candidate):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise Invalid(\n\t\t\t\"Class %s does not claim to provide %s\" % (\n\t\t\t\tclassName(candidate), iface.getName()\n\t\t\t\t),\n\t\t\tcandidate\n\t\t\t)\n\treturn verifyClass(iface, candidate)\n\n\n", "def verifyObject(iface, candidate, tentative=False):\n\tif isinstance(iface, InterfaceClass):\n\t\treturn _verifyObject(iface, candidate, tentative)\n\telif isinstance(iface, tuple):\n\t\t# This is a spec.\n\t\tfor i in iface:\n\t\t\tverifyObject(i, candidate, tentative)\n\telse:\n\t\traise TypeError('Expected interface or tuple of interfaces, got %r' %\n\t\t\tiface)\n\n\n", "def verifyObject(iface, candidate, tentative=False):\n\t\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid('%s does not provide %s' % (candidate, iface.__name__))\n\t\n\terrors = []\n\t\n\t# Check methods\n\tfor name, method in iface.namesAndDescriptions(1):\n\t\ttry:\n\t\t\tattrs = getattrs(candidate, name)\n\t\texcept TypeError:\n\t\t\traise Invalid('%s does not have a __getattribute__ method' % candidate)\n\t\t\n\t\tif not attrs:\n\t\t\terrors.append(Invalid('%s does not have a method %s' % (candidate, name)))\n\t\t\tcontinue\n\t\t\n\t\t# We're going to assume that the first attribute is the one we want.\n\t\tattr = attrs[0]\n\t\t\n\t\tif not callable(attr):\n\t\t\terrors.append(Invalid('%s.%s is not callable' % (candidate, name)))\n\t\t\tcontinue\n\t\t\n\t\tif method.positional:\n", "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t)\n\n\tinvalid = []\n\tfor method_name in iface.names(all=True):\n\t\tmethod = getattr(candidate, method_name, None)\n\t\tif method is None:\n\t\t\tinvalid.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"%r does not implement the '%s' method\" % (\n\t\t\t\t\t\tcandidate, method_name\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t)\n\t\t\tcontinue\n\t\tspec = iface[method_name]\n\t\tif not isinstance(spec, MethodTypes):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tvalidateInvariants(\n\t\t\t\tiface, candidate, method_name, method, spec.getTaggedValue(\"invariants\"),\n\t\t\t)\n\t\texcept Invalid as e:\n\n", "def verifyObject(iface, candidate, tentative=False):\n\ttry:\n\t\tif not tentative and not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t'%r does not claim to provide %r' % (candidate, iface))\n\texcept TypeError as e:\n\t\traise Invalid(\n\t\t\t'%r does not claim to provide %r' % (candidate, iface))\n\n\terrors = []\n\tfor name, d in iface.namesAndDescriptions(all=True):\n\t\tif d.getName() in ('__conform__', '__provides__'):\n\t\t\tcontinue\n\n\t\tif isinstance(d, Method):\n\t\t\ttry:\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\terrors.append(Invalid('%r does not provide %s'\n\t\t\t\t\t\t\t\t\t  % (candidate, name)))\n\t\t\t\tcontinue\n\n\t\t\tif not callable(method):\n\t\t\t\terrors.append(Invalid('%r is not callable' % (method\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\terrors = []\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\terrors.append(Invalid(\n\t\t\t\t'Object %r does not provide %r '\n\t\t\t\t'(but does provide %s)' % (\n\t\t\t\t\tcandidate, iface, [\n\t\t\t\t\t\tprovided.__name__ for provided\n\t\t\t\t\t\tin providedBy(candidate)\n\t\t\t\t\t])))\n\tfor method_name in iface:\n\t\tmethod = getattr(candidate, method_name, None)\n\t\tif method is None:\n\t\t\terrors.append(Invalid(\n\t\t\t\t'Object %r does not provide %s' % (\n\t\t\t\t\tcandidate, method_name)))\n\t\t\tcontinue\n\n\t\tmethod_iface = iface[method_name]\n\t\tif method_iface.getTaggedValue('_callable'):\n\t\t\ttry:\n\t\t\t\tverifyObject(method_iface, method, tentative)\n\t\t\texcept Invalid\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not provide %s\" % (candidate, iface.getName()))\n\terrors = []\n\tfor name, d in getMembers(iface, getter=True).items():\n\t\ttry:\n\t\t\td.bind(candidate)\n\t\texcept Exception as e:\n\t\t\terrors.append(e)\n\tif errors:\n\t\tif len(errors) == 1:\n\t\t\traise errors[0]\n\t\telse:\n\t\t\traise Invalid(\"\\n\".join([str(e) for e in errors]))\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t\t)\n\n\terrors = []\n\n\t# check methods\n\tfor name, method in iface.namesAndDescriptions(False):\n\t\tif isinstance(method, Method) and method.getSignatureInfo():\n\t\t\ttry:\n\t\t\t\tm = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\terrors.append(\n\t\t\t\t\t\"%r does not define %s\" % (candidate, name)\n\t\t\t\t)\n\t\t\telse:\n\t\t\t\tif hasattr(m, 'im_func'):\n\t\t\t\t\tm = m.im_func\n\t\t\t\tif isinstance(m, FunctionType):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tverifyClass(m, method)\n\t\t\t\t\texcept Invalid as e:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif isinstance(iface, interface.InterfaceClass):\n\t\tif not iface.isOrExtends(IComponent):\n\t\t\traise ValueError('%r is not a valid IComponent interface' % (iface,))\n\telse:\n\t\tiface = interface.providedBy(iface)\n\t\tif not iface.isOrExtends(IComponent):\n\t\t\traise ValueError('%r does not implement a valid IComponent interface' % (iface,))\n\tif not tentative:\n\t\tif not iface.providedBy(candidate):\n\t\t\traise Invalid('%r does not provide %s' % (candidate, iface.getName()))\n\treturn _verifyObject(iface, candidate)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tifaces = [iface]\n\tif not tentative:\n\t\tif iface.extends:\n\t\t\tifaces.extend(iface.extends)\n\t\tfor iface in ifaces:\n\t\t\tif not iface.providedBy(candidate):\n\t\t\t\traise Invalid(\"{} does not provide {}\".format(\n\t\t\t\t\tsafe_repr(candidate),\n\t\t\t\t\tiface.__identifier__,\n\t\t\t\t\t))\n\terrors = []\n\tfor iface in ifaces:\n\t\tfor name, descriptor in iface.namesAndDescriptions(1):\n\t\t\tif descriptor.getSignatureInfo():\n\t\t\t\ttry:\n\t\t\t\t\tdescriptor.bind(candidate)\n\t\t\t\texcept (AttributeError, TypeError) as e:\n\t\t\t\t\terrors.append(Invalid(\n\t\t\t\t\t\t\"{} is missing or has an incorrect signature ({})\"\n\t\t\t\t\t\t.format(name, e)\n\t\t\t\t\t\t))\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not iface.isOrExtends(candidate):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise Invalid(\n\t\t\t\"Class %s does not claim to provide %s\" % (\n\t\t\t\tclassName(candidate), iface.getName()\n\t\t\t\t),\n\t\t\tcandidate\n\t\t\t)\n\treturn verifyClass(iface, candidate)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif isinstance(iface, InterfaceClass):\n\t\treturn _verifyObject(iface, candidate, tentative)\n\telif isinstance(iface, tuple):\n\t\t# This is a spec.\n\t\tfor i in iface:\n\t\t\tverifyObject(i, candidate, tentative)\n\telse:\n\t\traise TypeError('Expected interface or tuple of interfaces, got %r' %\n\t\t\tiface)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\t\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid('%s does not provide %s' % (candidate, iface.__name__))\n\t\n\terrors = []\n\t\n\t# Check methods\n\tfor name, method in iface.namesAndDescriptions(1):\n\t\ttry:\n\t\t\tattrs = getattrs(candidate, name)\n\t\texcept TypeError:\n\t\t\traise Invalid('%s does not have a __getattribute__ method' % candidate)\n\t\t\n\t\tif not attrs:\n\t\t\terrors.append(Invalid('%s does not have a method %s' % (candidate, name)))\n\t\t\tcontinue\n\t\t\n\t\t# We're going to assume that the first attribute is the one we want.\n\t\tattr = attrs[0]\n\t\t\n\t\tif not callable(attr):\n\t\t\terrors.append(Invalid('%s.%s is not callable' % (candidate, name)))\n\t\t\tcontinue\n\t\t\n\t\tif method.positional:\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\tif not tentative and not iface.providedBy(candidate):\n\t\traise Invalid(\n\t\t\t\"%r does not implement %s\" % (candidate, iface.__identifier__)\n\t\t)\n\n\tinvalid = []\n\tfor method_name in iface.names(all=True):\n\t\tmethod = getattr(candidate, method_name, None)\n\t\tif method is None:\n\t\t\tinvalid.append(\n\t\t\t\tInvalid(\n\t\t\t\t\t\"%r does not implement the '%s' method\" % (\n\t\t\t\t\t\tcandidate, method_name\n\t\t\t\t\t)\n\t\t\t\t)\n\t\t\t)\n\t\t\tcontinue\n\t\tspec = iface[method_name]\n\t\tif not isinstance(spec, MethodTypes):\n\t\t\tcontinue\n\t\ttry:\n\t\t\tvalidateInvariants(\n\t\t\t\tiface, candidate, method_name, method, spec.getTaggedValue(\"invariants\"),\n\t\t\t)\n\t\texcept Invalid as e:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyObject(iface, candidate, tentative=False):\n\ttry:\n\t\tif not tentative and not iface.providedBy(candidate):\n\t\t\traise Invalid(\n\t\t\t\t'%r does not claim to provide %r' % (candidate, iface))\n\texcept TypeError as e:\n\t\traise Invalid(\n\t\t\t'%r does not claim to provide %r' % (candidate, iface))\n\n\terrors = []\n\tfor name, d in iface.namesAndDescriptions(all=True):\n\t\tif d.getName() in ('__conform__', '__provides__'):\n\t\t\tcontinue\n\n\t\tif isinstance(d, Method):\n\t\t\ttry:\n\t\t\t\tmethod = getattr(candidate, name)\n\t\t\texcept AttributeError:\n\t\t\t\terrors.append(Invalid('%r does not provide %s'\n\t\t\t\t\t\t\t\t\t  % (candidate, name)))\n\t\t\t\tcontinue\n\n\t\t\tif not callable(method):\n\t\t\t\terrors.append(Invalid('%r is not callable' % (method\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/verify.py", "project": "pexip/os-zope", "name": "verifyClass", "docstring": "Verify that the *candidate* might correctly provide *iface*.", "ques_id": "62b8b4c1eb7e40a82d2d1139", "code": ["def verifyClass(iface, candidate, tentative=False):\n\tif not issubclass(candidate, iface):\n\t\traise TypeError(\"%s is not a subclass of %s\" % (repr(candidate), repr(iface)))\n\tif not tentative:\n\t\tverifyImplements(candidate)\n\treturn candidate\n\n", "def verifyClass(iface, candidate, tentative=False):\n\t# FIXME: we need to check that the candidate is a subclass of the\n\t# interface class (or otherwise provides the interface).\n\treturn True\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, _InterfaceClass):\n\t\tiface = getInterface(iface)\n\tn=iface.__name__\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"%s is not a class.\"%candidate)\n\tif not issubclass(candidate, iface):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise TypeError(\"%s is not a subclass of %s.\"%(candidate, iface))\n\tif not iface.implementedBy(candidate):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise TypeError(\"%s does not implement %s.\"%(candidate, iface))\n\treturn True\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not issubclass(candidate, object):\n\t\tif not tentative:\n\t\t\traise TypeError(\"%r is not a new-style class\" % (candidate,))\n\t\treturn False\n\n\tif iface in candidate.__bases__:\n\t\treturn True\n\n\tfor base in candidate.__bases__:\n\t\tif verifyClass(iface, base, tentative):\n\t\t\treturn True\n\n\tif not tentative:\n\t\traise TypeError(\"%r does not implement %r\" % (candidate, iface))\n\treturn False\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif tentative:\n\t\treturn True\n\n\treturn verifyObject(iface, candidate)\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, type):\n\t\traise TypeError(\"iface must be a class\")\n\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"candidate must be a class\")\n\n\tif iface is candidate:\n\t\treturn True\n\n\tif candidate.__name__ == '<lambda>':\n\t\traise TypeError(\"candidate is a lambda, which cannot be verified\")\n\n\tif iface is object:\n\t\treturn True\n\n\tif candidate is object:\n\t\treturn False\n\n\tif not issubclass(candidate, iface):\n\t\treturn False\n\n\tif not tentative:\n\t\t# Verify that all required attributes are present\n\t\tfor name in iface.__dict__:\n\t\t\tifaceValue = iface.__dict__[name]\n\t\t\tcandidateValue = candidate.__dict__.get(name)\n\n\t\t\tif ifaceValue is None:\n\t\t\t\tcontinue\n\n\t\t\tif candidateValue is None:\n\t\t\t\traise TypeError(\"'%s' not provided in '%s'\" %\n\t\t\t\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not isInterface(iface):\n\t\traise TypeError(\"iface must be an Interface type\")\n\tiface = iface.__providedBy__\n\tif not iface.providedBy(candidate):\n\t\tif tentative:\n\t\t\treturn False\n\t\telse:\n\t\t\traise TypeError(\n\t\t\t\t\"Class %s does not provide the interface %s\" %\n\t\t\t\t(candidate.__name__, iface.__name__)\n\t\t\t)\n\treturn True\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif isinstance(iface, Interface):\n\t\tiface = iface.__provides__\n\n\tif isinstance(candidate, Interface):\n\t\tcandidate = candidate.__provides__\n\n\tif isinstance(candidate, ClassType):\n\t\treturn issubclass(candidate, iface)\n\telse:\n\t\treturn False\n\n\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, InterfaceClass):\n\t\traise TypeError(\"First argument must be a zope Interface\", iface)\n\tif not isinstance(candidate, (type, ClassType, TypeType)):\n\t\traise TypeError(\"Second argument must be a class.\", candidate)\n\tif not issubclass(candidate, InterfaceClass):\n\t\traise TypeError(\n\t\t\t\"Candidate %r doesn't subclass InterfaceClass.\" % candidate)\n\tif iface is candidate:\n\t\treturn True\n\tif candidate.__name__ == 'Implements':\n\t\treturn True\n\tif iface.__name__ == 'Interface':\n\t\treturn True\n\tif not tentative:\n\t\tif iface.__name__ == 'Implements':\n\t\t\traise TypeError(\n\t\t\t\t\"Class %r is not an interface class.\" % candidate)\n\t\tif candidate.__name__ == 'Interface':\n\t\t\traise TypeError(\n\t\t\t\t\"Class %r is not an interface class.\" % candidate)\n\tfor base in candidate.__bases__:\n\t\n", "def verifyClass(iface, candidate, tentative=False):\n\tif not isInterface(iface):\n\t\traise TypeError(\"not a interface\", iface)\n\tif not isClass(candidate):\n\t\traise TypeError(\"not a class\", candidate)\n\tif candidate.__dict__.get(\"__implements__\", None) is iface:\n\t\treturn True\n\tif tentative:\n\t\tfor base in candidate.__bases__:\n\t\t\tif verifyClass(iface, base, tentative=tentative):\n\t\t\t\treturn True\n\treturn False\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not issubclass(candidate, iface):\n\t\traise TypeError(\"%s is not a subclass of %s\" % (repr(candidate), repr(iface)))\n\tif not tentative:\n\t\tverifyImplements(candidate)\n\treturn candidate\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\t# FIXME: we need to check that the candidate is a subclass of the\n\t# interface class (or otherwise provides the interface).\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, _InterfaceClass):\n\t\tiface = getInterface(iface)\n\tn=iface.__name__\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"%s is not a class.\"%candidate)\n\tif not issubclass(candidate, iface):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise TypeError(\"%s is not a subclass of %s.\"%(candidate, iface))\n\tif not iface.implementedBy(candidate):\n\t\tif tentative:\n\t\t\treturn False\n\t\traise TypeError(\"%s does not implement %s.\"%(candidate, iface))\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not issubclass(candidate, object):\n\t\tif not tentative:\n\t\t\traise TypeError(\"%r is not a new-style class\" % (candidate,))\n\t\treturn False\n\n\tif iface in candidate.__bases__:\n\t\treturn True\n\n\tfor base in candidate.__bases__:\n\t\tif verifyClass(iface, base, tentative):\n\t\t\treturn True\n\n\tif not tentative:\n\t\traise TypeError(\"%r does not implement %r\" % (candidate, iface))\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif tentative:\n\t\treturn True\n\n\treturn verifyObject(iface, candidate)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, type):\n\t\traise TypeError(\"iface must be a class\")\n\n\tif not isinstance(candidate, type):\n\t\traise TypeError(\"candidate must be a class\")\n\n\tif iface is candidate:\n\t\treturn True\n\n\tif candidate.__name__ == '<lambda>':\n\t\traise TypeError(\"candidate is a lambda, which cannot be verified\")\n\n\tif iface is object:\n\t\treturn True\n\n\tif candidate is object:\n\t\treturn False\n\n\tif not issubclass(candidate, iface):\n\t\treturn False\n\n\tif not tentative:\n\t\t# Verify that all required attributes are present\n\t\tfor name in iface.__dict__:\n\t\t\tifaceValue = iface.__dict__[name]\n\t\t\tcandidateValue = candidate.__dict__.get(name)\n\n\t\t\tif ifaceValue is None:\n\t\t\t\tcontinue\n\n\t\t\tif candidateValue is None:\n\t\t\t\traise TypeError(\"'%s' not provided in '%s'\" %\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not isInterface(iface):\n\t\traise TypeError(\"iface must be an Interface type\")\n\tiface = iface.__providedBy__\n\tif not iface.providedBy(candidate):\n\t\tif tentative:\n\t\t\treturn False\n\t\telse:\n\t\t\traise TypeError(\n\t\t\t\t\"Class %s does not provide the interface %s\" %\n\t\t\t\t(candidate.__name__, iface.__name__)\n\t\t\t)\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif isinstance(iface, Interface):\n\t\tiface = iface.__provides__\n\n\tif isinstance(candidate, Interface):\n\t\tcandidate = candidate.__provides__\n\n\tif isinstance(candidate, ClassType):\n\t\treturn issubclass(candidate, iface)\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not isinstance(iface, InterfaceClass):\n\t\traise TypeError(\"First argument must be a zope Interface\", iface)\n\tif not isinstance(candidate, (type, ClassType, TypeType)):\n\t\traise TypeError(\"Second argument must be a class.\", candidate)\n\tif not issubclass(candidate, InterfaceClass):\n\t\traise TypeError(\n\t\t\t\"Candidate %r doesn't subclass InterfaceClass.\" % candidate)\n\tif iface is candidate:\n\t\treturn True\n\tif candidate.__name__ == 'Implements':\n\t\treturn True\n\tif iface.__name__ == 'Interface':\n\t\treturn True\n\tif not tentative:\n\t\tif iface.__name__ == 'Implements':\n\t\t\traise TypeError(\n\t\t\t\t\"Class %r is not an interface class.\" % candidate)\n\t\tif candidate.__name__ == 'Interface':\n\t\t\traise TypeError(\n\t\t\t\t\"Class %r is not an interface class.\" % candidate)\n\tfor base in candidate.__bases__:\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verifyClass(iface, candidate, tentative=False):\n\tif not isInterface(iface):\n\t\traise TypeError(\"not a interface\", iface)\n\tif not isClass(candidate):\n\t\traise TypeError(\"not a class\", candidate)\n\tif candidate.__dict__.get(\"__implements__\", None) is iface:\n\t\treturn True\n\tif tentative:\n\t\tfor base in candidate.__bases__:\n\t\t\tif verifyClass(iface, base, tentative=tentative):\n\t\t\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "src/zope/interface/advice.py", "project": "pexip/os-zope", "name": "determineMetaclass", "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__", "ques_id": "62b8b559eb7e40a82d2d11f6", "code": ["def determineMetaclass(bases, explicit_mc=None):\n\texplicit_mc = explicit_mc or None\n\tmetaclass = None\n\tif explicit_mc is not None:\n\t\tmetaclass = explicit_mc\n\telse:\n\t\tbases = [b for b in bases if b is not object]\n\t\tif bases:\n\t\t\tmetaclass = type(bases[0])\n\t\telse:\n\t\t\tmetaclass = type\n\treturn metaclass\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t# Initialize the metaclass to the explicit metaclass, if one was given\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\n\t# If the class has no bases, use the default metaclass\n\tif len(bases) == 0:\n\t\treturn type\n\n\t# If the bases are all of the same metaclass, use that metaclass\n\tmcs = set(type(b) for b in bases)\n\tif len(mcs) == 1:\n\t\treturn mcs.pop()\n\n\t# If the bases are all of the same type, use that type\n\ttypes = set(b for b in bases if isinstance(b, type))\n\tif len(types) == 1:\n\t\treturn types.pop()\n\n\t# If the bases are all of the same metaclass, use that metaclass\n\tmetaclasses = set(type(b) for b in bases if isinstance(b, type))\n\tif len(metaclasses) == 1:\n\t\treturn metaclasses.pop()\n\n\t# If the bases are all of the same metaclass\n", "def determineMetaclass(bases, explicit_mc=None):\n\tmetaclass = explicit_mc\n\tif not metaclass:\n\t\t# Explicitly requested metaclass, use it.\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\tmetaclass = base.__metaclass__\n\t\t\t\tbreak\n\n\tif not metaclass:\n\t\t# No explicit metaclass, use default.\n\t\tmetaclass = type\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__class__') and base.__class__ is not type:\n\t\t\t\tmetaclass = base.__class__\n\t\t\t\tbreak\n\n\t\tif metaclass is type:\n\t\t\t# Still no metaclass, use default.\n\t\t\tfor base in bases:\n\t\t\t\tif hasattr(base, '__class__'):\n\t\t\t\t\tmetaclass = base.__class__\n\t\t\t\t\tbreak\n\n\treturn metaclass\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\treturn explicit_mc\n\telif len(bases) == 0:\n\t\treturn type\n\telse:\n\t\t# Get the metaclass of the first base class\n\t\tmetaclass = type(bases[0])\n\t\t# If any of the bases are a subclass of the first one, we have\n\t\t# to use the most derived meta-class among them, so walk all\n\t\t# bases and see if any is a subclass of the first one.\n\t\tfor t in bases[1:]:\n\t\t\tif issubclass(type(t), metaclass):\n\t\t\t\tmetaclass = type(t)\n\t\treturn metaclass\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tmeta = explicit_mc\n\tif not meta:\n\t\t# If not explicitly set, check for __metaclass__ in bases\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\tmeta = base.__metaclass__\n\t\t\t\tbreak\n\t\telse:\n\t\t\t# If none of the bases have __metaclass__, use type\n\t\t\tmeta = type\n\treturn meta\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\tif len(bases) == 1:\n\t\treturn type(bases[0])\n\telif len(bases) == 0:\n\t\treturn type\n\telse:\n\t\tmetaclasses = {type(b) for b in bases}\n\t\tif len(metaclasses) > 1:\n\t\t\traise TypeError(\"Incompatible metaclasses %s\" % metaclasses)\n\t\treturn metaclasses.pop()\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\tbases = [base for base in bases if isinstance(base, ClassType)]\n\tif not bases:\n\t\t# NOTE: 'type' builtin returns None.\n\t\treturn None\n\telif len(bases) == 1:\n\t\treturn bases[0].__metaclass__\n\telse:\n\t\t# multiple inheritance\n\t\tmetaclasses = set(type(base) for base in bases)\n\t\tmetaclasses_type = type(metaclasses)\n\t\tif metaclasses_type is set:\n\t\t\treturn metaclasses.pop()\n\t\telse:\n\t\t\tassert metaclasses_type is frozenset, metaclasses_type\n\t\t\tmetaclasses = list(metaclasses)\n\t\t\tassert len(metaclasses) == 1\n\t\t\treturn metaclasses[0]\n\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\t# special case: __metaclass__ is None or type\n\tif explicit_mc is None or explicit_mc is type:\n\t\treturn type\n\t# special case: __metaclass__ is a type\n\tif isinstance(explicit_mc, type):\n\t\treturn explicit_mc\n\t# special case: __metaclass__ is a tuple of types\n\tif isinstance(explicit_mc, tuple):\n\t\treturn explicit_mc[0]\n\t# special case: __metaclass__ is a string\n\tif isinstance(explicit_mc, basestring):\n\t\treturn globals()[explicit_mc]\n\t# special case: __metaclass__ is a callable\n\tif callable(explicit_mc):\n\t\treturn explicit_mc\n\t# special case: __metaclass__ is a class\n\tif inspect.isclass(explicit_mc):\n\t\treturn explicit_mc\n\t# special case: __metaclass__ is a class method\n\tif isinstance(explicit_mc, (classmethod, staticmethod)):\n\t\treturn explicit_mc\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\telse:\n\t\t# check for __metaclass__ in bases\n\t\tmetaclasses = set(type(b) for b in bases)\n\t\tfor m in metaclasses:\n\t\t\tif issubclass(m, type):\n\t\t\t\tmetaclasses.remove(m)\n\t\tassert len(metaclasses) <= 1, \"More than one metaclass: %s\" % metaclasses\n\t\tmc = metaclasses.pop() if metaclasses else type\n\n\treturn mc\n\n\n", "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\treturn explicit_mc\n\tif len(bases) == 0:\n\t\treturn type\n\tif len(bases) == 1:\n\t\treturn bases[0].__class__\n\tbases = list(set(bases))\n\tmcs = [b.__class__ for b in bases]\n\tif not all(mcs[0] is mc for mc in mcs):\n\t\traise TypeError(\"Incompatible root metaclasses\")\n\treturn mcs[0]\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\texplicit_mc = explicit_mc or None\n\tmetaclass = None\n\tif explicit_mc is not None:\n\t\tmetaclass = explicit_mc\n\telse:\n\t\tbases = [b for b in bases if b is not object]\n\t\tif bases:\n\t\t\tmetaclass = type(bases[0])\n\t\telse:\n\t\t\tmetaclass = type\n\treturn metaclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t# Initialize the metaclass to the explicit metaclass, if one was given\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\n\t# If the class has no bases, use the default metaclass\n\tif len(bases) == 0:\n\t\treturn type\n\n\t# If the bases are all of the same metaclass, use that metaclass\n\tmcs = set(type(b) for b in bases)\n\tif len(mcs) == 1:\n\t\treturn mcs.pop()\n\n\t# If the bases are all of the same type, use that type\n\ttypes = set(b for b in bases if isinstance(b, type))\n\tif len(types) == 1:\n\t\treturn types.pop()\n\n\t# If the bases are all of the same metaclass, use that metaclass\n\tmetaclasses = set(type(b) for b in bases if isinstance(b, type))\n\tif len(metaclasses) == 1:\n\t\treturn metaclasses.pop()\n\n\t# If the bases are all of the same metaclass\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tmetaclass = explicit_mc\n\tif not metaclass:\n\t\t# Explicitly requested metaclass, use it.\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\tmetaclass = base.__metaclass__\n\t\t\t\tbreak\n\n\tif not metaclass:\n\t\t# No explicit metaclass, use default.\n\t\tmetaclass = type\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__class__') and base.__class__ is not type:\n\t\t\t\tmetaclass = base.__class__\n\t\t\t\tbreak\n\n\t\tif metaclass is type:\n\t\t\t# Still no metaclass, use default.\n\t\t\tfor base in bases:\n\t\t\t\tif hasattr(base, '__class__'):\n\t\t\t\t\tmetaclass = base.__class__\n\t\t\t\t\tbreak\n\n\treturn metaclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\treturn explicit_mc\n\telif len(bases) == 0:\n\t\treturn type\n\telse:\n\t\t# Get the metaclass of the first base class\n\t\tmetaclass = type(bases[0])\n\t\t# If any of the bases are a subclass of the first one, we have\n\t\t# to use the most derived meta-class among them, so walk all\n\t\t# bases and see if any is a subclass of the first one.\n\t\tfor t in bases[1:]:\n\t\t\tif issubclass(type(t), metaclass):\n\t\t\t\tmetaclass = type(t)\n\t\treturn metaclass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tmeta = explicit_mc\n\tif not meta:\n\t\t# If not explicitly set, check for __metaclass__ in bases\n\t\tfor base in bases:\n\t\t\tif hasattr(base, '__metaclass__'):\n\t\t\t\tmeta = base.__metaclass__\n\t\t\t\tbreak\n\t\telse:\n\t\t\t# If none of the bases have __metaclass__, use type\n\t\t\tmeta = type\n\treturn meta\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\tif len(bases) == 1:\n\t\treturn type(bases[0])\n\telif len(bases) == 0:\n\t\treturn type\n\telse:\n\t\tmetaclasses = {type(b) for b in bases}\n\t\tif len(metaclasses) > 1:\n\t\t\traise TypeError(\"Incompatible metaclasses %s\" % metaclasses)\n\t\treturn metaclasses.pop()\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\treturn explicit_mc\n\tbases = [base for base in bases if isinstance(base, ClassType)]\n\tif not bases:\n\t\t# NOTE: 'type' builtin returns None.\n\t\treturn None\n\telif len(bases) == 1:\n\t\treturn bases[0].__metaclass__\n\telse:\n\t\t# multiple inheritance\n\t\tmetaclasses = set(type(base) for base in bases)\n\t\tmetaclasses_type = type(metaclasses)\n\t\tif metaclasses_type is set:\n\t\t\treturn metaclasses.pop()\n\t\telse:\n\t\t\tassert metaclasses_type is frozenset, metaclasses_type\n\t\t\tmetaclasses = list(metaclasses)\n\t\t\tassert len(metaclasses) == 1\n\t\t\treturn metaclasses[0]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\t# special case: __metaclass__ is None or type\n\tif explicit_mc is None or explicit_mc is type:\n\t\treturn type\n\t# special case: __metaclass__ is a type\n\tif isinstance(explicit_mc, type):\n\t\treturn explicit_mc\n\t# special case: __metaclass__ is a tuple of types\n\tif isinstance(explicit_mc, tuple):\n\t\treturn explicit_mc[0]\n\t# special case: __metaclass__ is a string\n\tif isinstance(explicit_mc, basestring):\n\t\treturn globals()[explicit_mc]\n\t# special case: __metaclass__ is a callable\n\tif callable(explicit_mc):\n\t\treturn explicit_mc\n\t# special case: __metaclass__ is a class\n\tif inspect.isclass(explicit_mc):\n\t\treturn explicit_mc\n\t# special case: __metaclass__ is a class method\n\tif isinstance(explicit_mc, (classmethod, staticmethod)):\n\t\treturn explicit_mc\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc is not None:\n\t\tmc = explicit_mc\n\telse:\n\t\t# check for __metaclass__ in bases\n\t\tmetaclasses = set(type(b) for b in bases)\n\t\tfor m in metaclasses:\n\t\t\tif issubclass(m, type):\n\t\t\t\tmetaclasses.remove(m)\n\t\tassert len(metaclasses) <= 1, \"More than one metaclass: %s\" % metaclasses\n\t\tmc = metaclasses.pop() if metaclasses else type\n\n\treturn mc\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def determineMetaclass(bases, explicit_mc=None):\n\tif explicit_mc:\n\t\treturn explicit_mc\n\tif len(bases) == 0:\n\t\treturn type\n\tif len(bases) == 1:\n\t\treturn bases[0].__class__\n\tbases = list(set(bases))\n\tmcs = [b.__class__ for b in bases]\n\tif not all(mcs[0] is mc for mc in mcs):\n\t\traise TypeError(\"Incompatible root metaclasses\")\n\treturn mcs[0]\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/cache.py", "project": "pexip/os-python-cachetools", "name": "pop", "docstring": "D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\nIf key is not found, d is returned if given, otherwise KeyError is raised.", "ques_id": "62b8d22a48ba5a41d1c3f47d", "code": ["def pop(self, key, default=__marker):\n\tpass\n\t\n", "def pop(self, key, default=__marker):\n\tpass\n\t\n", "def pop(self, key, default=__marker):\n\ttry:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn value\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\n", "def pop(self, key, default=__marker):\n\tdef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n", "def pop(self, key, default=__marker):\n\tpass\n\t\n", "def pop(self, key, default=__marker):\n\tpass\n\t\n", "def pop(self, key, default=__marker):\n\tpass\n\n", "def pop(self, key, default=__marker):\n\ttry:\n\t\tvalue = dict.pop(self, key)\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\treturn value\n\n", "def pop(self, key, default=__marker):\n\tif default is self.__marker:\n\t\treturn self.data.pop(key)\n\treturn self.data.pop(key, default)\n\n", "def pop(self, key, default=__marker):\n\treturn self.__getitem__(key, default, True)\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\ttry:\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn value\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tdef popitem(self):\n\t\"\"\"\n\tD.popitem() -> (k, v), remove and return some (key, value) pair as a\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\ttry:\n\t\tvalue = dict.pop(self, key)\n\texcept KeyError:\n\t\tif default is self.__marker:\n\t\t\traise\n\t\treturn default\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\tif default is self.__marker:\n\t\treturn self.data.pop(key)\n\treturn self.data.pop(key, default)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def pop(self, key, default=__marker):\n\treturn self.__getitem__(key, default, True)\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lfu.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least frequently used.", "ques_id": "62b8d23748ba5a41d1c3f497", "code": ["def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\t_, key = self._removeLRU()\n\treturn (key, self._pop(key))\n\n", "def popitem(self):\n\tif self._cache:\n\t\treturn self._cache.popitem(last=False)\n\tif self._len:\n\t\tkey, value = self._mapping.popitem(last=False)\n\t\tself._len -= 1\n\t\treturn key, value\n\traise KeyError('dictionary is empty')\n\n\n", "def popitem(self):\n\ttry:\n\t\tkey = self.__map.popitem()[0]\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\telse:\n\t\tvalue = self.__cache.pop(key)\n\t\tself.__size -= 1\n\t\treturn (key, value)\n\n\n\tdef __getstate__(self):\n\t\treturn dict(self.__cache)\n\n\n\tdef __setstate__(self, state):\n\t\tself.__init__(state)\n\n\n\tdef __repr__(self):\n\t\treturn '%s(%r)' % (self.__class__.__name__, self.__cache)\n\n\n\tdef __len__(self):\n\t\treturn self.__size\n\n\n\tdef __contains__(self, key):\n\t\treturn key in self.__cache\n\n\n\tdef __getitem__(self, key):\n\t\tvalue = self.__cache[key]\n\t\tself.__map[key] = self.__counter[key] = self.__counter[key] + 1\n\t\treturn value\n", "def popitem(self):\n\traise NotImplementedError()\n\n", "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\n\tkey, _ = self._remove_node(self.head.next)\n\treturn key, self.__dict__.pop(key)\n\n\n", "def popitem(self):\n\tif self:\n\t\tkey, value = self.__root[1]\n\t\tself.__root = [self.__root, self.__root, None, None]\n\t\tself.__map.pop(key, None)\n\t\treturn key, value\n\telse:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "def popitem(self):\n\tif self.__root.left is self.__root or self.__root.right is self.__root:\n\t\traise KeyError('empty frequency map')\n\tlink = self.__root.left\n\tlink_prev = link.prev\n\tlink_next = link.next\n\tlink_prev.next = link_next\n\tlink_next.prev = link_prev\n\tlink.prev = None\n\tlink.next = None\n\treturn (link.key, self.__map.pop(link.key)[1])\n\n", "def popitem(self):\n\tif self.cache:\n\t\tkey, _ = self.heap[0]\n\t\treturn key, self.pop(key)\n\n\n", "def popitem(self):\n\tif self.root is None:\n\t\traise KeyError('dictionary is empty')\n\n\t# remove root\n\tnode = self.root\n\tif node.left is None or node.right is None:\n\t\tself.root = node.left or node.right\n\t\tif self.root is not None:\n\t\t\tself.root.parent = None\n\telse:\n\t\ts = node.next\n\t\tself.root = s\n\t\tself.unlink(s)\n\n\t# decrement count and re-balance tree\n\tself.count -= 1\n\tself.__update(node.parent)\n\n\treturn (node.key, node.value)\n\n\n", "def popitem(self):\n\tif self.maxsize is None:\n\t\traise RuntimeError('Cannot use popitem() on a fixed-size cache.')\n\tif not self:\n\t\traise KeyError('cache is empty')\n\n\tkey = next(iter(self._mapping))\n\tvalue = self.pop(key)\n\treturn key, value\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\t_, key = self._removeLRU()\n\treturn (key, self._pop(key))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self._cache:\n\t\treturn self._cache.popitem(last=False)\n\tif self._len:\n\t\tkey, value = self._mapping.popitem(last=False)\n\t\tself._len -= 1\n\t\treturn key, value\n\traise KeyError('dictionary is empty')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = self.__map.popitem()[0]\n\texcept KeyError:\n\t\traise KeyError('dictionary is empty')\n\telse:\n\t\tvalue = self.__cache.pop(key)\n\t\tself.__size -= 1\n\t\treturn (key, value)\n\n\n\tdef __getstate__(self):\n\t\treturn dict(self.__cache)\n\n\n\tdef __setstate__(self, state):\n\t\tself.__init__(state)\n\n\n\tdef __repr__(self):\n\t\treturn '%s(%r)' % (self.__class__.__name__, self.__cache)\n\n\n\tdef __len__(self):\n\t\treturn self.__size\n\n\n\tdef __contains__(self, key):\n\t\treturn key in self.__cache\n\n\n\tdef __getitem__(self, key):\n\t\tvalue = self.__cache[key]\n\t\tself.__map[key] = self.__counter[key] = self.__counter[key] + 1\n\t\treturn value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\traise NotImplementedError()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\n\tkey, _ = self._remove_node(self.head.next)\n\treturn key, self.__dict__.pop(key)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey, value = self.__root[1]\n\t\tself.__root = [self.__root, self.__root, None, None]\n\t\tself.__map.pop(key, None)\n\t\treturn key, value\n\telse:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.__root.left is self.__root or self.__root.right is self.__root:\n\t\traise KeyError('empty frequency map')\n\tlink = self.__root.left\n\tlink_prev = link.prev\n\tlink_next = link.next\n\tlink_prev.next = link_next\n\tlink_next.prev = link_prev\n\tlink.prev = None\n\tlink.next = None\n\treturn (link.key, self.__map.pop(link.key)[1])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.cache:\n\t\tkey, _ = self.heap[0]\n\t\treturn key, self.pop(key)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.root is None:\n\t\traise KeyError('dictionary is empty')\n\n\t# remove root\n\tnode = self.root\n\tif node.left is None or node.right is None:\n\t\tself.root = node.left or node.right\n\t\tif self.root is not None:\n\t\t\tself.root.parent = None\n\telse:\n\t\ts = node.next\n\t\tself.root = s\n\t\tself.unlink(s)\n\n\t# decrement count and re-balance tree\n\tself.count -= 1\n\tself.__update(node.parent)\n\n\treturn (node.key, node.value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.maxsize is None:\n\t\traise RuntimeError('Cannot use popitem() on a fixed-size cache.')\n\tif not self:\n\t\traise KeyError('cache is empty')\n\n\tkey = next(iter(self._mapping))\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/lru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair least recently used.", "ques_id": "62b8d23a48ba5a41d1c3f499", "code": ["def popitem(self):\n\tif self:\n\t\tkey, _, value = self.end[self.PREV]\n\t\tself.pop(key)\n\t\treturn key, value\n\telse:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\n\tdef __repr__(self):\n\t\tif not self:\n\t\t\treturn '%s()' % (self.__class__.__name__,)\n\t\treturn '%s(%r)' % (self.__class__.__name__, self.items())\n\n\tdef copy(self):\n\t\treturn self.__class__(self)\n\tdef __copy__(self):\n\t\treturn self.copy()\n\tdef __deepcopy__(self, memo):\n\t\timport copy\n\t\treturn self.__class__(copy.deepcopy(self.items(), memo))\n\n\tdef __reduce__(self):\n\t\treturn self.__class__, (self.items(),)\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, OrderedDict):\n\n", "def popitem(self):\n\treturn self.__map.popitem(last=False)\n\n", "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\n\tkey, value = self.end[PREV]\n\tself.__delitem__(key)\n\treturn (key, value)\n\n\n", "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tif self.lock:\n\t\traise RuntimeError('OrderedDict locked')\n\tkey, value = self.root.left.item\n\tself.__delitem__(key)\n\treturn key, value\n\n\n", "def popitem(self):\n\tif self:\n\t\tkey, _ = self.end[self.PREV]\n\t\tself.pop(key)\n\t\treturn key, self[key]\n\traise KeyError('dictionary is empty')\n\n\n", "def popitem(self):\n\tif self.__root is None:\n\t\traise KeyError('dictionary is empty')\n\t# sift down the root to the leftmost leaf.\n\tparent = None\n\tnode = self.__root\n\twhile node.left is not self.__end:\n\t\tparent = node\n\t\tnode = node.left\n\t# remove the leaf.\n\tif parent is None:\n\t\tself.__root = self.__end\n\telse:\n\t\tparent.left = self.__end\n\tk = node.key\n\tdel node\n\tself.__map.pop(k)\n\treturn (k, self.__map[k])\n\n\n", "def popitem(self):\n\tif self:\n\t\tkey = next(self.__map.__iter__())\n\t\treturn (key, self.pop(key))\n\telse:\n\t\traise KeyError('dictionary is empty')\n\n", "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\t(key, _) = self.end[PREV]\n\tself.__setitem__(key, self.__pop(self.end))\n\treturn (key, self.__map[key][VALUE])\n\n\n", "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\tif self.head is None:\n\t\traise AssertionError('head is None and len(%s) > 0' % self.__class__.__name__)\n\tk, v = self.head\n\tdel self[k]\n\treturn k, v\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\tif self:\n\t\tkey, _, value = self.end[self.PREV]\n\t\tself.pop(key)\n\t\treturn key, value\n\telse:\n\t\traise KeyError('%s is empty' % self.__class__.__name__)\n\n\tdef __repr__(self):\n\t\tif not self:\n\t\t\treturn '%s()' % (self.__class__.__name__,)\n\t\treturn '%s(%r)' % (self.__class__.__name__, self.items())\n\n\tdef copy(self):\n\t\treturn self.__class__(self)\n\tdef __copy__(self):\n\t\treturn self.copy()\n\tdef __deepcopy__(self, memo):\n\t\timport copy\n\t\treturn self.__class__(copy.deepcopy(self.items(), memo))\n\n\tdef __reduce__(self):\n\t\treturn self.__class__, (self.items(),)\n\n\tdef __eq__(self, other):\n\t\tif isinstance(other, OrderedDict):\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self.__map.popitem(last=False)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = next(iter(self))\n\texcept StopIteration:\n\t\traise KeyError('dictionary is empty')\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\n\tkey, value = self.end[PREV]\n\tself.__delitem__(key)\n\treturn (key, value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\tif self.lock:\n\t\traise RuntimeError('OrderedDict locked')\n\tkey, value = self.root.left.item\n\tself.__delitem__(key)\n\treturn key, value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey, _ = self.end[self.PREV]\n\t\tself.pop(key)\n\t\treturn key, self[key]\n\traise KeyError('dictionary is empty')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self.__root is None:\n\t\traise KeyError('dictionary is empty')\n\t# sift down the root to the leftmost leaf.\n\tparent = None\n\tnode = self.__root\n\twhile node.left is not self.__end:\n\t\tparent = node\n\t\tnode = node.left\n\t# remove the leaf.\n\tif parent is None:\n\t\tself.__root = self.__end\n\telse:\n\t\tparent.left = self.__end\n\tk = node.key\n\tdel node\n\tself.__map.pop(k)\n\treturn (k, self.__map[k])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif self:\n\t\tkey = next(self.__map.__iter__())\n\t\treturn (key, self.pop(key))\n\telse:\n\t\traise KeyError('dictionary is empty')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self:\n\t\traise KeyError('dictionary is empty')\n\t(key, _) = self.end[PREV]\n\tself.__setitem__(key, self.__pop(self.end))\n\treturn (key, self.__map[key][VALUE])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('dictionary is empty')\n\tif self.head is None:\n\t\traise AssertionError('head is None and len(%s) > 0' % self.__class__.__name__)\n\tk, v = self.head\n\tdel self[k]\n\treturn k, v\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/mru.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return the `(key, value)` pair most recently used.", "ques_id": "62b8d23c48ba5a41d1c3f49b", "code": ["def popitem(self):\n\treturn self.__map.popitem()\n\n\t@property\n\tdef size(self):\n\t\treturn self.__map.__sizeof__()\n\n\tdef __contains__(self, key):\n\t\treturn key in self.__map\n\n\tdef __getitem__(self, key):\n\t\treturn self.__map[key]\n\n\tdef __setitem__(self, key, value):\n\t\tself.__map[key] = value\n\n\tdef __delitem__(self, key):\n\t\tdel self.__map[key]\n\n\tdef __iter__(self):\n\t\treturn iter(self.__map)\n\n\tdef __sizeof__(self):\n\t\treturn self.__map.__sizeof__()\n\n\tdef __len__(self):\n\t\treturn len(self.__map)\n\n\tdef __str__(self):\n\t\treturn self.__map.__str__()\n\n\tdef __repr__(self):\n\t\treturn self.__map.__repr__()\n\n\n", "def popitem(self):\n\treturn self.__dict.popitem()\n\t\n", "def popitem(self):\n\ttry:\n\t\tkey = self.__end[self.__end_index]\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn (key, value)\n\texcept IndexError:\n\t\traise KeyError('dictionary is empty')\n\n\tdef __reduce__(self):\n\t\titems = [[k, self[k]] for k in self]\n\t\ttmp = self.__map, self.__start, self.__end, self.__end_index, self.__len\n\t\tdel self.__map, self.__start, self.__end, self.__end_index, self.__len\n\t\tinst_dict = vars(self).copy()\n\t\tself.__map, self.__start, self.__end, self.__end_index, self.__len = tmp\n\t\tif inst_dict:\n\t\t\treturn (self.__class__, (items,), inst_dict)\n\t\treturn self.__class__, (items,)\n\n\tdef keys(self):\n\t\treturn list(self)\n\n\tsetdefault\n", "def popitem(self):\n\t\n\tif self.__is_empty:\n\t\tif last_accessed_key == self.__default_key:\n\t\t\t# TODO: error message: no items in cache\n\t\t\traise KeyError('%s' % last_accessed_key)\n\t\telse:\n\t\t\t# TODO: error message: no items in cache\n\t\t\traise KeyError('%s' % self.__default_key)\n\t\t\t\n\telse:\n\t\t\n\t\tif last_accessed_key == self.__default_key:\n\t\t\tdel self.__default_key\n\t\t\tdel self.__default_value\n\t\t\t\n\t\t\tif len(self.__cache_dict) == 0:\n\t\t\t\tself.__is_empty = True\n\t\t\t\t\n\t\t\t\treturn (last_accessed_key, last_accessed_value)\n\t\t\telse:\n\t\t\t\tself.__default_key = self.__cache_dict.popitem()[0]\n\t\t\t\tself.__default_value = self\n", "def popitem(self):\n\tpass\n\n", "def popitem(self):\n\t(key, value) = self.__dict.popitem()\n\tself.__deque.remove(key)\n\treturn (key, value)\n\n", "def popitem(self):\n\ttry:\n\t\tkey = self.__container.popitem()[0]\n\t\tself.__priority[key] = None\n\t\treturn key\n\texcept KeyError:\n\t\traise KeyError('container is empty')\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\t\"Return a string representation of the cache.\n\t\t\"\"\"\n\t\treturn repr(self.__container)\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\t\"Return a string representation of the cache.\n\t\t\"\"\"\n\t\treturn repr(self.__container)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\t\"Return the length of the cache.\n\t\t\"\"\"\n\t\treturn len(self.__container)\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\t\"Iterate over the cache.\n\t\t\"\"\"\n\t\tfor i in self.__container:\n\t\t\tyield i\n\n\tdef __contains__(self, key):\n\t\t\"\"\"\n\t\t\"Check for the existence of a key in the cache.\n\t\t\"\"\"\n\t\treturn\n", "def popitem(self):\n\tself.__cache.popitem()\n\n", "def popitem(self):\n\tpass\n\n", "def popitem(self):\n\t\t\n\t\tif self.is_empty():\n\t\t\traise KeyError('Dictionary is empty.')\n\t\t\n\t\tkey = self.cache_list.last.key\n\t\tvalue = self.cache_list.last.value\n\t\t\n\t\tself.cache_list.delete_node(self.cache_list.last)\n\t\tdel self.cache_dict[key]\n\t\t\n\t\treturn key, value\n\t\t\n\tdef __repr__(self):\n\t\treturn repr(self.cache_dict)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\treturn self.__map.popitem()\n\n\t@property\n\tdef size(self):\n\t\treturn self.__map.__sizeof__()\n\n\tdef __contains__(self, key):\n\t\treturn key in self.__map\n\n\tdef __getitem__(self, key):\n\t\treturn self.__map[key]\n\n\tdef __setitem__(self, key, value):\n\t\tself.__map[key] = value\n\n\tdef __delitem__(self, key):\n\t\tdel self.__map[key]\n\n\tdef __iter__(self):\n\t\treturn iter(self.__map)\n\n\tdef __sizeof__(self):\n\t\treturn self.__map.__sizeof__()\n\n\tdef __len__(self):\n\t\treturn len(self.__map)\n\n\tdef __str__(self):\n\t\treturn self.__map.__str__()\n\n\tdef __repr__(self):\n\t\treturn self.__map.__repr__()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self.__dict.popitem()\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = self.__end[self.__end_index]\n\t\tvalue = self[key]\n\t\tdel self[key]\n\t\treturn (key, value)\n\texcept IndexError:\n\t\traise KeyError('dictionary is empty')\n\n\tdef __reduce__(self):\n\t\titems = [[k, self[k]] for k in self]\n\t\ttmp = self.__map, self.__start, self.__end, self.__end_index, self.__len\n\t\tdel self.__map, self.__start, self.__end, self.__end_index, self.__len\n\t\tinst_dict = vars(self).copy()\n\t\tself.__map, self.__start, self.__end, self.__end_index, self.__len = tmp\n\t\tif inst_dict:\n\t\t\treturn (self.__class__, (items,), inst_dict)\n\t\treturn self.__class__, (items,)\n\n\tdef keys(self):\n\t\treturn list(self)\n\n\tsetdefault\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\n\tif self.__is_empty:\n\t\tif last_accessed_key == self.__default_key:\n\t\t\t# TODO: error message: no items in cache\n\t\t\traise KeyError('%s' % last_accessed_key)\n\t\telse:\n\t\t\t# TODO: error message: no items in cache\n\t\t\traise KeyError('%s' % self.__default_key)\n\t\t\t\n\telse:\n\t\t\n\t\tif last_accessed_key == self.__default_key:\n\t\t\tdel self.__default_key\n\t\t\tdel self.__default_value\n\t\t\t\n\t\t\tif len(self.__cache_dict) == 0:\n\t\t\t\tself.__is_empty = True\n\t\t\t\t\n\t\t\t\treturn (last_accessed_key, last_accessed_value)\n\t\t\telse:\n\t\t\t\tself.__default_key = self.__cache_dict.popitem()[0]\n\t\t\t\tself.__default_value = self\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t(key, value) = self.__dict.popitem()\n\tself.__deque.remove(key)\n\treturn (key, value)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = self.__container.popitem()[0]\n\t\tself.__priority[key] = None\n\t\treturn key\n\texcept KeyError:\n\t\traise KeyError('container is empty')\n\n\tdef __repr__(self):\n\t\t\"\"\"\n\t\t\"Return a string representation of the cache.\n\t\t\"\"\"\n\t\treturn repr(self.__container)\n\n\tdef __str__(self):\n\t\t\"\"\"\n\t\t\"Return a string representation of the cache.\n\t\t\"\"\"\n\t\treturn repr(self.__container)\n\n\tdef __len__(self):\n\t\t\"\"\"\n\t\t\"Return the length of the cache.\n\t\t\"\"\"\n\t\treturn len(self.__container)\n\n\tdef __iter__(self):\n\t\t\"\"\"\n\t\t\"Iterate over the cache.\n\t\t\"\"\"\n\t\tfor i in self.__container:\n\t\t\tyield i\n\n\tdef __contains__(self, key):\n\t\t\"\"\"\n\t\t\"Check for the existence of a key in the cache.\n\t\t\"\"\"\n\t\treturn\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tself.__cache.popitem()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\t\t\n\t\tif self.is_empty():\n\t\t\traise KeyError('Dictionary is empty.')\n\t\t\n\t\tkey = self.cache_list.last.key\n\t\tvalue = self.cache_list.last.value\n\t\t\n\t\tself.cache_list.delete_node(self.cache_list.last)\n\t\tdel self.cache_dict[key]\n\t\t\n\t\treturn key, value\n\t\t\n\tdef __repr__(self):\n\t\treturn repr(self.cache_dict)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "cachetools/rr.py", "project": "pexip/os-python-cachetools", "name": "popitem", "docstring": "Remove and return a random `(key, value)` pair.", "ques_id": "62b8d23e48ba5a41d1c3f49e", "code": ["def popitem(self):\n\ttry:\n\t\tkey = self.__choice()\n\texcept IndexError:\n\t\traise KeyError(\"popitem(): dictionary is empty\")\n\tvalue = self.pop(key)\n\treturn key, value\n\n\n", "def popitem(self):\n\tkey = self.__choice(self.__keys)\n\treturn key, self.pop(key)\n\n", "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('popitem(): dictionary is empty')\n\tkey = self.__choice(list(self.keys()))\n\tvalue = self[key]\n\tdel self[key]\n\treturn (key, value)\n\n\n", "def popitem(self):\n\tkey = choice(self.keys())\n\treturn (key, self.pop(key))\n\t\n", "def popitem(self):\n\tkey = self.__choice()\n\treturn key, self.pop(key)\n\n", "def popitem(self):\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value\n\n\tdef setdefault(self, key, failobj=None):\n\t\t\"\"\"\n\t\tReturn the value for key if key is in the dict, else default\n\t\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = failobj\n\t\t\treturn failobj\n\t\treturn self[key]\n\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tUpdate the dict with the key/value pairs from other, overwriting existing keys\n\t\t\"\"\"\n\t\tnewdict = dict(*args, **kwargs)\n\t\tfor key in newdict:\n\t\t\tself[key] = newdict[key]\n\n\tdef values(self):\n\t\t\"\"\"\n\t\tReturn a list of all values in the dict\n\t\t\"\"\"\n\t\treturn [self[key] for key in self]\n", "def popitem(self):\n\tkey = self.__choice()\n\tvalue = self.pop(key)\n\treturn (key, value)\n\n\n", "def popitem(self):\n\tif not self.items:\n\t\traise KeyError('Dictionary is empty')\n\treturn self.__choice(self.items)\n\n", "def popitem(self):\n\treturn self.__choice(self.__items)\n\n\n", "def popitem(self):\n\tkey = self.__choice(self.keys())\n\tvalue = self.pop(key)\n\treturn key, value\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def popitem(self):\n\ttry:\n\t\tkey = self.__choice()\n\texcept IndexError:\n\t\traise KeyError(\"popitem(): dictionary is empty\")\n\tvalue = self.pop(key)\n\treturn key, value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tkey = self.__choice(self.__keys)\n\treturn key, self.pop(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif len(self) == 0:\n\t\traise KeyError('popitem(): dictionary is empty')\n\tkey = self.__choice(list(self.keys()))\n\tvalue = self[key]\n\tdel self[key]\n\treturn (key, value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tkey = choice(self.keys())\n\treturn (key, self.pop(key))\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tkey = self.__choice()\n\treturn key, self.pop(key)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tkey = self.__choice()\n\tvalue = self[key]\n\tdel self[key]\n\treturn key, value\n\n\tdef setdefault(self, key, failobj=None):\n\t\t\"\"\"\n\t\tReturn the value for key if key is in the dict, else default\n\t\t\"\"\"\n\t\tif key not in self:\n\t\t\tself[key] = failobj\n\t\t\treturn failobj\n\t\treturn self[key]\n\n\tdef update(self, *args, **kwargs):\n\t\t\"\"\"\n\t\tUpdate the dict with the key/value pairs from other, overwriting existing keys\n\t\t\"\"\"\n\t\tnewdict = dict(*args, **kwargs)\n\t\tfor key in newdict:\n\t\t\tself[key] = newdict[key]\n\n\tdef values(self):\n\t\t\"\"\"\n\t\tReturn a list of all values in the dict\n\t\t\"\"\"\n\t\treturn [self[key] for key in self]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tkey = self.__choice()\n\tvalue = self.pop(key)\n\treturn (key, value)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tif not self.items:\n\t\traise KeyError('Dictionary is empty')\n\treturn self.__choice(self.items)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\treturn self.__choice(self.__items)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def popitem(self):\n\tkey = self.__choice(self.keys())\n\tvalue = self.pop(key)\n\treturn key, value\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_in_regex", "docstring": "Create the in-style parameter regular expression.\n\nReturns the in-style parameter regular expression (:class:`re.Pattern`).", "ques_id": "62b43425903eeb48555d3ea1", "code": ["def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(rf\"\"\"\\A(?P<in_param>{self._param_re})\\Z\"\"\")\n\n\tdef _create_in_not_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the in-not-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn self._in_regex\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t# Build the regex.\n\tregex = self._regex_group_start\n\tif self._parameter_regex is not None:\n\t\tregex += self._parameter_regex\n\telse:\n\t\tregex += self._regex_group_end\n\tregex += self._regex_group_end\n\tregex += \"in\" + self._regex_group_start\n\tregex += self._regex_any_character\n\tregex += self._regex_group_end\n\treturn re.compile(regex, re.IGNORECASE)\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn self._re_compile(r'\\b{}\\b(?!\\s*\\=)'.format(self._param_name))\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the out-style parameter regular expression.\n\n\t\tReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\treturn self._re_compile(r'\\b{}\\b\\s*\\='.format(self._param_name))\n\n\tdef _get_desc(self, mode: str) -> str:\n\t\t\"\"\"\n\t\tGet the description for a mode.\n\n\t\t:param mode: the mode\n\t\t:return: the description for the mode\n\t\t\"\"\"\n\t\treturn self._desc.get(mode, '')\n\n\tdef _get_value(self, cmd: str, mode: str) -> str:\n\t\t\"\"\"\n\t\tGet the value for a mode.\n\n\t\t:param cmd: the command\n\t\t:param mode: the mode\n\t\t:return: the value for the mode\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'<\\s*{}\\s*>'.format(self.in_param_re_string))\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(\n\t\t\t\tr\"\\b(?P<name>[^\\s]+)\\s*(?P<value>(\\d+(\\.\\d+)?)|(True|False)|(\\\"[^\\\"]+\\\")|([^\\s]+))\"\n\t\t\t)\n\n\tdef __init__(self, *args, **kwargs) -> None:\n\t\"\"\"\n\tInitialize the parameter parser.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\tregex = r'^{0}(?P<param>.*?){1}$'.format(self.in_prefix, self.in_suffix)\n\t\treturn re.compile(regex, re.IGNORECASE)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'\\{([\\w.]+)\\}')\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "def _create_in_regex(self) -> Pattern:\n\treturn re.compile(r\"%in\\((?P<name>[a-zA-Z_]+),(?P<inlist>[^)]+)\\)\")\n\n", "def _create_in_regex(self) -> Pattern:\n\treturn re.compile(r'(?<![\\w/])\\{([\\w-]+?)\\.\\.\\.\\}')\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(rf\"\"\"\\A(?P<in_param>{self._param_re})\\Z\"\"\")\n\n\tdef _create_in_not_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the in-not-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn self._in_regex\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t# Build the regex.\n\tregex = self._regex_group_start\n\tif self._parameter_regex is not None:\n\t\tregex += self._parameter_regex\n\telse:\n\t\tregex += self._regex_group_end\n\tregex += self._regex_group_end\n\tregex += \"in\" + self._regex_group_start\n\tregex += self._regex_any_character\n\tregex += self._regex_group_end\n\treturn re.compile(regex, re.IGNORECASE)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn self._re_compile(r'\\b{}\\b(?!\\s*\\=)'.format(self._param_name))\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the out-style parameter regular expression.\n\n\t\tReturns the out-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\treturn self._re_compile(r'\\b{}\\b\\s*\\='.format(self._param_name))\n\n\tdef _get_desc(self, mode: str) -> str:\n\t\t\"\"\"\n\t\tGet the description for a mode.\n\n\t\t:param mode: the mode\n\t\t:return: the description for the mode\n\t\t\"\"\"\n\t\treturn self._desc.get(mode, '')\n\n\tdef _get_value(self, cmd: str, mode: str) -> str:\n\t\t\"\"\"\n\t\tGet the value for a mode.\n\n\t\t:param cmd: the command\n\t\t:param mode: the mode\n\t\t:return: the value for the mode\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'<\\s*{}\\s*>'.format(self.in_param_re_string))\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(\n\t\t\t\tr\"\\b(?P<name>[^\\s]+)\\s*(?P<value>(\\d+(\\.\\d+)?)|(True|False)|(\\\"[^\\\"]+\\\")|([^\\s]+))\"\n\t\t\t)\n\n\tdef __init__(self, *args, **kwargs) -> None:\n\t\"\"\"\n\tInitialize the parameter parser.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\tregex = r'^{0}(?P<param>.*?){1}$'.format(self.in_prefix, self.in_suffix)\n\t\treturn re.compile(regex, re.IGNORECASE)\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\t\treturn re.compile(r'\\{([\\w.]+)\\}')\n\n\tdef _create_out_regex(self) -> Pattern:\n\t\"\"\"\n\tCreate the out-style parameter regular expression.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\treturn re.compile(r\"%in\\((?P<name>[a-zA-Z_]+),(?P<inlist>[^)]+)\\)\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_in_regex(self) -> Pattern:\n\treturn re.compile(r'(?<![\\w/])\\{([\\w-]+?)\\.\\.\\.\\}')\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "sqlparams/__init__.py", "project": "cpburnz/python-sql-parameters", "name": "_create_converter", "docstring": "Create the parameter style converter.\n\nReturns the parameter style converter (:class:`._converting._Converter`).", "ques_id": "62b43426903eeb48555d3ea2", "code": ["def _create_converter(self) -> _converting._Converter:\n\tif self._converter is None:\n\t\tself._converter = _converting.Converter.create_from_parameter_style(self.parameter_style)\n\treturn self._converter\n\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self)\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self)\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting.Converter(self._parameters, self._format)\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._csv_parameters_style)\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self)\n\n", "def _create_converter(self) -> _converting._Converter:\n\treturn self._converter_factory(self)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _create_converter(self) -> _converting._Converter:\n\tif self._converter is None:\n\t\tself._converter = _converting.Converter.create_from_parameter_style(self.parameter_style)\n\treturn self._converter\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting.Converter(self._parameters, self._format)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self._csv_parameters_style)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn _converting._Converter(self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _create_converter(self) -> _converting._Converter:\n\treturn self._converter_factory(self)\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/isoparser.py", "project": "pexip/os-python-dateutil", "name": "isoparse", "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `\u00b1HH:MM`\n- `\u00b1HHMM`\n- `\u00b1HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0", "ques_id": "62b8966c755ee91dce50a154", "code": ["def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\tpass\n\t\n", "def isoparse(self, dt_str):\n\t# The tzinfos keyword is supported for backwards compatibility\n\t# with the dateutil 2.6 API. It is not documented.\n\twith _get_string_stream(dt_str) as fp:\n\t\t# Return a datetime.\n\t\treturn self._parse(fp, **self._get_parse_defaults(self._parse_iso))\n\n\n", "def isoparse(self, dt_str):\n\tpass\n\t\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\t# The tzinfos keyword is supported for backwards compatibility\n\t# with the dateutil 2.6 API. It is not documented.\n\twith _get_string_stream(dt_str) as fp:\n\t\t# Return a datetime.\n\t\treturn self._parse(fp, **self._get_parse_defaults(self._parse_iso))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def isoparse(self, dt_str):\n\tpass\n\t\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/parser/_parser.py", "project": "pexip/os-python-dateutil", "name": "parse", "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.", "ques_id": "62b896de755ee91dce50a183", "code": ["def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tdef _convert(val, val_type, default_unit=None, from_unit=None):\n\t\t\"\"\"\n\t\tConvert the given value to the given type and adjust the unit\n\t\taccordingly.\n\n\t\t:param val:\n\t\t    The value to convert.\n\n\t\t:param val_type:\n\t\t    The type to which to convert the value.\n\n\t\t:param default_unit:\n\t\t    The unit to use if no unit was given.\n\n\t\t:param from_unit:\n\t\t    The unit of the value.\n\t\t\"\"\"\n\t\tmult = 1\n\t\tif from_unit is not None:\n\t\t\tif from_unit == \"week\":\n\t\t\t\tfrom_unit = \"day\"\n\t\t\t\tmult = 7\n\n\t\t\tif val_type == \"microsecond\":\n\t\t\t\tval_type = \"second\"\n\t\t\t\tmult *= 1e6\n\n\t\t\tif from_unit != val_type:\n\t\t\t\tif from_unit == \"second\" and val_type == \"minute\":\n\t\t\t\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif timestr is None:\n\t\traise TypeError(\"Cannot parse None\")\n\n\ttry:\n\t\tif isinstance(timestr, unicode):\n\t\t\ttimestr = timestr.encode(\"utf-8\")\n\texcept NameError:\n\t\tpass\n\n\tif not isinstance(timestr, str):\n\t\traise TypeError(\"strptime() argument 1 must be str, not %s\" %\n\t\t                type(timestr).__name__)\n\n\tif len(timestr) == 0:\n\t\traise ValueError(\"empty string\")\n\n\td = _timelex.split(timestr)\n\ttokens = [x for x in d if x[0]]\n\tif len(tokens) == 0:\n\t\traise ValueError(\"empty string\")\n\n\tres, skipped_tokens = self._parse(tokens, **kwargs)\n\n\tif default is not None:\n\t\tif not isinstance(default, datetime.datetime):\n\t\t\traise TypeError(\"Default argument must be a \"\n\t\t\t                \"\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif default is not None and not isinstance(default, datetime.datetime):\n\t\traise TypeError(\"default must be a datetime instance\")\n\n\tif not isinstance(timestr, str):\n\t\traise TypeError(\"argument must be a string or a character buffer\")\n\n\t# Pre-process the string, stripping out comments\n\t# and cleaning up whitespace\n\ttimestr = cStringIO.StringIO(timestr).readline()\n\ttimestr = re.sub(r\"(?<!\\\\)#.*\", \"\", timestr).strip()\n\n\t# Extract the parts from the string, using the best match\n\ttry:\n\t\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\texcept IndexError:\n\t\traise ParserError(\"no datetime information was found\")\n\n\t# If the default is not specified, use the current date\n\t# and time.\n\tif default is None:\n\t\tdefault = datetime.datetime.now()\n\n\t# If no year was specified, use the year from the default\n\tif res.year is None\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"type '%s'\" % type(timestr))\n\n\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"{0}.\".format(type(timestr).__name__))\n\n\tif default is None:\n\t\tdefault = datetime.datetime.now(self.default_tzinfo)\n\n\tif not isinstance(default, datetime.datetime):\n\t\traise ValueError(\"Default must be a datetime object, not {0}\"\n\t\t\t\t\t\t .format(type(default).__name__))\n\n\tdefault = default.replace(tzinfo=self.default_tzinfo)\n\n\tif kwargs.get('fuzzy_with_tokens', False):\n\t\tret = self._parse(timestr, **kwargs)\n\t\tif ret is None:\n\t\t\treturn None, None\n\t\telse:\n\t\t\tdt, tokens = ret\n\telse:\n\t\tdt, tokens = self._parse(timestr, **kwargs)\n\n\tif dt is None:\n\t\traise\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, str):\n\t\ttimestr = timestr.decode(UNICODE_ENCODING)\n\n\tif not isinstance(timestr, text_type):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"{0}\".format(type(timestr).__name__))\n\n\tif default is not None and not isinstance(default, datetime):\n\t\traise TypeError(\"Default must be a datetime instance, not {0}\"\n\t\t\t\t\t\t.format(type(default).__name__))\n\n\tif not isinstance(tzinfos, (dict, type(None))):\n\t\ttry:\n\t\t\ttzinfos = dict(tzinfos)\n\t\texcept ValueError:\n\t\t\traise TypeError(\"tzinfos must be a dict or an iterable of tuples\")\n\n\tif tzinfos is not None:\n\t\ttzinfos = tzinfos.copy()\n\t\tfor tzname in list(tzinfos.keys()):\n\t\t\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(default, datetime):\n\t\tdefault = default.replace(tzinfo=None)\n\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif default is None:\n\t\tdefault = datetime.datetime.now().replace(microsecond=0)\n\telif isinstance(default, datetime.datetime):\n\t\tdefault = default.replace(microsecond=0)\n\telse:\n\t\traise TypeError(\"parse() argument 2 must be datetime.datetime, not %s\" %\n\t\t\t\t\t\ttype(default))\n\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\n\ttzprec = res.pop(\"tzprecision\", None)\n\ttzname = res.pop(\"tzname\", None)\n\n\t# Convert the tzinfo, if needed.\n\tif ignoretz:\n\t\ttz = None\n\telif tzname:\n\t\ttz = tz_factory(tzname, res.get(\"tzoffset\"), res.get(\"tzlocal\"))\n\t\tif tz is None:\n\t\t\traise ParserError(\"Unknown time zone %r\" % tzname)\n\telif tzinfos:\n\t\ttz = tzinfos.get(tz\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t_, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _,\n", "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"type '%s'\" % type(timestr))\n\n\tif default is None:\n\t\tdefault = self.default\n\n\tif isinstance(default, datetime.datetime):\n\t\tif default.tzinfo is None:\n\t\t\tdefault = default.replace(tzinfo=self.default_tzinfo)\n\t\tdefault_datetime = default\n\t\tdefault = default.timetuple()[:6]\n\telse:\n\t\tdefault_datetime = datetime.datetime(*default[:6],\n\t\t\t\t\t\t\t\t\t\t\t tzinfo=self.default_tzinfo)\n\n\t# We need to convert the default datetime to UTC, as this will be\n\t# the base for all calculations.\n\tdefault_datetime = default_datetime.astimezone(self.UTC)\n\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\n\tif res is None\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tdef _convert(val, val_type, default_unit=None, from_unit=None):\n\t\t\"\"\"\n\t\tConvert the given value to the given type and adjust the unit\n\t\taccordingly.\n\n\t\t:param val:\n\t\t    The value to convert.\n\n\t\t:param val_type:\n\t\t    The type to which to convert the value.\n\n\t\t:param default_unit:\n\t\t    The unit to use if no unit was given.\n\n\t\t:param from_unit:\n\t\t    The unit of the value.\n\t\t\"\"\"\n\t\tmult = 1\n\t\tif from_unit is not None:\n\t\t\tif from_unit == \"week\":\n\t\t\t\tfrom_unit = \"day\"\n\t\t\t\tmult = 7\n\n\t\t\tif val_type == \"microsecond\":\n\t\t\t\tval_type = \"second\"\n\t\t\t\tmult *= 1e6\n\n\t\t\tif from_unit != val_type:\n\t\t\t\tif from_unit == \"second\" and val_type == \"minute\":\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif timestr is None:\n\t\traise TypeError(\"Cannot parse None\")\n\n\ttry:\n\t\tif isinstance(timestr, unicode):\n\t\t\ttimestr = timestr.encode(\"utf-8\")\n\texcept NameError:\n\t\tpass\n\n\tif not isinstance(timestr, str):\n\t\traise TypeError(\"strptime() argument 1 must be str, not %s\" %\n\t\t                type(timestr).__name__)\n\n\tif len(timestr) == 0:\n\t\traise ValueError(\"empty string\")\n\n\td = _timelex.split(timestr)\n\ttokens = [x for x in d if x[0]]\n\tif len(tokens) == 0:\n\t\traise ValueError(\"empty string\")\n\n\tres, skipped_tokens = self._parse(tokens, **kwargs)\n\n\tif default is not None:\n\t\tif not isinstance(default, datetime.datetime):\n\t\t\traise TypeError(\"Default argument must be a \"\n\t\t\t                \"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif default is not None and not isinstance(default, datetime.datetime):\n\t\traise TypeError(\"default must be a datetime instance\")\n\n\tif not isinstance(timestr, str):\n\t\traise TypeError(\"argument must be a string or a character buffer\")\n\n\t# Pre-process the string, stripping out comments\n\t# and cleaning up whitespace\n\ttimestr = cStringIO.StringIO(timestr).readline()\n\ttimestr = re.sub(r\"(?<!\\\\)#.*\", \"\", timestr).strip()\n\n\t# Extract the parts from the string, using the best match\n\ttry:\n\t\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\texcept IndexError:\n\t\traise ParserError(\"no datetime information was found\")\n\n\t# If the default is not specified, use the current date\n\t# and time.\n\tif default is None:\n\t\tdefault = datetime.datetime.now()\n\n\t# If no year was specified, use the year from the default\n\tif res.year is None\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"type '%s'\" % type(timestr))\n\n\treturn self._parse(timestr, default, ignoretz, tzinfos, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"{0}.\".format(type(timestr).__name__))\n\n\tif default is None:\n\t\tdefault = datetime.datetime.now(self.default_tzinfo)\n\n\tif not isinstance(default, datetime.datetime):\n\t\traise ValueError(\"Default must be a datetime object, not {0}\"\n\t\t\t\t\t\t .format(type(default).__name__))\n\n\tdefault = default.replace(tzinfo=self.default_tzinfo)\n\n\tif kwargs.get('fuzzy_with_tokens', False):\n\t\tret = self._parse(timestr, **kwargs)\n\t\tif ret is None:\n\t\t\treturn None, None\n\t\telse:\n\t\t\tdt, tokens = ret\n\telse:\n\t\tdt, tokens = self._parse(timestr, **kwargs)\n\n\tif dt is None:\n\t\traise\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(timestr, str):\n\t\ttimestr = timestr.decode(UNICODE_ENCODING)\n\n\tif not isinstance(timestr, text_type):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"{0}\".format(type(timestr).__name__))\n\n\tif default is not None and not isinstance(default, datetime):\n\t\traise TypeError(\"Default must be a datetime instance, not {0}\"\n\t\t\t\t\t\t.format(type(default).__name__))\n\n\tif not isinstance(tzinfos, (dict, type(None))):\n\t\ttry:\n\t\t\ttzinfos = dict(tzinfos)\n\t\texcept ValueError:\n\t\t\traise TypeError(\"tzinfos must be a dict or an iterable of tuples\")\n\n\tif tzinfos is not None:\n\t\ttzinfos = tzinfos.copy()\n\t\tfor tzname in list(tzinfos.keys()):\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif isinstance(default, datetime):\n\t\tdefault = default.replace(tzinfo=None)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif default is None:\n\t\tdefault = datetime.datetime.now().replace(microsecond=0)\n\telif isinstance(default, datetime.datetime):\n\t\tdefault = default.replace(microsecond=0)\n\telse:\n\t\traise TypeError(\"parse() argument 2 must be datetime.datetime, not %s\" %\n\t\t\t\t\t\ttype(default))\n\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\n\ttzprec = res.pop(\"tzprecision\", None)\n\ttzname = res.pop(\"tzname\", None)\n\n\t# Convert the tzinfo, if needed.\n\tif ignoretz:\n\t\ttz = None\n\telif tzname:\n\t\ttz = tz_factory(tzname, res.get(\"tzoffset\"), res.get(\"tzlocal\"))\n\t\tif tz is None:\n\t\t\traise ParserError(\"Unknown time zone %r\" % tzname)\n\telif tzinfos:\n\t\ttz = tzinfos.get(tz\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\t_, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, \\\n\t\t_, _, _, _, _, _,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n\tif not isinstance(timestr, basestring):\n\t\traise TypeError(\"Parser must be a string or character stream, not \"\n\t\t\t\t\t\t\"type '%s'\" % type(timestr))\n\n\tif default is None:\n\t\tdefault = self.default\n\n\tif isinstance(default, datetime.datetime):\n\t\tif default.tzinfo is None:\n\t\t\tdefault = default.replace(tzinfo=self.default_tzinfo)\n\t\tdefault_datetime = default\n\t\tdefault = default.timetuple()[:6]\n\telse:\n\t\tdefault_datetime = datetime.datetime(*default[:6],\n\t\t\t\t\t\t\t\t\t\t\t tzinfo=self.default_tzinfo)\n\n\t# We need to convert the default datetime to UTC, as this will be\n\t# the base for all calculations.\n\tdefault_datetime = default_datetime.astimezone(self.UTC)\n\n\tres, skipped_tokens = self._parse(timestr, **kwargs)\n\n\tif res is None\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/tz/_common.py", "project": "pexip/os-python-dateutil", "name": "fromutc", "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.", "ques_id": "62b8a4a4755ee91dce50a3d3", "code": ["def fromutc(self, dt):\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\n\tdt = dt.astimezone(self)\n\tdt = dt.replace(tzinfo=None)\n\n\tnew_timestamp = self._utc_transition_times[bisect.bisect(self._utc_transition_times, dt.timestamp())]\n\tnew_datetime = datetime.datetime.utcfromtimestamp(new_timestamp)\n\treturn new_datetime.replace(tzinfo=self)\n\n", "def fromutc(self, dt):\n\t\ttz = self.tzinfo\n\n\t\t# If the original datetime is not timezone-aware, raise an exception.\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError, 'fromutc: dt is not timezone-aware'\n\n\t\t# If the original datetime is naive, raise an exception.\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError, 'fromutc: dt is naive'\n\n\t\t# If the timezone-aware datetime is in a \"fold\", we need to figure out\n\t\t# if it's the first or second occurrence of the ambiguous datetime. We\n\t\t# do this by comparing the offset of the original datetime with the\n\t\t# offset of the datetime one hour before. If they differ, we know that\n\t\t# the datetime is within the fold and we use the offset of the datetime\n\t\t# one hour before to determine the correct result.\n\n\t\tdt_fold = dt.replace(tzinfo=tz)\n\t\tdt_fold_prev = dt.\n", "def fromutc(self, dt):\n\tif not self._is_dst(dt):\n\t\treturn dt + self.stdoffset\n\telse:\n\t\treturn dt + self.dst(dt)\n\n\n", "def fromutc(self, dt):\n\t# We can't use dt.astimezone() for this, since it's been\n\t# deprecated and removed, and is also only available in\n\t# Python 2.7 onwards.\n\t#\n\t# We use the fact that we know we're in a unambiguous datetime\n\t# to determine whether we're in a fold period.\n\tif self.is_ambiguous(dt):\n\t\tif dt.replace(tzinfo=None) == self.localize(dt, is_dst=False).replace(tzinfo=None):\n\t\t\tis_dst = False\n\t\telse:\n\t\t\tis_dst = True\n\telse:\n\t\tis_dst = dt.tzinfo._isdst\n\n\tif dt.tzinfo == self:\n\t\t# If we're converting from the same timezone, we can do this\n\t\t# trivially.\n\t\treturn self.localize(dt, is_dst=is_dst)\n\n\t# We don't know anything about the timezone we're converting from,\n\t# so we can't do anything other than rely on the\n", "def fromutc(self, dt):\n\tdt = _ensure_datetime(dt)\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"fromutc() requires a timezone-aware datetime\")\n\treturn dt.replace(tzinfo=self, fold=self._fold_ambiguous_time(dt))\n\n\tdef normalize(self, dt, is_dst=False):\n\t\t\"\"\"\n\t\tGiven a timezone-aware datetime in a given timezone, calculates a\n", "def fromutc(self, dt):\n\t# dt is assumed to be in the current timezone, so we can just call\n\t# normalize() to get the correct fold value.\n\tdt = self.normalize(dt)\n\treturn self._utc_to_local(dt.utcoffset(), dt, dt.fold)\n\n\n\n", "def fromutc(self, dt):\n\t\tif not is_aware(dt):\n\t\t\traise ValueError(\"fromutc() requires an aware datetime\")\n\n\t\t# Check for ambiguous times, and correct them if necessary.\n\t\tif self._tzinfo._ambiguous:\n\t\t\tif self._tzinfo._fold and dt.fold == 0:\n\t\t\t\t# The datetime is in a fold.\n\t\t\t\t# This is the first occurrence of the ambiguous datetime.\n\t\t\t\t# We need to add a day to the datetime.\n\t\t\t\tdt += timedelta(days=1)\n\t\t\telse:\n\t\t\t\t# The datetime is not in a fold, or it is in a fold but\n\t\t\t\t# it's not the first occurrence.\n\t\t\t\t# We can't do anything with it.\n\t\t\t\tpass\n\n\t\tdst = dt.tzinfo.dst(dt)\n\n\t\treturn datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond,\n\n", "def fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\tif not dt.tzinfo._tzname:\n\t\traise ValueError('fromutc() requires that dt.tzinfo is a valid timezone')\n\tdt = dt.astimezone(self)\n\tif self._tzname(dt):\n\t\tdt.fold = 0\n\treturn dt\n\n\n", "def fromutc(self, dt):\n\t\tif self._tzinfo._tzinfos is None:\n\t\t\treturn dt + self._offset\n\n\t\tif dt.tzinfo is not self._tzinfo:\n\t\t\traise ValueError('dt must be in the same timezone as the converter')\n\n\t\tutc_datetime = dt.astimezone(_utc)\n\n\t\t# Find the appropriate UTC offset for the datetime and timezone\n\t\t# self._tzinfo.\n\t\tutc_offset = self._tzinfo.utcoffset(utc_datetime)\n\t\tif utc_offset is None:\n\t\t\traise ValueError('dt is not a valid datetime in the timezone')\n\n\t\t# Determine if the datetime is ambiguous.\n\t\tis_ambiguous = self._tzinfo.is_ambiguous(utc_datetime)\n\n\t\t# If the datetime is not ambiguous, or we're not on the fold, or\n\t\t# the fold is None, then simply return the datetime with the\n\t\t# new UTC offset.\n\t\tif not is_ambiguous or not self._fold\n", "def fromutc(self, dt):\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"fromutc: dt is not timezone-aware\")\n\t\t\n\t\tdt = dt.astimezone(self.utc)\n\t\t\n\t\t# Check whether this datetime is in a fold\n\t\tdt_fold = dt.replace(fold=1)\n\t\tdt_unfold = dt_fold.replace(fold=0)\n\t\tif dt_fold > dt_unfold:\n\t\t\t# dt_fold is the first occurrence of this datetime\n\t\t\tdt = dt_fold\n\t\t\n\t\t# Now we can find the utcoffset\n\t\toffset = self.utcoffset(dt)\n\t\t# And convert the datetime to a UTC-based one\n\t\tdt = dt - offset\n\t\t# Now make it timezone-aware again\n\t\treturn dt.replace(tzinfo=self)\n\t\n\tdef utcoffset(self, dt):\n\t\t\"\"\"\n\t\tReturns the timezone offset in minutes east of UTC\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def fromutc(self, dt):\n\tif dt.tzinfo is self:\n\t\treturn dt\n\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\n\tdt = dt.astimezone(self)\n\tdt = dt.replace(tzinfo=None)\n\n\tnew_timestamp = self._utc_transition_times[bisect.bisect(self._utc_transition_times, dt.timestamp())]\n\tnew_datetime = datetime.datetime.utcfromtimestamp(new_timestamp)\n\treturn new_datetime.replace(tzinfo=self)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\ttz = self.tzinfo\n\n\t\t# If the original datetime is not timezone-aware, raise an exception.\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError, 'fromutc: dt is not timezone-aware'\n\n\t\t# If the original datetime is naive, raise an exception.\n\t\tif dt.tzinfo.utcoffset(dt) is None:\n\t\t\traise ValueError, 'fromutc: dt is naive'\n\n\t\t# If the timezone-aware datetime is in a \"fold\", we need to figure out\n\t\t# if it's the first or second occurrence of the ambiguous datetime. We\n\t\t# do this by comparing the offset of the original datetime with the\n\t\t# offset of the datetime one hour before. If they differ, we know that\n\t\t# the datetime is within the fold and we use the offset of the datetime\n\t\t# one hour before to determine the correct result.\n\n\t\tdt_fold = dt.replace(tzinfo=tz)\n\t\tdt_fold_prev = dt.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tif not self._is_dst(dt):\n\t\treturn dt + self.stdoffset\n\telse:\n\t\treturn dt + self.dst(dt)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t# We can't use dt.astimezone() for this, since it's been\n\t# deprecated and removed, and is also only available in\n\t# Python 2.7 onwards.\n\t#\n\t# We use the fact that we know we're in a unambiguous datetime\n\t# to determine whether we're in a fold period.\n\tif self.is_ambiguous(dt):\n\t\tif dt.replace(tzinfo=None) == self.localize(dt, is_dst=False).replace(tzinfo=None):\n\t\t\tis_dst = False\n\t\telse:\n\t\t\tis_dst = True\n\telse:\n\t\tis_dst = dt.tzinfo._isdst\n\n\tif dt.tzinfo == self:\n\t\t# If we're converting from the same timezone, we can do this\n\t\t# trivially.\n\t\treturn self.localize(dt, is_dst=is_dst)\n\n\t# We don't know anything about the timezone we're converting from,\n\t# so we can't do anything other than rely on the\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tdt = _ensure_datetime(dt)\n\tif dt.tzinfo is None:\n\t\traise ValueError(\"fromutc() requires a timezone-aware datetime\")\n\treturn dt.replace(tzinfo=self, fold=self._fold_ambiguous_time(dt))\n\n\tdef normalize(self, dt, is_dst=False):\n\t\t\"\"\"\n\t\tGiven a timezone-aware datetime in a given timezone, calculates a\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t# dt is assumed to be in the current timezone, so we can just call\n\t# normalize() to get the correct fold value.\n\tdt = self.normalize(dt)\n\treturn self._utc_to_local(dt.utcoffset(), dt, dt.fold)\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\tif not is_aware(dt):\n\t\t\traise ValueError(\"fromutc() requires an aware datetime\")\n\n\t\t# Check for ambiguous times, and correct them if necessary.\n\t\tif self._tzinfo._ambiguous:\n\t\t\tif self._tzinfo._fold and dt.fold == 0:\n\t\t\t\t# The datetime is in a fold.\n\t\t\t\t# This is the first occurrence of the ambiguous datetime.\n\t\t\t\t# We need to add a day to the datetime.\n\t\t\t\tdt += timedelta(days=1)\n\t\t\telse:\n\t\t\t\t# The datetime is not in a fold, or it is in a fold but\n\t\t\t\t# it's not the first occurrence.\n\t\t\t\t# We can't do anything with it.\n\t\t\t\tpass\n\n\t\tdst = dt.tzinfo.dst(dt)\n\n\t\treturn datetime(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond,\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\tif dt.tzinfo is None:\n\t\traise ValueError('fromutc() requires a timezone-aware datetime')\n\tif not dt.tzinfo._tzname:\n\t\traise ValueError('fromutc() requires that dt.tzinfo is a valid timezone')\n\tdt = dt.astimezone(self)\n\tif self._tzname(dt):\n\t\tdt.fold = 0\n\treturn dt\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\tif self._tzinfo._tzinfos is None:\n\t\t\treturn dt + self._offset\n\n\t\tif dt.tzinfo is not self._tzinfo:\n\t\t\traise ValueError('dt must be in the same timezone as the converter')\n\n\t\tutc_datetime = dt.astimezone(_utc)\n\n\t\t# Find the appropriate UTC offset for the datetime and timezone\n\t\t# self._tzinfo.\n\t\tutc_offset = self._tzinfo.utcoffset(utc_datetime)\n\t\tif utc_offset is None:\n\t\t\traise ValueError('dt is not a valid datetime in the timezone')\n\n\t\t# Determine if the datetime is ambiguous.\n\t\tis_ambiguous = self._tzinfo.is_ambiguous(utc_datetime)\n\n\t\t# If the datetime is not ambiguous, or we're not on the fold, or\n\t\t# the fold is None, then simply return the datetime with the\n\t\t# new UTC offset.\n\t\tif not is_ambiguous or not self._fold\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fromutc(self, dt):\n\t\tif dt.tzinfo is None:\n\t\t\traise ValueError(\"fromutc: dt is not timezone-aware\")\n\t\t\n\t\tdt = dt.astimezone(self.utc)\n\t\t\n\t\t# Check whether this datetime is in a fold\n\t\tdt_fold = dt.replace(fold=1)\n\t\tdt_unfold = dt_fold.replace(fold=0)\n\t\tif dt_fold > dt_unfold:\n\t\t\t# dt_fold is the first occurrence of this datetime\n\t\t\tdt = dt_fold\n\t\t\n\t\t# Now we can find the utcoffset\n\t\toffset = self.utcoffset(dt)\n\t\t# And convert the datetime to a UTC-based one\n\t\tdt = dt - offset\n\t\t# Now make it timezone-aware again\n\t\treturn dt.replace(tzinfo=self)\n\t\n\tdef utcoffset(self, dt):\n\t\t\"\"\"\n\t\tReturns the timezone offset in minutes east of UTC\n", "is_pass": false, "return_code": 1}]}
{"file_path": "dateutil/utils.py", "project": "pexip/os-python-dateutil", "name": "default_tzinfo", "docstring": "Sets the ``tzinfo`` parameter on naive datetimes only\n\nThis is useful for example when you are provided a datetime that may have\neither an implicit or explicit time zone, such as when parsing a time zone\nstring.\n\n.. doctest::\n\n    >>> from dateutil.tz import tzoffset\n    >>> from dateutil.parser import parse\n    >>> from dateutil.utils import default_tzinfo\n    >>> dflt_tz = tzoffset(\"EST\", -18000)\n    >>> print(default_tzinfo(parse('2014-01-01 12:30 UTC'), dflt_tz))\n    2014-01-01 12:30:00+00:00\n    >>> print(default_tzinfo(parse('2014-01-01 12:30'), dflt_tz))\n    2014-01-01 12:30:00-05:00\n\n:param dt:\n    The datetime on which to replace the time zone\n\n:param tzinfo:\n    The :py:class:`datetime.tzinfo` subclass instance to assign to\n    ``dt`` if (and only if) it is naive.\n\n:return:\n    Returns an aware :py:class:`datetime.datetime`.", "ques_id": "62b8a7b2755ee91dce50a4a7", "code": ["def default_tzinfo(dt, tzinfo):\n\tif tzinfo is None or dt.tzinfo is not None:\n\t\treturn dt\n\treturn dt.replace(tzinfo=tzinfo)\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n\n", "def default_tzinfo(dt, tzinfo):\n\tif getattr(dt, 'tzinfo', None) is not None:\n\t\treturn dt\n\treturn dt.replace(tzinfo=tzinfo)\n\n\n", "def default_tzinfo(dt, tzinfo):\n\treturn dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt\n\n\n"], "level": "self_contained", "generate_results": [{"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif tzinfo is None or dt.tzinfo is not None:\n\t\treturn dt\n\treturn dt.replace(tzinfo=tzinfo)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\tdt = dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\telse:\n\t\treturn dt\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif dt.tzinfo is None:\n\t\treturn dt.replace(tzinfo=tzinfo)\n\treturn dt\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\tif getattr(dt, 'tzinfo', None) is not None:\n\t\treturn dt\n\treturn dt.replace(tzinfo=tzinfo)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def default_tzinfo(dt, tzinfo):\n\treturn dt.replace(tzinfo=tzinfo) if dt.tzinfo is None else dt\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "src/lithium/testcases.py", "project": "MozillaSecurity/lithium", "name": "set_cut_chars", "docstring": "Set the bytes used to delimit slice points.\n\nArgs:\n    before: Split file before these delimiters.\n    after: Split file after these delimiters.", "ques_id": "6305f9991d275c6667163c50", "code": ["def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_before = before\n\tself.cut_after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_cut_regex(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_segment_limit(self, limit: int) -> None:\n\t\t\"\"\"\n\t\tSet the maximum number of segments.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_cut_pattern(self, pattern: str) -> None:\n\t\"\"\"\n\tSet the regex pattern used to delimit slice points.\n\t\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.before_cut = before\n\t\tself.after_cut = after\n\n\tdef set_split_size(self, size: int) -> None:\n\t\"\"\"\n\tSet the maximum size of each split file to size bytes.\n\t\"\"\"\n\t\tself.split_size = size\n\n\tdef set_split_num(self, num: int) -> None:\n\t\"\"\"\n\tSet the number of split files to create.\n\t\"\"\"\n\t\tself.split_num = num\n\n\tdef set_split_file(self, filename: str) -> None:\n\t\"\"\"\n\tSet the file to split.\n\t\"\"\"\n\t\tself.split_file = filename\n\n\tdef set_output_dir(self, dirname: str) -> None:\n\t\"\"\"\n\tSet the directory to store the split files.\n\t\"\"\"\n\t\tself.output_dir = dirname\n\n\tdef set_split_ext(self, ext: str) -> None:\n\t\"\"\"\n\tSet the extension of the split files.\n\t\"\"\"\n\t\tself.split_ext = ext\n\n\tdef split_by_size(self) ->\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself._before = before\n\t\tself._after = after\n\n\tdef set_cut_bytes(self, before: int, after: int) -> None:\n\t\"\"\"\n\tSet the number of bytes before and after the delimiter.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_output_path(self, output_path: str) -> None:\n\t\t\"\"\"\n\t\tSet the output folder.\n\t\t\n\t\tArgs:\n\t\t\toutput_path: The output folder.\n\t\t\"\"\"\n\t\tself.output_path = output_path\n\t\t\n\tdef set_input_path(self, input_path: str) -> None:\n\t\t\"\"\"\n\t\tSet the input file.\n\t\t\n\t\tArgs:\n\t\t\tinput_path: The input file.\n\t\t\"\"\"\n\t\tself.input_path = input_path\n\t\t\n\tdef set_output_format(self, output_format: str) -> None:\n\t\t\"\"\"\n\t\tSet the output filename format.\n\t\t\n\t\tArgs:\n\t\t\toutput_format: The output filename format.\n\t\t\"\"\"\n\t\tself.output_format = output_format\n\t\t\n\tdef set_output_extension(self, output_extension: str) -> None:\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_file_name(self, name: bytes) -> None:\n\t\t\"\"\"\n\t\tSet the file name.\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_chars_before = before\n\tself.cut_chars_after = after\n\n", "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_output_format(self, format: str) -> None:\n\t\t\"\"\"\n\t\tSet the output format.\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_before = before\n\tself.cut_after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_cut_regex(self, before: bytes, after: bytes) -> None:\n\t\"\"\"\n\tSet the bytes used to delimit slice points.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_segment_limit(self, limit: int) -> None:\n\t\t\"\"\"\n\t\tSet the maximum number of segments.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_cut_pattern(self, pattern: str) -> None:\n\t\"\"\"\n\tSet the regex pattern used to delimit slice points.\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.before_cut = before\n\t\tself.after_cut = after\n\n\tdef set_split_size(self, size: int) -> None:\n\t\"\"\"\n\tSet the maximum size of each split file to size bytes.\n\t\"\"\"\n\t\tself.split_size = size\n\n\tdef set_split_num(self, num: int) -> None:\n\t\"\"\"\n\tSet the number of split files to create.\n\t\"\"\"\n\t\tself.split_num = num\n\n\tdef set_split_file(self, filename: str) -> None:\n\t\"\"\"\n\tSet the file to split.\n\t\"\"\"\n\t\tself.split_file = filename\n\n\tdef set_output_dir(self, dirname: str) -> None:\n\t\"\"\"\n\tSet the directory to store the split files.\n\t\"\"\"\n\t\tself.output_dir = dirname\n\n\tdef set_split_ext(self, ext: str) -> None:\n\t\"\"\"\n\tSet the extension of the split files.\n\t\"\"\"\n\t\tself.split_ext = ext\n\n\tdef split_by_size(self) ->\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself._before = before\n\t\tself._after = after\n\n\tdef set_cut_bytes(self, before: int, after: int) -> None:\n\t\"\"\"\n\tSet the number of bytes before and after the delimiter.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_output_path(self, output_path: str) -> None:\n\t\t\"\"\"\n\t\tSet the output folder.\n\t\t\n\t\tArgs:\n\t\t\toutput_path: The output folder.\n\t\t\"\"\"\n\t\tself.output_path = output_path\n\t\t\n\tdef set_input_path(self, input_path: str) -> None:\n\t\t\"\"\"\n\t\tSet the input file.\n\t\t\n\t\tArgs:\n\t\t\tinput_path: The input file.\n\t\t\"\"\"\n\t\tself.input_path = input_path\n\t\t\n\tdef set_output_format(self, output_format: str) -> None:\n\t\t\"\"\"\n\t\tSet the output filename format.\n\t\t\n\t\tArgs:\n\t\t\toutput_format: The output filename format.\n\t\t\"\"\"\n\t\tself.output_format = output_format\n\t\t\n\tdef set_output_extension(self, output_extension: str) -> None:\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_file_name(self, name: bytes) -> None:\n\t\t\"\"\"\n\t\tSet the file name.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\tself.cut_chars_before = before\n\tself.cut_chars_after = after\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def set_cut_chars(self, before: bytes, after: bytes) -> None:\n\t\tself.cut_before = before\n\t\tself.cut_after = after\n\n\tdef set_output_format(self, format: str) -> None:\n\t\t\"\"\"\n\t\tSet the output format.\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.", "ques_id": "6306292052e177c0ba469f09", "code": ["def identify_request(request: RequestType):\n\ttry:\n\t\trequest_body = request.get_body()\n\t\tif isinstance(request_body, str):\n\t\t\ttry:\n\t\t\t\trequest_body = json.loads(request_body)\n\t\t\texcept ValueError:\n\t\t\t\ttry:\n\t\t\t\t\trequest_body = ElementTree.fromstring(request_body)\n\t\t\t\texcept ElementTree.ParseError:\n\t\t\t\t\treturn False\n\t\t\t\telse:\n\t\t\t\t\tif request_body.tag == Magic_ENV_TAG:\n\t\t\t\t\t\treturn True\n\t\t\t\t\telse:\n\t\t\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\tif request_body.get(\"events\") is not None:\n\t\t\t\t\treturn True\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\texcept Exception as identifier_error:\n\t\traise identifier_error\n\t\treturn False\n\n", "def identify_request(request: RequestType):\n\tif isinstance(request, dict) and 'events' in request:\n\t\treturn True\n\tif isinstance(request, ET.Element) and request.tag == Magic_ENV_TAG:\n\t\treturn True\n\treturn False\n\n", "def identify_request(request: RequestType):\n\tif request.is_json:\n\t\ttry:\n\t\t\trequest_json = request.json\n\t\t\tif 'events' in request_json:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\treturn False\n\telif request.is_xml:\n\t\ttry:\n\t\t\trequest_xml = request.data\n\t\t\troot = ET.fromstring(request_xml)\n\t\t\tif root.tag == Magic_ENV_TAG:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "def identify_request(request: RequestType):\n\tif request.headers.get('Content-Type') and request.headers.get('Content-Type').startswith('application/json'):\n\t\ttry:\n\t\t\trequest_json = request.json\n\t\t\treturn \"events\" in request_json\n\t\texcept:\n\t\t\treturn False\n\telif request.headers.get('Content-Type') and request.headers.get('Content-Type').startswith('text/xml'):\n\t\trequest_xml = request.data\n\t\t# print(\"request_xml: \", request_xml)\n\t\ttry:\n\t\t\trequest_xml_tree = ET.fromstring(request_xml)\n\t\t\t# print(\"request_xml_tree: \", request_xml_tree)\n\t\t\treturn request_xml_tree.tag == Magic_ENV_TAG\n\t\texcept:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "def identify_request(request: RequestType):\n\ttry:\n\t\tevents = request.json.get('events')\n\t\tif events is not None:\n\t\t\treturn True\n\texcept Exception:\n\t\tpass\n\t\n\ttry:\n\t\troot = request.xml.getroot()\n\t\tif root.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept Exception:\n\t\tpass\n\t\n\treturn False\n\n\n", "def identify_request(request: RequestType):\n\tif request.json is not None:\n\t\tif \"events\" in request.json:\n\t\t\treturn True\n\telif request.content_type == \"text/xml\":\n\t\ttree = ET.fromstring(request.data)\n\t\tif tree.tag == MAGIC_ENV_TAG:\n\t\t\treturn True\n\treturn False\n\n\n", "def identify_request(request: RequestType):\n\tif request.get_json():\n\t\treturn True\n\telif request.data:\n\t\tif ET.fromstring(request.data).tag == Magic_ENV_TAG:\n\t\t\treturn True\n\treturn False\n\n\n", "def identify_request(request: RequestType):\n\ttry:\n\t\tif request.json().get('events', None):\n\t\t\treturn True\n\texcept:\n\t\tpass\n\ttry:\n\t\tif request.xml.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\treturn False\n\n", "def identify_request(request: RequestType):\n\t# Check whether the request body contains events\n\ttry:\n\t\tevents = request['events']\n\t\treturn True\n\texcept KeyError:\n\t\tpass\n\n\t# Check whether the request body contains an XML file with the tag of Magic_ENV_TAG\n\ttry:\n\t\troot = ET.fromstring(request.body)\n\t\tif root.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept ET.ParseError:\n\t\tpass\n\n\treturn False\n\n\n", "def identify_request(request: RequestType):\n\tif (request.method == \"POST\" and \"application/json\" in request.headers.get(\"content-type\", \"\")):\n\t\tdata = request.json\n\t\tif (isinstance(data, dict) and \"events\" in data):\n\t\t\treturn True\n\t\telif (isinstance(data, list) and isinstance(data[0], dict) and \"events\" in data[0]):\n\t\t\treturn True\n\telif (request.method == \"POST\" and \"application/xml\" in request.headers.get(\"content-type\", \"\")):\n\t\tdata = request.data.decode()\n\t\tif (\"<{}>\".format(Magic_ENV_TAG) in data):\n\t\t\treturn True\n\treturn False\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType):\n\ttry:\n\t\trequest_body = request.get_body()\n\t\tif isinstance(request_body, str):\n\t\t\ttry:\n\t\t\t\trequest_body = json.loads(request_body)\n\t\t\texcept ValueError:\n\t\t\t\ttry:\n\t\t\t\t\trequest_body = ElementTree.fromstring(request_body)\n\t\t\t\texcept ElementTree.ParseError:\n\t\t\t\t\treturn False\n\t\t\t\telse:\n\t\t\t\t\tif request_body.tag == Magic_ENV_TAG:\n\t\t\t\t\t\treturn True\n\t\t\t\t\telse:\n\t\t\t\t\t\treturn False\n\t\t\telse:\n\t\t\t\tif request_body.get(\"events\") is not None:\n\t\t\t\t\treturn True\n\t\t\t\telse:\n\t\t\t\t\treturn False\n\t\telse:\n\t\t\treturn False\n\texcept Exception as identifier_error:\n\t\traise identifier_error\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif isinstance(request, dict) and 'events' in request:\n\t\treturn True\n\tif isinstance(request, ET.Element) and request.tag == Magic_ENV_TAG:\n\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif request.is_json:\n\t\ttry:\n\t\t\trequest_json = request.json\n\t\t\tif 'events' in request_json:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\treturn False\n\telif request.is_xml:\n\t\ttry:\n\t\t\trequest_xml = request.data\n\t\t\troot = ET.fromstring(request_xml)\n\t\t\tif root.tag == Magic_ENV_TAG:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept Exception as e:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif request.headers.get('Content-Type') and request.headers.get('Content-Type').startswith('application/json'):\n\t\ttry:\n\t\t\trequest_json = request.json\n\t\t\treturn \"events\" in request_json\n\t\texcept:\n\t\t\treturn False\n\telif request.headers.get('Content-Type') and request.headers.get('Content-Type').startswith('text/xml'):\n\t\trequest_xml = request.data\n\t\t# print(\"request_xml: \", request_xml)\n\t\ttry:\n\t\t\trequest_xml_tree = ET.fromstring(request_xml)\n\t\t\t# print(\"request_xml_tree: \", request_xml_tree)\n\t\t\treturn request_xml_tree.tag == Magic_ENV_TAG\n\t\texcept:\n\t\t\treturn False\n\telse:\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\ttry:\n\t\tevents = request.json.get('events')\n\t\tif events is not None:\n\t\t\treturn True\n\texcept Exception:\n\t\tpass\n\t\n\ttry:\n\t\troot = request.xml.getroot()\n\t\tif root.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept Exception:\n\t\tpass\n\t\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif request.json is not None:\n\t\tif \"events\" in request.json:\n\t\t\treturn True\n\telif request.content_type == \"text/xml\":\n\t\ttree = ET.fromstring(request.data)\n\t\tif tree.tag == MAGIC_ENV_TAG:\n\t\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif request.get_json():\n\t\treturn True\n\telif request.data:\n\t\tif ET.fromstring(request.data).tag == Magic_ENV_TAG:\n\t\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\ttry:\n\t\tif request.json().get('events', None):\n\t\t\treturn True\n\texcept:\n\t\tpass\n\ttry:\n\t\tif request.xml.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept:\n\t\tpass\n\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\t# Check whether the request body contains events\n\ttry:\n\t\tevents = request['events']\n\t\treturn True\n\texcept KeyError:\n\t\tpass\n\n\t# Check whether the request body contains an XML file with the tag of Magic_ENV_TAG\n\ttry:\n\t\troot = ET.fromstring(request.body)\n\t\tif root.tag == Magic_ENV_TAG:\n\t\t\treturn True\n\texcept ET.ParseError:\n\t\tpass\n\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType):\n\tif (request.method == \"POST\" and \"application/json\" in request.headers.get(\"content-type\", \"\")):\n\t\tdata = request.json\n\t\tif (isinstance(data, dict) and \"events\" in data):\n\t\t\treturn True\n\t\telif (isinstance(data, list) and isinstance(data[0], dict) and \"events\" in data[0]):\n\t\t\treturn True\n\telif (request.method == \"POST\" and \"application/xml\" in request.headers.get(\"content-type\", \"\")):\n\t\tdata = request.data.decode()\n\t\tif (\"<{}>\".format(Magic_ENV_TAG) in data):\n\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/matrix/protocol.py", "project": "jaywink/federation", "name": "identify_request", "docstring": "Try to identify whether this is a Matrix request", "ques_id": "6306292152e177c0ba469f0d", "code": ["def identify_request(request: RequestType) -> bool:\n\ttry:\n\t\tif request.json().get(\"events\") is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception as e:\n\t\tlogger.error(\"Cannot identify the request.\")\n\t\tlogger.error(e)\n\t\treturn False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.json\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn True if request.get('events') else False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\tif 'events' in request.json.keys():\n\t\treturn True\n\treturn False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn isinstance(request, dict) and 'events' in request\n\n\n", "def identify_request(request: RequestType) -> bool:\n\tif 'events' in request.json:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.form\n\n\n", "def identify_request(request: RequestType) -> bool:\n\tif isinstance(request.json, dict):\n\t\treturn True\n\treturn False\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.json\n\n\n", "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.json\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def identify_request(request: RequestType) -> bool:\n\ttry:\n\t\tif request.json().get(\"events\") is not None:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept Exception as e:\n\t\tlogger.error(\"Cannot identify the request.\")\n\t\tlogger.error(e)\n\t\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.json\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn True if request.get('events') else False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif 'events' in request.json.keys():\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn isinstance(request, dict) and 'events' in request\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif 'events' in request.json:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.form\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\tif isinstance(request.json, dict):\n\t\treturn True\n\treturn False\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.json\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def identify_request(request: RequestType) -> bool:\n\treturn \"events\" in request.json\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/utils.py", "project": "jaywink/federation", "name": "format_dt", "docstring": "Format a datetime in the way that D* nodes expect.", "ques_id": "6306292252e177c0ba469f11", "code": ["def format_dt(dt):\n\tif dt is None:\n\t\treturn\n\telse:\n\t\tdt = ensure_timezone(dt)\n\t\treturn dt.strftime('%H:%M')\n\n\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime('%A, %B %d, %Y %H:%M%p')\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).astimezone(tzone).replace(tzinfo=None)\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).time()\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%m/%d/%Y %I:%M:%S %p\")\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%I:%M %p\")\n\n", "def format_dt(dt):\n\t\n\treturn ensure_timezone(dt).time()\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%B %d, %Y %I:%M %p\")\n\n\n", "def format_dt(dt):\n\treturn ensure_timezone(dt).isoformat()\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def format_dt(dt):\n\tif dt is None:\n\t\treturn\n\telse:\n\t\tdt = ensure_timezone(dt)\n\t\treturn dt.strftime('%H:%M')\n\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%Y-%m-%d %H:%M:%S\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime('%A, %B %d, %Y %H:%M%p')\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).astimezone(tzone).replace(tzinfo=None)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).time()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%m/%d/%Y %I:%M:%S %p\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%I:%M %p\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\t\n\treturn ensure_timezone(dt).time()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).strftime(\"%B %d, %Y %I:%M %p\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def format_dt(dt):\n\treturn ensure_timezone(dt).isoformat()\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "find_tags", "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.", "ques_id": "6306292352e177c0ba469f1d", "code": ["def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tlines = text.split('\\n')\n\tin_code = False\n\tfor i, line in enumerate(lines):\n\t\tif '```' in line:\n\t\t\tin_code = not in_code\n\t\tif in_code:\n\t\t\tcontinue\n\t\tfor j, word in enumerate(line.split(' ')):\n\t\t\tif word.startswith('#') and len(word) > 1:\n\t\t\t\ttags.add(word[1:])\n\t\t\t\tif replacer:\n\t\t\t\t\tlines[i] = ' '.join(line.split(' ')[:j] + [replacer(word)] + line.split(' ')[j + 1:])\n\n\treturn tags, '\\n'.join(lines)\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttag_words = set()\n\n\t# first, we find the code blocks, then we ignore tags inside them\n\tcode_blocks = list(find_code_blocks(text))\n\n\toffsets = []  # (start, end) for each code block\n\n\tfor code_block in code_blocks:\n\t\tstart = text.find(code_block)\n\t\tend = start + len(code_block)\n\t\toffsets.append((start, end))\n\n\t# then we find all our tags\n\tfor match in RE_TAGS.finditer(text):\n\t\ttag = match.group(1)\n\t\tindex = match.start()\n\n\t\t# check if we're in a code block\n\t\tif any(index >= start and index < end for start, end in offsets):\n\t\t\tcontinue\n\n\t\ttag_words.add(tag)\n\n\t\tif replacer:\n\t\t\ttext = text[:index] + replacer(tag) + text[index + len(tag):]\n\n\treturn tag_words, text\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\t# do not replace tags inside code blocks\n\tcode_blocks = []\n\tfor match in re.finditer(r'```.*?```', text, re.DOTALL):\n\t\tcode_blocks.append(match.group(0))\n\n\ttext = re.sub(r'```.*?```', r'\\x00', text, flags=re.DOTALL)\n\n\ttags = set()\n\tfor match in re.finditer(r'\\b#\\w+\\b', text):\n\t\ttags.add(replacer(match.group(0)))\n\n\ttext = re.sub(r'\\b#\\w+\\b', replacer, text)\n\n\t# now put back the code blocks\n\tfor code_block in code_blocks:\n\t\ttext = text.replace('\\x00', code_block, 1)\n\n\treturn tags, text\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttext = text.strip()\n\ttag_set = set()\n\tif text:\n\t\ttag_words = findall(r\"#\\S+\", text)\n\t\tfor tag_word in tag_words:\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(tag_word, replacer(tag_word))\n\t\t\ttag_set.add(tag_word)\n\treturn tag_set, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tin_block = False\n\tblock_start = \"\"\n\tfor match in TAG_REGEX.finditer(text):\n\t\tif not in_block:\n\t\t\tif match.group(\"block_start\"):\n\t\t\t\tin_block = True\n\t\t\t\tblock_start = match.group(\"block_start\")\n\t\t\t\tcontinue\n\t\t\ttag = match.group(\"tag\")\n\t\t\tif tag:\n\t\t\t\ttags.add(tag)\n\t\t\t\tif replacer:\n\t\t\t\t\ttext = replacer(text, match)\n\t\telse:\n\t\t\tif match.group(block_start):\n\t\t\t\tin_block = False\n\treturn tags, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttag_matches = set()\n\n\t# Find all tags in the text\n\tfor match in TAG_REGEX.finditer(text):\n\t\ttag_matches.add(match.group(1))\n\n\tif replacer:\n\t\ttext = TAG_REGEX.sub(lambda match: replacer(match.group(1)), text)\n\n\treturn tag_matches, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\n\t# some tags are not words but rather \"words with a dash\"\n\ttag_finder = re.compile(r'\\b(?:[a-z0-9]+-?)+\\b')\n\n\t# We don't want to lose any whitespace in the original text, so we must\n\t# be careful to preserve it.\n\tdef repl(match):\n\t\tif not all(c in string.ascii_letters + '-' for c in match.group(0)):\n\t\t\treturn match.group(0)\n\t\tif replacer:\n\t\t\treturn replacer(match.group(0))\n\t\telse:\n\t\t\ttags.add(match.group(0))\n\t\t\treturn match.group(0)\n\n\tnew_text = tag_finder.sub(repl, text)\n\treturn tags, new_text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\tmatches = re.findall(TAG_REGEX, text)\n\ttags = set()\n\tfor match in matches:\n\t\tif match:\n\t\t\ttags.add(match.lower())\n\t\t\ttext = text.replace(match, replacer(match))\n\treturn tags, text\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttag_set = set()\n\tnew_text = []\n\tin_code = False\n\tfor line in text.splitlines():\n\t\tif in_code:\n\t\t\tif line.lstrip().startswith('~~~'):\n\t\t\t\tin_code = False\n\t\t\tnew_text.append(line)\n\t\telif line.lstrip().startswith('~~~'):\n\t\t\tin_code = True\n\t\t\tnew_text.append(line)\n\t\telse:\n\t\t\tfor word in line.split():\n\t\t\t\tif word.startswith('#') and len(word) > 1:\n\t\t\t\t\ttag_set.add(word[1:])\n\t\t\t\t\tif replacer:\n\t\t\t\t\t\tword = replacer(word)\n\t\t\t\tnew_text.append(word)\n\t\t\tnew_text.append('')\n\treturn tag_set, '\\n'.join(new_text)\n\n\n", "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\ttext, code_blocks = extract_code_blocks(text)\n\n\t# find all the tags\n\tfor word in text.split():\n\t\tif word.startswith(\"#\") and len(word) > 1:\n\t\t\ttag = word[1:]\n\t\t\ttags.add(tag)\n\n\t\t\t# replace if needed\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(word, replacer(tag))\n\n\t# re-insert the code blocks\n\ttext = reinsert_code_blocks(text, code_blocks)\n\n\treturn tags, text\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tlines = text.split('\\n')\n\tin_code = False\n\tfor i, line in enumerate(lines):\n\t\tif '```' in line:\n\t\t\tin_code = not in_code\n\t\tif in_code:\n\t\t\tcontinue\n\t\tfor j, word in enumerate(line.split(' ')):\n\t\t\tif word.startswith('#') and len(word) > 1:\n\t\t\t\ttags.add(word[1:])\n\t\t\t\tif replacer:\n\t\t\t\t\tlines[i] = ' '.join(line.split(' ')[:j] + [replacer(word)] + line.split(' ')[j + 1:])\n\n\treturn tags, '\\n'.join(lines)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttag_words = set()\n\n\t# first, we find the code blocks, then we ignore tags inside them\n\tcode_blocks = list(find_code_blocks(text))\n\n\toffsets = []  # (start, end) for each code block\n\n\tfor code_block in code_blocks:\n\t\tstart = text.find(code_block)\n\t\tend = start + len(code_block)\n\t\toffsets.append((start, end))\n\n\t# then we find all our tags\n\tfor match in RE_TAGS.finditer(text):\n\t\ttag = match.group(1)\n\t\tindex = match.start()\n\n\t\t# check if we're in a code block\n\t\tif any(index >= start and index < end for start, end in offsets):\n\t\t\tcontinue\n\n\t\ttag_words.add(tag)\n\n\t\tif replacer:\n\t\t\ttext = text[:index] + replacer(tag) + text[index + len(tag):]\n\n\treturn tag_words, text\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\n\t# do not replace tags inside code blocks\n\tcode_blocks = []\n\tfor match in re.finditer(r'```.*?```', text, re.DOTALL):\n\t\tcode_blocks.append(match.group(0))\n\n\ttext = re.sub(r'```.*?```', r'\\x00', text, flags=re.DOTALL)\n\n\ttags = set()\n\tfor match in re.finditer(r'\\b#\\w+\\b', text):\n\t\ttags.add(replacer(match.group(0)))\n\n\ttext = re.sub(r'\\b#\\w+\\b', replacer, text)\n\n\t# now put back the code blocks\n\tfor code_block in code_blocks:\n\t\ttext = text.replace('\\x00', code_block, 1)\n\n\treturn tags, text\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttext = text.strip()\n\ttag_set = set()\n\tif text:\n\t\ttag_words = findall(r\"#\\S+\", text)\n\t\tfor tag_word in tag_words:\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(tag_word, replacer(tag_word))\n\t\t\ttag_set.add(tag_word)\n\treturn tag_set, text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\tin_block = False\n\tblock_start = \"\"\n\tfor match in TAG_REGEX.finditer(text):\n\t\tif not in_block:\n\t\t\tif match.group(\"block_start\"):\n\t\t\t\tin_block = True\n\t\t\t\tblock_start = match.group(\"block_start\")\n\t\t\t\tcontinue\n\t\t\ttag = match.group(\"tag\")\n\t\t\tif tag:\n\t\t\t\ttags.add(tag)\n\t\t\t\tif replacer:\n\t\t\t\t\ttext = replacer(text, match)\n\t\telse:\n\t\t\tif match.group(block_start):\n\t\t\t\tin_block = False\n\treturn tags, text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttag_matches = set()\n\n\t# Find all tags in the text\n\tfor match in TAG_REGEX.finditer(text):\n\t\ttag_matches.add(match.group(1))\n\n\tif replacer:\n\t\ttext = TAG_REGEX.sub(lambda match: replacer(match.group(1)), text)\n\n\treturn tag_matches, text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\n\t# some tags are not words but rather \"words with a dash\"\n\ttag_finder = re.compile(r'\\b(?:[a-z0-9]+-?)+\\b')\n\n\t# We don't want to lose any whitespace in the original text, so we must\n\t# be careful to preserve it.\n\tdef repl(match):\n\t\tif not all(c in string.ascii_letters + '-' for c in match.group(0)):\n\t\t\treturn match.group(0)\n\t\tif replacer:\n\t\t\treturn replacer(match.group(0))\n\t\telse:\n\t\t\ttags.add(match.group(0))\n\t\t\treturn match.group(0)\n\n\tnew_text = tag_finder.sub(repl, text)\n\treturn tags, new_text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\tif replacer is None:\n\t\treplacer = lambda x: x\n\tmatches = re.findall(TAG_REGEX, text)\n\ttags = set()\n\tfor match in matches:\n\t\tif match:\n\t\t\ttags.add(match.lower())\n\t\t\ttext = text.replace(match, replacer(match))\n\treturn tags, text\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttag_set = set()\n\tnew_text = []\n\tin_code = False\n\tfor line in text.splitlines():\n\t\tif in_code:\n\t\t\tif line.lstrip().startswith('~~~'):\n\t\t\t\tin_code = False\n\t\t\tnew_text.append(line)\n\t\telif line.lstrip().startswith('~~~'):\n\t\t\tin_code = True\n\t\t\tnew_text.append(line)\n\t\telse:\n\t\t\tfor word in line.split():\n\t\t\t\tif word.startswith('#') and len(word) > 1:\n\t\t\t\t\ttag_set.add(word[1:])\n\t\t\t\t\tif replacer:\n\t\t\t\t\t\tword = replacer(word)\n\t\t\t\tnew_text.append(word)\n\t\t\tnew_text.append('')\n\treturn tag_set, '\\n'.join(new_text)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n\ttags = set()\n\ttext, code_blocks = extract_code_blocks(text)\n\n\t# find all the tags\n\tfor word in text.split():\n\t\tif word.startswith(\"#\") and len(word) > 1:\n\t\t\ttag = word[1:]\n\t\t\ttags.add(tag)\n\n\t\t\t# replace if needed\n\t\t\tif replacer:\n\t\t\t\ttext = text.replace(word, replacer(tag))\n\n\t# re-insert the code blocks\n\ttext = reinsert_code_blocks(text, code_blocks)\n\n\treturn tags, text\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "process_text_links", "docstring": "Process links in text, adding some attributes and linkifying textual links.", "ques_id": "6306292352e177c0ba469f1e", "code": ["def process_text_links(text):\n\tif text is None:\n\t\treturn text\n\n\t# Do some processing on links (e.g. add 'target=\"_blank\"' attribute)\n\t# and also linkify textual links.\n\t# regex for a link like https://stackoverflow.com/questions/827557/how-do-you-validate-a-url-with-a-regular-expression-in-python\n\thtml_link_regex = re.compile(r'^(?:(?:https?):\\/\\/)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}\n", "def process_text_links(text):\n\t# Find all links in the text\n\tlinks = re.findall(r'(\\w+://[^\\s]+)', text)\n\tfor link in links:\n\t\tif '://' in link:\n\t\t\t# Linkify the link\n\t\t\ttext = text.replace(link, '<a href=\"%s\" class=\"text-link\" target=\"_blank\">%s</a>' % (link, link))\n\t\t\t\n\treturn text\n\n", "def process_text_links(text):\n\t# Scan the text for links and add attributes to them.\n\ttree = html.fromstring(text)\n\tfor link in tree.xpath('//a'):\n\t\thref = link.attrib.get('href')\n\t\tif href is None:\n\t\t\tcontinue\n\t\t# Strip trailing slashes and query strings.\n\t\thref = href.rstrip('/')\n\t\thref = href.split('?', 1)[0]\n\t\t# Ignore mailto: links.\n\t\tif href.startswith('mailto:'):\n\t\t\tcontinue\n\t\t# Ignore links to the current page.\n\t\tif href == request.path:\n\t\t\tcontinue\n\t\t# Check if the link is internal or external.\n\t\tif href.startswith('/'):\n\t\t\t# Internal link.\n\t\t\t# Check if it points to a page that exists.\n\t\t\tif Page.query.filter_by(path=href).first() is not None:\n\t\t\t\t# Page exists.\n\t\t\t\t# Add the rel and target attributes.\n\t\t\t\n", "def process_text_links(text):\n\t# Convert links to HTML\n\ttext = re.sub('(https?://[^ ]+)', '<a href=\"\\\\1\" target=\"_blank\">\\\\1</a>', text)\n\t# Convert links to text\n\ttext = re.sub('<a href=\"https?://m.youtube.com/watch\\?v=([^\"]+)\">[^<]+</a>', '<a href=\"https://www.youtube.com/watch?v=\\\\1\" target=\"_blank\" class=\"youtube\" data-youtube=\"\\\\1\">YouTube</a>', text)\n\ttext = re.sub('<a href=\"https?://www.youtube.com/watch\\?v=([^\"]+)\">[^<]+</a>', '<a href=\"https://www.youtube.com/watch?v=\\\\1\" target=\"_blank\" class=\"youtube\" data-youtube=\"\\\\1\">YouTube</a>', text)\n\ttext = re.sub('<a href=\"https?://(?:www.)?youtu.be/([^\"]+)\">[^<]+</a>', '<a href=\"\n", "def process_text_links(text):\n\tlinks = re.findall(r'\\[(?P<text>.*?)\\]\\((?P<url>.*?)\\)', text)\n\tfor link_text, link_url in links:\n\t\t# Add attributes\n\t\tif link_url.endswith('.pdf'):\n\t\t\tlink_attributes = ' target=\"_blank\" class=\"pdf-link\"'\n\t\telif link_url.startswith('http'):\n\t\t\tlink_attributes = ' target=\"_blank\"'\n\t\telse:\n\t\t\tlink_attributes = ''\n\t\t# Add link\n\t\ttext = text.replace('[%s](%s)' % (link_text, link_url), '<a%s href=\"%s\">%s</a>' % (link_attributes, link_url, link_text))\n\treturn text\n\n\n", "def process_text_links(text):\n\t# Add some attributes:\n\t# target=\"_blank\" to open in a new tab\n\t# class=\"link\" to style links consistently\n\ttext = re.sub(r'<a href=\"(.+?)\">(.+?)</a>', r'<a href=\"\\1\" target=\"_blank\" class=\"link\">\\2</a>', text)\n\t# Linkify links\n\tp = re.compile(r'(https?://[^\\s\\[\\]]+)')\n\ttext = p.sub(r'<a href=\"\\1\" target=\"_blank\" class=\"link\">\\1</a>', text)\n\treturn text\n\n", "def process_text_links(text):\n\tfrom django.utils.html import urlize\n\n\tdef replace_link(match):\n\t\tmatch = match.groupdict()\n\t\t#if match['url_text']:\n\t\t#\treturn '<a href=\"%s\" rel=\"nofollow\">%s</a>' % (match['url'], match['url_text'])\n\t\t#else:\n\t\t#\treturn '<a href=\"%s\" rel=\"nofollow\">%s</a>' % (match['url'], match['url'])\n\t\treturn urlize(match.group(0))\n\n\t#return re.sub(r'(?P<url>https?://(?:[a-z0-9][a-z0-9\\._\\-]*[a-z0-9]\\.[a-z]+|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})(?::\\d+)?(?:/[^\\s]*)?)|(?P<url_text>\\[url=(?P\n", "def process_text_links(text):\n\ttext = re.sub(r\"<a href=['\\\"]([^'\\\"]*?)['\\\"]\", r'<a href=\"\\1\" class=\"text-link\" target=\"_blank\"', text)\n\ttext = re.sub(r\"<a href=['\\\"]mailto:([^'\\\"]*?)['\\\"]\", r'<a href=\"mailto:\\1\" class=\"mail-link\"', text)\n\ttext = re.sub(r\"<a href=['\\\"]tel:([^'\\\"]*?)['\\\"]\", r'<a href=\"tel:\\1\" class=\"tel-link\"', text)\n\treturn text\n\n", "def process_text_links(text):\n\ttext = re.sub(r'(?<!\\[)\\[(.*?)\\]\\((.*?)\\)', r'[\\g<1>]({target=\"_blank\" href=\"\\g<2>\"})', text)\n\ttext = re.sub(r'<(http.*?)>', r'[\\g<1>]({target=\"_blank\" href=\"\\g<1>\"})', text)\n\treturn text\n\n", "def process_text_links(text):\n\t# TODO:\n\t# - [ ] Ignore links in code blocks.\n\t# - [ ] Ignore links inside Markdown links.\n\t# - [ ] Add `target=\"_blank\" rel=\"noopener\"` attributes to external links.\n\t# - [ ] Maybe add `target=\"_blank\"` attributes to internal links that open\n\t#       in a new tab.\n\t# - [ ] Maybe add `rel=\"nofollow\"` attributes to external links.\n\t# - [ ] Maybe add `rel=\"noopener\"` attributes to external links.\n\t# - [ ] Maybe add `rel=\"noreferrer\"` attributes to external links.\n\t# - [ ] Maybe add `rel=\"noindex\"` attributes to links that should not be\n\t#       indexed.\n\t# - [ ] Maybe add `title` attributes to external links.\n\t# - [ ] Maybe add `title` attributes to internal links.\n\t# - [ ] Maybe add `title` attributes to file links.\n\t# - [ ] Maybe add `title` attributes to image links.\n\t# - [ ] Maybe add `title` attributes to video links.\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def process_text_links(text):\n\tif text is None:\n\t\treturn text\n\n\t# Do some processing on links (e.g. add 'target=\"_blank\"' attribute)\n\t# and also linkify textual links.\n\t# regex for a link like https://stackoverflow.com/questions/827557/how-do-you-validate-a-url-with-a-regular-expression-in-python\n\thtml_link_regex = re.compile(r'^(?:(?:https?):\\/\\/)(?:\\S+(?::\\S*)?@)?(?:(?!(?:10|127)(?:\\.\\d{1,3}){3})(?!(?:169\\.254|192\\.168)(?:\\.\\d{1,3}){2})(?!172\\.(?:1[6-9]|2\\d|3[0-1])(?:\\.\\d{1,3}){2})(?:[1-9]\\d?|1\\d\\d|2[01]\\d|22[0-3])(?:\\.(?:1?\\d{1,2}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t# Find all links in the text\n\tlinks = re.findall(r'(\\w+://[^\\s]+)', text)\n\tfor link in links:\n\t\tif '://' in link:\n\t\t\t# Linkify the link\n\t\t\ttext = text.replace(link, '<a href=\"%s\" class=\"text-link\" target=\"_blank\">%s</a>' % (link, link))\n\t\t\t\n\treturn text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\t# Scan the text for links and add attributes to them.\n\ttree = html.fromstring(text)\n\tfor link in tree.xpath('//a'):\n\t\thref = link.attrib.get('href')\n\t\tif href is None:\n\t\t\tcontinue\n\t\t# Strip trailing slashes and query strings.\n\t\thref = href.rstrip('/')\n\t\thref = href.split('?', 1)[0]\n\t\t# Ignore mailto: links.\n\t\tif href.startswith('mailto:'):\n\t\t\tcontinue\n\t\t# Ignore links to the current page.\n\t\tif href == request.path:\n\t\t\tcontinue\n\t\t# Check if the link is internal or external.\n\t\tif href.startswith('/'):\n\t\t\t# Internal link.\n\t\t\t# Check if it points to a page that exists.\n\t\t\tif Page.query.filter_by(path=href).first() is not None:\n\t\t\t\t# Page exists.\n\t\t\t\t# Add the rel and target attributes.\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\t# Convert links to HTML\n\ttext = re.sub('(https?://[^ ]+)', '<a href=\"\\\\1\" target=\"_blank\">\\\\1</a>', text)\n\t# Convert links to text\n\ttext = re.sub('<a href=\"https?://m.youtube.com/watch\\?v=([^\"]+)\">[^<]+</a>', '<a href=\"https://www.youtube.com/watch?v=\\\\1\" target=\"_blank\" class=\"youtube\" data-youtube=\"\\\\1\">YouTube</a>', text)\n\ttext = re.sub('<a href=\"https?://www.youtube.com/watch\\?v=([^\"]+)\">[^<]+</a>', '<a href=\"https://www.youtube.com/watch?v=\\\\1\" target=\"_blank\" class=\"youtube\" data-youtube=\"\\\\1\">YouTube</a>', text)\n\ttext = re.sub('<a href=\"https?://(?:www.)?youtu.be/([^\"]+)\">[^<]+</a>', '<a href=\"\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\tlinks = re.findall(r'\\[(?P<text>.*?)\\]\\((?P<url>.*?)\\)', text)\n\tfor link_text, link_url in links:\n\t\t# Add attributes\n\t\tif link_url.endswith('.pdf'):\n\t\t\tlink_attributes = ' target=\"_blank\" class=\"pdf-link\"'\n\t\telif link_url.startswith('http'):\n\t\t\tlink_attributes = ' target=\"_blank\"'\n\t\telse:\n\t\t\tlink_attributes = ''\n\t\t# Add link\n\t\ttext = text.replace('[%s](%s)' % (link_text, link_url), '<a%s href=\"%s\">%s</a>' % (link_attributes, link_url, link_text))\n\treturn text\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\t# Add some attributes:\n\t# target=\"_blank\" to open in a new tab\n\t# class=\"link\" to style links consistently\n\ttext = re.sub(r'<a href=\"(.+?)\">(.+?)</a>', r'<a href=\"\\1\" target=\"_blank\" class=\"link\">\\2</a>', text)\n\t# Linkify links\n\tp = re.compile(r'(https?://[^\\s\\[\\]]+)')\n\ttext = p.sub(r'<a href=\"\\1\" target=\"_blank\" class=\"link\">\\1</a>', text)\n\treturn text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\tfrom django.utils.html import urlize\n\n\tdef replace_link(match):\n\t\tmatch = match.groupdict()\n\t\t#if match['url_text']:\n\t\t#\treturn '<a href=\"%s\" rel=\"nofollow\">%s</a>' % (match['url'], match['url_text'])\n\t\t#else:\n\t\t#\treturn '<a href=\"%s\" rel=\"nofollow\">%s</a>' % (match['url'], match['url'])\n\t\treturn urlize(match.group(0))\n\n\t#return re.sub(r'(?P<url>https?://(?:[a-z0-9][a-z0-9\\._\\-]*[a-z0-9]\\.[a-z]+|\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})(?::\\d+)?(?:/[^\\s]*)?)|(?P<url_text>\\[url=(?P\n", "is_pass": false, "return_code": 1}, {"generate_code": "def process_text_links(text):\n\ttext = re.sub(r\"<a href=['\\\"]([^'\\\"]*?)['\\\"]\", r'<a href=\"\\1\" class=\"text-link\" target=\"_blank\"', text)\n\ttext = re.sub(r\"<a href=['\\\"]mailto:([^'\\\"]*?)['\\\"]\", r'<a href=\"mailto:\\1\" class=\"mail-link\"', text)\n\ttext = re.sub(r\"<a href=['\\\"]tel:([^'\\\"]*?)['\\\"]\", r'<a href=\"tel:\\1\" class=\"tel-link\"', text)\n\treturn text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\ttext = re.sub(r'(?<!\\[)\\[(.*?)\\]\\((.*?)\\)', r'[\\g<1>]({target=\"_blank\" href=\"\\g<2>\"})', text)\n\ttext = re.sub(r'<(http.*?)>', r'[\\g<1>]({target=\"_blank\" href=\"\\g<1>\"})', text)\n\treturn text\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def process_text_links(text):\n\t# TODO:\n\t# - [ ] Ignore links in code blocks.\n\t# - [ ] Ignore links inside Markdown links.\n\t# - [ ] Add `target=\"_blank\" rel=\"noopener\"` attributes to external links.\n\t# - [ ] Maybe add `target=\"_blank\"` attributes to internal links that open\n\t#       in a new tab.\n\t# - [ ] Maybe add `rel=\"nofollow\"` attributes to external links.\n\t# - [ ] Maybe add `rel=\"noopener\"` attributes to external links.\n\t# - [ ] Maybe add `rel=\"noreferrer\"` attributes to external links.\n\t# - [ ] Maybe add `rel=\"noindex\"` attributes to links that should not be\n\t#       indexed.\n\t# - [ ] Maybe add `title` attributes to external links.\n\t# - [ ] Maybe add `title` attributes to internal links.\n\t# - [ ] Maybe add `title` attributes to file links.\n\t# - [ ] Maybe add `title` attributes to image links.\n\t# - [ ] Maybe add `title` attributes to video links.\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "fetch_content_type", "docstring": "Fetch the HEAD of the remote url to determine the content type.", "ques_id": "6306292652e177c0ba469f34", "code": ["def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, headers={\"User-Agent\": USER_AGENT})\n\t\tresponse.raise_for_status()\n\texcept (requests.exceptions.RequestException, HTTPError) as e:\n\t\tprint(e)\n\t\treturn None\n\telse:\n\t\treturn response.headers.get(\"Content-Type\")\n\t\t\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\theaders = {\n\t\t'User-Agent': USER_AGENT\n\t}\n\ttry:\n\t\tresponse = requests.head(url, headers=headers)\n\texcept (ConnectionError, Timeout):\n\t\treturn None\n\treturn response.headers.get('Content-Type')\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, headers={'User-Agent': USER_AGENT})\n\texcept requests.exceptions.ConnectionError:\n\t\t# No internet connection.\n\t\treturn None\n\telse:\n\t\treturn response.headers.get('Content-Type')\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\t# Set the head of the request through the URL and USER_AGENT.\n\t\treq = urllib.request.Request(\n\t\t\turl,\n\t\t\tdata=None,\n\t\t\theaders={\n\t\t\t\t'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n\t\t\t}\n\t\t)\n\n\t\t# Open the URL and read the content.\n\t\twith urllib.request.urlopen(req) as response:\n\t\t\t# Get the content type of the URL.\n\t\t\tcontent_type = response.getheader('Content-Type')\n\t\t\treturn content_type\n\texcept urllib.error.HTTPError:\n\t\treturn None\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\theaders: Dict[str, str] = {\n\t\t\"User-Agent\": USER_AGENT\n\t}\n\trequest: Request = Request(url, headers=headers)\n\ttry:\n\t\twith urlopen(request) as response:\n\t\t\tif response.code == 200:\n\t\t\t\tcontent_type: str = response.getheader(\"Content-Type\")\n\t\t\t\treturn content_type\n\t\t\telse:\n\t\t\t\treturn None\n\texcept HTTPError:\n\t\treturn None\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\trequest = urllib.request.Request(url, headers={'User-Agent': USER_AGENT})\n\ttry:\n\t\twith urllib.request.urlopen(request, timeout=10) as response:\n\t\t\treturn response.headers.get('Content-Type')\n\texcept (urllib.error.HTTPError, urllib.error.URLError):\n\t\treturn None\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = urlopen(url)\n\t\tif response.code == 200:\n\t\t\treturn response.getheader('Content-Type')\n\texcept Exception:\n\t\tpass\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\trequest = urllib.request.Request(url, headers={'User-Agent': USER_AGENT})\n\ttry:\n\t\twith urllib.request.urlopen(request) as response:\n\t\t\treturn response.getheader('Content-Type')\n\texcept Exception as e:\n\t\tprint(\"Failed to get content type: {}\".format(e))\n\t\treturn None\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\theaders = {'User-Agent': USER_AGENT}\n\trequest = urllib.request.Request(url, headers=headers)\n\ttry:\n\t\tresponse = urllib.request.urlopen(request, timeout=3)\n\texcept (urllib.error.URLError, urllib.error.HTTPError):\n\t\treturn None\n\telse:\n\t\treturn response.headers.get_content_type()\n\n\n", "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, headers=USER_AGENT)\n\t\treturn response.headers.get(\"content-type\")\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, headers={\"User-Agent\": USER_AGENT})\n\t\tresponse.raise_for_status()\n\texcept (requests.exceptions.RequestException, HTTPError) as e:\n\t\tprint(e)\n\t\treturn None\n\telse:\n\t\treturn response.headers.get(\"Content-Type\")\n\t\t\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\theaders = {\n\t\t'User-Agent': USER_AGENT\n\t}\n\ttry:\n\t\tresponse = requests.head(url, headers=headers)\n\texcept (ConnectionError, Timeout):\n\t\treturn None\n\treturn response.headers.get('Content-Type')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, headers={'User-Agent': USER_AGENT})\n\texcept requests.exceptions.ConnectionError:\n\t\t# No internet connection.\n\t\treturn None\n\telse:\n\t\treturn response.headers.get('Content-Type')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\t# Set the head of the request through the URL and USER_AGENT.\n\t\treq = urllib.request.Request(\n\t\t\turl,\n\t\t\tdata=None,\n\t\t\theaders={\n\t\t\t\t'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.47 Safari/537.36'\n\t\t\t}\n\t\t)\n\n\t\t# Open the URL and read the content.\n\t\twith urllib.request.urlopen(req) as response:\n\t\t\t# Get the content type of the URL.\n\t\t\tcontent_type = response.getheader('Content-Type')\n\t\t\treturn content_type\n\texcept urllib.error.HTTPError:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\theaders: Dict[str, str] = {\n\t\t\"User-Agent\": USER_AGENT\n\t}\n\trequest: Request = Request(url, headers=headers)\n\ttry:\n\t\twith urlopen(request) as response:\n\t\t\tif response.code == 200:\n\t\t\t\tcontent_type: str = response.getheader(\"Content-Type\")\n\t\t\t\treturn content_type\n\t\t\telse:\n\t\t\t\treturn None\n\texcept HTTPError:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\trequest = urllib.request.Request(url, headers={'User-Agent': USER_AGENT})\n\ttry:\n\t\twith urllib.request.urlopen(request, timeout=10) as response:\n\t\t\treturn response.headers.get('Content-Type')\n\texcept (urllib.error.HTTPError, urllib.error.URLError):\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = urlopen(url)\n\t\tif response.code == 200:\n\t\t\treturn response.getheader('Content-Type')\n\texcept Exception:\n\t\tpass\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\trequest = urllib.request.Request(url, headers={'User-Agent': USER_AGENT})\n\ttry:\n\t\twith urllib.request.urlopen(request) as response:\n\t\t\treturn response.getheader('Content-Type')\n\texcept Exception as e:\n\t\tprint(\"Failed to get content type: {}\".format(e))\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\theaders = {'User-Agent': USER_AGENT}\n\trequest = urllib.request.Request(url, headers=headers)\n\ttry:\n\t\tresponse = urllib.request.urlopen(request, timeout=3)\n\texcept (urllib.error.URLError, urllib.error.HTTPError):\n\t\treturn None\n\telse:\n\t\treturn response.headers.get_content_type()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def fetch_content_type(url: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.head(url, headers=USER_AGENT)\n\t\treturn response.headers.get(\"content-type\")\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/text.py", "project": "jaywink/federation", "name": "test_tag", "docstring": "Test a word whether it could be accepted as a tag.", "ques_id": "6306292a52e177c0ba469f41", "code": ["def test_tag(tag: str) -> bool:\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\treturn set(tag) <= LEEGAL_TAG_CHARS\n\n", "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n\n", "def test_tag(tag: str) -> bool:\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif not char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n\n", "def test_tag(tag: str) -> bool:\n\tfor c in tag:\n\t\tif (not c in LEGAL_TAG_CHARS):\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif char in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\tif tag:\n\t\tfor char in tag:\n\t\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\t\treturn False\n\treturn True\n\n", "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def test_tag(tag: str) -> bool:\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\treturn set(tag) <= LEEGAL_TAG_CHARS\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif not char in LEEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tfor c in tag:\n\t\tif (not c in LEGAL_TAG_CHARS):\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif char in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tfor char in LEEGAL_TAG_CHARS:\n\t\tif char in tag:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tif tag:\n\t\tfor char in tag:\n\t\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def test_tag(tag: str) -> bool:\n\tfor char in tag:\n\t\tif char not in LEGAL_TAG_CHARS:\n\t\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "xml_children_as_dict", "docstring": "Turn the children of node <xml> into a dict, keyed by tag name.\n\nThis is only a shallow conversation - child nodes are not recursively processed.", "ques_id": "6306298b52e177c0ba469fdc", "code": ["def xml_children_as_dict(node):\n\treturn dict((child.tag, child) for child in node.getchildren())\n\n\n", "def xml_children_as_dict(node):\n\tchildren = {}\n\tfor child in node:\n\t\tif child.tag not in children:\n\t\t\tchildren[child.tag] = []\n\t\tchildren[child.tag].append(child)\n\treturn children\n\n", "def xml_children_as_dict(node):\n\treturn dict((c.tag, c.text) for c in node.getchildren())\n\n", "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == child.ELEMENT_NODE:\n\t\t\ttag = child.tagName\n\t\t\tif tag in d:\n\t\t\t\tprint('Warning: tag \"%s\" already exists in dict, overwriting.' % tag)\n\t\t\td[tag] = child\n\treturn d\n\n", "def xml_children_as_dict(node):\n\treturn dict((child.tag, child) for child in node)\n\n", "def xml_children_as_dict(node):\n\td = {}\n\tfor c in node.getchildren():\n\t\tif c.tag not in d:\n\t\t\td[c.tag] = []\n\t\td[c.tag].append(c)\n\treturn d\n\n", "def xml_children_as_dict(node):\n\tassert node.tag == \"xml\"\n\treturn dict([(e.tag, e) for e in node.getchildren()])\n\n", "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == child.ELEMENT_NODE:\n\t\t\td[child.tagName] = child\n\treturn d\n\n", "def xml_children_as_dict(node):\n\tchildren = node.getchildren()\n\tif len(children) == 0:\n\t\treturn None\n\td = dict()\n\tfor c in children:\n\t\tif c.tag not in d:\n\t\t\td[c.tag] = []\n\t\td[c.tag].append(c)\n\treturn d\n\n", "def xml_children_as_dict(node):\n\tchildren = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType != child.ELEMENT_NODE:\n\t\t\tcontinue\n\t\tif child.tagName in children:\n\t\t\tif not isinstance(children[child.tagName], list):\n\t\t\t\tchildren[child.tagName] = [children[child.tagName]]\n\t\t\tchildren[child.tagName].append(child)\n\t\telse:\n\t\t\tchildren[child.tagName] = child\n\treturn children\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def xml_children_as_dict(node):\n\treturn dict((child.tag, child) for child in node.getchildren())\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\tchildren = {}\n\tfor child in node:\n\t\tif child.tag not in children:\n\t\t\tchildren[child.tag] = []\n\t\tchildren[child.tag].append(child)\n\treturn children\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\treturn dict((c.tag, c.text) for c in node.getchildren())\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == child.ELEMENT_NODE:\n\t\t\ttag = child.tagName\n\t\t\tif tag in d:\n\t\t\t\tprint('Warning: tag \"%s\" already exists in dict, overwriting.' % tag)\n\t\t\td[tag] = child\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\treturn dict((child.tag, child) for child in node)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor c in node.getchildren():\n\t\tif c.tag not in d:\n\t\t\td[c.tag] = []\n\t\td[c.tag].append(c)\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\tassert node.tag == \"xml\"\n\treturn dict([(e.tag, e) for e in node.getchildren()])\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\td = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType == child.ELEMENT_NODE:\n\t\t\td[child.tagName] = child\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\tchildren = node.getchildren()\n\tif len(children) == 0:\n\t\treturn None\n\td = dict()\n\tfor c in children:\n\t\tif c.tag not in d:\n\t\t\td[c.tag] = []\n\t\td[c.tag].append(c)\n\treturn d\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def xml_children_as_dict(node):\n\tchildren = {}\n\tfor child in node.childNodes:\n\t\tif child.nodeType != child.ELEMENT_NODE:\n\t\t\tcontinue\n\t\tif child.tagName in children:\n\t\t\tif not isinstance(children[child.tagName], list):\n\t\t\t\tchildren[child.tagName] = [children[child.tagName]]\n\t\t\tchildren[child.tagName].append(child)\n\t\telse:\n\t\t\tchildren[child.tagName] = child\n\treturn children\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/entities/diaspora/mappers.py", "project": "jaywink/federation", "name": "check_sender_and_entity_handle_match", "docstring": "Ensure that sender and entity handles match.\n\nBasically we've already verified the sender is who they say when receiving the payload. However, the sender might\nbe trying to set another author in the payload itself, since Diaspora has the sender in both the payload headers\nAND the object. We must ensure they're the same.", "ques_id": "6306299052e177c0ba469fe8", "code": ["def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == \"\":\n\t\treturn True\n\telse:\n\t\ttry:\n\t\t\tif sender_handle == entity_handle:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept:\n\t\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\treturn True if sender_handle == entity_handle else False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\treturn sender_handle == entity_handle\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == \"\":\n\t\treturn True\n\telse:\n\t\ttry:\n\t\t\tif sender_handle == entity_handle:\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept:\n\t\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\treturn True if sender_handle == entity_handle else False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\treturn sender_handle == entity_handle\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def check_sender_and_entity_handle_match(sender_handle, entity_handle):\n\tif sender_handle == entity_handle:\n\t\treturn True\n\telse:\n\t\treturn False\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "federation/hostmeta/generators.py", "project": "jaywink/federation", "name": "get_nodeinfo_well_known_document", "docstring": "Generate a NodeInfo .well-known document.\n\nSee spec: http://nodeinfo.diaspora.software\n\n:arg url: The full base url with protocol, ie https://example.com\n:arg document_path: Custom NodeInfo document path if supplied (optional)\n:returns: dict", "ques_id": "630629b952e177c0ba46a043", "code": ["def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\turl = url.rstrip('/')\n\tnodeinfo_path = url + document_path\n\treturn {'url': url, 'document_path': document_path}\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path or '/.well-known/nodeinfo',\n\t\t'get_document': functools.partial(get_nodeinfo_well_known_document_content, url, document_path),\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = '/nodeinfo/2.0'\n\treturn {\n\t\t'url': url.rstrip('/') + document_path,\n\t\t'document_path': document_path\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tresponse = requests.get(url)\n\tnodeinfo = response.json()\n\tif document_path:\n\t\tnodeinfo_url = urljoin(url, document_path)\n\telse:\n\t\tnodeinfo_url = urljoin(url, nodeinfo[\"links\"][0][\"href\"])\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t\t\"nodeinfo\": nodeinfo,\n\t\t\"nodeinfo_url\": nodeinfo_url,\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = urljoin(url, \"/.well-known/nodeinfo\")\n\n\tresponse = requests.get(document_path, timeout=5)\n\tresponse.raise_for_status()\n\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t\t\"response\": response\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif not document_path:\n\t\tdocument_path = \"%s/%s\" % (url, NODEINFO_WELL_KNOWN_PATH)\n\t\n\treturn get_document(document_path)\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = \"/.well-known/nodeinfo\"\n\t\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\turl = url.rstrip('/')\n\tif document_path is None:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\treturn {'url': url,\n\t\t\t'document_path': document_path}\n\n\n", "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = DEFAULT_WELL_KNOWN_PATH\n\n\tnodeinfo2_document_path = os.path.join(document_path, \"nodeinfo\", \"2.0\")\n\tnodeinfo2_url = urljoin(url, nodeinfo2_document_path)\n\n\treturn {\n\t\t\"url\": nodeinfo2_url,\n\t\t\"document_path\": nodeinfo2_document_path,\n\t}\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\turl = url.rstrip('/')\n\tnodeinfo_path = url + document_path\n\treturn {'url': url, 'document_path': document_path}\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\treturn {\n\t\t'url': url,\n\t\t'document_path': document_path or '/.well-known/nodeinfo',\n\t\t'get_document': functools.partial(get_nodeinfo_well_known_document_content, url, document_path),\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = '/nodeinfo/2.0'\n\treturn {\n\t\t'url': url.rstrip('/') + document_path,\n\t\t'document_path': document_path\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tresponse = requests.get(url)\n\tnodeinfo = response.json()\n\tif document_path:\n\t\tnodeinfo_url = urljoin(url, document_path)\n\telse:\n\t\tnodeinfo_url = urljoin(url, nodeinfo[\"links\"][0][\"href\"])\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t\t\"nodeinfo\": nodeinfo,\n\t\t\"nodeinfo_url\": nodeinfo_url,\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = urljoin(url, \"/.well-known/nodeinfo\")\n\n\tresponse = requests.get(document_path, timeout=5)\n\tresponse.raise_for_status()\n\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path,\n\t\t\"response\": response\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif not document_path:\n\t\tdocument_path = \"%s/%s\" % (url, NODEINFO_WELL_KNOWN_PATH)\n\t\n\treturn get_document(document_path)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = \"/.well-known/nodeinfo\"\n\t\n\treturn {\n\t\t\"url\": url,\n\t\t\"document_path\": document_path\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\turl = url.rstrip('/')\n\tif document_path is None:\n\t\tdocument_path = '/.well-known/nodeinfo'\n\treturn {'url': url,\n\t\t\t'document_path': document_path}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nodeinfo_well_known_document(url, document_path=None):\n\tif document_path is None:\n\t\tdocument_path = DEFAULT_WELL_KNOWN_PATH\n\n\tnodeinfo2_document_path = os.path.join(document_path, \"nodeinfo\", \"2.0\")\n\tnodeinfo2_url = urljoin(url, nodeinfo2_document_path)\n\n\treturn {\n\t\t\"url\": nodeinfo2_url,\n\t\t\"document_path\": nodeinfo2_document_path,\n\t}\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/protocols/diaspora/signatures.py", "project": "jaywink/federation", "name": "verify_relayable_signature", "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.", "ques_id": "630629d052e177c0ba46a0a1", "code": ["def verify_relayable_signature(public_key, doc, signature):\n\t#\n\t# Verify the signature of the document\n\t#\n\t# If the signature is valid, this will also verify the digest of the\n\t# document.  Otherwise, the signature wasn't valid, so we don't need to\n\t# verify the digest.\n\tif not xmlsec.check_signature(doc, public_key, base64.b64decode(signature)):\n\t\treturn False\n\n\t#\n\t# Verify the digest of the document.\n\t#\n\t# Since the signature was valid, we know that the digest of the document\n\t# was also valid.  This is just a sanity check to be sure that the\n\t# signature element was not tampered with in any way.\n\t#\n\t# Pull the digest from the signature element.\n\tsignature_element = doc.getElementsByTagNameNS(xmlsec.DSigNs, \"Signature\")[0]\n\tdigest_value = signature_element.getElementsByTagNameNS(xmlsec.DSigNs, \"DigestValue\")[0]\n\tdigest = base64.b64decode(digest_value.child\n", "def verify_relayable_signature(public_key, doc, signature):\n\tif isinstance(public_key, basestring):\n\t\tpublic_key = RSA.importKey(public_key)\n\t\t\n\tsignature_node = doc.find('{%s}Signature' % XMLConstants.XMLDSIG_NAMESPACE)\n\tif signature_node is None:\n\t\traise Exception(\"XML signature not found\")\n\t\n\tsigned_info_node = signature_node.find('{%s}SignedInfo' % XMLConstants.XMLDSIG_NAMESPACE)\n\tif signed_info_node is None:\n\t\traise Exception(\"XML signature SignedInfo not found\")\n\t\n\tsignature_method_node = signed_info_node.find('{%s}SignatureMethod' % XMLConstants.XMLDSIG_NAMESPACE)\n\tif signature_method_node is None:\n\t\traise Exception(\"XML signature SignatureMethod not found\")\n\t\n\tsignature_method = signature_method_node.get('Algorithm')\n\tif signature_method != XMLConstants.RSA_SHA1_SIGNATURE_\n", "def verify_relayable_signature(public_key, doc, signature):\n\t# The SAML spec says the Signature is inside a Signature element.\n\t# (this is a saml Response message)\n\tsignature_element = doc.getElementsByTagNameNS(\n\t\t\"http://www.w3.org/2000/09/xmldsig#\", \"Signature\")\n\tassert len(signature_element) == 1\n\tsignature_element = signature_element[0]\n\t# The signature itself is the first child of the Signature element.\n\tsignature_element = signature_element.firstChild\n\t# The signature is base64-encoded.\n\tsignature_element = base64.b64decode(signature_element.nodeValue)\n\tassert len(signature_element) == 256\n\t# The signature is an RSA signature over the SHA-1 digest of the\n\t# concatenation of the two signed elements.\n\tdigest = sha1(str(doc.getElementsByTagNameNS(\n\t\t\"urn:oasis:names:tc:SAML:2.0:assertion\", \"Issuer\"))\n\t\t+ str(doc.\n", "def verify_relayable_signature(public_key, doc, signature):\n\tcert = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, public_key)\n\tcert_pub_key = cert.get_pubkey()\n\tcert_pub_key.verify_init()\n\tcert_pub_key.verify_update(doc)\n\treturn cert_pub_key.verify_final(signature)\n\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\treturn verify(public_key, doc, signature)\n\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\ttry:\n\t\trelayable_signed_elements = doc.xpath('.//samlp:Response', namespaces={'samlp': 'urn:oasis:names:tc:SAML:2.0:protocol'})\n\t\trelayable_signed_elements += doc.xpath('.//saml:Assertion', namespaces={'saml': 'urn:oasis:names:tc:SAML:2.0:assertion'})\n\texcept:\n\t\trelayable_signed_elements = []\n\t\tpass\n\t#print(public_key)\n\tfor element in relayable_signed_elements:\n\t\ttry:\n\t\t\txmlsec.tree.add_ids(element, [\"ID\"])\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\txmlsec.tree.add_ids(element, [\"id\"])\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\txmlsec.tree.add_ids(element, [\"Id\"])\n\t\texcept:\n\t\t\tpass\n\t\t\n", "def verify_relayable_signature(public_key, doc, signature):\n\t# Find the Signature element.\n\tsignature_element = doc.find('.//{http://www.w3.org/2000/09/xmldsig#}Signature')\n\tif signature_element is None:\n\t\traise Exception('No Signature element found in the document')\n\n\t# Create a new XML document that has the same nodes, but without the\n\t# Signature element.\n\tdoc2 = copy.deepcopy(doc)\n\tsignature_element.getparent().remove(signature_element)\n\n\t# Canonicalize the document and serialize it as XML.\n\tcanonicalized_doc = etree.tostring(doc2, method='c14n')\n\n\t# Verify the signature on the document.\n\tpublic_key.verify(signature, canonicalized_doc)\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\txmlsec.initialize()\n\txmlsec.add_ids(doc, [\"ID\"])\n\tctx = xmlsec.SignatureContext()\n\tkey = xmlsec.Key.from_memory(public_key, xmlsec.KeyFormat.PEM, None)\n\tkey.load_cert_from_memory(public_key, xmlsec.KeyFormat.PEM)\n\tsignature_node = ctx.find_signature(doc.getroot())\n\tctx.key = key\n\tctx.verify(signature_node)\n\txmlsec.shutdown()\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\t# TODO: Implement XML Signature verification\n\treturn True\n\n", "def verify_relayable_signature(public_key, doc, signature):\n\tdoc.xpath('//*[@id]')\n\tsigned_elements = [node for node in doc.iter() if node.get('id')]\n\n\tfor element in signed_elements:\n\t\tcanonical_element = canonicalize_element(element)\n\t\ttry:\n\t\t\tpublic_key.verify(signature, canonical_element)\n\t\texcept:\n\t\t\traise SignatureError('Invalid signature')\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t#\n\t# Verify the signature of the document\n\t#\n\t# If the signature is valid, this will also verify the digest of the\n\t# document.  Otherwise, the signature wasn't valid, so we don't need to\n\t# verify the digest.\n\tif not xmlsec.check_signature(doc, public_key, base64.b64decode(signature)):\n\t\treturn False\n\n\t#\n\t# Verify the digest of the document.\n\t#\n\t# Since the signature was valid, we know that the digest of the document\n\t# was also valid.  This is just a sanity check to be sure that the\n\t# signature element was not tampered with in any way.\n\t#\n\t# Pull the digest from the signature element.\n\tsignature_element = doc.getElementsByTagNameNS(xmlsec.DSigNs, \"Signature\")[0]\n\tdigest_value = signature_element.getElementsByTagNameNS(xmlsec.DSigNs, \"DigestValue\")[0]\n\tdigest = base64.b64decode(digest_value.child\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tif isinstance(public_key, basestring):\n\t\tpublic_key = RSA.importKey(public_key)\n\t\t\n\tsignature_node = doc.find('{%s}Signature' % XMLConstants.XMLDSIG_NAMESPACE)\n\tif signature_node is None:\n\t\traise Exception(\"XML signature not found\")\n\t\n\tsigned_info_node = signature_node.find('{%s}SignedInfo' % XMLConstants.XMLDSIG_NAMESPACE)\n\tif signed_info_node is None:\n\t\traise Exception(\"XML signature SignedInfo not found\")\n\t\n\tsignature_method_node = signed_info_node.find('{%s}SignatureMethod' % XMLConstants.XMLDSIG_NAMESPACE)\n\tif signature_method_node is None:\n\t\traise Exception(\"XML signature SignatureMethod not found\")\n\t\n\tsignature_method = signature_method_node.get('Algorithm')\n\tif signature_method != XMLConstants.RSA_SHA1_SIGNATURE_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t# The SAML spec says the Signature is inside a Signature element.\n\t# (this is a saml Response message)\n\tsignature_element = doc.getElementsByTagNameNS(\n\t\t\"http://www.w3.org/2000/09/xmldsig#\", \"Signature\")\n\tassert len(signature_element) == 1\n\tsignature_element = signature_element[0]\n\t# The signature itself is the first child of the Signature element.\n\tsignature_element = signature_element.firstChild\n\t# The signature is base64-encoded.\n\tsignature_element = base64.b64decode(signature_element.nodeValue)\n\tassert len(signature_element) == 256\n\t# The signature is an RSA signature over the SHA-1 digest of the\n\t# concatenation of the two signed elements.\n\tdigest = sha1(str(doc.getElementsByTagNameNS(\n\t\t\"urn:oasis:names:tc:SAML:2.0:assertion\", \"Issuer\"))\n\t\t+ str(doc.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tcert = OpenSSL.crypto.load_certificate(OpenSSL.crypto.FILETYPE_PEM, public_key)\n\tcert_pub_key = cert.get_pubkey()\n\tcert_pub_key.verify_init()\n\tcert_pub_key.verify_update(doc)\n\treturn cert_pub_key.verify_final(signature)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\treturn verify(public_key, doc, signature)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\ttry:\n\t\trelayable_signed_elements = doc.xpath('.//samlp:Response', namespaces={'samlp': 'urn:oasis:names:tc:SAML:2.0:protocol'})\n\t\trelayable_signed_elements += doc.xpath('.//saml:Assertion', namespaces={'saml': 'urn:oasis:names:tc:SAML:2.0:assertion'})\n\texcept:\n\t\trelayable_signed_elements = []\n\t\tpass\n\t#print(public_key)\n\tfor element in relayable_signed_elements:\n\t\ttry:\n\t\t\txmlsec.tree.add_ids(element, [\"ID\"])\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\txmlsec.tree.add_ids(element, [\"id\"])\n\t\texcept:\n\t\t\tpass\n\t\ttry:\n\t\t\txmlsec.tree.add_ids(element, [\"Id\"])\n\t\texcept:\n\t\t\tpass\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t# Find the Signature element.\n\tsignature_element = doc.find('.//{http://www.w3.org/2000/09/xmldsig#}Signature')\n\tif signature_element is None:\n\t\traise Exception('No Signature element found in the document')\n\n\t# Create a new XML document that has the same nodes, but without the\n\t# Signature element.\n\tdoc2 = copy.deepcopy(doc)\n\tsignature_element.getparent().remove(signature_element)\n\n\t# Canonicalize the document and serialize it as XML.\n\tcanonicalized_doc = etree.tostring(doc2, method='c14n')\n\n\t# Verify the signature on the document.\n\tpublic_key.verify(signature, canonicalized_doc)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\txmlsec.initialize()\n\txmlsec.add_ids(doc, [\"ID\"])\n\tctx = xmlsec.SignatureContext()\n\tkey = xmlsec.Key.from_memory(public_key, xmlsec.KeyFormat.PEM, None)\n\tkey.load_cert_from_memory(public_key, xmlsec.KeyFormat.PEM)\n\tsignature_node = ctx.find_signature(doc.getroot())\n\tctx.key = key\n\tctx.verify(signature_node)\n\txmlsec.shutdown()\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\t# TODO: Implement XML Signature verification\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def verify_relayable_signature(public_key, doc, signature):\n\tdoc.xpath('//*[@id]')\n\tsigned_elements = [node for node in doc.iter() if node.get('id')]\n\n\tfor element in signed_elements:\n\t\tcanonical_element = canonicalize_element(element)\n\t\ttry:\n\t\t\tpublic_key.verify(signature, canonical_element)\n\t\texcept:\n\t\t\traise SignatureError('Invalid signature')\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "parse_diaspora_webfinger", "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html", "ques_id": "630629e052e177c0ba46a0c4", "code": ["def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\twf = json.loads(document)\n\t\treturn wf\n\texcept Exception as e:\n\t\traise ValueError(document) from e\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\tdata = json.loads(document)\n\thcard_url = data[\"links\"][0][\"href\"]\n\treturn {\"hcard_url\": hcard_url}\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\tparsed_webfinger = json.loads(document)\n\thcard_url = parsed_webfinger['links'][0]['href']\n\n\treturn {'hcard_url': hcard_url}\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger = json.loads(document)\n\treturn {\n\t\t\"hcard_url\": webfinger[\"links\"][0][\"href\"],\n\t\t\"guid\": webfinger[\"subject\"].split(\":\")[-1]\n\t}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger = json.loads(document)\n\treturn webfinger\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\tjson_document = json.loads(document)\n\tlinks = json_document.get(\"links\")\n\tfor link in links:\n\t\tif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n\t\t\treturn link.get(\"href\")\n\treturn None\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\twebfinger = json.loads(document)\n\t\treturn webfinger\n\texcept (ValueError, TypeError):\n\t\treturn {}\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger = json.loads(document)\n\thcard_url = webfinger[\"links\"][0][\"href\"]\n\treturn {\"hcard_url\": hcard_url}\n\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\tdocument = json.loads(document)\n\thref = document['links'][0]['href']\n\tif href[-1] != '/':\n\t\thref += '/'\n\treturn {'hcard_url': href}\n\n", "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger_json = json.loads(document)\n\treturn webfinger_json\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\twf = json.loads(document)\n\t\treturn wf\n\texcept Exception as e:\n\t\traise ValueError(document) from e\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\tdata = json.loads(document)\n\thcard_url = data[\"links\"][0][\"href\"]\n\treturn {\"hcard_url\": hcard_url}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\tparsed_webfinger = json.loads(document)\n\thcard_url = parsed_webfinger['links'][0]['href']\n\n\treturn {'hcard_url': hcard_url}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger = json.loads(document)\n\treturn {\n\t\t\"hcard_url\": webfinger[\"links\"][0][\"href\"],\n\t\t\"guid\": webfinger[\"subject\"].split(\":\")[-1]\n\t}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger = json.loads(document)\n\treturn webfinger\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\tjson_document = json.loads(document)\n\tlinks = json_document.get(\"links\")\n\tfor link in links:\n\t\tif link.get(\"rel\") == \"http://microformats.org/profile/hcard\":\n\t\t\treturn link.get(\"href\")\n\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\ttry:\n\t\twebfinger = json.loads(document)\n\t\treturn webfinger\n\texcept (ValueError, TypeError):\n\t\treturn {}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger = json.loads(document)\n\thcard_url = webfinger[\"links\"][0][\"href\"]\n\treturn {\"hcard_url\": hcard_url}\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\tdocument = json.loads(document)\n\thref = document['links'][0]['href']\n\tif href[-1] != '/':\n\t\thref += '/'\n\treturn {'hcard_url': href}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def parse_diaspora_webfinger(document: str) -> Dict:\n\twebfinger_json = json.loads(document)\n\treturn webfinger_json\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "try_retrieve_webfinger_document", "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.", "ques_id": "630629e152e177c0ba46a0d1", "code": ["def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept ValueError:\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tif not is_webfinger_handle(handle):\n\t\treturn None\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept (ConnectionError, ConnectTimeout, ReadTimeout):\n\t\tlogger.debug(\"Could not retrieve webfinger document for handle {}\".format(handle), exc_info=True)\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tif not handle:\n\t\treturn None\n\n\t# TODO: Try to use the webfinger document to find the right URL to use.\n\t# TODO: Maybe we should also try to find the right URL to use.\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept:\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t# TODO: We could consider using an LRU cache here.\n\ttry:\n\t\treturn get_webfinger_document(handle)\n\texcept (DNSLookupError, HTTPError, Timeout, URLError):\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn requests.get(webfinger_url(handle)).text\n\texcept Exception as e:\n\t\tlogger.info(\"Failed to retrieve webfinger document for {}: {}\".format(handle, e))\n\t\treturn None\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tretrieve_url = \"https://{}/.well-known/webfinger?resource=acct:{}\".format(\n\t\tZOTERO_DOMAIN, handle\n\t)\n\ttry:\n\t\tresponse = requests.get(retrieve_url)\n\t\tif response.status_code == 200:\n\t\t\treturn json.loads(response.text)[\"links\"][0][\"href\"]\n\texcept Exception:\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\t# See https://tools.ietf.org/html/rfc7033#section-4.1\n\t\tif not handle.startswith('acct:'):\n\t\t\thandle = 'acct:' + handle\n\t\turl = 'https://{host}/.well-known/webfinger?resource={handle}'.format(\n\t\t\thost=handle.split(':')[1].split('@')[1],\n\t\t\thandle=handle,\n\t\t)\n\t\treturn requests.get(url).text\n\texcept:\n\t\treturn None\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\tpass\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.get(\"https://\" + handle + \"/.well-known/webfinger\")\n\t\tresponse.raise_for_status()\n\t\treturn response.text\n\texcept:\n\t\treturn None\n\n\n", "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept WebfingerException as e:\n\t\tlogger.warning(f'Webfinger failed for {handle}: {e}')\n\t\treturn None\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept ValueError:\n\t\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tif not is_webfinger_handle(handle):\n\t\treturn None\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept (ConnectionError, ConnectTimeout, ReadTimeout):\n\t\tlogger.debug(\"Could not retrieve webfinger document for handle {}\".format(handle), exc_info=True)\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tif not handle:\n\t\treturn None\n\n\t# TODO: Try to use the webfinger document to find the right URL to use.\n\t# TODO: Maybe we should also try to find the right URL to use.\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept:\n\t\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\t# TODO: We could consider using an LRU cache here.\n\ttry:\n\t\treturn get_webfinger_document(handle)\n\texcept (DNSLookupError, HTTPError, Timeout, URLError):\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn requests.get(webfinger_url(handle)).text\n\texcept Exception as e:\n\t\tlogger.info(\"Failed to retrieve webfinger document for {}: {}\".format(handle, e))\n\t\treturn None\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\tretrieve_url = \"https://{}/.well-known/webfinger?resource=acct:{}\".format(\n\t\tZOTERO_DOMAIN, handle\n\t)\n\ttry:\n\t\tresponse = requests.get(retrieve_url)\n\t\tif response.status_code == 200:\n\t\t\treturn json.loads(response.text)[\"links\"][0][\"href\"]\n\texcept Exception:\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\t# See https://tools.ietf.org/html/rfc7033#section-4.1\n\t\tif not handle.startswith('acct:'):\n\t\t\thandle = 'acct:' + handle\n\t\turl = 'https://{host}/.well-known/webfinger?resource={handle}'.format(\n\t\t\thost=handle.split(':')[1].split('@')[1],\n\t\t\thandle=handle,\n\t\t)\n\t\treturn requests.get(url).text\n\texcept:\n\t\treturn None\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept Exception:\n\t\tpass\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\tresponse = requests.get(\"https://\" + handle + \"/.well-known/webfinger\")\n\t\tresponse.raise_for_status()\n\t\treturn response.text\n\texcept:\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n\ttry:\n\t\treturn retrieve_webfinger_document(handle)\n\texcept WebfingerException as e:\n\t\tlogger.warning(f'Webfinger failed for {handle}: {e}')\n\t\treturn None\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_and_parse_diaspora_webfinger", "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict", "ques_id": "630629e152e177c0ba46a0d2", "code": ["def retrieve_and_parse_diaspora_webfinger(handle):\n\trequest = requests.get(make_webfinger_url(handle))\n\ttry:\n\t\treturn json.loads(request.text)\n\texcept ValueError:\n\t\treturn parse_legacy_webfinger(request.text)\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\tresponse = retrieve_remote_webfinger(handle)\n\n\t# If the response is a webfinger document\n\tif response.get('subject') == handle:\n\t\t# return the parsed webfinger document\n\t\treturn parse_diaspora_webfinger(response)\n\t# Otherwise return None\n\treturn None\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\treturn parse_diaspora_webfinger(retrieve_diaspora_webfinger(handle))\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\turl = \"https://%s/.well-known/host-meta\" % handle.split(\"@\")[1]\n\ttry:\n\t\twebfinger = requests.get(url, timeout=5)\n\texcept requests.exceptions.RequestException as e:\n\t\tlogger.error(\"Failed to retrieve webfinger for %s: %s\" % (handle, e))\n\t\treturn False\n\n\ttry:\n\t\twebfinger.raise_for_status()\n\texcept requests.exceptions.HTTPError as e:\n\t\tlogger.error(\"Failed to retrieve webfinger for %s: %s\" % (handle, e))\n\t\treturn False\n\n\ttry:\n\t\twebfinger = xmltodict.parse(webfinger.text)\n\texcept xmltodict.expat.ExpatError as e:\n\t\tlogger.error(\"Failed to parse webfinger for %s: %s\" % (handle, e))\n\t\treturn False\n\n\tif \"XRD\" in webfinger.keys():\n\t\tif \"Link\" in webfinger[\"XRD\"].keys():\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\tresponse = requests.get(\n\t\turl=urljoin(\n\t\t\t'https://' + handle.split('@')[-1],\n\t\t\t'/.well-known/webfinger?resource={}'.format(handle)\n\t\t),\n\t\theaders={'Accept': 'application/xrd+xml'}\n\t)\n\n\tif response.status_code == 200:\n\t\treturn parse_diaspora_webfinger(response.text)\n\telse:\n\t\traise requests.HTTPError(response.status_code)\n\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\tif handle.startswith('acct:'):\n\t\thandle = handle[5:]\n\tif handle.endswith('@'):\n\t\thandle = handle[:-1]\n\tif '@' not in handle:\n\t\traise ValueError('No domain in handle')\n\n\ts = requests.get('https://' + handle.split('@')[1] + '/.well-known/webfinger?resource=acct:' + handle)\n\tdata = json.loads(s.content)\n\n\tif 'links' not in data:\n\t\traise ValueError('No links in webfinger')\n\n\turl = None\n\tfor link in data['links']:\n\t\tif link['rel'] != 'http://joindiaspora.com/seed_location':\n\t\t\tcontinue\n\t\turl = link['href']\n\t\tbreak\n\n\tif url is None:\n\t\traise ValueError('No Diaspora seed URL in webfinger')\n\n\treturn {'seed_url': url, 'handle': handle}\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\tret = None\n\tif handle:\n\t\tif handle.startswith('acct:'):\n\t\t\thandle = handle[5:]\n\t\tparts = handle.split('@')\n\t\tif len(parts) == 1:\n\t\t\treturn None\n\t\turl = 'https://%s/.well-known/host-meta' % parts[1]\n\t\ttry:\n\t\t\tresponse = urllib.request.urlopen(url)\n\t\texcept urllib.error.HTTPError as e:\n\t\t\tif e.code == 404:\n\t\t\t\treturn None\n\t\t\traise\n\t\texcept urllib.error.URLError as e:\n\t\t\treturn None\n\t\tcontent = response.read()\n\t\ttry:\n\t\t\tret = lrdd.parse_lrdd(content, handle)\n\t\texcept:\n\t\t\tpass\n\treturn ret\n\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\treturn _retrieve_and_parse_webfinger(\n\t\thandle,\n\t\tDIASPORA_WEBFINGER_REL_TYPES,\n\t\tDIASPORA_WEBFINGER_RELS)\n\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\twebfinger = retrieve_and_parse_webfinger(handle)\n\n\t# TODO: Check that the webfinger is indeed for a Diaspora* user\n\t# (see https://github.com/joindiaspora/diaspora/pull/3201)\n\n\tif not webfinger:\n\t\treturn None\n\n\tif webfinger[\"links\"][\"rel\"] != \"http://joindiaspora.com/seed_location\":\n\t\treturn None\n\n\treturn webfinger\n\n", "def retrieve_and_parse_diaspora_webfinger(handle):\n\tresponse = retrieve_diaspora_webfinger(handle)\n\treturn parse_diaspora_webfinger(response)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\trequest = requests.get(make_webfinger_url(handle))\n\ttry:\n\t\treturn json.loads(request.text)\n\texcept ValueError:\n\t\treturn parse_legacy_webfinger(request.text)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\tresponse = retrieve_remote_webfinger(handle)\n\n\t# If the response is a webfinger document\n\tif response.get('subject') == handle:\n\t\t# return the parsed webfinger document\n\t\treturn parse_diaspora_webfinger(response)\n\t# Otherwise return None\n\treturn None\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\treturn parse_diaspora_webfinger(retrieve_diaspora_webfinger(handle))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\turl = \"https://%s/.well-known/host-meta\" % handle.split(\"@\")[1]\n\ttry:\n\t\twebfinger = requests.get(url, timeout=5)\n\texcept requests.exceptions.RequestException as e:\n\t\tlogger.error(\"Failed to retrieve webfinger for %s: %s\" % (handle, e))\n\t\treturn False\n\n\ttry:\n\t\twebfinger.raise_for_status()\n\texcept requests.exceptions.HTTPError as e:\n\t\tlogger.error(\"Failed to retrieve webfinger for %s: %s\" % (handle, e))\n\t\treturn False\n\n\ttry:\n\t\twebfinger = xmltodict.parse(webfinger.text)\n\texcept xmltodict.expat.ExpatError as e:\n\t\tlogger.error(\"Failed to parse webfinger for %s: %s\" % (handle, e))\n\t\treturn False\n\n\tif \"XRD\" in webfinger.keys():\n\t\tif \"Link\" in webfinger[\"XRD\"].keys():\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\tresponse = requests.get(\n\t\turl=urljoin(\n\t\t\t'https://' + handle.split('@')[-1],\n\t\t\t'/.well-known/webfinger?resource={}'.format(handle)\n\t\t),\n\t\theaders={'Accept': 'application/xrd+xml'}\n\t)\n\n\tif response.status_code == 200:\n\t\treturn parse_diaspora_webfinger(response.text)\n\telse:\n\t\traise requests.HTTPError(response.status_code)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\tif handle.startswith('acct:'):\n\t\thandle = handle[5:]\n\tif handle.endswith('@'):\n\t\thandle = handle[:-1]\n\tif '@' not in handle:\n\t\traise ValueError('No domain in handle')\n\n\ts = requests.get('https://' + handle.split('@')[1] + '/.well-known/webfinger?resource=acct:' + handle)\n\tdata = json.loads(s.content)\n\n\tif 'links' not in data:\n\t\traise ValueError('No links in webfinger')\n\n\turl = None\n\tfor link in data['links']:\n\t\tif link['rel'] != 'http://joindiaspora.com/seed_location':\n\t\t\tcontinue\n\t\turl = link['href']\n\t\tbreak\n\n\tif url is None:\n\t\traise ValueError('No Diaspora seed URL in webfinger')\n\n\treturn {'seed_url': url, 'handle': handle}\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\tret = None\n\tif handle:\n\t\tif handle.startswith('acct:'):\n\t\t\thandle = handle[5:]\n\t\tparts = handle.split('@')\n\t\tif len(parts) == 1:\n\t\t\treturn None\n\t\turl = 'https://%s/.well-known/host-meta' % parts[1]\n\t\ttry:\n\t\t\tresponse = urllib.request.urlopen(url)\n\t\texcept urllib.error.HTTPError as e:\n\t\t\tif e.code == 404:\n\t\t\t\treturn None\n\t\t\traise\n\t\texcept urllib.error.URLError as e:\n\t\t\treturn None\n\t\tcontent = response.read()\n\t\ttry:\n\t\t\tret = lrdd.parse_lrdd(content, handle)\n\t\texcept:\n\t\t\tpass\n\treturn ret\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\treturn _retrieve_and_parse_webfinger(\n\t\thandle,\n\t\tDIASPORA_WEBFINGER_REL_TYPES,\n\t\tDIASPORA_WEBFINGER_RELS)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\twebfinger = retrieve_and_parse_webfinger(handle)\n\n\t# TODO: Check that the webfinger is indeed for a Diaspora* user\n\t# (see https://github.com/joindiaspora/diaspora/pull/3201)\n\n\tif not webfinger:\n\t\treturn None\n\n\tif webfinger[\"links\"][\"rel\"] != \"http://joindiaspora.com/seed_location\":\n\t\treturn None\n\n\treturn webfinger\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_and_parse_diaspora_webfinger(handle):\n\tresponse = retrieve_diaspora_webfinger(handle)\n\treturn parse_diaspora_webfinger(response)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/diaspora.py", "project": "jaywink/federation", "name": "retrieve_diaspora_host_meta", "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance", "ques_id": "630629e252e177c0ba46a0d6", "code": ["def retrieve_diaspora_host_meta(host):\n\treturn retrieve_host_meta(host, 'application/xrd+xml')\n\n\n", "def retrieve_diaspora_host_meta(host):\n\twith urlopen('https://{}/.well-known/host-meta'.format(host)) as request:\n\t\thost_meta = request.read()\n\treturn XRD(host_meta)\n\n", "def retrieve_diaspora_host_meta(host):\n\treq = requests.get(\"https://%s/.well-known/host-meta\" % host)\n\n\tif req.status_code != 200:\n\t\traise DiasporaError(\"Failed to retrieve host-meta from %s\" % host)\n\n\treturn xrd.parse(req.text)\n\n\n", "def retrieve_diaspora_host_meta(host):\n\tresponse = requests.get(urljoin(host, \"/.well-known/host-meta\"))\n\tif response.status_code != 200:\n\t\traise ValueError(\"Could not retrieve host-meta\")\n\txrd = xmldict.load(response.text)\n\treturn xrd\n\n\n", "def retrieve_diaspora_host_meta(host):\n\turl = 'https://{0}/.well-known/host-meta'.format(host)\n\treturn retrieve(url)\n\n", "def retrieve_diaspora_host_meta(host):\n\thost_meta_url = urljoin(host, \"/.well-known/host-meta\")\n\thost_meta = retrieve_xml(host_meta_url)\n\treturn host_meta\n\n", "def retrieve_diaspora_host_meta(host):\n\turl = \"https://%s/.well-known/host-meta\" % host\n\tdata = get_url(url)\n\treturn xrd.XRD(data)\n\n\n", "def retrieve_diaspora_host_meta(host):\n\treturn XRD.from_url(urljoin(host, '/.well-known/host-meta'))\n\n\n", "def retrieve_diaspora_host_meta(host):\n\ttry:\n\t\tresponse = requests.get('https://%s/.well-known/host-meta' % host)\n\t\tresponse.raise_for_status()\n\t\treturn xrd.parse_xrd(response.text)\n\texcept Exception:\n\t\treturn None\n\n\n", "def retrieve_diaspora_host_meta(host):\n\ttry:\n\t\tr = requests.get(\"https://%s/.well-known/host-meta\" % host)\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\tif r.status_code != 200:\n\t\treturn None\n\treturn XRD(r.text)\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def retrieve_diaspora_host_meta(host):\n\treturn retrieve_host_meta(host, 'application/xrd+xml')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\twith urlopen('https://{}/.well-known/host-meta'.format(host)) as request:\n\t\thost_meta = request.read()\n\treturn XRD(host_meta)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\treq = requests.get(\"https://%s/.well-known/host-meta\" % host)\n\n\tif req.status_code != 200:\n\t\traise DiasporaError(\"Failed to retrieve host-meta from %s\" % host)\n\n\treturn xrd.parse(req.text)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\tresponse = requests.get(urljoin(host, \"/.well-known/host-meta\"))\n\tif response.status_code != 200:\n\t\traise ValueError(\"Could not retrieve host-meta\")\n\txrd = xmldict.load(response.text)\n\treturn xrd\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\turl = 'https://{0}/.well-known/host-meta'.format(host)\n\treturn retrieve(url)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\thost_meta_url = urljoin(host, \"/.well-known/host-meta\")\n\thost_meta = retrieve_xml(host_meta_url)\n\treturn host_meta\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\turl = \"https://%s/.well-known/host-meta\" % host\n\tdata = get_url(url)\n\treturn xrd.XRD(data)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\treturn XRD.from_url(urljoin(host, '/.well-known/host-meta'))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\ttry:\n\t\tresponse = requests.get('https://%s/.well-known/host-meta' % host)\n\t\tresponse.raise_for_status()\n\t\treturn xrd.parse_xrd(response.text)\n\texcept Exception:\n\t\treturn None\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def retrieve_diaspora_host_meta(host):\n\ttry:\n\t\tr = requests.get(\"https://%s/.well-known/host-meta\" % host)\n\texcept requests.exceptions.ConnectionError:\n\t\treturn None\n\tif r.status_code != 200:\n\t\treturn None\n\treturn XRD(r.text)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "federation/utils/network.py", "project": "jaywink/federation", "name": "send_document", "docstring": "Helper method to send a document via POST.\n\nAdditional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n:arg url: Full url to send to, including protocol\n:arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n:arg timeout: Seconds to wait for response (defaults to 10)\n:arg method: Method to use, defaults to post\n:returns: Tuple of status code (int or None) and error (exception class instance or None)", "ques_id": "630629e752e177c0ba46a0fb", "code": ["def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout, method, \"application/json\", *args, **kwargs)\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn requests.post(url, files=data, timeout=timeout, *args, **kwargs)\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout=timeout, method=method, *args, **kwargs)\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_data(url, data, timeout=timeout, method=method, *args, **kwargs)\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, \"application/json\", timeout, method, *args, **kwargs)\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout=timeout, method=\"post\", *args, **kwargs)\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\telif method == \"get\":\n\t\treturn requests.get(url, params=data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\traise ValueError(\"Only POST and GET methods are supported.\")\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\treturn requests.get(url, data=data, timeout=timeout, *args, **kwargs)\n\n\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tresponse = HttpResponse(content_type=\"application/pdf\")\n\tresponse.write(data)\n\treturn response\n\t\n", "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout=timeout, method=method, *args, **kwargs)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout, method, \"application/json\", *args, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn requests.post(url, files=data, timeout=timeout, *args, **kwargs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout=timeout, method=method, *args, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_data(url, data, timeout=timeout, method=method, *args, **kwargs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, \"application/json\", timeout, method, *args, **kwargs)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout=timeout, method=\"post\", *args, **kwargs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\telif method == \"get\":\n\t\treturn requests.get(url, params=data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\traise ValueError(\"Only POST and GET methods are supported.\")\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tif method == \"post\":\n\t\treturn requests.post(url, data=data, timeout=timeout, *args, **kwargs)\n\telse:\n\t\treturn requests.get(url, data=data, timeout=timeout, *args, **kwargs)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\tresponse = HttpResponse(content_type=\"application/pdf\")\n\tresponse.write(data)\n\treturn response\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n\treturn send_response(url, data, timeout=timeout, method=method, *args, **kwargs)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/utils/dict_utils.py", "project": "redhat-openstack/infrared", "name": "dict_insert", "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain", "ques_id": "6306091073426c38ae68acac", "code": ["def dict_insert(dic, val, key, *keys):\n\tif not key:\n\t\treturn\n\n\tif not keys:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, *keys)\n\n", "def dict_insert(dic, val, key, *keys):\n\tkeys = list(keys)\n\tkeys.insert(0, key)\n\n\tcurrent = dic\n\tfor key in keys[:-1]:\n\t\tcurrent = current.setdefault(key, {})\n\tcurrent[keys[-1]] = val\n\n", "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn dic\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\treturn dic\n\n", "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\telse:\n\t\tif not dic.has_key(key):\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n", "def dict_insert(dic, val, key, *keys):\n\tif len(keys):\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\n\t\tdict_insert(dic[key], val, *keys)\n\telse:\n\t\tdic[key] = val\n\n", "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tif not key in dic:\n\t\t\tdic[key] = val\n\t\t\treturn True\n\t\treturn False\n\tif not key in dic:\n\t\tdic[key] = {}\n\treturn dict_insert(dic[key], val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, *keys)\n\n\n", "def dict_insert(dic, val, key, *keys):\n\tif keys:\n\t\tsub_key = keys[0]\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, sub_key, *keys[1:])\n\telse:\n\t\tdic[key] = val\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not key:\n\t\treturn\n\n\tif not keys:\n\t\tdic[key] = val\n\telse:\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, *keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tkeys = list(keys)\n\tkeys.insert(0, key)\n\n\tcurrent = dic\n\tfor key in keys[:-1]:\n\t\tcurrent = current.setdefault(key, {})\n\tcurrent[keys[-1]] = val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn dic\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\treturn dic\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif len(keys) == 0:\n\t\tdic[key] = val\n\t\treturn\n\telse:\n\t\tif not dic.has_key(key):\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, *keys)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif len(keys):\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\n\t\tdict_insert(dic[key], val, *keys)\n\telse:\n\t\tdic[key] = val\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\tif key not in dic:\n\t\tdic[key] = {}\n\tdict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tif not key in dic:\n\t\t\tdic[key] = val\n\t\t\treturn True\n\t\treturn False\n\tif not key in dic:\n\t\tdic[key] = {}\n\treturn dict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif not keys:\n\t\tdic[key] = val\n\t\treturn\n\n\tif key not in dic:\n\t\tdic[key] = {}\n\n\tdict_insert(dic[key], val, *keys)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def dict_insert(dic, val, key, *keys):\n\tif keys:\n\t\tsub_key = keys[0]\n\t\tif key not in dic:\n\t\t\tdic[key] = {}\n\t\tdict_insert(dic[key], val, sub_key, *keys[1:])\n\telse:\n\t\tdic[key] = val\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_complex_types.py", "project": "redhat-openstack/infrared", "name": "list_of_file_names", "docstring": "Create a new IniType complex type", "ques_id": "6306091a73426c38ae68acc8", "code": ["def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\tcli.Sequence(\n\t\t\tcli.FileType(\n\t\t\t\t\"r\",\n\t\t\t\tdefaultValue=spec_option.default_value,\n\t\t\t\tdescription=spec_option.description,\n\t\t\t\tname=spec_option.name,\n\t\t\t\tsettings_dirs=settings_dirs,\n\t\t\t\tvalid_values=spec_option.valid_values,\n\t\t\t)\n\t\t)\n\t)\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(default=[], metavar=\"[FILENAME]\",\n\t\tdescr=_(\"TODO\"),\n\t\tread=read_file_names, write=write_file_names,\n\t\tvalidate_default=False,\n\t\tvalidate=validate_file_names,\n\t\tcompleter=completer_file_names(settings_dirs, spec_option))\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\"),\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\"),\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\"),\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\")\n\t)\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\tspec_option,\n\t\tcli.String(\n\t\t\t\"filename\",\n\t\t\trequired=False,\n\t\t\thelp=\"The name of a settings file to add to the list.\"\n\t\t\t),\n\t\thelp=\"Add a settings file to the list.\",\n\t\tgroup=cli.FormatterGroup(\n\t\t\t\"Settings File Options\",\n\t\t\tcli.listing_group,\n\t\t\tcli.FormatterGroup.HELP_OPTIONAL,\n\t\t\t),\n\t\tconverter=lambda value: [os.path.normpath(os.path.join(settings_dir, filename)) for filename in value],\n\t\tmetavar=\"<filename>\",\n\t\t)\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tresult = cli.ListOfFileNames(settings_dirs, spec_option)\n\tassert isinstance(result, cli.IniType)\n\treturn result\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\tlist_of_file_names = cli.ListOfFileNames()\n\tlist_of_file_names.settings_dirs = settings_dirs\n\tlist_of_file_names.spec_option = spec_option\n\treturn list_of_file_names\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\t\"list of file names\",\n\t\tsettings_dirs,\n\t\tspec_option,\n\t\tcli.FileName(),\n\t)\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n\n", "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n\n"], "level": "plib_runnable", "generate_results": [{"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\tcli.Sequence(\n\t\t\tcli.FileType(\n\t\t\t\t\"r\",\n\t\t\t\tdefaultValue=spec_option.default_value,\n\t\t\t\tdescription=spec_option.description,\n\t\t\t\tname=spec_option.name,\n\t\t\t\tsettings_dirs=settings_dirs,\n\t\t\t\tvalid_values=spec_option.valid_values,\n\t\t\t)\n\t\t)\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(default=[], metavar=\"[FILENAME]\",\n\t\tdescr=_(\"TODO\"),\n\t\tread=read_file_names, write=write_file_names,\n\t\tvalidate_default=False,\n\t\tvalidate=validate_file_names,\n\t\tcompleter=completer_file_names(settings_dirs, spec_option))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\"),\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\"),\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\"),\n\t\tcli.FileOption(settings_dirs, spec_option, None, None, None, \"name\")\n\t)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\tspec_option,\n\t\tcli.String(\n\t\t\t\"filename\",\n\t\t\trequired=False,\n\t\t\thelp=\"The name of a settings file to add to the list.\"\n\t\t\t),\n\t\thelp=\"Add a settings file to the list.\",\n\t\tgroup=cli.FormatterGroup(\n\t\t\t\"Settings File Options\",\n\t\t\tcli.listing_group,\n\t\t\tcli.FormatterGroup.HELP_OPTIONAL,\n\t\t\t),\n\t\tconverter=lambda value: [os.path.normpath(os.path.join(settings_dir, filename)) for filename in value],\n\t\tmetavar=\"<filename>\",\n\t\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tresult = cli.ListOfFileNames(settings_dirs, spec_option)\n\tassert isinstance(result, cli.IniType)\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\tlist_of_file_names = cli.ListOfFileNames()\n\tlist_of_file_names.settings_dirs = settings_dirs\n\tlist_of_file_names.spec_option = spec_option\n\treturn list_of_file_names\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(\n\t\t\"list of file names\",\n\t\tsettings_dirs,\n\t\tspec_option,\n\t\tcli.FileName(),\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def list_of_file_names(settings_dirs, spec_option):\n\treturn cli.ListOfFileNames(settings_dirs, spec_option)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "ansible_config_manager", "docstring": "Gets the ansible config manager.", "ques_id": "6306091b73426c38ae68acd7", "code": ["def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\n", "def ansible_config_manager(cls):\n\tservice_name = cls.__class__.__name__\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER, service_name, service_name)\n\n", "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\n", "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "def ansible_config_manager(cls):\n\treturn cls.ansible_config_manager\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\tservice_name = cls.__class__.__name__\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER, service_name, service_name)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def ansible_config_manager(cls):\n\treturn cls.ansible_config_manager\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "workspace_manager", "docstring": "Gets the workspace manager.", "ques_id": "6306091b73426c38ae68acd9", "code": ["def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def workspace_manager(cls):\n\treturn cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/services/__init__.py", "project": "redhat-openstack/infrared", "name": "plugins_manager", "docstring": "Gets the plugin manager.", "ques_id": "6306091b73426c38ae68acda", "code": ["def plugins_manager(cls):\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n", "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "def plugins_manager(cls):\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n", "def plugins_manager(cls):\n\tservice = cls._get_service()\n\treturn getattr(service, 'PLUGINS_MANAGER')\n\n", "def plugins_manager(cls):\n\tif not hasattr(cls, '_plugins_manager'):\n\t\tcls._plugins_manager = cls._get_service(ServiceName.PLUGINS_MANAGER)\n\treturn cls._plugins_manager\n\n\n", "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n\n", "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n\n", "def plugins_manager(cls):\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tservice = cls._get_service()\n\treturn getattr(service, 'PLUGINS_MANAGER')\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\tif not hasattr(cls, '_plugins_manager'):\n\t\tcls._plugins_manager = cls._get_service(ServiceName.PLUGINS_MANAGER)\n\treturn cls._plugins_manager\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(cls.PLUGINS_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def plugins_manager(cls):\n\treturn cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_content", "docstring": "validates that spec (YAML) content has all required fields\n\n:param spec_content: content of spec file\n:raise IRValidatorException: when mandatory data\nis missing in spec file\n:return: Dictionary with data loaded from a spec (YAML) file", "ques_id": "6306091c73426c38ae68acdc", "code": ["def validate_from_content(cls, spec_content=None):\n\timport yaml\n\ttry:\n\t\tdata = yaml.load(spec_content, Loader=yaml.FullLoader)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: {}\".format(exc))\n\tif data is None:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: No data found in spec file\")\n\tif 'version' not in data:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: No version found in spec file\")\n\tif data['version'] != cls.spec_version:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: Unsupported version: {}. \"\n\t\t\t\"Supported version: {}\".format(data['version'], cls.spec_version))\n\tif 'name' not in data:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: No name found in spec file\")\n\tif 'description' not in data:\n\t\traise IRValidator\n", "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"spec content is not provided\")\n\n\tif isinstance(spec_content, str):\n\t\tspec_content = yaml.safe_load(spec_content)\n\n\ttry:\n\t\tvalidator = cls()\n\t\tvalidator.validate(spec_content)\n\t\treturn spec_content\n\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(\"Can't validate spec content: %s\" % e)\n\n\n", "def validate_from_content(cls, spec_content=None):\n\tmandatory_fields = ['name', 'version', 'summary', 'license', 'url']\n\tif spec_content is None:\n\t\traise IRValidatorException(\n\t\t\t\"Spec content is not provided.\")\n\telif not isinstance(spec_content, dict):\n\t\traise IRValidatorException(\n\t\t\t\"Spec content provided is not a dictionary.\")\n\tfor field in mandatory_fields:\n\t\tif field not in spec_content:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Mandatory field '{0}' is missing in spec.\".format(field))\n\treturn spec_content\n\n\n", "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\tspec_content = cls.spec_content\n\tif spec_content is None:\n\t\t# this should not happen, but just in case:\n\t\traise IRValidatorException(\n\t\t\t'No spec content provided. Cannot validate spec file')\n\tdata = yaml.safe_load(spec_content)\n\tif data is None:\n\t\traise IRValidatorException('Spec file is empty')\n\tif 'version' not in data:\n\t\traise IRValidatorException('Spec file version is missing')\n\treturn data\n\n\n", "def validate_from_content(cls, spec_content=None):\n\tdata = cls.load_from_content(spec_content)\n\n\tif 'name' not in data:\n\t\traise IRValidatorException('Name is required')\n\n\treturn data\n\n\t@classmethod\n\tdef validate(cls, spec_file):\n\t\t\"\"\"\n\t\tvalidates that spec (YAML) file has all required fields\n\n", "def validate_from_content(cls, spec_content=None):\n\tdata = cls.load_from_content(spec_content)\n\tcls.__validate_required_data(data)\n\treturn data\n\n\t@classmethod\n\tdef load_from_content(cls, spec_content=None):\n\t\t\"\"\"\n\t\tloads spec (YAML) content into a dictionary\n\n", "def validate_from_content(cls, spec_content=None):\n\tdata = cls.load_from_content(spec_content)\n\tif data is None:\n\t\traise IRValidatorException(\"No data in spec file\")\n\tif \"name\" not in data:\n\t\traise IRValidatorException(\"name is missing in spec\")\n\telif \"version\" not in data:\n\t\traise IRValidatorException(\"version is missing in spec\")\n\telif \"description\" not in data:\n\t\traise IRValidatorException(\"description is missing in spec\")\n\telif \"keywords\" not in data:\n\t\traise IRValidatorException(\"keywords is missing in spec\")\n\telif \"type\" not in data:\n\t\traise IRValidatorException(\"type is missing in spec\")\n\telif \"subtype\" not in data:\n\t\traise IRValidatorException(\"subtype is missing in spec\")\n\telif \"template\" not in data:\n\t\traise IRValidatorException(\"template is missing in spec\")\n\telif \"image_name\" not in data:\n\t\traise IRValidatorException(\"image_name is missing in spec\")\n\telif\n", "def validate_from_content(cls, spec_content=None):\n\tdata = yaml.load(spec_content)\n\n\t# check if data is valid\n\tcls.validate(data)\n\n\treturn data\n\n\n", "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\tspec_content = cls.spec_content\n\n\ttry:\n\t\tyaml_data = yaml.load(spec_content)\n\texcept Exception, e:\n\t\traise IRValidatorException(\n\t\t\t\t\"Error parsing spec file: \" + str(e))\n\n\tfor field in cls.mandatory_fields:\n\t\tif field not in yaml_data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Mandatory field '\" + field + \"' not found in spec file!\")\n\n\treturn yaml_data\n\n\n", "def validate_from_content(cls, spec_content=None):\n\t# TODO: validate data in spec file\n\n\treturn cls.load_from_content(spec_content)\n\n\t# try:\n\t# \tyaml.load(spec_content, Loader=yaml.FullLoader)\n\t# except yaml.YAMLError as ex:\n\t# \traise IRValidatorException(\n\t# \t\tf'Exception loading spec file: {ex}')\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_content(cls, spec_content=None):\n\timport yaml\n\ttry:\n\t\tdata = yaml.load(spec_content, Loader=yaml.FullLoader)\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: {}\".format(exc))\n\tif data is None:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: No data found in spec file\")\n\tif 'version' not in data:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: No version found in spec file\")\n\tif data['version'] != cls.spec_version:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: Unsupported version: {}. \"\n\t\t\t\"Supported version: {}\".format(data['version'], cls.spec_version))\n\tif 'name' not in data:\n\t\traise IRValidatorException(\n\t\t\t\"Error parsing spec file: No name found in spec file\")\n\tif 'description' not in data:\n\t\traise IRValidator\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\traise IRValidatorException(\"spec content is not provided\")\n\n\tif isinstance(spec_content, str):\n\t\tspec_content = yaml.safe_load(spec_content)\n\n\ttry:\n\t\tvalidator = cls()\n\t\tvalidator.validate(spec_content)\n\t\treturn spec_content\n\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(\"Can't validate spec content: %s\" % e)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tmandatory_fields = ['name', 'version', 'summary', 'license', 'url']\n\tif spec_content is None:\n\t\traise IRValidatorException(\n\t\t\t\"Spec content is not provided.\")\n\telif not isinstance(spec_content, dict):\n\t\traise IRValidatorException(\n\t\t\t\"Spec content provided is not a dictionary.\")\n\tfor field in mandatory_fields:\n\t\tif field not in spec_content:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Mandatory field '{0}' is missing in spec.\".format(field))\n\treturn spec_content\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\tspec_content = cls.spec_content\n\tif spec_content is None:\n\t\t# this should not happen, but just in case:\n\t\traise IRValidatorException(\n\t\t\t'No spec content provided. Cannot validate spec file')\n\tdata = yaml.safe_load(spec_content)\n\tif data is None:\n\t\traise IRValidatorException('Spec file is empty')\n\tif 'version' not in data:\n\t\traise IRValidatorException('Spec file version is missing')\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tdata = cls.load_from_content(spec_content)\n\n\tif 'name' not in data:\n\t\traise IRValidatorException('Name is required')\n\n\treturn data\n\n\t@classmethod\n\tdef validate(cls, spec_file):\n\t\t\"\"\"\n\t\tvalidates that spec (YAML) file has all required fields\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tdata = cls.load_from_content(spec_content)\n\tcls.__validate_required_data(data)\n\treturn data\n\n\t@classmethod\n\tdef load_from_content(cls, spec_content=None):\n\t\t\"\"\"\n\t\tloads spec (YAML) content into a dictionary\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tdata = cls.load_from_content(spec_content)\n\tif data is None:\n\t\traise IRValidatorException(\"No data in spec file\")\n\tif \"name\" not in data:\n\t\traise IRValidatorException(\"name is missing in spec\")\n\telif \"version\" not in data:\n\t\traise IRValidatorException(\"version is missing in spec\")\n\telif \"description\" not in data:\n\t\traise IRValidatorException(\"description is missing in spec\")\n\telif \"keywords\" not in data:\n\t\traise IRValidatorException(\"keywords is missing in spec\")\n\telif \"type\" not in data:\n\t\traise IRValidatorException(\"type is missing in spec\")\n\telif \"subtype\" not in data:\n\t\traise IRValidatorException(\"subtype is missing in spec\")\n\telif \"template\" not in data:\n\t\traise IRValidatorException(\"template is missing in spec\")\n\telif \"image_name\" not in data:\n\t\traise IRValidatorException(\"image_name is missing in spec\")\n\telif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tdata = yaml.load(spec_content)\n\n\t# check if data is valid\n\tcls.validate(data)\n\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\tif spec_content is None:\n\t\tspec_content = cls.spec_content\n\n\ttry:\n\t\tyaml_data = yaml.load(spec_content)\n\texcept Exception, e:\n\t\traise IRValidatorException(\n\t\t\t\t\"Error parsing spec file: \" + str(e))\n\n\tfor field in cls.mandatory_fields:\n\t\tif field not in yaml_data:\n\t\t\traise IRValidatorException(\n\t\t\t\t\"Mandatory field '\" + field + \"' not found in spec file!\")\n\n\treturn yaml_data\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_from_content(cls, spec_content=None):\n\t# TODO: validate data in spec file\n\n\treturn cls.load_from_content(spec_content)\n\n\t# try:\n\t# \tyaml.load(spec_content, Loader=yaml.FullLoader)\n\t# except yaml.YAMLError as ex:\n\t# \traise IRValidatorException(\n\t# \t\tf'Exception loading spec file: {ex}')\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/utils/validators.py", "project": "redhat-openstack/infrared", "name": "validate_from_file", "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file", "ques_id": "6306091c73426c38ae68acdd", "code": ["def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file or not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"Path to YAML file not provided OR file not found\")\n\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tyaml_data = yaml.load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"Error in loading YAML file: %s\" % exc)\n\n\tif not isinstance(yaml_data, dict):\n\t\traise IRValidatorException(\"Error in loading YAML file: No data found\")\n\n\treturn cls.validate(yaml_data)\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\treturn cls.validate(cls.load(yaml_file))\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\twith open(yaml_file) as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tprint(exc)\n\n\t\tif not cls.validate(data):\n\t\t\traise IRValidatorException(\"Invalid YAML file: \" + yaml_file)\n\treturn data\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tdata = yaml.load(stream)\n\texcept yaml.YAMLError as err:\n\t\tprint(err)\n\t\traise IRValidatorException(\"Invalid YAML file: \" + str(yaml_file))\n\texcept OSError as err:\n\t\tprint(err)\n\t\traise IRValidatorException(\"Invalid YAML file: \" + str(yaml_file))\n\treturn cls.validate(data)\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\traise IRValidatorException(\"Missing YAML file\")\n\n\tif os.path.exists(yaml_file) is False:\n\t\traise IRValidatorException(\"YAML file does not exist\")\n\n\twith open(yaml_file, \"r\") as f:\n\t\ttry:\n\t\t\tyaml_data = yaml.load(f)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(\"Error while loading the YAML file\")\n\n\tif cls.__class__.__name__ == 'IncidentReport':\n\t\treturn cls(yaml_data)\n\telse:\n\t\treturn cls.validate_dict(yaml_data)\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None or not os.path.exists(yaml_file):\n\t\traise IRValidatorException(\"Invalid YAML file provided\")\n\n\ttry:\n\t\tdata = yaml.load(open(yaml_file))\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Invalid YAML file provided\")\n\n\trequired_fields = ['name', 'id', 'version']\n\tfor field in required_fields:\n\t\tif field not in data:\n\t\t\traise IRValidatorException(\"Required field %s missing from YAML file %s\" % (field, yaml_file))\n\n\treturn data\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file:\n\t\tyaml_file = os.environ['IR_CONFIG_FILE']\n\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"YAML file '{}' does not exist\".format(yaml_file))\n\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream, Loader=yaml.FullLoader)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"YAML file '{}' has an error: {}\".format(yaml_file, exc))\n\n\t\tif not data:\n\t\t\traise IRValidatorException(\"YAML file '{}' is empty\".format(yaml_file))\n\n\t\tif not cls.__validate_keys(data):\n\t\t\traise IRValidatorException(\"Missing required keys in YAML file '{}'\".format(yaml_file))\n\n\treturn data\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\treturn cls().from_file(yaml_file=yaml_file)\n\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\twith open(yaml_file) as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream)\n\t\texcept IOError as e:\n\t\t\traise IRValidatorException(e)\n\ttry:\n\t\tcls.validate(data)\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(e)\n\treturn data\n\n\n", "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file:\n\t\traise IRValidatorException(\"YAML file is required\")\n\n\ttry:\n\t\twith open(yaml_file, \"r\") as yaml_stream:\n\t\t\tyaml_data = yaml.load(yaml_stream)\n\texcept Exception as e:\n\t\traise IRValidatorException(\"YAML file could not be loaded: {}\".format(e))\n\n\treturn cls.validate(yaml_data)\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file or not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"Path to YAML file not provided OR file not found\")\n\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tyaml_data = yaml.load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"Error in loading YAML file: %s\" % exc)\n\n\tif not isinstance(yaml_data, dict):\n\t\traise IRValidatorException(\"Error in loading YAML file: No data found\")\n\n\treturn cls.validate(yaml_data)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\treturn cls.validate(cls.load(yaml_file))\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\twith open(yaml_file) as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream)\n\t\texcept yaml.YAMLError as exc:\n\t\t\tprint(exc)\n\n\t\tif not cls.validate(data):\n\t\t\traise IRValidatorException(\"Invalid YAML file: \" + yaml_file)\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\ttry:\n\t\twith open(yaml_file, 'r') as stream:\n\t\t\tdata = yaml.load(stream)\n\texcept yaml.YAMLError as err:\n\t\tprint(err)\n\t\traise IRValidatorException(\"Invalid YAML file: \" + str(yaml_file))\n\texcept OSError as err:\n\t\tprint(err)\n\t\traise IRValidatorException(\"Invalid YAML file: \" + str(yaml_file))\n\treturn cls.validate(data)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None:\n\t\traise IRValidatorException(\"Missing YAML file\")\n\n\tif os.path.exists(yaml_file) is False:\n\t\traise IRValidatorException(\"YAML file does not exist\")\n\n\twith open(yaml_file, \"r\") as f:\n\t\ttry:\n\t\t\tyaml_data = yaml.load(f)\n\t\texcept yaml.YAMLError as e:\n\t\t\traise IRValidatorException(\"Error while loading the YAML file\")\n\n\tif cls.__class__.__name__ == 'IncidentReport':\n\t\treturn cls(yaml_data)\n\telse:\n\t\treturn cls.validate_dict(yaml_data)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif yaml_file is None or not os.path.exists(yaml_file):\n\t\traise IRValidatorException(\"Invalid YAML file provided\")\n\n\ttry:\n\t\tdata = yaml.load(open(yaml_file))\n\texcept yaml.YAMLError as exc:\n\t\traise IRValidatorException(\"Invalid YAML file provided\")\n\n\trequired_fields = ['name', 'id', 'version']\n\tfor field in required_fields:\n\t\tif field not in data:\n\t\t\traise IRValidatorException(\"Required field %s missing from YAML file %s\" % (field, yaml_file))\n\n\treturn data\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file:\n\t\tyaml_file = os.environ['IR_CONFIG_FILE']\n\n\tif not os.path.isfile(yaml_file):\n\t\traise IRValidatorException(\"YAML file '{}' does not exist\".format(yaml_file))\n\n\twith open(yaml_file, 'r') as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream, Loader=yaml.FullLoader)\n\t\texcept yaml.YAMLError as exc:\n\t\t\traise IRValidatorException(\"YAML file '{}' has an error: {}\".format(yaml_file, exc))\n\n\t\tif not data:\n\t\t\traise IRValidatorException(\"YAML file '{}' is empty\".format(yaml_file))\n\n\t\tif not cls.__validate_keys(data):\n\t\t\traise IRValidatorException(\"Missing required keys in YAML file '{}'\".format(yaml_file))\n\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\treturn cls().from_file(yaml_file=yaml_file)\n\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\twith open(yaml_file) as stream:\n\t\ttry:\n\t\t\tdata = yaml.load(stream)\n\t\texcept IOError as e:\n\t\t\traise IRValidatorException(e)\n\ttry:\n\t\tcls.validate(data)\n\texcept IRValidatorException as e:\n\t\traise IRValidatorException(e)\n\treturn data\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def validate_from_file(cls, yaml_file=None):\n\tif not yaml_file:\n\t\traise IRValidatorException(\"YAML file is required\")\n\n\ttry:\n\t\twith open(yaml_file, \"r\") as yaml_stream:\n\t\t\tyaml_data = yaml.load(yaml_stream)\n\texcept Exception as e:\n\t\traise IRValidatorException(\"YAML file could not be loaded: {}\".format(e))\n\n\treturn cls.validate(yaml_data)\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "_include_groups", "docstring": "Resolves the include dict directive in the spec files.", "ques_id": "6306091d73426c38ae68ace5", "code": ["def _include_groups(self, parser_dict):\n\tfor group in parser_dict.get('groups', []):\n\t\tfor include_dict in group.get('include', []):\n\t\t\tif not isinstance(include_dict, dict):\n\t\t\t\tcontinue\n\t\t\tinclude_file = include_dict.get('file')\n\t\t\tif include_file is None:\n\t\t\t\tcontinue\n\t\t\tinclude_file = os.path.join(self._spec_dir, include_file)\n\t\t\tparser = SpecParser(include_file)\n\t\t\tparser_dict = parser.parse()\n\t\t\tself._include_groups(parser_dict)\n\t\t\tgroup['groups'].extend(parser_dict.get('groups', []))\n\t\t\tgroup['packages'].extend(parser_dict.get('packages', []))\n\t\t\tgroup['includes'].extend(parser_dict.get('includes', []))\n\n\tfor package in parser_dict.get('packages', []):\n\t\tfor include_dict in package.get('include', []):\n\t\t\tif not isinstance(include_dict,\n", "def _include_groups(self, parser_dict):\n\tgroup_list = []\n\tfor group in parser_dict[\"include\"]:\n\t\tgroup_list.append(self._group_from_name(group))\n\treturn group_list\n\n", "def _include_groups(self, parser_dict):\n\t\n\t# First check if the include directive is present\n\tif 'include' in parser_dict:\n\t\t# Get all the groups to include\n\t\tgroups_to_include = parser_dict['include']\n\t\t\n\t\t# Get the current config and the current group\n\t\tconfig = self.config\n\t\tcurrent_group = self.current_group\n\t\t\n\t\t# Check if the config is a dict\n\t\tif not isinstance(config, dict):\n\t\t\traise TypeError('Config must be a dictionary')\n\t\t\n\t\t# Check if the current group is a string\n\t\tif not isinstance(current_group, str):\n\t\t\traise TypeError('Current group must be a string')\n\t\t\n\t\t# Check if the current group is in the config\n\t\tif not current_group in config:\n\t\t\traise KeyError('Current group not in config')\n\t\t\n\t\t# Get the current group from the config\n\t\tcurrent_group_config = config[current_group]\n\t\t\n\t\t# Check if the current group config is a dict\n\t\tif\n", "def _include_groups(self, parser_dict):\n\tif 'include' in parser_dict:\n\t\tif 'groups' in parser_dict['include']:\n\t\t\tfor group in parser_dict['include']['groups']:\n\t\t\t\t# add the group if it is not already there\n\t\t\t\tif group not in self.groups:\n\t\t\t\t\tself.groups.append(group)\n\t\t\t\t\t# now add the groups that this group includes\n\t\t\t\t\tif group in self.group_include:\n\t\t\t\t\t\tself.groups = self.groups + self.group_include[group]\n\t\t\t\t\t# now add the fields that this group includes\n\t\t\t\t\tif group in self.group_fields:\n\t\t\t\t\t\tself.fields = self.fields + self.group_fields[group]\n\n\t\tif 'fields' in parser_dict['include']:\n\t\t\tfor field in parser_dict['include']['fields']:\n\t\t\t\t# add the field if it is not already there\n\t\t\t\tif field not in self.fields:\n\t\t\t\t\n", "def _include_groups(self, parser_dict):\n\tfor key, value in parser_dict.iteritems():\n\t\tif isinstance(value, list):\n\t\t\t# Found a list, this means we need to add the values of the\n\t\t\t# dict to the list.\n\t\t\tfor val in value:\n\t\t\t\tif isinstance(val, dict):\n\t\t\t\t\t# Found a dict in the list, this means we need to add\n\t\t\t\t\t# the values of the dict to the list.\n\t\t\t\t\tif val.has_key(\"include_groups\"):\n\t\t\t\t\t\t# We found an include_groups directive.\n\t\t\t\t\t\tgroups = val[\"include_groups\"]\n\t\t\t\t\t\tif not isinstance(groups, list):\n\t\t\t\t\t\t\traise TypeError(\"The include_groups directive \"\n\t\t\t\t\t\t\t\t\"should be a list.\")\n\t\t\t\t\t\t# We need to include all the values from the specified\n\t\t\t\t\t\t# group.\n\t\t\t\t\t\tfor group in groups:\n\t\t\t\t\t\t\t\n", "def _include_groups(self, parser_dict):\n\t\n\t# build a list of the group names and a map from group names to their\n\t# group objects\n\tgroup_names = []\n\tgroup_map = {}\n\t\n\tfor group in self._groups:\n\t\tgroup_names.append(group.name)\n\t\tgroup_map[group.name] = group\n\t\n\t# process the include groups\n\tfor name in parser_dict['include']:\n\t\tif name not in group_names:\n\t\t\traise TemplateError('unknown group \"%s\"' % name)\n\t\tgroup_map[name].add_to_template(self)\n\t\n\t# remove the include list from the parser dict\n\tdel parser_dict['include']\n\n\n", "def _include_groups(self, parser_dict):\n\tif self.include_groups:\n\t\tfor group in self.include_groups:\n\t\t\tparser_dict.update(group.parser_dict)\n\n\n", "def _include_groups(self, parser_dict):\n\tif 'include' in parser_dict and isinstance(parser_dict['include'], dict):\n\t\tfor group_name in parser_dict['include']:\n\t\t\tif group_name in self.config_data['groups']:\n\t\t\t\t# Add the files in the include dict to the files list.\n\t\t\t\tfor filename in self.config_data['groups'][group_name]:\n\t\t\t\t\t# Check if the file is already in the list.\n\t\t\t\t\tif filename not in parser_dict['files']:\n\t\t\t\t\t\tparser_dict['files'].append(filename)\n\t\t\telse:\n\t\t\t\t# Group name is not valid.\n\t\t\t\traise ValueError('Group name {} is not valid.'.format(group_name))\n\n\t\t# Remove the include dict from the parser dict.\n\t\tdel parser_dict['include']\n\n\n", "def _include_groups(self, parser_dict):\n\tif 'include' not in parser_dict:\n\t\treturn\n\tinclude_groups = parser_dict['include']\n\tif not include_groups:\n\t\treturn\n\tif isinstance(include_groups, str):\n\t\tinclude_groups = [include_groups]\n\tfor group in include_groups:\n\t\tif group not in self.groups:\n\t\t\tcontinue\n\t\tself._include_groups(self.groups[group])\n\t\tfor key, value in self.groups[group].items():\n\t\t\tif key not in parser_dict:\n\t\t\t\tparser_dict[key] = []\n\t\t\tparser_dict[key] += value\n\tdel parser_dict['include']\n\n\n", "def _include_groups(self, parser_dict):\n\tif 'include-groups' not in parser_dict:\n\t\treturn\n\n\tfor group in parser_dict['include-groups']:\n\t\tself._include_group(group)\n\n\tdel parser_dict['include-groups']\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _include_groups(self, parser_dict):\n\tfor group in parser_dict.get('groups', []):\n\t\tfor include_dict in group.get('include', []):\n\t\t\tif not isinstance(include_dict, dict):\n\t\t\t\tcontinue\n\t\t\tinclude_file = include_dict.get('file')\n\t\t\tif include_file is None:\n\t\t\t\tcontinue\n\t\t\tinclude_file = os.path.join(self._spec_dir, include_file)\n\t\t\tparser = SpecParser(include_file)\n\t\t\tparser_dict = parser.parse()\n\t\t\tself._include_groups(parser_dict)\n\t\t\tgroup['groups'].extend(parser_dict.get('groups', []))\n\t\t\tgroup['packages'].extend(parser_dict.get('packages', []))\n\t\t\tgroup['includes'].extend(parser_dict.get('includes', []))\n\n\tfor package in parser_dict.get('packages', []):\n\t\tfor include_dict in package.get('include', []):\n\t\t\tif not isinstance(include_dict,\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tgroup_list = []\n\tfor group in parser_dict[\"include\"]:\n\t\tgroup_list.append(self._group_from_name(group))\n\treturn group_list\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\n\t# First check if the include directive is present\n\tif 'include' in parser_dict:\n\t\t# Get all the groups to include\n\t\tgroups_to_include = parser_dict['include']\n\t\t\n\t\t# Get the current config and the current group\n\t\tconfig = self.config\n\t\tcurrent_group = self.current_group\n\t\t\n\t\t# Check if the config is a dict\n\t\tif not isinstance(config, dict):\n\t\t\traise TypeError('Config must be a dictionary')\n\t\t\n\t\t# Check if the current group is a string\n\t\tif not isinstance(current_group, str):\n\t\t\traise TypeError('Current group must be a string')\n\t\t\n\t\t# Check if the current group is in the config\n\t\tif not current_group in config:\n\t\t\traise KeyError('Current group not in config')\n\t\t\n\t\t# Get the current group from the config\n\t\tcurrent_group_config = config[current_group]\n\t\t\n\t\t# Check if the current group config is a dict\n\t\tif\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif 'include' in parser_dict:\n\t\tif 'groups' in parser_dict['include']:\n\t\t\tfor group in parser_dict['include']['groups']:\n\t\t\t\t# add the group if it is not already there\n\t\t\t\tif group not in self.groups:\n\t\t\t\t\tself.groups.append(group)\n\t\t\t\t\t# now add the groups that this group includes\n\t\t\t\t\tif group in self.group_include:\n\t\t\t\t\t\tself.groups = self.groups + self.group_include[group]\n\t\t\t\t\t# now add the fields that this group includes\n\t\t\t\t\tif group in self.group_fields:\n\t\t\t\t\t\tself.fields = self.fields + self.group_fields[group]\n\n\t\tif 'fields' in parser_dict['include']:\n\t\t\tfor field in parser_dict['include']['fields']:\n\t\t\t\t# add the field if it is not already there\n\t\t\t\tif field not in self.fields:\n\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tfor key, value in parser_dict.iteritems():\n\t\tif isinstance(value, list):\n\t\t\t# Found a list, this means we need to add the values of the\n\t\t\t# dict to the list.\n\t\t\tfor val in value:\n\t\t\t\tif isinstance(val, dict):\n\t\t\t\t\t# Found a dict in the list, this means we need to add\n\t\t\t\t\t# the values of the dict to the list.\n\t\t\t\t\tif val.has_key(\"include_groups\"):\n\t\t\t\t\t\t# We found an include_groups directive.\n\t\t\t\t\t\tgroups = val[\"include_groups\"]\n\t\t\t\t\t\tif not isinstance(groups, list):\n\t\t\t\t\t\t\traise TypeError(\"The include_groups directive \"\n\t\t\t\t\t\t\t\t\"should be a list.\")\n\t\t\t\t\t\t# We need to include all the values from the specified\n\t\t\t\t\t\t# group.\n\t\t\t\t\t\tfor group in groups:\n\t\t\t\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\t\n\t# build a list of the group names and a map from group names to their\n\t# group objects\n\tgroup_names = []\n\tgroup_map = {}\n\t\n\tfor group in self._groups:\n\t\tgroup_names.append(group.name)\n\t\tgroup_map[group.name] = group\n\t\n\t# process the include groups\n\tfor name in parser_dict['include']:\n\t\tif name not in group_names:\n\t\t\traise TemplateError('unknown group \"%s\"' % name)\n\t\tgroup_map[name].add_to_template(self)\n\t\n\t# remove the include list from the parser dict\n\tdel parser_dict['include']\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif self.include_groups:\n\t\tfor group in self.include_groups:\n\t\t\tparser_dict.update(group.parser_dict)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif 'include' in parser_dict and isinstance(parser_dict['include'], dict):\n\t\tfor group_name in parser_dict['include']:\n\t\t\tif group_name in self.config_data['groups']:\n\t\t\t\t# Add the files in the include dict to the files list.\n\t\t\t\tfor filename in self.config_data['groups'][group_name]:\n\t\t\t\t\t# Check if the file is already in the list.\n\t\t\t\t\tif filename not in parser_dict['files']:\n\t\t\t\t\t\tparser_dict['files'].append(filename)\n\t\t\telse:\n\t\t\t\t# Group name is not valid.\n\t\t\t\traise ValueError('Group name {} is not valid.'.format(group_name))\n\n\t\t# Remove the include dict from the parser dict.\n\t\tdel parser_dict['include']\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif 'include' not in parser_dict:\n\t\treturn\n\tinclude_groups = parser_dict['include']\n\tif not include_groups:\n\t\treturn\n\tif isinstance(include_groups, str):\n\t\tinclude_groups = [include_groups]\n\tfor group in include_groups:\n\t\tif group not in self.groups:\n\t\t\tcontinue\n\t\tself._include_groups(self.groups[group])\n\t\tfor key, value in self.groups[group].items():\n\t\t\tif key not in parser_dict:\n\t\t\t\tparser_dict[key] = []\n\t\t\tparser_dict[key] += value\n\tdel parser_dict['include']\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _include_groups(self, parser_dict):\n\tif 'include-groups' not in parser_dict:\n\t\treturn\n\n\tfor group in parser_dict['include-groups']:\n\t\tself._include_group(group)\n\n\tdel parser_dict['include-groups']\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_spec_defaults", "docstring": "Resolve arguments' values from spec and other sources.", "ques_id": "6306092373426c38ae68acfa", "code": ["def get_spec_defaults(self):\n\t# defaults from spec\n\tdefaults = self._get_defaults()\n\t# defaults from args\n\tdefaults = self._get_defaults(defaults)\n\treturn defaults\n\n", "def get_spec_defaults(self):\n\tif self._spec_defaults is None:\n\t\tself._spec_defaults = self._get_defaults()\n\treturn self._spec_defaults\n\n", "def get_spec_defaults(self):\n\tif not self.spec_defaults:\n\t\tself.spec_defaults = self._get_defaults()\n\treturn self.spec_defaults\n\n", "def get_spec_defaults(self):\n\ttry:\n\t\tself._get_defaults()\n\texcept TypeError as e:\n\t\traise PortageException(\"Error getting default args for '%s': %s\" % (self.name, e))\n\treturn self.args\n\n\n", "def get_spec_defaults(self):\n\treturn self._get_defaults(self.spec)\n\n", "def get_spec_defaults(self):\n\treturn self._get_defaults()\n\n\n", "def get_spec_defaults(self):\n\tdefaults = self._get_defaults()\n\t# We do not want to resolve defaults of default arguments\n\t# (it would be nice to allow default arguments to reference previously defined arguments,\n\t# but that would require a lot of work)\n\tdefaults = dict((arg, value) for arg, value in defaults.iteritems() if arg not in self.spec.defaults)\n\treturn defaults\n\n", "def get_spec_defaults(self):\n\tspec_defaults = self.spec.get(\"defaults\", {})\n\tif spec_defaults:\n\t\tself._get_defaults(spec_defaults)\n\n\tif self.defaults_file:\n\t\tself._get_defaults(self._load_defaults_file(self.defaults_file))\n\n\n", "def get_spec_defaults(self):\n\tfor arg in self.spec.arguments:\n\t\tself._get_defaults(arg)\n\n\n", "def get_spec_defaults(self):\n\tself._get_defaults()\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_spec_defaults(self):\n\t# defaults from spec\n\tdefaults = self._get_defaults()\n\t# defaults from args\n\tdefaults = self._get_defaults(defaults)\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tif self._spec_defaults is None:\n\t\tself._spec_defaults = self._get_defaults()\n\treturn self._spec_defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tif not self.spec_defaults:\n\t\tself.spec_defaults = self._get_defaults()\n\treturn self.spec_defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\ttry:\n\t\tself._get_defaults()\n\texcept TypeError as e:\n\t\traise PortageException(\"Error getting default args for '%s': %s\" % (self.name, e))\n\treturn self.args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\treturn self._get_defaults(self.spec)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\treturn self._get_defaults()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tdefaults = self._get_defaults()\n\t# We do not want to resolve defaults of default arguments\n\t# (it would be nice to allow default arguments to reference previously defined arguments,\n\t# but that would require a lot of work)\n\tdefaults = dict((arg, value) for arg, value in defaults.iteritems() if arg not in self.spec.defaults)\n\treturn defaults\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tspec_defaults = self.spec.get(\"defaults\", {})\n\tif spec_defaults:\n\t\tself._get_defaults(spec_defaults)\n\n\tif self.defaults_file:\n\t\tself._get_defaults(self._load_defaults_file(self.defaults_file))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tfor arg in self.spec.arguments:\n\t\tself._get_defaults(arg)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_spec_defaults(self):\n\tself._get_defaults()\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_deprecated_args", "docstring": "Returning dict with options which deprecate others.", "ques_id": "6306092973426c38ae68ad01", "code": ["def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.get('deprecated_options'):\n\t\t\tdeprecated_args[option_spec['name']] = option_spec['deprecated_options']\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor opt_spec in self.spec_helper.iterate_option_specs():\n\t\tif opt_spec.deprecates:\n\t\t\tdeprecated_args[opt_spec.name] = opt_spec.deprecates\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif 'deprecated' in option_spec:\n\t\t\tdeprecated_args[option_spec['deprecated']] = option_spec['name']\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tdeprecated_args[option.name] = option.deprecated_alternatives\n\n\treturn deprecated_args\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor opt in self.spec_helper.iterate_option_specs():\n\t\tdeprecated_args.update(opt.deprecated_args)\n\treturn deprecated_args\n\n\t\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.deprecated_for:\n\t\t\tdeprecated_args[option_spec.deprecated_for] = option_spec.deprecated_for\n\treturn deprecated_args\n\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor arg in self.spec_helper.iterate_option_specs():\n\t\tif arg.deprecates_options:\n\t\t\tfor deprecated_arg in arg.deprecates_options:\n\t\t\t\tif deprecated_arg in deprecated_args:\n\t\t\t\t\tdeprecated_args[deprecated_arg].append(arg.name)\n\t\t\t\telse:\n\t\t\t\t\tdeprecated_args[deprecated_arg] = [arg.name]\n\treturn deprecated_args\n\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif spec.deprecated_option and spec.deprecated_option in self._options:\n\t\t\tdeprecated_args[spec.deprecated_option] = self._options[spec.deprecated_option]\n\treturn deprecated_args\n\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.deprecated:\n\t\t\tdeprecated_args[option_spec.deprecated] = option_spec.name\n\treturn deprecated_args\n\n\n", "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option.deprecated_by:\n\t\t\tdeprecated_args[option.name] = option.deprecated_by\n\treturn deprecated_args\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.get('deprecated_options'):\n\t\t\tdeprecated_args[option_spec['name']] = option_spec['deprecated_options']\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor opt_spec in self.spec_helper.iterate_option_specs():\n\t\tif opt_spec.deprecates:\n\t\t\tdeprecated_args[opt_spec.name] = opt_spec.deprecates\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif 'deprecated' in option_spec:\n\t\t\tdeprecated_args[option_spec['deprecated']] = option_spec['name']\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tdeprecated_args[option.name] = option.deprecated_alternatives\n\n\treturn deprecated_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor opt in self.spec_helper.iterate_option_specs():\n\t\tdeprecated_args.update(opt.deprecated_args)\n\treturn deprecated_args\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.deprecated_for:\n\t\t\tdeprecated_args[option_spec.deprecated_for] = option_spec.deprecated_for\n\treturn deprecated_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor arg in self.spec_helper.iterate_option_specs():\n\t\tif arg.deprecates_options:\n\t\t\tfor deprecated_arg in arg.deprecates_options:\n\t\t\t\tif deprecated_arg in deprecated_args:\n\t\t\t\t\tdeprecated_args[deprecated_arg].append(arg.name)\n\t\t\t\telse:\n\t\t\t\t\tdeprecated_args[deprecated_arg] = [arg.name]\n\treturn deprecated_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor spec in self.spec_helper.iterate_option_specs():\n\t\tif spec.deprecated_option and spec.deprecated_option in self._options:\n\t\t\tdeprecated_args[spec.deprecated_option] = self._options[spec.deprecated_option]\n\treturn deprecated_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option_spec in self.spec_helper.iterate_option_specs():\n\t\tif option_spec.deprecated:\n\t\t\tdeprecated_args[option_spec.deprecated] = option_spec.name\n\treturn deprecated_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_deprecated_args(self):\n\tdeprecated_args = {}\n\tfor option in self.spec_helper.iterate_option_specs():\n\t\tif option.deprecated_by:\n\t\t\tdeprecated_args[option.name] = option.deprecated_by\n\treturn deprecated_args\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_arg_deprecation", "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files", "ques_id": "6306092c73426c38ae68ad02", "code": ["def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\tfor i in cli_args:\n\t\tif cli_args[i] is not None:\n\t\t\tif cli_args[i].get('deprecated') is not None:\n\t\t\t\tdeprecated_args[i] = cli_args[i].get('deprecated')\n\tfor i in answer_file_args:\n\t\tif answer_file_args[i] is not None:\n\t\t\tif answer_file_args[i].get('deprecated') is not None:\n\t\t\t\tdeprecated_args[i] = answer_file_args[i].get('deprecated')\n\tif len(deprecated_args) > 0:\n\t\tself.logger.error(\"The following arguments are deprecated. \"\n\t\t\t\t\t\t  \"Please check the documentation to see if there is a replacement.\")\n\t\tfor key, value in deprecated_args.items():\n\t\t\tself.logger.error(\"Argument: \" + key + \" - \" + value)\n\t\tsys.exit(1)\n\n\tdef validate_arg\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tself._validate_arg_deprecation(cli_args, answer_file_args)\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = [\"--virtualenv-name\", \"--virtualenv-template\", \"--virtualenv-path\", \"--virtualenv-requirements\", \"--virtualenv-script\",\n\t\t\t\t\t\t \"--virtualenv-version\", \"--virtualenv-system-site-packages\", \"--virtualenv-prompt\", \"--virtualenv-pip\", \"--virtualenv-setuptools\",\n\t\t\t\t\t\t \"--virtualenv-distribute\", \"--virtualenv-wheel\", \"--virtualenv-clear\", \"--virtualenv-no-download\", \"--virtualenv-extra-search-dir\",\n\t\t\t\t\t\t \"--virtualenv-python-exe\", \"--virtualenv-always-copy\", \"--virtualenv-never-download\", \"--virtualenv-download\", \"--virtualenv-site-packages\",\n\t\t\t\t\t\t \"--virtualenv-no-setuptools\", \"--virtualenv-no-wheel\", \"--virtualenv-relocatable\", \"--virtualenv-prompt-template\", \"--virtualenv-activate-script\",\n\t\t\t\t\t\t \"--virtualenv\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\tif cli_args.get('--enable-unsupported') or answer_file_args.get('--enable-unsupported'):\n\t\t\tlogger.warning(\"'--enable-unsupported' is deprecated. Please use '--enable-unsupported-plugins' instead.\")\n\t\t\tif cli_args.get('--enable-unsupported'):\n\t\t\t\tcli_args['--enable-unsupported-plugins'] = True\n\t\t\tif answer_file_args.get('--enable-unsupported'):\n\t\t\t\tanswer_file_args['--enable-unsupported-plugins'] = True\n\t\tif cli_args.get('--disable-unsupported') or answer_file_args.get('--disable-unsupported'):\n\t\t\tlogger.warning(\"'--disable-unsupported' is deprecated. Please use '--disable-unsupported-plugins' instead.\")\n\t\t\tif cli_args.get('--disable-unsupported'):\n\t\t\t\tcli_args['--disable-unsupported-plugins'] = True\n\t\t\tif answer_file_args.\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {'--azure_storage_container', '--azure_storage_account', '--azure_storage_key', '--azure_storage_type', '--azure_storage_sas_token',\n\t\t\t\t\t   '--azure_blob_storage_url'}\n\tdeprecated_args_map = {'--azure_storage_container': '--azure_container_name', '--azure_storage_account': '--azure_account_name',\n\t\t\t\t\t\t   '--azure_storage_key': '--azure_account_key', '--azure_storage_type': '--azure_storage_type',\n\t\t\t\t\t\t   '--azure_storage_sas_token': '--azure_sas_token', '--azure_blob_storage_url': '--azure_blob_url'}\n\tfor arg in deprecated_args:\n\t\tif arg in cli_args or arg in answer_file_args:\n\t\t\tlogger.warning(\"Deprec\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = [\"--no-prompt\", \"--no-banner\", \"--no-inform\", \"--no-connect\", \"--insecure\"]\n\tfor arg in deprecated_args:\n\t\tif cli_args.get(arg) or answer_file_args.get(arg):\n\t\t\tself.logger.warning(\"Argument '{0}' is deprecated and will be ignored.\".format(arg))\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tfor arg in self.deprecated_args:\n\t\tif cli_args.get(arg) or answer_file_args.get(arg):\n\t\t\tprint('Warning: argument \"{}\" is deprecated. Refer to the '\n\t\t\t\t  'documentation for more details.'.format(arg))\n\n\t\t\tif cli_args.get(arg):\n\t\t\t\tprint('This argument was found in the CLI.')\n\t\t\telif answer_file_args.get(arg):\n\t\t\t\tprint('This argument was found in {}.'.format(self.answer_file_path))\n\t\t\tprint('\\n')\n\n\t\t\t# Remove deprecated argument from the dict\n\t\t\tif cli_args.get(arg):\n\t\t\t\tdel cli_args[arg]\n\t\t\telif answer_file_args.get(arg):\n\t\t\t\tdel answer_file_args[arg]\n\n\treturn cli_args, answer_file_args\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tfor arg in cli_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tlogger.warning(\"The argument '--%s' is deprecated. Please use '--%s'.\" % (arg, self._deprecated_args.get(arg)))\n\tfor arg in answer_file_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tlogger.warning(\"The argument '%s' is deprecated. Please use '%s'.\" % (arg, self._deprecated_args.get(arg)))\n\n\t# if the deprecated argument is used, then replace it with the new argument name\n\tfor arg in cli_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tcli_args[self._deprecated_args.get(arg)] = cli_args[arg]\n\tfor arg in answer_file_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tanswer_file_args[self._deprecated_args.get(arg)] = answer_file_args[arg]\n\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args_list = []\n\tfor key in cli_args:\n\t\tif key in self.deprecated_args:\n\t\t\tdeprecated_args_list.append(key)\n\tfor key in answer_file_args:\n\t\tif key in self.deprecated_args:\n\t\t\tdeprecated_args_list.append(key)\n\tif deprecated_args_list:\n\t\traise Exception(\"The following arguments are deprecated:\\n{}.\".format(\"\\n\".join(deprecated_args_list)))\n\n\tdef validate_arg_dependencies(self, cli_args, answer_file_args):\n\t\t\"\"\"\n\t\tValidates the required arguments dependencies.\n\n", "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = [\n\t\t'--user-name',\n\t\t'--user-password',\n\t\t'--user-private-key-path',\n\t\t'--user-public-key-path',\n\t\t'--user-private-key-passphrase',\n\t\t'--install-agent',\n\t\t'--node-ip',\n\t\t'--node-name',\n\t\t'--node-instance-id',\n\t\t'--node-os-user',\n\t\t'--node-os-password',\n\t\t'--node-os-user-groups',\n\t\t'--node-os-user-key-path',\n\t\t'--node-os-user-public-key-path',\n\t\t'--node-os-user-private-key-passphrase',\n\t\t'--node-os-user-home-directory',\n\t\t'--install-agent-on-all-nodes',\n\t\t'--validate-all-agents',\n\t\t'--existing-agent-key',\n\t\t'--agent-key-path',\n\t\t'\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {}\n\tfor i in cli_args:\n\t\tif cli_args[i] is not None:\n\t\t\tif cli_args[i].get('deprecated') is not None:\n\t\t\t\tdeprecated_args[i] = cli_args[i].get('deprecated')\n\tfor i in answer_file_args:\n\t\tif answer_file_args[i] is not None:\n\t\t\tif answer_file_args[i].get('deprecated') is not None:\n\t\t\t\tdeprecated_args[i] = answer_file_args[i].get('deprecated')\n\tif len(deprecated_args) > 0:\n\t\tself.logger.error(\"The following arguments are deprecated. \"\n\t\t\t\t\t\t  \"Please check the documentation to see if there is a replacement.\")\n\t\tfor key, value in deprecated_args.items():\n\t\t\tself.logger.error(\"Argument: \" + key + \" - \" + value)\n\t\tsys.exit(1)\n\n\tdef validate_arg\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tself._validate_arg_deprecation(cli_args, answer_file_args)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = [\"--virtualenv-name\", \"--virtualenv-template\", \"--virtualenv-path\", \"--virtualenv-requirements\", \"--virtualenv-script\",\n\t\t\t\t\t\t \"--virtualenv-version\", \"--virtualenv-system-site-packages\", \"--virtualenv-prompt\", \"--virtualenv-pip\", \"--virtualenv-setuptools\",\n\t\t\t\t\t\t \"--virtualenv-distribute\", \"--virtualenv-wheel\", \"--virtualenv-clear\", \"--virtualenv-no-download\", \"--virtualenv-extra-search-dir\",\n\t\t\t\t\t\t \"--virtualenv-python-exe\", \"--virtualenv-always-copy\", \"--virtualenv-never-download\", \"--virtualenv-download\", \"--virtualenv-site-packages\",\n\t\t\t\t\t\t \"--virtualenv-no-setuptools\", \"--virtualenv-no-wheel\", \"--virtualenv-relocatable\", \"--virtualenv-prompt-template\", \"--virtualenv-activate-script\",\n\t\t\t\t\t\t \"--virtualenv\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\t\tif cli_args.get('--enable-unsupported') or answer_file_args.get('--enable-unsupported'):\n\t\t\tlogger.warning(\"'--enable-unsupported' is deprecated. Please use '--enable-unsupported-plugins' instead.\")\n\t\t\tif cli_args.get('--enable-unsupported'):\n\t\t\t\tcli_args['--enable-unsupported-plugins'] = True\n\t\t\tif answer_file_args.get('--enable-unsupported'):\n\t\t\t\tanswer_file_args['--enable-unsupported-plugins'] = True\n\t\tif cli_args.get('--disable-unsupported') or answer_file_args.get('--disable-unsupported'):\n\t\t\tlogger.warning(\"'--disable-unsupported' is deprecated. Please use '--disable-unsupported-plugins' instead.\")\n\t\t\tif cli_args.get('--disable-unsupported'):\n\t\t\t\tcli_args['--disable-unsupported-plugins'] = True\n\t\t\tif answer_file_args.\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = {'--azure_storage_container', '--azure_storage_account', '--azure_storage_key', '--azure_storage_type', '--azure_storage_sas_token',\n\t\t\t\t\t   '--azure_blob_storage_url'}\n\tdeprecated_args_map = {'--azure_storage_container': '--azure_container_name', '--azure_storage_account': '--azure_account_name',\n\t\t\t\t\t\t   '--azure_storage_key': '--azure_account_key', '--azure_storage_type': '--azure_storage_type',\n\t\t\t\t\t\t   '--azure_storage_sas_token': '--azure_sas_token', '--azure_blob_storage_url': '--azure_blob_url'}\n\tfor arg in deprecated_args:\n\t\tif arg in cli_args or arg in answer_file_args:\n\t\t\tlogger.warning(\"Deprec\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = [\"--no-prompt\", \"--no-banner\", \"--no-inform\", \"--no-connect\", \"--insecure\"]\n\tfor arg in deprecated_args:\n\t\tif cli_args.get(arg) or answer_file_args.get(arg):\n\t\t\tself.logger.warning(\"Argument '{0}' is deprecated and will be ignored.\".format(arg))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tfor arg in self.deprecated_args:\n\t\tif cli_args.get(arg) or answer_file_args.get(arg):\n\t\t\tprint('Warning: argument \"{}\" is deprecated. Refer to the '\n\t\t\t\t  'documentation for more details.'.format(arg))\n\n\t\t\tif cli_args.get(arg):\n\t\t\t\tprint('This argument was found in the CLI.')\n\t\t\telif answer_file_args.get(arg):\n\t\t\t\tprint('This argument was found in {}.'.format(self.answer_file_path))\n\t\t\tprint('\\n')\n\n\t\t\t# Remove deprecated argument from the dict\n\t\t\tif cli_args.get(arg):\n\t\t\t\tdel cli_args[arg]\n\t\t\telif answer_file_args.get(arg):\n\t\t\t\tdel answer_file_args[arg]\n\n\treturn cli_args, answer_file_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tfor arg in cli_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tlogger.warning(\"The argument '--%s' is deprecated. Please use '--%s'.\" % (arg, self._deprecated_args.get(arg)))\n\tfor arg in answer_file_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tlogger.warning(\"The argument '%s' is deprecated. Please use '%s'.\" % (arg, self._deprecated_args.get(arg)))\n\n\t# if the deprecated argument is used, then replace it with the new argument name\n\tfor arg in cli_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tcli_args[self._deprecated_args.get(arg)] = cli_args[arg]\n\tfor arg in answer_file_args:\n\t\tif arg in self._deprecated_args:\n\t\t\tanswer_file_args[self._deprecated_args.get(arg)] = answer_file_args[arg]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args_list = []\n\tfor key in cli_args:\n\t\tif key in self.deprecated_args:\n\t\t\tdeprecated_args_list.append(key)\n\tfor key in answer_file_args:\n\t\tif key in self.deprecated_args:\n\t\t\tdeprecated_args_list.append(key)\n\tif deprecated_args_list:\n\t\traise Exception(\"The following arguments are deprecated:\\n{}.\".format(\"\\n\".join(deprecated_args_list)))\n\n\tdef validate_arg_dependencies(self, cli_args, answer_file_args):\n\t\t\"\"\"\n\t\tValidates the required arguments dependencies.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_arg_deprecation(self, cli_args, answer_file_args):\n\tdeprecated_args = [\n\t\t'--user-name',\n\t\t'--user-password',\n\t\t'--user-private-key-path',\n\t\t'--user-public-key-path',\n\t\t'--user-private-key-passphrase',\n\t\t'--install-agent',\n\t\t'--node-ip',\n\t\t'--node-name',\n\t\t'--node-instance-id',\n\t\t'--node-os-user',\n\t\t'--node-os-password',\n\t\t'--node-os-user-groups',\n\t\t'--node-os-user-key-path',\n\t\t'--node-os-user-public-key-path',\n\t\t'--node-os-user-private-key-passphrase',\n\t\t'--node-os-user-home-directory',\n\t\t'--install-agent-on-all-nodes',\n\t\t'--validate-all-agents',\n\t\t'--existing-agent-key',\n\t\t'--agent-key-path',\n\t\t'\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_parser_option_specs", "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options", "ques_id": "6306092d73426c38ae68ad04", "code": ["def get_parser_option_specs(self, command_name):\n\tif not self.__config_parser.has_section(command_name):\n\t\treturn []\n\treturn [self.__config_parser.get(command_name, option) for option in self.__config_parser.options(command_name)]\n\n", "def get_parser_option_specs(self, command_name):\n\treturn self.command_parser_option_specs[command_name]\n\n\n", "def get_parser_option_specs(self, command_name):\n\treturn self._parser_options[command_name]\n\n", "def get_parser_option_specs(self, command_name):\n\tparser_option_specs = []\n\tparser_option_specs.extend(self.get_parser_options_specs(command_name))\n\treturn parser_option_specs\n\n\tdef get_parser_options_specs(self, command_name):\n\t\t\"\"\"\n\t\tGets all the options for the specified command\n\n", "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn parser.option_list\n\n\tdef get_parser_option_spec(self, command_name, option_name):\n\t\t\"\"\"\n\t\tGets an option for the specified command\n\n", "def get_parser_option_specs(self, command_name):\n\tparser_option_specs = []\n\tfor option_spec in self.option_specs:\n\t\tif option_spec.command_name == command_name:\n\t\t\tparser_option_specs.append(option_spec)\n\treturn parser_option_specs\n\n", "def get_parser_option_specs(self, command_name):\n\tself.logger.debug('get_parser_option_specs(%s)', command_name)\n\tcmd = self.get_command(command_name)\n\tif cmd is None:\n\t\treturn None\n\treturn cmd.option_specs\n\n", "def get_parser_option_specs(self, command_name):\n\tparser_options = []\n\n\tparser_option_spec = get_parser_option_spec(self, command_name)\n\tif parser_option_spec:\n\t\tfor option_name in parser_option_spec:\n\t\t\tparser_options.append(option_name)\n\n\treturn parser_options\n\n\n", "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn self._get_option_specs(parser)\n\n\tdef get_parser_required_options(self, command_name):\n\t\t\"\"\"\n\t\tGets all the required options for the specified command\n\n", "def get_parser_option_specs(self, command_name):\n\toption_specs = self.parser.option_list_for_command(command_name)\n\treturn [option for option in option_specs]\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_parser_option_specs(self, command_name):\n\tif not self.__config_parser.has_section(command_name):\n\t\treturn []\n\treturn [self.__config_parser.get(command_name, option) for option in self.__config_parser.options(command_name)]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\treturn self.command_parser_option_specs[command_name]\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\treturn self._parser_options[command_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tparser_option_specs = []\n\tparser_option_specs.extend(self.get_parser_options_specs(command_name))\n\treturn parser_option_specs\n\n\tdef get_parser_options_specs(self, command_name):\n\t\t\"\"\"\n\t\tGets all the options for the specified command\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn parser.option_list\n\n\tdef get_parser_option_spec(self, command_name, option_name):\n\t\t\"\"\"\n\t\tGets an option for the specified command\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tparser_option_specs = []\n\tfor option_spec in self.option_specs:\n\t\tif option_spec.command_name == command_name:\n\t\t\tparser_option_specs.append(option_spec)\n\treturn parser_option_specs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tself.logger.debug('get_parser_option_specs(%s)', command_name)\n\tcmd = self.get_command(command_name)\n\tif cmd is None:\n\t\treturn None\n\treturn cmd.option_specs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tparser_options = []\n\n\tparser_option_spec = get_parser_option_spec(self, command_name)\n\tif parser_option_spec:\n\t\tfor option_name in parser_option_spec:\n\t\t\tparser_options.append(option_name)\n\n\treturn parser_options\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\tparser = self.get_parser(command_name)\n\treturn self._get_option_specs(parser)\n\n\tdef get_parser_required_options(self, command_name):\n\t\t\"\"\"\n\t\tGets all the required options for the specified command\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_parser_option_specs(self, command_name):\n\toption_specs = self.parser.option_list_for_command(command_name)\n\treturn [option for option in option_specs]\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/helper.py", "project": "redhat-openstack/infrared", "name": "get_option_spec", "docstring": "Gets the specification for the specified option name.", "ques_id": "6306092d73426c38ae68ad05", "code": ["def get_option_spec(self, command_name, argument_name):\n\t#TODO: do a cache for this\n\treturn self.get_parser_option_specs(command_name)[argument_name]\n\n", "def get_option_spec(self, command_name, argument_name):\n\tif command_name not in self.parser_option_specs:\n\t\treturn None\n\tif argument_name not in self.parser_option_specs[command_name]:\n\t\treturn None\n\treturn self.parser_option_specs[command_name][argument_name]\n\n\t\t\n", "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\tp = self._get_parser(command_name)\n\t\treturn p.get_option_spec(argument_name)\n\texcept:\n\t\treturn None\n\n\tdef get_option_help(self, command_name, argument_name):\n\t\"\"\"\n\tGets the help for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\ttry:\n\t\tp = self._get_parser(command_name)\n\t\treturn p.get_option_help(argument_name)\n\texcept:\n\t\treturn None\n\n\tdef get_positional_help(self, command_name, argument_name):\n\t\"\"\"\n\tGets the help for the specified positional argument name. Get positional arguments of the given command_name with self.get_parser_positional_specs()\n\t\"\"\"\n\ttry:\n\t\tp = self._get_parser(command_name)\n\t\treturn p.get_positional_help(argument_name)\n\texcept:\n\t\treturn None\n\n\tdef get_positional\n", "def get_option_spec(self, command_name, argument_name):\n\tcommand_options = self.get_parser_option_specs(command_name)\n\treturn command_options.get(argument_name)\n\n\tdef get_parser_option_specs(self, command_name):\n\t\t\"\"\"\n\t\tGets a dictionary of option specifications for the given parser.\n\t\t\"\"\"\n\t\tif not self.parsers.has_key(command_name):\n\t\t\traise Exception('No parser exists with the name %s' % command_name)\n\t\treturn self._get_parser_option_specs(self.parsers[command_name])\n\n\tdef _get_parser_option_specs(self, parser):\n\t\t\"\"\"\n\t\tGets a dictionary of option specifications for the given parser.\n\t\t\"\"\"\n\t\toption_specs = {}\n\t\tfor option in parser._actions:\n\t\t\tif isinstance(option, argparse._StoreAction):\n\t\t\t\toption_specs[option.dest] = option\n\t\treturn option_specs\n\n", "def get_option_spec(self, command_name, argument_name):\n\tparser = self.get_parser_for_command(command_name)\n\treturn parser.get_option_spec(argument_name)\n\n\t\n", "def get_option_spec(self, command_name, argument_name):\n\tcommand_specs = self.get_parser_specs()\n\tif not command_name in command_specs:\n\t\treturn None\n\tcommand_spec = command_specs[command_name]\n\tif not hasattr(command_spec, 'options'):\n\t\treturn None\n\tfor option_spec in command_spec.options:\n\t\tif option_spec.dest == argument_name:\n\t\t\treturn option_spec\n\treturn None\n\t\n", "def get_option_spec(self, command_name, argument_name):\n\treturn self.get_parser().get_option_spec(command_name, argument_name)\n\n\n", "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\treturn self.get_parser_option_specs()[command_name][argument_name]\n\texcept KeyError:\n\t\treturn None\n\n\tdef get_option_group_spec(self, group_name):\n\t\t\"\"\"\n\t\tGets the specification for the specified option group name.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.get_parser_option_group_specs()[group_name]\n\t\texcept KeyError:\n\t\t\treturn None\n\n\tdef get_command_spec(self, command_name):\n\t\t\"\"\"\n\t\tGets the specification for the specified command name.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.get_parser_command_specs()[command_name]\n\t\texcept KeyError:\n\t\t\treturn None\n\n\tdef get_argument_spec(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the specification for the specified positional argument name. Get arguments of the given command_name with self.get_parser_argument_specs()\n\t\t\"\"\"\n\t\t\n", "def get_option_spec(self, command_name, argument_name):\n\t# self.get_command_option_specs(command_name)\n\treturn __get_option_spec(self, command_name, argument_name)\n\n", "def get_option_spec(self, command_name, argument_name):\n\tcmd = self.__get_command(command_name)\n\tif cmd is None:\n\t\treturn None\n\treturn cmd.get_option_spec(argument_name)\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t#TODO: do a cache for this\n\treturn self.get_parser_option_specs(command_name)[argument_name]\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tif command_name not in self.parser_option_specs:\n\t\treturn None\n\tif argument_name not in self.parser_option_specs[command_name]:\n\t\treturn None\n\treturn self.parser_option_specs[command_name][argument_name]\n\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\tp = self._get_parser(command_name)\n\t\treturn p.get_option_spec(argument_name)\n\texcept:\n\t\treturn None\n\n\tdef get_option_help(self, command_name, argument_name):\n\t\"\"\"\n\tGets the help for the specified option name. Get options of the given command_name with self.get_parser_option_specs()\n\t\"\"\"\n\ttry:\n\t\tp = self._get_parser(command_name)\n\t\treturn p.get_option_help(argument_name)\n\texcept:\n\t\treturn None\n\n\tdef get_positional_help(self, command_name, argument_name):\n\t\"\"\"\n\tGets the help for the specified positional argument name. Get positional arguments of the given command_name with self.get_parser_positional_specs()\n\t\"\"\"\n\ttry:\n\t\tp = self._get_parser(command_name)\n\t\treturn p.get_positional_help(argument_name)\n\texcept:\n\t\treturn None\n\n\tdef get_positional\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tcommand_options = self.get_parser_option_specs(command_name)\n\treturn command_options.get(argument_name)\n\n\tdef get_parser_option_specs(self, command_name):\n\t\t\"\"\"\n\t\tGets a dictionary of option specifications for the given parser.\n\t\t\"\"\"\n\t\tif not self.parsers.has_key(command_name):\n\t\t\traise Exception('No parser exists with the name %s' % command_name)\n\t\treturn self._get_parser_option_specs(self.parsers[command_name])\n\n\tdef _get_parser_option_specs(self, parser):\n\t\t\"\"\"\n\t\tGets a dictionary of option specifications for the given parser.\n\t\t\"\"\"\n\t\toption_specs = {}\n\t\tfor option in parser._actions:\n\t\t\tif isinstance(option, argparse._StoreAction):\n\t\t\t\toption_specs[option.dest] = option\n\t\treturn option_specs\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tparser = self.get_parser_for_command(command_name)\n\treturn parser.get_option_spec(argument_name)\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tcommand_specs = self.get_parser_specs()\n\tif not command_name in command_specs:\n\t\treturn None\n\tcommand_spec = command_specs[command_name]\n\tif not hasattr(command_spec, 'options'):\n\t\treturn None\n\tfor option_spec in command_spec.options:\n\t\tif option_spec.dest == argument_name:\n\t\t\treturn option_spec\n\treturn None\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\treturn self.get_parser().get_option_spec(command_name, argument_name)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\ttry:\n\t\treturn self.get_parser_option_specs()[command_name][argument_name]\n\texcept KeyError:\n\t\treturn None\n\n\tdef get_option_group_spec(self, group_name):\n\t\t\"\"\"\n\t\tGets the specification for the specified option group name.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.get_parser_option_group_specs()[group_name]\n\t\texcept KeyError:\n\t\t\treturn None\n\n\tdef get_command_spec(self, command_name):\n\t\t\"\"\"\n\t\tGets the specification for the specified command name.\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn self.get_parser_command_specs()[command_name]\n\t\texcept KeyError:\n\t\t\treturn None\n\n\tdef get_argument_spec(self, command_name, argument_name):\n\t\t\"\"\"\n\t\tGets the specification for the specified positional argument name. Get arguments of the given command_name with self.get_parser_argument_specs()\n\t\t\"\"\"\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\t# self.get_command_option_specs(command_name)\n\treturn __get_option_spec(self, command_name, argument_name)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_option_spec(self, command_name, argument_name):\n\tcmd = self.__get_command(command_name)\n\tif cmd is None:\n\t\treturn None\n\treturn cmd.get_option_spec(argument_name)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_silent_args", "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names", "ques_id": "6306092d73426c38ae68ad06", "code": ["def get_silent_args(self, args):\n\treturn [a[1:] for a in args if a.startswith(\"-\")]\n\n\tdef get_args_with_values(self, args):\n\t\t\"\"\"\n\t\tlist of argument with values\n\n", "def get_silent_args(self, args):\n\t\tsilent_args = []\n\t\tfor i in args:\n\t\t\tif i.startswith('-'):\n\t\t\t\tsilent_args.append(i)\n\t\t\telse:\n\t\t\t\tpass\n\t\treturn silent_args\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\"\"\"\n\t\tlist of silenced keyworded argument\n\n", "def get_silent_args(self, args):\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('--') and '=' in arg:\n\t\t\tsilent_args.append(arg.split('=')[0][2:])\n\treturn silent_args\n\t\n", "def get_silent_args(self, args):\n\targs = args or {}\n\tsilent = args.get('silent', [])\n\treturn silent\n\n", "def get_silent_args(self, args):\n\tsilent = []\n\tfor arg in args:\n\t\tif arg[0] == '-':\n\t\t\tsilent.append(arg)\n\treturn silent\n\n", "def get_silent_args(self, args):\n\t\treturn [arg[1:] for arg in args if arg.startswith(\"-\")]\n\n\tdef get_args(self, args):\n\t\t\"\"\"\n\tReturns a dictionary of the arguments.\n\n", "def get_silent_args(self, args):\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-s'):\n\t\t\tsilent_args.append(arg.replace('-s', ''))\n\treturn silent_args\n\n\tdef get_file_path(self, args):\n\t\t\"\"\"\n\t\tgets file path from the arguments\n\n", "def get_silent_args(self, args):\n\tsilenced_args = []\n\tfor key, value in args.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor k, v in value.items():\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\tfor k2, v2 in v.items():\n\t\t\t\t\t\tif k2 == \"silent\":\n\t\t\t\t\t\t\tsilenced_args.append(k)\n\t\t\t\telse:\n\t\t\t\t\tif k == \"silent\":\n\t\t\t\t\t\tsilenced_args.append(key)\n\treturn silenced_args\n\n\tdef get_args_value(self, args):\n\t\t\"\"\"\n\t\tGet the value of argument.\n\n\t\t:param args: The received arguments.\n\t\t:return: dict, the arguments' name and value\n\t\t\"\"\"\n\t\targs_value = dict()\n\t\tfor key, value in args.items():\n\t\t\tif isinstance(value, dict):\n\t\t\t\tfor k, v in value.items():\n\t\t\t\t\tif isinstance\n", "def get_silent_args(self, args):\n\t\treturn [arg for arg in args if arg[0] in self.silent_args and arg[1] is not None]\n\n\tdef get_named_args(self, args):\n\t\t\"\"\"\n\t\tlist of named argument\n\n", "def get_silent_args(self, args):\n\t\tsilent_args = []\n\t\tfor arg in args:\n\t\t\tif arg in self.silent_args:\n\t\t\t\tsilent_args.append(arg)\n\t\treturn silent_args\n\n\tdef get_verbose_args(self, args):\n\t\t\"\"\"\n\t\tlist of verbose argument\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def get_silent_args(self, args):\n\treturn [a[1:] for a in args if a.startswith(\"-\")]\n\n\tdef get_args_with_values(self, args):\n\t\t\"\"\"\n\t\tlist of argument with values\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\tsilent_args = []\n\t\tfor i in args:\n\t\t\tif i.startswith('-'):\n\t\t\t\tsilent_args.append(i)\n\t\t\telse:\n\t\t\t\tpass\n\t\treturn silent_args\n\n\tdef get_silent_kwargs(self, kwargs):\n\t\"\"\"\n\t\tlist of silenced keyworded argument\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('--') and '=' in arg:\n\t\t\tsilent_args.append(arg.split('=')[0][2:])\n\treturn silent_args\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n\targs = args or {}\n\tsilent = args.get('silent', [])\n\treturn silent\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n\tsilent = []\n\tfor arg in args:\n\t\tif arg[0] == '-':\n\t\t\tsilent.append(arg)\n\treturn silent\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def get_silent_args(self, args):\n\t\treturn [arg[1:] for arg in args if arg.startswith(\"-\")]\n\n\tdef get_args(self, args):\n\t\t\"\"\"\n\tReturns a dictionary of the arguments.\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\tsilent_args = []\n\tfor arg in args:\n\t\tif arg.startswith('-s'):\n\t\t\tsilent_args.append(arg.replace('-s', ''))\n\treturn silent_args\n\n\tdef get_file_path(self, args):\n\t\t\"\"\"\n\t\tgets file path from the arguments\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\tsilenced_args = []\n\tfor key, value in args.items():\n\t\tif isinstance(value, dict):\n\t\t\tfor k, v in value.items():\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\tfor k2, v2 in v.items():\n\t\t\t\t\t\tif k2 == \"silent\":\n\t\t\t\t\t\t\tsilenced_args.append(k)\n\t\t\t\telse:\n\t\t\t\t\tif k == \"silent\":\n\t\t\t\t\t\tsilenced_args.append(key)\n\treturn silenced_args\n\n\tdef get_args_value(self, args):\n\t\t\"\"\"\n\t\tGet the value of argument.\n\n\t\t:param args: The received arguments.\n\t\t:return: dict, the arguments' name and value\n\t\t\"\"\"\n\t\targs_value = dict()\n\t\tfor key, value in args.items():\n\t\t\tif isinstance(value, dict):\n\t\t\t\tfor k, v in value.items():\n\t\t\t\t\tif isinstance\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\treturn [arg for arg in args if arg[0] in self.silent_args and arg[1] is not None]\n\n\tdef get_named_args(self, args):\n\t\t\"\"\"\n\t\tlist of named argument\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_silent_args(self, args):\n\t\tsilent_args = []\n\t\tfor arg in args:\n\t\t\tif arg in self.silent_args:\n\t\t\t\tsilent_args.append(arg)\n\t\treturn silent_args\n\n\tdef get_verbose_args(self, args):\n\t\t\"\"\"\n\t\tlist of verbose argument\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_requires_args", "docstring": "Check if all the required arguments have been provided.", "ques_id": "6306092d73426c38ae68ad07", "code": ["def validate_requires_args(self, args):\n\tself.validate_args(args)\n\tmissing_args = []\n\tfor arg in self.inputs.values():\n\t\tif arg[\"required\"] and arg[\"value\"] == None:\n\t\t\tmissing_args.append(arg[\"name\"])\n\tif len(missing_args) > 0:\n\t\tprint \"The following arguments are required: \" + \", \".join(missing_args)\n\t\tsys.exit(1)\n\n", "def validate_requires_args(self, args):\n\tif not self.plugin_requires_args():\n\t\treturn True\n\n\tfor req in self.plugin_requires:\n\t\tif req not in args:\n\t\t\treturn False\n\treturn True\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tReturn True if args are valid.\n\t\t\"\"\"\n\t\tif not self.plugin_requires_args():\n\t\t\treturn True\n\t\traise NotImplementedError\n\n\tdef plugin_requires_args(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires arguments.\n\t\t\"\"\"\n\t\treturn len(self.plugin_requires) > 0\n\n\tdef plugin_requires_root(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires root privileges.\n\t\t\"\"\"\n\t\treturn self.plugin_root_required\n\n\tdef plugin_requires_sudo(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires sudo privileges.\n\t\t\"\"\"\n\t\treturn self.plugin_sudo_required\n\n\tdef plugin_requires_admin(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires admin privileges\n", "def validate_requires_args(self, args):\n\treturn True\n\n", "def validate_requires_args(self, args):\n\tfor arg in self.__class__.required_args:\n\t\tif arg not in args:\n\t\t\traise TypeError(\n\t\t\t\t\"%s() missing required argument '%s'\" % (self.__class__.__name__, arg))\n\treturn True\n\n\n", "def validate_requires_args(self, args):\n\treturn len(args) > 0\n\n\n", "def validate_requires_args(self, args):\n\tif not args.file_name:\n\t\tself.parser.error(\"Missing file name\")\n\tif not args.output_path:\n\t\tself.parser.error(\"Missing output path\")\n\n\n", "def validate_requires_args(self, args):\n\tif not args:\n\t\treturn False\n\n\tfor arg in args:\n\t\tif not getattr(self, arg, None):\n\t\t\treturn False\n\n\treturn True\n\n", "def validate_requires_args(self, args):\n\tif not args.f:\n\t\tself.error(\"Please provide the input file.\")\n\n\tif not args.o:\n\t\tself.error(\"Please provide the output file.\")\n\n\treturn args\n\n", "def validate_requires_args(self, args):\n\tfor arg in args:\n\t\tif not arg.get('required', True):\n\t\t\tcontinue\n\t\tif not arg['name'] in self.parameters:\n\t\t\traise ValueError('Missing required parameter \\'%s\\'' % arg['name'])\n\n\tdef validate_parameter_values(self, args):\n\t\t\"\"\"\n\t\tValidate parameter values.\n\t\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg['name'] in self.parameters:\n\t\t\t\tcontinue\n\t\t\tvalue = self.parameters[arg['name']]\n\t\t\tif 'options' in arg and not value in arg['options']:\n\t\t\t\traise ValueError('Parameter \\'%s\\' must be one of: %s' % (arg['name'], ', '.join(arg['options'])))\n\n\tdef validate_parameter_types(self, args):\n\t\t\"\"\"\n\t\tValidate the types of the provided parameters.\n\t\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg['name'] in self.parameters:\n\t\t\t\t\n", "def validate_requires_args(self, args):\n\trequires_args = self.get_requires_args()\n\tfor arg in requires_args:\n\t\tif arg not in args:\n\t\t\traise Exception(\"Argument error: '\" + arg + \"' is required.\")\n\n\tdef get_requires_args(self):\n\t\t\"\"\"\n\t\tReturn the list of arguments that are required for this command.\n\t\t\"\"\"\n\t\treturn self.requires_args\n\n\tdef get_help(self):\n\t\t\"\"\"\n\t\tReturn the help text for this command.\n\t\t\"\"\"\n\t\treturn self.help\n\n\tdef get_name(self):\n\t\t\"\"\"\n\t\tReturn the name of this command.\n\t\t\"\"\"\n\t\treturn self.name\n\n\tdef get_usage(self):\n\t\t\"\"\"\n\t\tReturn the usage text for this command.\n\t\t\"\"\"\n\t\treturn self.usage\n\n\tdef get_subcommands(self):\n\t\t\"\"\"\n\t\tReturn the dictionary of subcommands of this command.\n\t\t\"\"\"\n\t\treturn self.subcommands\n\n\tdef get_subcommand_names(self):\n\t\t\"\"\"\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_requires_args(self, args):\n\tself.validate_args(args)\n\tmissing_args = []\n\tfor arg in self.inputs.values():\n\t\tif arg[\"required\"] and arg[\"value\"] == None:\n\t\t\tmissing_args.append(arg[\"name\"])\n\tif len(missing_args) > 0:\n\t\tprint \"The following arguments are required: \" + \", \".join(missing_args)\n\t\tsys.exit(1)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif not self.plugin_requires_args():\n\t\treturn True\n\n\tfor req in self.plugin_requires:\n\t\tif req not in args:\n\t\t\treturn False\n\treturn True\n\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tReturn True if args are valid.\n\t\t\"\"\"\n\t\tif not self.plugin_requires_args():\n\t\t\treturn True\n\t\traise NotImplementedError\n\n\tdef plugin_requires_args(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires arguments.\n\t\t\"\"\"\n\t\treturn len(self.plugin_requires) > 0\n\n\tdef plugin_requires_root(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires root privileges.\n\t\t\"\"\"\n\t\treturn self.plugin_root_required\n\n\tdef plugin_requires_sudo(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires sudo privileges.\n\t\t\"\"\"\n\t\treturn self.plugin_sudo_required\n\n\tdef plugin_requires_admin(self):\n\t\t\"\"\"\n\t\tReturn True if the plugin requires admin privileges\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tfor arg in self.__class__.required_args:\n\t\tif arg not in args:\n\t\t\traise TypeError(\n\t\t\t\t\"%s() missing required argument '%s'\" % (self.__class__.__name__, arg))\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\treturn len(args) > 0\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif not args.file_name:\n\t\tself.parser.error(\"Missing file name\")\n\tif not args.output_path:\n\t\tself.parser.error(\"Missing output path\")\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif not args:\n\t\treturn False\n\n\tfor arg in args:\n\t\tif not getattr(self, arg, None):\n\t\t\treturn False\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tif not args.f:\n\t\tself.error(\"Please provide the input file.\")\n\n\tif not args.o:\n\t\tself.error(\"Please provide the output file.\")\n\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\tfor arg in args:\n\t\tif not arg.get('required', True):\n\t\t\tcontinue\n\t\tif not arg['name'] in self.parameters:\n\t\t\traise ValueError('Missing required parameter \\'%s\\'' % arg['name'])\n\n\tdef validate_parameter_values(self, args):\n\t\t\"\"\"\n\t\tValidate parameter values.\n\t\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg['name'] in self.parameters:\n\t\t\t\tcontinue\n\t\t\tvalue = self.parameters[arg['name']]\n\t\t\tif 'options' in arg and not value in arg['options']:\n\t\t\t\traise ValueError('Parameter \\'%s\\' must be one of: %s' % (arg['name'], ', '.join(arg['options'])))\n\n\tdef validate_parameter_types(self, args):\n\t\t\"\"\"\n\t\tValidate the types of the provided parameters.\n\t\t\"\"\"\n\t\tfor arg in args:\n\t\t\tif not arg['name'] in self.parameters:\n\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_requires_args(self, args):\n\trequires_args = self.get_requires_args()\n\tfor arg in requires_args:\n\t\tif arg not in args:\n\t\t\traise Exception(\"Argument error: '\" + arg + \"' is required.\")\n\n\tdef get_requires_args(self):\n\t\t\"\"\"\n\t\tReturn the list of arguments that are required for this command.\n\t\t\"\"\"\n\t\treturn self.requires_args\n\n\tdef get_help(self):\n\t\t\"\"\"\n\t\tReturn the help text for this command.\n\t\t\"\"\"\n\t\treturn self.help\n\n\tdef get_name(self):\n\t\t\"\"\"\n\t\tReturn the name of this command.\n\t\t\"\"\"\n\t\treturn self.name\n\n\tdef get_usage(self):\n\t\t\"\"\"\n\t\tReturn the usage text for this command.\n\t\t\"\"\"\n\t\treturn self.usage\n\n\tdef get_subcommands(self):\n\t\t\"\"\"\n\t\tReturn the dictionary of subcommands of this command.\n\t\t\"\"\"\n\t\treturn self.subcommands\n\n\tdef get_subcommand_names(self):\n\t\t\"\"\"\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_get_conditionally_required_args", "docstring": "List arguments with ``required_when`` condition matched.\n\n:param command_name: the command name.\n:param options_spec:  the list of command spec options.\n:param args: the received input arguments\n:return: list, list of argument names with matched ``required_when``\n    condition", "ques_id": "6306092d73426c38ae68ad08", "code": ["def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tresult = []\n\tfor option in options_spec:\n\t\twhen = option.get('required_when')\n\t\tif when:\n\t\t\tif self._match_when_condition(command_name, when, args):\n\t\t\t\tresult.append(option['name'])\n\treturn result\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option in options_spec:\n\t\tif option['name'] in args:\n\t\t\tif option.get('required_when'):\n\t\t\t\tif option['required_when'] == args[option['name']]:\n\t\t\t\t\tconditionally_required_args.append(option['name'])\n\t\t\tif option.get('required_when_not'):\n\t\t\t\tif option['required_when_not'] != args[option['name']]:\n\t\t\t\t\tconditionally_required_args.append(option['name'])\n\treturn conditionally_required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\targs_with_condition = []\n\tfor option_spec in options_spec:\n\t\tif 'required_when' in option_spec:\n\t\t\tcondition = option_spec['required_when']\n\t\t\tif _is_condition_matched(condition, args):\n\t\t\t\targs_with_condition.append(option_spec['name'])\n\treturn args_with_condition\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif 'required_when' in option and self._is_required_when_string(option, args):\n\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor opt in options_spec:\n\t\tif opt.get('required_when'):\n\t\t\tif self._is_required_when_matched(opt, args):\n\t\t\t\trequired_args.append(opt['name'])\n\treturn required_args\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor arg_spec in options_spec:\n\t\tif arg_spec.get(\"required_when\") is not None:\n\t\t\tfor condition_arg, condition_arg_value in \\\n\t\t\t\targ_spec[\"required_when\"].items():\n\t\t\t\tif not args.get(condition_arg):\n\t\t\t\t\tcontinue\n\t\t\t\tif args[condition_arg] == condition_arg_value:\n\t\t\t\t\tconditionally_required_args.append(arg_spec[\"name\"])\n\n\treturn conditionally_required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option in options_spec:\n\t\toption_name = option.get(\"name\")\n\t\tif not option_name:\n\t\t\t# we are not interested in positional arguments\n\t\t\tcontinue\n\t\tif \"required_when\" not in option:\n\t\t\tcontinue\n\t\trequired_when = option.get(\"required_when\")\n\t\tif required_when.get(\"command\") != command_name:\n\t\t\tcontinue\n\t\tif type(required_when.get(\"arguments\")) is not list:\n\t\t\t# TODO: raise error\n\t\t\tcontinue\n\t\tif set(required_when.get(\"arguments\")).issubset(set(args)):\n\t\t\tconditionally_required_args.append(option_name)\n\treturn conditionally_required_args\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tif not options_spec:\n\t\treturn []\n\targ_names = []\n\tfor arg_spec in options_spec:\n\t\tif arg_spec.get('required_when') is not None:\n\t\t\targ_names_to_match = arg_spec.get('required_when')\n\t\t\tif isinstance(arg_names_to_match, str):\n\t\t\t\targ_names_to_match = [arg_names_to_match]\n\t\t\tif arg_names_to_match:\n\t\t\t\tif self.__is_required_when_cond_match(command_name,\n\t\t\t\t                                      arg_names_to_match,\n\t\t\t\t                                      args):\n\t\t\t\t\targ_names.append(arg_spec.get('name'))\n\treturn arg_names\n\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option in options_spec:\n\t\tif 'required_when' in option:\n\t\t\tcondition_name = option['required_when'][0]\n\t\t\tcondition_value = option['required_when'][1]\n\t\t\tif self._is_condition_matched(condition_name, condition_value,\n\t\t\t                              options_spec, args):\n\t\t\t\tconditionally_required_args.append(option['name'])\n\treturn conditionally_required_args\n\n", "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif (option.get('required_when') and\n\t\t\t\toption['required_when'](command_name, args)):\n\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tresult = []\n\tfor option in options_spec:\n\t\twhen = option.get('required_when')\n\t\tif when:\n\t\t\tif self._match_when_condition(command_name, when, args):\n\t\t\t\tresult.append(option['name'])\n\treturn result\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option in options_spec:\n\t\tif option['name'] in args:\n\t\t\tif option.get('required_when'):\n\t\t\t\tif option['required_when'] == args[option['name']]:\n\t\t\t\t\tconditionally_required_args.append(option['name'])\n\t\t\tif option.get('required_when_not'):\n\t\t\t\tif option['required_when_not'] != args[option['name']]:\n\t\t\t\t\tconditionally_required_args.append(option['name'])\n\treturn conditionally_required_args\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\targs_with_condition = []\n\tfor option_spec in options_spec:\n\t\tif 'required_when' in option_spec:\n\t\t\tcondition = option_spec['required_when']\n\t\t\tif _is_condition_matched(condition, args):\n\t\t\t\targs_with_condition.append(option_spec['name'])\n\treturn args_with_condition\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif 'required_when' in option and self._is_required_when_string(option, args):\n\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor opt in options_spec:\n\t\tif opt.get('required_when'):\n\t\t\tif self._is_required_when_matched(opt, args):\n\t\t\t\trequired_args.append(opt['name'])\n\treturn required_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor arg_spec in options_spec:\n\t\tif arg_spec.get(\"required_when\") is not None:\n\t\t\tfor condition_arg, condition_arg_value in \\\n\t\t\t\targ_spec[\"required_when\"].items():\n\t\t\t\tif not args.get(condition_arg):\n\t\t\t\t\tcontinue\n\t\t\t\tif args[condition_arg] == condition_arg_value:\n\t\t\t\t\tconditionally_required_args.append(arg_spec[\"name\"])\n\n\treturn conditionally_required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option in options_spec:\n\t\toption_name = option.get(\"name\")\n\t\tif not option_name:\n\t\t\t# we are not interested in positional arguments\n\t\t\tcontinue\n\t\tif \"required_when\" not in option:\n\t\t\tcontinue\n\t\trequired_when = option.get(\"required_when\")\n\t\tif required_when.get(\"command\") != command_name:\n\t\t\tcontinue\n\t\tif type(required_when.get(\"arguments\")) is not list:\n\t\t\t# TODO: raise error\n\t\t\tcontinue\n\t\tif set(required_when.get(\"arguments\")).issubset(set(args)):\n\t\t\tconditionally_required_args.append(option_name)\n\treturn conditionally_required_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tif not options_spec:\n\t\treturn []\n\targ_names = []\n\tfor arg_spec in options_spec:\n\t\tif arg_spec.get('required_when') is not None:\n\t\t\targ_names_to_match = arg_spec.get('required_when')\n\t\t\tif isinstance(arg_names_to_match, str):\n\t\t\t\targ_names_to_match = [arg_names_to_match]\n\t\t\tif arg_names_to_match:\n\t\t\t\tif self.__is_required_when_cond_match(command_name,\n\t\t\t\t                                      arg_names_to_match,\n\t\t\t\t                                      args):\n\t\t\t\t\targ_names.append(arg_spec.get('name'))\n\treturn arg_names\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\tconditionally_required_args = []\n\tfor option in options_spec:\n\t\tif 'required_when' in option:\n\t\t\tcondition_name = option['required_when'][0]\n\t\t\tcondition_value = option['required_when'][1]\n\t\t\tif self._is_condition_matched(condition_name, condition_value,\n\t\t\t                              options_spec, args):\n\t\t\t\tconditionally_required_args.append(option['name'])\n\treturn conditionally_required_args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n\trequired_args = []\n\tfor option in options_spec:\n\t\tif (option.get('required_when') and\n\t\t\t\toption['required_when'](command_name, args)):\n\t\t\trequired_args.append(option['name'])\n\treturn required_args\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_length_args", "docstring": "Check if value of arguments is not longer than length specified.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad09", "code": ["def validate_length_args(self, args):\n\tfor key, value in args.items():\n\t\tif key in self.length_args:\n\t\t\tif len(value) > self.length_args[key]:\n\t\t\t\treturn False\n\treturn True\n\n", "def validate_length_args(self, args):\n\tif len(args) > self.length:\n\t\traise TypeError(self.error_msg)\n\n", "def validate_length_args(self, args):\n\tif len(args) > self.length:\n\t\traise ValueError('Argument must be {} characters or less.'.format(self.length))\n\n\treturn args\n\n\n", "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tif arg[1] and (len(arg[1]) > int(arg[2])):\n\t\t\traise ValueError('Value of the argument \\\"{}\\\" is not valid'.format(arg[0]))\n\n", "def validate_length_args(self, args):\n\tfor key, value in args.items():\n\t\tif key in self.length and len(value) > self.length[key]:\n\t\t\traise InvalidUsage(description=\"Value of {} is longer than {}\".format(key, self.length[key]), status_code=400)\n\n\t\tif key in self.min_length and len(value) < self.min_length[key]:\n\t\t\traise InvalidUsage(description=\"Value of {} is shorter than {}\".format(key, self.min_length[key]), status_code=400)\n\n\t\tif key in self.max_length and len(value) > self.max_length[key]:\n\t\t\traise InvalidUsage(description=\"Value of {} is longer than {}\".format(key, self.max_length[key]), status_code=400)\n\n\treturn True\n\n\n", "def validate_length_args(self, args):\n\tfor key in args:\n\t\tif args[key] and len(args[key]) > self.length[key]:\n\t\t\treturn False\n\treturn True\n\n\n", "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tif len(args[arg]) > self.length[arg]:\n\t\t\traise ValueError('length of ' + arg + ' is longer than ' + str(self.length[arg]))\n\n\n", "def validate_length_args(self, args):\n\tfor key, value in args.items():\n\t\tif not isinstance(value, str):\n\t\t\tcontinue\n\t\tif self._length_args.get(key) is None:\n\t\t\tcontinue\n\t\tif len(value) > self._length_args[key]:\n\t\t\traise Exception('The length of {} should not be longer than {}'.format(key, self._length_args[key]))\n\n\treturn True\n\n\n", "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tif len(args[arg]) > self.length[arg]:\n\t\t\treturn False\n\treturn True\n\n", "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tfor arg_name in arg:\n\t\t\targ_value = arg[arg_name]\n\t\t\tif len(arg_value) > arg_name.length:\n\t\t\t\treturn True\n\treturn False\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_length_args(self, args):\n\tfor key, value in args.items():\n\t\tif key in self.length_args:\n\t\t\tif len(value) > self.length_args[key]:\n\t\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tif len(args) > self.length:\n\t\traise TypeError(self.error_msg)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tif len(args) > self.length:\n\t\traise ValueError('Argument must be {} characters or less.'.format(self.length))\n\n\treturn args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tif arg[1] and (len(arg[1]) > int(arg[2])):\n\t\t\traise ValueError('Value of the argument \\\"{}\\\" is not valid'.format(arg[0]))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tfor key, value in args.items():\n\t\tif key in self.length and len(value) > self.length[key]:\n\t\t\traise InvalidUsage(description=\"Value of {} is longer than {}\".format(key, self.length[key]), status_code=400)\n\n\t\tif key in self.min_length and len(value) < self.min_length[key]:\n\t\t\traise InvalidUsage(description=\"Value of {} is shorter than {}\".format(key, self.min_length[key]), status_code=400)\n\n\t\tif key in self.max_length and len(value) > self.max_length[key]:\n\t\t\traise InvalidUsage(description=\"Value of {} is longer than {}\".format(key, self.max_length[key]), status_code=400)\n\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tfor key in args:\n\t\tif args[key] and len(args[key]) > self.length[key]:\n\t\t\treturn False\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tif len(args[arg]) > self.length[arg]:\n\t\t\traise ValueError('length of ' + arg + ' is longer than ' + str(self.length[arg]))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tfor key, value in args.items():\n\t\tif not isinstance(value, str):\n\t\t\tcontinue\n\t\tif self._length_args.get(key) is None:\n\t\t\tcontinue\n\t\tif len(value) > self._length_args[key]:\n\t\t\traise Exception('The length of {} should not be longer than {}'.format(key, self._length_args[key]))\n\n\treturn True\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tif len(args[arg]) > self.length[arg]:\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_length_args(self, args):\n\tfor arg in args:\n\t\tfor arg_name in arg:\n\t\t\targ_value = arg[arg_name]\n\t\t\tif len(arg_value) > arg_name.length:\n\t\t\t\treturn True\n\treturn False\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_choices_args", "docstring": "Check if value of choice arguments is one of the available choices.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0a", "code": ["def validate_choices_args(self, args):\n\tif self.choices:\n\t\tfor choice in self.choices:\n\t\t\tif args.strip() in choice:\n\t\t\t\treturn True\n\t\treturn False\n\treturn True\n\n", "def validate_choices_args(self, args):\n\t\tfor arg in args:\n\t\t\tif arg not in self.choices:\n\t\t\t\traise ValueError(f'Invalid choice: {arg}. (choose from {self.choices})')\n\t\t\t\t\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if number of received arguments is valid, and validate the value of the received arguments.\n\t\t\"\"\"\n\t\tif len(args) != self.nargs:\n\t\t\traise ValueError(f'command {self.name} requires {self.nargs} argument(s)')\n\t\tif self.choices:\n\t\t\tself.validate_choices_args(args)\n\t\tif self.min_args:\n\t\t\tself.validate_min_args(args)\n\t\tif self.max_args:\n\t\t\tself.validate_max_args(args)\n\t\tif self.arg_type:\n\t\t\tself.validate_arg_type(args)\n\t\t\t\n\t\t\t\n\tdef validate_min_args(self, args):\n\n", "def validate_choices_args(self, args):\n\tfor arg in (args or []):\n\t\tif arg not in self.choices:\n\t\t\traise CommandError('\"%s\" is not a valid choice.' % arg)\n\treturn args\n\n\n", "def validate_choices_args(self, args):\n\t\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise Exception('Invalid value for argument: {}. Please make sure it is one of the available choices.'.format(arg))\n\t\t\t\n", "def validate_choices_args(self, args):\n\tif args.available_choices:\n\t\tchoices = args.available_choices.split(\",\")\n\t\tif choices[0] == \"\":\n\t\t\tchoices.pop(0)\n\t\t# Check if the given value is one of the available choices\n\t\tif args.value not in choices:\n\t\t\tsys.exit(\"Error: Invalid value for {}. The value must be one of the following: {}\".format(args.key, args.available_choices))\n\n\n", "def validate_choices_args(self, args):\n\tfor arg in args:\n\t\tif not args[arg] in self.choices[arg]:\n\t\t\traise argparse.ArgumentTypeError(\"{} is not a valid {}\".format(args[arg], arg))\n\n\treturn args\n\n", "def validate_choices_args(self, args):\n\tif not args:\n\t\treturn\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise argparse.ArgumentTypeError(\"invalid choice: '%s' (choose from %s)\" % (arg, self.choices))\n\treturn args\n\n", "def validate_choices_args(self, args):\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise ValueError(\"Invalid choice: {}. (choose from {})\".format(arg, self.choices))\n\n", "def validate_choices_args(self, args):\n\tfor arg in args:\n\t\tif arg not in self.choices and arg is not None:\n\t\t\tself.logger.error(\"Argument {0} does not match any of the available choices. Available choices: {1}\".format(arg, self.choices))\n\t\t\traise InvalidArguments(\"Argument {0} does not match any of the available choices. Available choices: {1}\".format(arg, self.choices))\n\n\n", "def validate_choices_args(self, args):\n\t\tif not isinstance(args, dict):\n\t\t\traise TypeError(\"Arguments must be a dictionary.\")\n\n\t\tfor key, value in args.items():\n\t\t\tif key not in self._choices:\n\t\t\t\traise KeyError(\n\t\t\t\t\t\"Argument {} is not a valid choice.\".format(key))\n\n\t\t\tif value not in self._choices[key]:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Value {} of argument {} is not a valid choice.\".format(value, key))\n\n\tdef validate_arguments(self, args):\n\t\t\"\"\"\n\t\tCheck if the given args are valid. \\n:param args: The received arguments.\n\t\t\"\"\"\n\t\tfor key, value in args.items():\n\t\t\tif key not in self._args:\n\t\t\t\traise KeyError(\n\t\t\t\t\t\"Argument {} is not a valid argument.\".format(key))\n\n\t\t\tif isinstance(value, str):\n\t\t\t\tvalue = value.strip()\n\n\t\t\tif not self._args\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_choices_args(self, args):\n\tif self.choices:\n\t\tfor choice in self.choices:\n\t\t\tif args.strip() in choice:\n\t\t\t\treturn True\n\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\tfor arg in args:\n\t\t\tif arg not in self.choices:\n\t\t\t\traise ValueError(f'Invalid choice: {arg}. (choose from {self.choices})')\n\t\t\t\t\n\tdef validate_args(self, args):\n\t\t\"\"\"\n\t\tCheck if number of received arguments is valid, and validate the value of the received arguments.\n\t\t\"\"\"\n\t\tif len(args) != self.nargs:\n\t\t\traise ValueError(f'command {self.name} requires {self.nargs} argument(s)')\n\t\tif self.choices:\n\t\t\tself.validate_choices_args(args)\n\t\tif self.min_args:\n\t\t\tself.validate_min_args(args)\n\t\tif self.max_args:\n\t\t\tself.validate_max_args(args)\n\t\tif self.arg_type:\n\t\t\tself.validate_arg_type(args)\n\t\t\t\n\t\t\t\n\tdef validate_min_args(self, args):\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg in (args or []):\n\t\tif arg not in self.choices:\n\t\t\traise CommandError('\"%s\" is not a valid choice.' % arg)\n\treturn args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise Exception('Invalid value for argument: {}. Please make sure it is one of the available choices.'.format(arg))\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tif args.available_choices:\n\t\tchoices = args.available_choices.split(\",\")\n\t\tif choices[0] == \"\":\n\t\t\tchoices.pop(0)\n\t\t# Check if the given value is one of the available choices\n\t\tif args.value not in choices:\n\t\t\tsys.exit(\"Error: Invalid value for {}. The value must be one of the following: {}\".format(args.key, args.available_choices))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg in args:\n\t\tif not args[arg] in self.choices[arg]:\n\t\t\traise argparse.ArgumentTypeError(\"{} is not a valid {}\".format(args[arg], arg))\n\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tif not args:\n\t\treturn\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise argparse.ArgumentTypeError(\"invalid choice: '%s' (choose from %s)\" % (arg, self.choices))\n\treturn args\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg in args:\n\t\tif arg not in self.choices:\n\t\t\traise ValueError(\"Invalid choice: {}. (choose from {})\".format(arg, self.choices))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\tfor arg in args:\n\t\tif arg not in self.choices and arg is not None:\n\t\t\tself.logger.error(\"Argument {0} does not match any of the available choices. Available choices: {1}\".format(arg, self.choices))\n\t\t\traise InvalidArguments(\"Argument {0} does not match any of the available choices. Available choices: {1}\".format(arg, self.choices))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_choices_args(self, args):\n\t\tif not isinstance(args, dict):\n\t\t\traise TypeError(\"Arguments must be a dictionary.\")\n\n\t\tfor key, value in args.items():\n\t\t\tif key not in self._choices:\n\t\t\t\traise KeyError(\n\t\t\t\t\t\"Argument {} is not a valid choice.\".format(key))\n\n\t\t\tif value not in self._choices[key]:\n\t\t\t\traise ValueError(\n\t\t\t\t\t\"Value {} of argument {} is not a valid choice.\".format(value, key))\n\n\tdef validate_arguments(self, args):\n\t\t\"\"\"\n\t\tCheck if the given args are valid. \\n:param args: The received arguments.\n\t\t\"\"\"\n\t\tfor key, value in args.items():\n\t\t\tif key not in self._args:\n\t\t\t\traise KeyError(\n\t\t\t\t\t\"Argument {} is not a valid argument.\".format(key))\n\n\t\t\tif isinstance(value, str):\n\t\t\t\tvalue = value.strip()\n\n\t\t\tif not self._args\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "validate_min_max_args", "docstring": "Check if value of arguments is between minimum and maximum values.\n\n:param args: The received arguments.", "ques_id": "6306092e73426c38ae68ad0b", "code": ["def validate_min_max_args(self, args):\n\t# TODO: make sure this works for int and float\n\ttry:\n\t\tif (args[1] < args[2]):\n\t\t\tif (args[1] <= args[0] <= args[2]):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\tif (args[2] <= args[0] <= args[1]):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\texcept:\n\t\treturn False\n\n\tdef validate_range_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of the given args is between range value\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif (args[0] in range(args[1], args[2])):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept:\n\t\t\treturn False\n\n\n\tdef validate_type_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of the given args is of the given type\n\t\t\n", "def validate_min_max_args(self, args):\n\tif self.min_value and self.max_value:\n\t\tif self.min_value > self.max_value:\n\t\t\traise ValueError(\"Min value must be less than max value\")\n\t\telif self.min_value > args:\n\t\t\traise ValueError(\"Argument must be greater than or equal to {}\".format(self.min_value))\n\t\telif self.max_value < args:\n\t\t\traise ValueError(\"Argument must be less than or equal to {}\".format(self.max_value))\n\telif self.min_value:\n\t\tif self.min_value > args:\n\t\t\traise ValueError(\"Argument must be greater than or equal to {}\".format(self.min_value))\n\telif self.max_value:\n\t\tif self.max_value < args:\n\t\t\traise ValueError(\"Argument must be less than or equal to {}\".format(self.max_value))\n\n\n", "def validate_min_max_args(self, args):\n\tfor key, value in args.iteritems():\n\t\tif key in self.min_max_args:\n\t\t\tmin_value, max_value = self.min_max_args[key]\n\t\t\tif value < min_value or value > max_value:\n\t\t\t\traise ValueError(\"%s must be between %d and %d\" % (key, min_value, max_value))\n\n\n", "def validate_min_max_args(self, args):\n\tif isinstance(args, dict):\n\t\tfor arg in args:\n\t\t\tif args[arg] < arg.min_value or args[arg] > arg.max_value:\n\t\t\t\traise ValueError('value of the argument is out of range')\n\telse:\n\t\tif args.value < args.min_value or args.value > args.max_value:\n\t\t\traise ValueError('value of the argument is out of range')\n\n\n", "def validate_min_max_args(self, args):\n\tfor arg in args:\n\t\targname = arg[0]\n\t\targmin = arg[1]\n\t\targmax = arg[2]\n\t\targval = self.get_arg(argname)\n\n\t\tif type(argval) is str:\n\t\t\tif argmin is not None and argmax is not None:\n\t\t\t\tif len(argval) < argmin or len(argval) > argmax:\n\t\t\t\t\tself.set_arg_error(argname, \"Value must be between %d and %d characters\" % (argmin, argmax))\n\t\t\telif argmin is not None:\n\t\t\t\tif len(argval) < argmin:\n\t\t\t\t\tself.set_arg_error(argname, \"Value must be at least %d characters\" % argmin)\n\t\t\telif argmax is not None:\n\t\t\t\tif len(argval) > argmax:\n\t\t\t\t\tself.set_arg_error(argname, \"Value must be at most %d characters\" % argmax)\n", "def validate_min_max_args(self, args):\n\targ_name = args[0]\n\tmin_value = args[1]\n\tmax_value = args[2]\n\targ_value = args[3]\n\tif arg_value < min_value or arg_value > max_value:\n\t\traise argparse.ArgumentTypeError(\"{} must be between {} and {}\".format(arg_name, min_value, max_value))\n\telse:\n\t\treturn arg_value\n\n", "def validate_min_max_args(self, args):\n\t\t\t\n\tif int(args) > self.MAX_VALUE or int(args) < self.MIN_VALUE:\n\t\treturn False\n\t\t\n\treturn True\n\t\t\n", "def validate_min_max_args(self, args):\n\tpass\n\n", "def validate_min_max_args(self, args):\n\t# check if the value of the given args is between minimum and maximum values\n\tfor arg in args:\n\t\tif arg.value < arg.minimum or arg.value > arg.maximum:\n\t\t\tself.logger.error('Value of the given args is outside the permitted range')\n\t\t\treturn False\n\treturn True\n\n", "def validate_min_max_args(self, args):\n\tif len(args) != 2:\n\t\treturn False\n\n\tif not (isinstance(args[0], int) and isinstance(args[1], int)):\n\t\treturn False\n\n\tif args[0] < 0 or args[1] > 9999:\n\t\treturn False\n\n\treturn args[0] < args[1]\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def validate_min_max_args(self, args):\n\t# TODO: make sure this works for int and float\n\ttry:\n\t\tif (args[1] < args[2]):\n\t\t\tif (args[1] <= args[0] <= args[2]):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\telse:\n\t\t\tif (args[2] <= args[0] <= args[1]):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\texcept:\n\t\treturn False\n\n\tdef validate_range_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of the given args is between range value\n\t\t\"\"\"\n\t\ttry:\n\t\t\tif (args[0] in range(args[1], args[2])):\n\t\t\t\treturn True\n\t\t\telse:\n\t\t\t\treturn False\n\t\texcept:\n\t\t\treturn False\n\n\n\tdef validate_type_args(self, args):\n\t\t\"\"\"\n\t\tCheck if value of the given args is of the given type\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\tif self.min_value and self.max_value:\n\t\tif self.min_value > self.max_value:\n\t\t\traise ValueError(\"Min value must be less than max value\")\n\t\telif self.min_value > args:\n\t\t\traise ValueError(\"Argument must be greater than or equal to {}\".format(self.min_value))\n\t\telif self.max_value < args:\n\t\t\traise ValueError(\"Argument must be less than or equal to {}\".format(self.max_value))\n\telif self.min_value:\n\t\tif self.min_value > args:\n\t\t\traise ValueError(\"Argument must be greater than or equal to {}\".format(self.min_value))\n\telif self.max_value:\n\t\tif self.max_value < args:\n\t\t\traise ValueError(\"Argument must be less than or equal to {}\".format(self.max_value))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\tfor key, value in args.iteritems():\n\t\tif key in self.min_max_args:\n\t\t\tmin_value, max_value = self.min_max_args[key]\n\t\t\tif value < min_value or value > max_value:\n\t\t\t\traise ValueError(\"%s must be between %d and %d\" % (key, min_value, max_value))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\tif isinstance(args, dict):\n\t\tfor arg in args:\n\t\t\tif args[arg] < arg.min_value or args[arg] > arg.max_value:\n\t\t\t\traise ValueError('value of the argument is out of range')\n\telse:\n\t\tif args.value < args.min_value or args.value > args.max_value:\n\t\t\traise ValueError('value of the argument is out of range')\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\tfor arg in args:\n\t\targname = arg[0]\n\t\targmin = arg[1]\n\t\targmax = arg[2]\n\t\targval = self.get_arg(argname)\n\n\t\tif type(argval) is str:\n\t\t\tif argmin is not None and argmax is not None:\n\t\t\t\tif len(argval) < argmin or len(argval) > argmax:\n\t\t\t\t\tself.set_arg_error(argname, \"Value must be between %d and %d characters\" % (argmin, argmax))\n\t\t\telif argmin is not None:\n\t\t\t\tif len(argval) < argmin:\n\t\t\t\t\tself.set_arg_error(argname, \"Value must be at least %d characters\" % argmin)\n\t\t\telif argmax is not None:\n\t\t\t\tif len(argval) > argmax:\n\t\t\t\t\tself.set_arg_error(argname, \"Value must be at most %d characters\" % argmax)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\targ_name = args[0]\n\tmin_value = args[1]\n\tmax_value = args[2]\n\targ_value = args[3]\n\tif arg_value < min_value or arg_value > max_value:\n\t\traise argparse.ArgumentTypeError(\"{} must be between {} and {}\".format(arg_name, min_value, max_value))\n\telse:\n\t\treturn arg_value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t\t\t\n\tif int(args) > self.MAX_VALUE or int(args) < self.MIN_VALUE:\n\t\treturn False\n\t\t\n\treturn True\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\tpass\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\t# check if the value of the given args is between minimum and maximum values\n\tfor arg in args:\n\t\tif arg.value < arg.minimum or arg.value > arg.maximum:\n\t\t\tself.logger.error('Value of the given args is outside the permitted range')\n\t\t\treturn False\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def validate_min_max_args(self, args):\n\tif len(args) != 2:\n\t\treturn False\n\n\tif not (isinstance(args[0], int) and isinstance(args[1], int)):\n\t\treturn False\n\n\tif args[0] < 0 or args[1] > 9999:\n\t\treturn False\n\n\treturn args[0] < args[1]\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "create_complex_argumet_type", "docstring": "Build the complex argument type\n\n:param subcommand: the command name\n:param type_name: the complex type name\n:param option_name: the option name\n:param spec_option: option's specifications\n:return: the complex type instance", "ques_id": "6306092e73426c38ae68ad0d", "code": ["def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\ttry:\n\t\tcomplex_type = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise ArgumentError(\n\t\t\t'Type %s is not supported for option %s' %\n\t\t\t(type_name, option_name))\n\n\tcomplex_action = complex_type.complex_action\n\tif complex_action is None:\n\t\traise ArgumentError(\n\t\t\t'Type %s is not supported for option %s' %\n\t\t\t(type_name, option_name))\n\n\treturn complex_action(option_name, (self.vars, self.defaults,\n\t                                    self.plugin_path), subcommand,\n\t                      spec_option)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\treturn COMPLEX_TYPES[type_name](option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif type_name in self.COMPLEX_TYPES:\n\t\treturn self.COMPLEX_TYPES[type_name](option_name,\n\t\t\t\t\t\t\t\t\t\t\t(self.vars, self.defaults, self.plugin_path),\n\t\t\t\t\t\t\t\t\t\t\tsubcommand, spec_option)\n\telse:\n\t\traise NotImplementedError(\"%s is not a valid option type\" % type_name)\n\t\t\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type = COMPLEX_TYPES[type_name]\n\tvalue = complex_type.complex_action(option_name,\n\t\t\t\t\t    (self.vars, self.defaults, self.plugin_path),\n\t\t\t\t\t    subcommand, spec_option)\n\treturn value\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_action = COMPLEX_TYPES[type_name]\n\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif type_name not in COMPLEX_TYPES:\n\t\traise ArgumentError(\n\t\t\t_(\"Invalid complex argument type: %s\") % type_name\n\t\t)\n\treturn COMPLEX_TYPES[type_name](option_name,\n\t\t(self.vars, self.defaults, self.plugin_path),\n\t\tsubcommand, spec_option\n\t)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\ttry:\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\t\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\texcept KeyError:\n\t\traise OptionError('%s is not a valid type' % type_name, self.spec_option)\n\t\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type = COMPLEX_TYPES[type_name]\n\treturn complex_type(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type_name = complex_type_name(type_name)\n\tcomplex_action = COMPLEX_TYPES[complex_type_name]\n\treturn complex_action(option_name, self.vars, self.defaults, self.plugin_path, subcommand, spec_option)\n\n", "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type_func = COMPLEX_TYPES[type_name]\n\treturn complex_type_func(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\ttry:\n\t\tcomplex_type = COMPLEX_TYPES[type_name]\n\texcept KeyError:\n\t\traise ArgumentError(\n\t\t\t'Type %s is not supported for option %s' %\n\t\t\t(type_name, option_name))\n\n\tcomplex_action = complex_type.complex_action\n\tif complex_action is None:\n\t\traise ArgumentError(\n\t\t\t'Type %s is not supported for option %s' %\n\t\t\t(type_name, option_name))\n\n\treturn complex_action(option_name, (self.vars, self.defaults,\n\t                                    self.plugin_path), subcommand,\n\t                      spec_option)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\treturn COMPLEX_TYPES[type_name](option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif type_name in self.COMPLEX_TYPES:\n\t\treturn self.COMPLEX_TYPES[type_name](option_name,\n\t\t\t\t\t\t\t\t\t\t\t(self.vars, self.defaults, self.plugin_path),\n\t\t\t\t\t\t\t\t\t\t\tsubcommand, spec_option)\n\telse:\n\t\traise NotImplementedError(\"%s is not a valid option type\" % type_name)\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type = COMPLEX_TYPES[type_name]\n\tvalue = complex_type.complex_action(option_name,\n\t\t\t\t\t    (self.vars, self.defaults, self.plugin_path),\n\t\t\t\t\t    subcommand, spec_option)\n\treturn value\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_action = COMPLEX_TYPES[type_name]\n\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tif type_name not in COMPLEX_TYPES:\n\t\traise ArgumentError(\n\t\t\t_(\"Invalid complex argument type: %s\") % type_name\n\t\t)\n\treturn COMPLEX_TYPES[type_name](option_name,\n\t\t(self.vars, self.defaults, self.plugin_path),\n\t\tsubcommand, spec_option\n\t)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\ttry:\n\t\tcomplex_action = COMPLEX_TYPES[type_name]\n\t\treturn complex_action(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\texcept KeyError:\n\t\traise OptionError('%s is not a valid type' % type_name, self.spec_option)\n\t\n", "is_pass": true, "return_code": 0}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type = COMPLEX_TYPES[type_name]\n\treturn complex_type(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type_name = complex_type_name(type_name)\n\tcomplex_action = COMPLEX_TYPES[complex_type_name]\n\treturn complex_action(option_name, self.vars, self.defaults, self.plugin_path, subcommand, spec_option)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n\tcomplex_type_func = COMPLEX_TYPES[type_name]\n\treturn complex_type_func(option_name, (self.vars, self.defaults, self.plugin_path), subcommand, spec_option)\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "get_nested_custom_and_control_args", "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)", "ques_id": "6306092e73426c38ae68ad0f", "code": ["def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\t# Group args into control, nested, and custom args.\n\tfor name, value in args.items():\n\t\tif name.startswith(\"_\"):\n\t\t\tcontrol_args[name] = value\n\t\telif name in self.custom_args:\n\t\t\tcustom_args[name] = value\n\t\telse:\n\t\t\tnested_args[name] = value\n\n\treturn (control_args, nested_args, custom_args)\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args, nested_args = {}, {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--custom-'):\n\t\t\t# custom args\n\t\t\tnested_args[arg[9:]] = args[arg]\n\t\telse:\n\t\t\t# control args\n\t\t\tcontrol_args[arg] = args[arg]\n\n\treturn control_args, nested_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tnested_args = {}\n\tcustom_args = {}\n\tcontrol_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"custom_\"):\n\t\t\tcustom_args[arg.replace(\"custom_\", \"\", 1)] = args[arg]\n\t\telif arg.startswith(\"control_\"):\n\t\t\tcontrol_args[arg.replace(\"control_\", \"\", 1)] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {\n\t\t'workdir': self.workdir,\n\t\t'logdir': self.logdir,\n\t\t'artifactdir': self.artifactdir,\n\t\t'playbookdir': self.playbookdir,\n\t\t'playbook_args': self.playbook_args,\n\t\t'playbook_vars': self.playbook_vars,\n\t\t'verbose': self.verbose,\n\t\t'host_inventory': self.host_inventory,\n\t\t'host_inventory_file': self.host_inventory_file,\n\t\t'inventory_file': self.inventory_file,\n\t\t'inventory_directory': self.inventory_directory,\n\t\t'custom_inventory': self.custom_inventory,\n\t\t'extra_vars': self.extra_vars,\n\t\t'extra_vars_file': self.extra_vars_file,\n\t\t'output_dir': self.output_dir,\n\t\t'custom_output_dir': self.custom_output_dir,\n\t\t'output_file': self.output\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\n\t# control arguments are prefixed with '--'\n\t# and are used to control the IR behavior\n\t# and are not passed to the Ansible playbooks.\n\t# They do not have to be in the spec yml file.\n\tcontrol_args_prefix = '--'\n\n\t# custom arguments are prefixed with 'custom_'\n\t# and are used instead of the nested variables.\n\t# They have to be in the spec yml file.\n\tcustom_args_prefix = 'custom_'\n\n\tfor arg in args:\n\t\t# split the argument into name and value\n\t\tname, value = arg.split('=')\n\n\t\t# remove prefixes and suffixes\n\t\tname = name.lstrip(control_args_prefix)\n\t\tname = name.lstrip(custom_args_prefix)\n\n\t\t# if it is a control argument, add it to the control dict\n\t\tif arg.startswith(control_args_prefix):\n\t\t\tcontrol_args[name] = value\n\n\t\t# if it is a custom argument\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith('control-'):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg.startswith('nested-'):\n\t\t\tnested_args[arg[len('nested-'):]] = args[arg]\n\t\telif arg.startswith('custom-'):\n\t\t\tcustom_args[arg[len('custom-'):]] = args[arg]\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg['control']:\n\t\t\tcontrol_args[arg['name']] = arg['value']\n\t\telse:\n\t\t\tnested_args[arg['name']] = arg['value']\n\n\treturn (control_args, nested_args)\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\t# TODO: improve this, this is ugly as hell.\n\tfor arg in args:\n\t\tif arg == '--list':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--pretty':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--debug':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--verbose':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--force':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg.startswith('--custom-'):\n\t\t\t_, key = arg.split('--custom-', 1)\n\t\t\tcustom_args[key] = True\n\t\telse:\n\t\t\tsplit = arg.split('=', 1)\n\t\t\tif len(split) == 1:\n\t\t\t\tkey = split[0]\n\t\t\t\tvalue = True\n\t\t\t\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith(':'):\n\t\t\tcontrol_args[arg[1:]] = args[arg]\n\t\telif arg.startswith('::'):\n\t\t\tcustom_args[arg[2:]] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n\n", "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor k, v in args.items():\n\t\tif k[:2] == '--':\n\t\t\tcontrol_args[k[2:]] = v\n\t\telif k[:1] == '_':\n\t\t\tnested_args[k] = v\n\t\telif k[:1] == '-':\n\t\t\tcustom_args[k[1:]] = v\n\t\telse:\n\t\t\traise ValueError('Error: wrong usage of argument: {}'.format(k))\n\n\treturn control_args, nested_args, custom_args\n\n\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\t# Group args into control, nested, and custom args.\n\tfor name, value in args.items():\n\t\tif name.startswith(\"_\"):\n\t\t\tcontrol_args[name] = value\n\t\telif name in self.custom_args:\n\t\t\tcustom_args[name] = value\n\t\telse:\n\t\t\tnested_args[name] = value\n\n\treturn (control_args, nested_args, custom_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args, nested_args = {}, {}\n\n\tfor arg in args:\n\t\tif arg.startswith('--custom-'):\n\t\t\t# custom args\n\t\t\tnested_args[arg[9:]] = args[arg]\n\t\telse:\n\t\t\t# control args\n\t\t\tcontrol_args[arg] = args[arg]\n\n\treturn control_args, nested_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tnested_args = {}\n\tcustom_args = {}\n\tcontrol_args = {}\n\tfor arg in args:\n\t\tif arg.startswith(\"custom_\"):\n\t\t\tcustom_args[arg.replace(\"custom_\", \"\", 1)] = args[arg]\n\t\telif arg.startswith(\"control_\"):\n\t\t\tcontrol_args[arg.replace(\"control_\", \"\", 1)] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {\n\t\t'workdir': self.workdir,\n\t\t'logdir': self.logdir,\n\t\t'artifactdir': self.artifactdir,\n\t\t'playbookdir': self.playbookdir,\n\t\t'playbook_args': self.playbook_args,\n\t\t'playbook_vars': self.playbook_vars,\n\t\t'verbose': self.verbose,\n\t\t'host_inventory': self.host_inventory,\n\t\t'host_inventory_file': self.host_inventory_file,\n\t\t'inventory_file': self.inventory_file,\n\t\t'inventory_directory': self.inventory_directory,\n\t\t'custom_inventory': self.custom_inventory,\n\t\t'extra_vars': self.extra_vars,\n\t\t'extra_vars_file': self.extra_vars_file,\n\t\t'output_dir': self.output_dir,\n\t\t'custom_output_dir': self.custom_output_dir,\n\t\t'output_file': self.output\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\n\t# control arguments are prefixed with '--'\n\t# and are used to control the IR behavior\n\t# and are not passed to the Ansible playbooks.\n\t# They do not have to be in the spec yml file.\n\tcontrol_args_prefix = '--'\n\n\t# custom arguments are prefixed with 'custom_'\n\t# and are used instead of the nested variables.\n\t# They have to be in the spec yml file.\n\tcustom_args_prefix = 'custom_'\n\n\tfor arg in args:\n\t\t# split the argument into name and value\n\t\tname, value = arg.split('=')\n\n\t\t# remove prefixes and suffixes\n\t\tname = name.lstrip(control_args_prefix)\n\t\tname = name.lstrip(custom_args_prefix)\n\n\t\t# if it is a control argument, add it to the control dict\n\t\tif arg.startswith(control_args_prefix):\n\t\t\tcontrol_args[name] = value\n\n\t\t# if it is a custom argument\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\tfor arg in args:\n\t\tif arg.startswith('control-'):\n\t\t\tcontrol_args[arg] = args[arg]\n\t\telif arg.startswith('nested-'):\n\t\t\tnested_args[arg[len('nested-'):]] = args[arg]\n\t\telif arg.startswith('custom-'):\n\t\t\tcustom_args[arg[len('custom-'):]] = args[arg]\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\n\tfor arg in args:\n\t\tif arg['control']:\n\t\t\tcontrol_args[arg['name']] = arg['value']\n\t\telse:\n\t\t\tnested_args[arg['name']] = arg['value']\n\n\treturn (control_args, nested_args)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\t# TODO: improve this, this is ugly as hell.\n\tfor arg in args:\n\t\tif arg == '--list':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--pretty':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--debug':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--verbose':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg == '--force':\n\t\t\tcontrol_args[arg] = True\n\t\telif arg.startswith('--custom-'):\n\t\t\t_, key = arg.split('--custom-', 1)\n\t\t\tcustom_args[key] = True\n\t\telse:\n\t\t\tsplit = arg.split('=', 1)\n\t\t\tif len(split) == 1:\n\t\t\t\tkey = split[0]\n\t\t\t\tvalue = True\n\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor arg in args:\n\t\tif arg.startswith(':'):\n\t\t\tcontrol_args[arg[1:]] = args[arg]\n\t\telif arg.startswith('::'):\n\t\t\tcustom_args[arg[2:]] = args[arg]\n\t\telse:\n\t\t\tnested_args[arg] = args[arg]\n\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_nested_custom_and_control_args(self, args):\n\tcontrol_args = {}\n\tnested_args = {}\n\tcustom_args = {}\n\n\tfor k, v in args.items():\n\t\tif k[:2] == '--':\n\t\t\tcontrol_args[k[2:]] = v\n\t\telif k[:1] == '_':\n\t\t\tnested_args[k] = v\n\t\telif k[:1] == '-':\n\t\t\tcustom_args[k[1:]] = v\n\t\telse:\n\t\t\traise ValueError('Error: wrong usage of argument: {}'.format(k))\n\n\treturn control_args, nested_args, custom_args\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/settings.py", "project": "redhat-openstack/infrared", "name": "merge_extra_vars", "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars", "ques_id": "6306092e73426c38ae68ad11", "code": ["def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\tvars_dict[key] = value\n\treturn vars_dict\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\tvars_dict[key] = value\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif not extra_vars:\n\t\treturn\n\n\tfor extra_var in extra_vars:\n\t\t# extra-vars can be specified in the form of:\n\t\t#    extra_var=value\n\t\t#    extra_var=\"value with spaces\"\n\t\t#    extra_var=value1,value2\n\t\t#    extra_var=value1,value2,value3\n\t\t#    extra_var={'key1':'value1', 'key2':'value2'}\n\t\t#    extra_var=[1,2,3]\n\t\t#    extra_var=@filename.yml\n\t\t#    extra_var=@filename.json\n\t\t#\n\t\t# Split the string by the first '=', the left side is the\n\t\t# variable name, the right the value.\n\t\tparts = extra_var.split('=', 1)\n\n\t\t# If there is more than one '=' in the string, then the value\n\t\t# contained additional '=' chars, so join everything back\n\t\t# together except for the first '=', which\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif not extra_vars:\n\t\treturn\n\tfor var in extra_vars:\n\t\ttry:\n\t\t\tk, v = var.split('=', 1)\n\t\t\tif k.startswith('@'):\n\t\t\t\t# Read vars from a file\n\t\t\t\ttry:\n\t\t\t\t\twith open(v, 'r') as f:\n\t\t\t\t\t\tvars_dict.update(yaml.load(f))\n\t\t\t\texcept Exception:\n\t\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tvars_dict[k] = v\n\t\texcept ValueError:\n\t\t\t# Ignore malformed argument\n\t\t\tpass\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split('=')\n\t\t\tvars_dict[key] = value\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\t(key, value) = extra_var.split('=', 1)\n\t\t\tvars_dict[key] = value\n\treturn vars_dict\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise errors.AnsibleError(\"Invalid extra var definition: %s\" % extra_var)\n\t\t\tvars_dict[key] = utils.template(value, vars_dict)\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars is None:\n\t\treturn\n\n\tfor extra_var in extra_vars:\n\t\tif extra_var.startswith('@'):\n\t\t\t# Load extra-vars from file\n\t\t\twith open(extra_var[1:]) as f:\n\t\t\t\tnew_vars = yaml.safe_load(f)\n\t\t\t\tif not isinstance(new_vars, dict):\n\t\t\t\t\traise AnsibleError('Extra vars file must contain a dictionary')\n\t\t\t\tvars_dict.update(new_vars)\n\t\telse:\n\t\t\t# Parse extra-vars into a dictionary\n\t\t\ttry:\n\t\t\t\tnew_vars = json.loads(extra_var)\n\t\t\texcept ValueError:\n\t\t\t\tnew_vars = shlex.split(extra_var)\n\n\t\t\tif not isinstance(new_vars, dict):\n\t\t\t\tnew_vars = dict(new_vars)\n\n\t\t\tvars_dict.update(new\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise RuntimeError(\"Invalid extra-var: '%s'\" % extra_var)\n\t\t\tvars_dict[key] = value\n\n\n", "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\t# FIXME: Allow overriding?\n\t\t\tassert extra_var not in vars_dict, 'Duplicate variable name \"%s\"' % extra_var\n\t\t\tvars_dict[extra_var] = True\n\treturn vars_dict\n\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split(\"=\", 1)\n\t\t\tvars_dict[key] = value\n\treturn vars_dict\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\tvars_dict[key] = value\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif not extra_vars:\n\t\treturn\n\n\tfor extra_var in extra_vars:\n\t\t# extra-vars can be specified in the form of:\n\t\t#    extra_var=value\n\t\t#    extra_var=\"value with spaces\"\n\t\t#    extra_var=value1,value2\n\t\t#    extra_var=value1,value2,value3\n\t\t#    extra_var={'key1':'value1', 'key2':'value2'}\n\t\t#    extra_var=[1,2,3]\n\t\t#    extra_var=@filename.yml\n\t\t#    extra_var=@filename.json\n\t\t#\n\t\t# Split the string by the first '=', the left side is the\n\t\t# variable name, the right the value.\n\t\tparts = extra_var.split('=', 1)\n\n\t\t# If there is more than one '=' in the string, then the value\n\t\t# contained additional '=' chars, so join everything back\n\t\t# together except for the first '=', which\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif not extra_vars:\n\t\treturn\n\tfor var in extra_vars:\n\t\ttry:\n\t\t\tk, v = var.split('=', 1)\n\t\t\tif k.startswith('@'):\n\t\t\t\t# Read vars from a file\n\t\t\t\ttry:\n\t\t\t\t\twith open(v, 'r') as f:\n\t\t\t\t\t\tvars_dict.update(yaml.load(f))\n\t\t\t\texcept Exception:\n\t\t\t\t\tpass\n\t\t\telse:\n\t\t\t\tvars_dict[k] = v\n\t\texcept ValueError:\n\t\t\t# Ignore malformed argument\n\t\t\tpass\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\tkey, value = extra_var.split('=')\n\t\t\tvars_dict[key] = value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\t(key, value) = extra_var.split('=', 1)\n\t\t\tvars_dict[key] = value\n\treturn vars_dict\n\n", "is_pass": true, "return_code": 0}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise errors.AnsibleError(\"Invalid extra var definition: %s\" % extra_var)\n\t\t\tvars_dict[key] = utils.template(value, vars_dict)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars is None:\n\t\treturn\n\n\tfor extra_var in extra_vars:\n\t\tif extra_var.startswith('@'):\n\t\t\t# Load extra-vars from file\n\t\t\twith open(extra_var[1:]) as f:\n\t\t\t\tnew_vars = yaml.safe_load(f)\n\t\t\t\tif not isinstance(new_vars, dict):\n\t\t\t\t\traise AnsibleError('Extra vars file must contain a dictionary')\n\t\t\t\tvars_dict.update(new_vars)\n\t\telse:\n\t\t\t# Parse extra-vars into a dictionary\n\t\t\ttry:\n\t\t\t\tnew_vars = json.loads(extra_var)\n\t\t\texcept ValueError:\n\t\t\t\tnew_vars = shlex.split(extra_var)\n\n\t\t\tif not isinstance(new_vars, dict):\n\t\t\t\tnew_vars = dict(new_vars)\n\n\t\t\tvars_dict.update(new\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\ttry:\n\t\t\t\tkey, value = extra_var.split('=', 1)\n\t\t\texcept ValueError:\n\t\t\t\traise RuntimeError(\"Invalid extra-var: '%s'\" % extra_var)\n\t\t\tvars_dict[key] = value\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def merge_extra_vars(vars_dict, extra_vars=None):\n\tif extra_vars:\n\t\tfor extra_var in extra_vars:\n\t\t\t# FIXME: Allow overriding?\n\t\t\tassert extra_var not in vars_dict, 'Duplicate variable name \"%s\"' % extra_var\n\t\t\tvars_dict[extra_var] = True\n\treturn vars_dict\n\n\n", "is_pass": true, "return_code": 0}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "ansible_playbook", "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.", "ques_id": "6306092f73426c38ae68ad13", "code": ["def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tenv_vars = {'ANSIBLE_CONFIG': ir_workspace.ansible_cfg_file}\n\tif extra_vars is not None:\n\t\tenv_vars['ANSIBLE_EXTRA_VARS'] = json.dumps(extra_vars)\n\treturn _ansible_cli(ir_workspace, ir_plugin, playbook_path, env_vars, 'ansible-playbook',\n\t\t\t\t\t\tverbose=verbose, ansible_args=ansible_args)\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tcmd = ['ansible-playbook']\n\tif extra_vars:\n\t\tcmd.extend(['-e', '{}'.format(\n\t\t\tjson.dumps(extra_vars))])\n\tif verbose:\n\t\tcmd.extend(['-{}'.format(\"v\" * verbose)])\n\tif ansible_args:\n\t\tfor k, v in ansible_args.items():\n\t\t\tcmd.extend(['-{}'.format(k), '{}'.format(v)])\n\tcmd.append(playbook_path)\n\tLOG.debug(\"Running ansible-playbook: %s\", cmd)\n\tLOG.debug(\"cwd = %s\", ir_workspace.workdir)\n\n\t# set the working directory to the current workspace\n\tos.chdir(ir_workspace.workdir)\n\n\t# TODO(obara): remove the following workaround when we move to\n\t# Ansible 2.4\n\t#\n\t# The following is a workaround for the following issue:\n\t# https://github.com/ansible/\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tif ansible_args is None:\n\t\tansible_args = {}\n\tansible_args.update({\"verbose\": verbose})\n\tif \"inventory\" not in ansible_args:\n\t\tansible_args.update({\"inventory\": ir_workspace.inventory_path})\n\tif \"extra_vars\" not in ansible_args:\n\t\tansible_args.update({\"extra_vars\": extra_vars})\n\tansible_cmd = [\"ansible-playbook\"]\n\n\tfor key, value in ansible_args.items():\n\t\tansible_cmd.extend([\n\t\t\t\"--{}\".format(key.replace(\"_\", \"-\")),\n\t\t\tstr(value)\n\t\t])\n\n\tansible_cmd.append(playbook_path)\n\treturn ir_plugin.cli.run(ansible_cmd, output=True)\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif verbose is None:\n\t\tverbose = 0\n\n\tif extra_vars is None:\n\t\textra_vars = {}\n\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tverbose = 'v' * int(verbose)\n\tansible_args['verbosity'] = verbose\n\n\tinv_path = os.path.join(ir_workspace.inventory_path,\n\t                        ir_workspace.inventory_name)\n\n\tansible_args['inventory-file'] = inv_path\n\tansible_args['extra-vars'] = extra_vars\n\n\tif ir_plugin.plugin_path:\n\t\tplugin_path = ir_plugin.plugin_path\n\telse:\n\t\tplugin_path = ir_plugin.path\n\n\tansible_args['extra-vars']['plugin_path'] = plugin_path\n\n\tansible_args['extra-vars']['plugin_name'] = ir_plugin.name\n\n\tansible_args['extra-vars']['base_dir'] = os.getcwd()\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\tcmd = [\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t]\n\n\tif verbose:\n\t\tcmd.append('-{}'.format(verbose))\n\n\tif ir_plugin.settings.get('ansible_debug', False):\n\t\tcmd.append('-vvvv')\n\n\tif extra_vars:\n\t\tfor var, value in extra_vars.items():\n\t\t\tcmd.append('-e \"{0}={1}\"'.format(var, value))\n\n\tif ansible_args:\n\t\tfor arg, value in ansible_args.items():\n\t\t\tif value:\n\t\t\t\tcmd.append('--{0}={1}'.format(arg, value))\n\t\t\telse:\n\t\t\t\tcmd.append('--{0}'.format(arg))\n\n\treturn subprocess.check_call(cmd, cwd=ir_workspace.path)\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\tif not os.path.exists(playbook_path):\n\t\tir_plugin.log.error(\"Cannot find playbook: %s\", playbook_path)\n\t\treturn False\n\n\t# Common args for ansible-playbook\n\tcommon_args = [\"ansible-playbook\", \"-i\",\n\t               \"{}/inventory\".format(ir_workspace.inventory_path),\n\t               \"-e\", \"infrared_workspace={}\".format(ir_workspace.path)]\n\n\t# Add verbosity level\n\tif verbose:\n\t\tcommon_args.extend([\"-{}\".format(verbose)])\n\n\t# Add extra vars\n\textra_vars = dict(extra_vars, **ir_plugin.get_extra_vars())\n\tfor var, val in extra_vars.items():\n\t\tcommon_args.extend([\"-e\", \"{}={}\".format(var, val)])\n\n\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tansible_args = ansible_args or {}\n\thosts_file = ir_workspace.hosts_file\n\t# if no hosts file is found, we assume that the user wants to run\n\t# against a localhost\n\tif not os.path.exists(hosts_file):\n\t\thosts_file = \"localhost\"\n\n\tinvocation_args = [\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t\t'-i',\n\t\thosts_file,\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_vars_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_secrets_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_defaults_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_extra_vars_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not verbose:\n\t\tverbose = \"v\" * ir_workspace.verbosity\n\n\tif not extra_vars:\n\t\textra_vars = {}\n\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\targs = [\"ansible-playbook\",\n\t        \"-i\", \"localhost,\",\n\t        \"-c\", \"local\",\n\t        \"-e\", \"ansible_python_interpreter={}\".format(sys.executable),\n\t        \"-e\", \"infrared_workspace={}\".format(ir_workspace.path),\n\t        \"-e\", \"infrared_plugin={}\".format(ir_plugin.name),\n\t        \"-e\", \"infrared_verbosity={}\".format(ir_workspace.verbosity),\n\t        \"-e\", \"infrared_log_file={}\".format(ir_workspace.log_file),\n\t        \"-e\", \"infrared_log_level={}\".format(ir_workspace.log_level),\n\t        \"-e\", \"infrared_debug={}\".format(ir_\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tif ansible_args is None:\n\t\tansible_args = {}\n\tif verbose is None:\n\t\tverbose = 0\n\n\t# TODO: replace with a logger object\n\tLOG.debug(\"Invoking ansible-playbook {0}\".format(playbook_path))\n\n\t# TODO: make sure we have ansible-playbook on the path\n\tcmd = [\"ansible-playbook\", \"-i\", ir_workspace.inventory_path,\n\t       playbook_path]\n\n\tif verbose:\n\t\tcmd.append(\"-{0}\".format(\"v\" * verbose))\n\n\tfor (key, value) in extra_vars.items():\n\t\tcmd.append(\"-e\")\n\t\tcmd.append(\"{0}={1}\".format(key, value))\n\n\tfor (key, value) in ansible_args.items():\n\t\tcmd.append(\"--{0}\".format(key))\n\t\tif value:\n\t\t\tcmd.append(\"{0}\".\n", "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\textra_vars = extra_vars or {}\n\tansible_args = ansible_args or {}\n\n\t# TODO(yfried): add support for --vault-password-file\n\t#               to be able to run playbooks with vaulted\n\t#               variables\n\tansible_cmd = [\n\t\t\"ansible-playbook\",\n\t\t\"-i\", ir_workspace.inventory_path,\n\t\t\"--ssh-common-args\", \"-o StrictHostKeyChecking=no\",\n\t\t\"--become\",\n\t\t\"--become-user=root\",\n\t\t\"--become-method=sudo\",\n\t\t\"--forks=50\",\n\t\t\"--skip-tags\", \"undefined_variables\",\n\t\t\"--limit\", \"all\",\n\t\t\"--extra-vars\", \" \".join(\n\t\t\t[\"{}={}\".format(k, json.dumps(v)) for k, v in extra_vars.items()])\n\t]\n\n\tif verbose:\n\t\tansible_cmd.append(\"-{}\".\n"], "level": "file_runnable", "generate_results": [{"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tenv_vars = {'ANSIBLE_CONFIG': ir_workspace.ansible_cfg_file}\n\tif extra_vars is not None:\n\t\tenv_vars['ANSIBLE_EXTRA_VARS'] = json.dumps(extra_vars)\n\treturn _ansible_cli(ir_workspace, ir_plugin, playbook_path, env_vars, 'ansible-playbook',\n\t\t\t\t\t\tverbose=verbose, ansible_args=ansible_args)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tcmd = ['ansible-playbook']\n\tif extra_vars:\n\t\tcmd.extend(['-e', '{}'.format(\n\t\t\tjson.dumps(extra_vars))])\n\tif verbose:\n\t\tcmd.extend(['-{}'.format(\"v\" * verbose)])\n\tif ansible_args:\n\t\tfor k, v in ansible_args.items():\n\t\t\tcmd.extend(['-{}'.format(k), '{}'.format(v)])\n\tcmd.append(playbook_path)\n\tLOG.debug(\"Running ansible-playbook: %s\", cmd)\n\tLOG.debug(\"cwd = %s\", ir_workspace.workdir)\n\n\t# set the working directory to the current workspace\n\tos.chdir(ir_workspace.workdir)\n\n\t# TODO(obara): remove the following workaround when we move to\n\t# Ansible 2.4\n\t#\n\t# The following is a workaround for the following issue:\n\t# https://github.com/ansible/\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tif ansible_args is None:\n\t\tansible_args = {}\n\tansible_args.update({\"verbose\": verbose})\n\tif \"inventory\" not in ansible_args:\n\t\tansible_args.update({\"inventory\": ir_workspace.inventory_path})\n\tif \"extra_vars\" not in ansible_args:\n\t\tansible_args.update({\"extra_vars\": extra_vars})\n\tansible_cmd = [\"ansible-playbook\"]\n\n\tfor key, value in ansible_args.items():\n\t\tansible_cmd.extend([\n\t\t\t\"--{}\".format(key.replace(\"_\", \"-\")),\n\t\t\tstr(value)\n\t\t])\n\n\tansible_cmd.append(playbook_path)\n\treturn ir_plugin.cli.run(ansible_cmd, output=True)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif verbose is None:\n\t\tverbose = 0\n\n\tif extra_vars is None:\n\t\textra_vars = {}\n\n\tif ansible_args is None:\n\t\tansible_args = {}\n\n\tverbose = 'v' * int(verbose)\n\tansible_args['verbosity'] = verbose\n\n\tinv_path = os.path.join(ir_workspace.inventory_path,\n\t                        ir_workspace.inventory_name)\n\n\tansible_args['inventory-file'] = inv_path\n\tansible_args['extra-vars'] = extra_vars\n\n\tif ir_plugin.plugin_path:\n\t\tplugin_path = ir_plugin.plugin_path\n\telse:\n\t\tplugin_path = ir_plugin.path\n\n\tansible_args['extra-vars']['plugin_path'] = plugin_path\n\n\tansible_args['extra-vars']['plugin_name'] = ir_plugin.name\n\n\tansible_args['extra-vars']['base_dir'] = os.getcwd()\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\tcmd = [\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t]\n\n\tif verbose:\n\t\tcmd.append('-{}'.format(verbose))\n\n\tif ir_plugin.settings.get('ansible_debug', False):\n\t\tcmd.append('-vvvv')\n\n\tif extra_vars:\n\t\tfor var, value in extra_vars.items():\n\t\t\tcmd.append('-e \"{0}={1}\"'.format(var, value))\n\n\tif ansible_args:\n\t\tfor arg, value in ansible_args.items():\n\t\t\tif value:\n\t\t\t\tcmd.append('--{0}={1}'.format(arg, value))\n\t\t\telse:\n\t\t\t\tcmd.append('--{0}'.format(arg))\n\n\treturn subprocess.check_call(cmd, cwd=ir_workspace.path)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not extra_vars:\n\t\textra_vars = {}\n\tif not ansible_args:\n\t\tansible_args = {}\n\tif not os.path.exists(playbook_path):\n\t\tir_plugin.log.error(\"Cannot find playbook: %s\", playbook_path)\n\t\treturn False\n\n\t# Common args for ansible-playbook\n\tcommon_args = [\"ansible-playbook\", \"-i\",\n\t               \"{}/inventory\".format(ir_workspace.inventory_path),\n\t               \"-e\", \"infrared_workspace={}\".format(ir_workspace.path)]\n\n\t# Add verbosity level\n\tif verbose:\n\t\tcommon_args.extend([\"-{}\".format(verbose)])\n\n\t# Add extra vars\n\textra_vars = dict(extra_vars, **ir_plugin.get_extra_vars())\n\tfor var, val in extra_vars.items():\n\t\tcommon_args.extend([\"-e\", \"{}={}\".format(var, val)])\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tansible_args = ansible_args or {}\n\thosts_file = ir_workspace.hosts_file\n\t# if no hosts file is found, we assume that the user wants to run\n\t# against a localhost\n\tif not os.path.exists(hosts_file):\n\t\thosts_file = \"localhost\"\n\n\tinvocation_args = [\n\t\t'ansible-playbook',\n\t\tplaybook_path,\n\t\t'-i',\n\t\thosts_file,\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_vars_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_secrets_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_defaults_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin.abs_plugin_extra_vars_file),\n\t\t'-e',\n\t\t'@{}'.format(ir_plugin\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif not verbose:\n\t\tverbose = \"v\" * ir_workspace.verbosity\n\n\tif not extra_vars:\n\t\textra_vars = {}\n\n\tif not ansible_args:\n\t\tansible_args = {}\n\n\targs = [\"ansible-playbook\",\n\t        \"-i\", \"localhost,\",\n\t        \"-c\", \"local\",\n\t        \"-e\", \"ansible_python_interpreter={}\".format(sys.executable),\n\t        \"-e\", \"infrared_workspace={}\".format(ir_workspace.path),\n\t        \"-e\", \"infrared_plugin={}\".format(ir_plugin.name),\n\t        \"-e\", \"infrared_verbosity={}\".format(ir_workspace.verbosity),\n\t        \"-e\", \"infrared_log_file={}\".format(ir_workspace.log_file),\n\t        \"-e\", \"infrared_log_level={}\".format(ir_workspace.log_level),\n\t        \"-e\", \"infrared_debug={}\".format(ir_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\tif extra_vars is None:\n\t\textra_vars = {}\n\tif ansible_args is None:\n\t\tansible_args = {}\n\tif verbose is None:\n\t\tverbose = 0\n\n\t# TODO: replace with a logger object\n\tLOG.debug(\"Invoking ansible-playbook {0}\".format(playbook_path))\n\n\t# TODO: make sure we have ansible-playbook on the path\n\tcmd = [\"ansible-playbook\", \"-i\", ir_workspace.inventory_path,\n\t       playbook_path]\n\n\tif verbose:\n\t\tcmd.append(\"-{0}\".format(\"v\" * verbose))\n\n\tfor (key, value) in extra_vars.items():\n\t\tcmd.append(\"-e\")\n\t\tcmd.append(\"{0}={1}\".format(key, value))\n\n\tfor (key, value) in ansible_args.items():\n\t\tcmd.append(\"--{0}\".format(key))\n\t\tif value:\n\t\t\tcmd.append(\"{0}\".\n", "is_pass": false, "return_code": 1}, {"generate_code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n\textra_vars = extra_vars or {}\n\tansible_args = ansible_args or {}\n\n\t# TODO(yfried): add support for --vault-password-file\n\t#               to be able to run playbooks with vaulted\n\t#               variables\n\tansible_cmd = [\n\t\t\"ansible-playbook\",\n\t\t\"-i\", ir_workspace.inventory_path,\n\t\t\"--ssh-common-args\", \"-o StrictHostKeyChecking=no\",\n\t\t\"--become\",\n\t\t\"--become-user=root\",\n\t\t\"--become-method=sudo\",\n\t\t\"--forks=50\",\n\t\t\"--skip-tags\", \"undefined_variables\",\n\t\t\"--limit\", \"all\",\n\t\t\"--extra-vars\", \" \".join(\n\t\t\t[\"{}={}\".format(k, json.dumps(v)) for k, v in extra_vars.items()])\n\t]\n\n\tif verbose:\n\t\tansible_cmd.append(\"-{}\".\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/execute.py", "project": "redhat-openstack/infrared", "name": "_run_playbook", "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results", "ques_id": "6306093273426c38ae68ad15", "code": ["def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\textra_vars_file = ir_workspace.build_file('extra_vars')\n\twith open(extra_vars_file, 'w') as extravars_file:\n\t\tjson.dump(vars_dict, extravars_file, indent=4)\n\n\tcmd = cli_args + [\n\t\t'-e',\n\t\t'@{}'.format(extra_vars_file),\n\t\t'-e',\n\t\t'ansible_python_interpreter=/usr/bin/python'\n\t]\n\n\tresult = ir_plugin.execute(cmd, cwd=ir_workspace.workspace_dir)\n\tif result['rc'] != 0:\n\t\traise Exception(\n\t\t\t\"Failed to execute ansible playbook. \"\n\t\t\t\"Command: {}\\nOutput:\\n{}\".format(\n\t\t\t\t\" \".join(cmd), result['stdout']))\n\n\treturn result\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_workspace.copy_artifacts_to(ir_plugin.tree)\n\tvars_dict[\"ir_workspace\"] = ir_workspace.path\n\tvars_dict[\"ir_plugin\"] = ir_plugin.name\n\tvars_dict[\"ir_plugin_path\"] = ir_plugin.path\n\tvars_dict[\"ir_plugin_artifacts\"] = ir_plugin.artifacts_path\n\tvars_dict[\"ir_plugin_tree\"] = ir_plugin.tree\n\tvars_dict[\"ir_plugin_tests\"] = ir_plugin.tests_path\n\tvars_dict[\"ir_plugin_venv\"] = ir_plugin.venv_path\n\tvars_dict[\"ir_plugin_cli\"] = ir_plugin.cli_path\n\tvars_dict[\"ir_plugin_rpms\"] = ir_plugin.rpms_path\n\tvars_dict[\"ir_plugin_logs\"] = ir_plugin.logs_path\n\tvars_dict[\"ir_plugin_docs\"] = ir_plugin.docs_path\n\tvars_dict[\"ir_plugin\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\textra_vars = _get_extra_vars(cli_args, vars_dict, ir_workspace, ir_plugin)\n\n\treturn ansible_runner.run(\n\t\tprivate_data_dir=ir_workspace.cache_dir,\n\t\thost_pattern='all',\n\t\tbecome=True,\n\t\tbecome_method='sudo',\n\t\tbecome_user='root',\n\t\tcheck=False,\n\t\tverbosity=cli_args.verbose_level,\n\t\textra_vars=extra_vars,\n\t\tidentity_file=ir_workspace.get_ssh_key_path(),\n\t\tinventory=ir_workspace.get_inventory_file_path(),\n\t)\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_workspace.update_vars(vars_dict)\n\tir_workspace.update_vars(ir_plugin.get_plugin_vars())\n\n\tcli_args.extend([\"-e\", \"ansible_python_interpreter={}\".format(sys.executable)])\n\tcli_args.extend([\"-e\", \"ansible_ssh_python={}\".format(sys.executable)])\n\tcli_args.extend(\n\t\t[\"-e\", \"ansible_winrm_transport=plaintext\"])\n\n\tcli_args.extend(ir_plugin.get_plugin_args())\n\n\tansible_args = cli_args + [\"-e\", \"infrared_workspace={}\".format(\n\t\tir_workspace.workspace_path)]\n\tansible_args.extend([\"-e\", \"infrared_playbook={}\".format(\n\t\tir_plugin.playbook_file)])\n\n\tif ir_workspace.verbose:\n\t\tansible_args.append('-vvvv')\n\n\t\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_plugin.logger.info(\"Running Ansible\")\n\n\t# ansible-playbook command line arguments\n\tansible_args = [\"-i\", ir_plugin.inventory_file]\n\t# Ansible extra vars\n\tansible_vars = [\n\t\t\"--extra-vars\",\n\t\t\"@{}\".format(ir_plugin.vars_file)\n\t]\n\t# Ansible tags\n\tansible_tags = [\n\t\t\"--tags\",\n\t\t\"{}\".format(\",\".join(ir_plugin.tags))\n\t]\n\n\t# Command to run\n\tcmd = [\"ansible-playbook\"] + cli_args + ansible_args + ansible_vars + \\\n\t\tansible_tags\n\n\tir_plugin.logger.info(\"Running ansible-playbook command: {}\".format(cmd))\n\n\t# Running the playbook\n\tir_plugin.logger.info(\"Running the playbooks\")\n\ttry:\n\t\tresult = subprocess.check_output(\n\t\t\tcmd, stderr=subprocess.STDOUT, c\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_workspace.logger.debug(\n\t\t\"Running Ansible playbook with the following vars: {0}\".format(\n\t\t\tstr(vars_dict)))\n\tir_workspace.logger.debug(\n\t\t\"Running Ansible playbook with the following cli args: {0}\".format(\n\t\t\tstr(cli_args)))\n\tir_workspace.logger.info(\n\t\t\"Executing playbook: {0}\".format(ir_plugin.get_playbook_path()))\n\treturn ir_workspace.run_ansible_playbook(\n\t\tir_plugin.get_playbook_path(),\n\t\tcli_args,\n\t\textra_vars=vars_dict,\n\t\tverbosity=cli_args.verbosity)\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t# Use the Ansible cli module to run the playbook\n\tcli_args += [\n\t\t\"-i\", ir_workspace.inventory_path,\n\t\t\"--extra-vars\", json.dumps(vars_dict),\n\t\t\"--forks\", str(ir_plugin.parallel_count)]\n\tcli_args += ir_plugin.cli_args\n\tansible_results = ansible_runner.run(\n\t\tprivate_data_dir=ir_workspace.workdir,\n\t\tproject_dir=ir_plugin.plugin_path,\n\t\tplaybook=ir_plugin.playbook,\n\t\tident=ir_plugin.name,\n\t\tverbosity=ir_plugin.verbosity,\n\t\tcli_args=cli_args)\n\n\tif ansible_results.is_successful():\n\t\tir_logger.info(\"Playbook '{}' ran successfully\".format(\n\t\t\tir_plugin.playbook))\n\telse:\n\t\tir_logger.error(\"Playbook '{}' failed\".format(ir_plugin.playbook))\n\n\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t# pylint: disable=too-many-locals\n\t# pylint: disable=unused-argument\n\n\t# adding the cli arguments to the list\n\tcli_args = cli_args.split()\n\t# adding the extra vars\n\tcli_args.append('--extra-vars')\n\tcli_args.append(json.dumps(vars_dict))\n\n\t# adding the --private-key arg if exists\n\tprivate_key = ir_workspace.get_setting('private_key')\n\tif private_key:\n\t\tcli_args.append('--private-key={}'.format(private_key))\n\n\t# adding the --ssh-common-args arg if exists\n\tssh_common_args = ir_workspace.get_setting('ssh_common_args')\n\tif ssh_common_args:\n\t\tcli_args.append('--ssh-common-args={}'.format(ssh_common_args))\n\n\t# adding the --ssh-extra-args arg if exists\n\tssh_extra_args = ir_workspace.get_setting('ssh_extra\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tlogger.debug(\"Using cli_args: {}\".format(cli_args))\n\tlogger.debug(\n\t\t\"Using vars_dict: {}\".format(json.dumps(vars_dict, indent=4)))\n\n\t# TODO: replace this with AnsiblePlaybookExecutor\n\tif ir_workspace.is_test_mode():\n\t\tlogger.info(\"Test mode enabled, skipping playbook execution.\")\n\t\treturn (0, [], [])\n\n\tir_plugin.set_vars_to_file(vars_dict)\n\n\tansible_playbook_path = \\\n\t\tir_plugin.get_playbook_path(cli_args[0])\n\tif not ansible_playbook_path:\n\t\traise exceptions.InfraredPluginError(\n\t\t\t\"Playbook not found: {}\".format(cli_args[0]))\n\n\t# TODO: add support for ansible.cfg\n\ttry:\n\t\tir_workspace.run_ansible_playbook(\n\t\t\tansible_playbook_path,\n\t\t\tcli_\n", "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t# ###########################################################\n\t# #### We need to get the name of the current playbook #### #\n\t# ###########################################################\n\ttry:\n\t\tcli_args.remove('--debug')\n\t\tcli_args.insert(0, '--debug')\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tcli_args.remove('--skip-tags')\n\t\tcli_args.insert(0, '--skip-tags')\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tcli_args.remove('--tags')\n\t\tcli_args.insert(0, '--tags')\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tcli_args.remove('-e')\n\t\tcli_args.insert(0, '-e')\n\texcept ValueError:\n\t\tpass\n\n\tcli_args.insert(0, 'ansible-playbook')\n\tcmd = cli_args\n\targs = cmd[1:]\n\n\t# #######################################################\n\t# #### We need to get the name of the current plugin\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\textra_vars_file = ir_workspace.build_file('extra_vars')\n\twith open(extra_vars_file, 'w') as extravars_file:\n\t\tjson.dump(vars_dict, extravars_file, indent=4)\n\n\tcmd = cli_args + [\n\t\t'-e',\n\t\t'@{}'.format(extra_vars_file),\n\t\t'-e',\n\t\t'ansible_python_interpreter=/usr/bin/python'\n\t]\n\n\tresult = ir_plugin.execute(cmd, cwd=ir_workspace.workspace_dir)\n\tif result['rc'] != 0:\n\t\traise Exception(\n\t\t\t\"Failed to execute ansible playbook. \"\n\t\t\t\"Command: {}\\nOutput:\\n{}\".format(\n\t\t\t\t\" \".join(cmd), result['stdout']))\n\n\treturn result\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_workspace.copy_artifacts_to(ir_plugin.tree)\n\tvars_dict[\"ir_workspace\"] = ir_workspace.path\n\tvars_dict[\"ir_plugin\"] = ir_plugin.name\n\tvars_dict[\"ir_plugin_path\"] = ir_plugin.path\n\tvars_dict[\"ir_plugin_artifacts\"] = ir_plugin.artifacts_path\n\tvars_dict[\"ir_plugin_tree\"] = ir_plugin.tree\n\tvars_dict[\"ir_plugin_tests\"] = ir_plugin.tests_path\n\tvars_dict[\"ir_plugin_venv\"] = ir_plugin.venv_path\n\tvars_dict[\"ir_plugin_cli\"] = ir_plugin.cli_path\n\tvars_dict[\"ir_plugin_rpms\"] = ir_plugin.rpms_path\n\tvars_dict[\"ir_plugin_logs\"] = ir_plugin.logs_path\n\tvars_dict[\"ir_plugin_docs\"] = ir_plugin.docs_path\n\tvars_dict[\"ir_plugin\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\textra_vars = _get_extra_vars(cli_args, vars_dict, ir_workspace, ir_plugin)\n\n\treturn ansible_runner.run(\n\t\tprivate_data_dir=ir_workspace.cache_dir,\n\t\thost_pattern='all',\n\t\tbecome=True,\n\t\tbecome_method='sudo',\n\t\tbecome_user='root',\n\t\tcheck=False,\n\t\tverbosity=cli_args.verbose_level,\n\t\textra_vars=extra_vars,\n\t\tidentity_file=ir_workspace.get_ssh_key_path(),\n\t\tinventory=ir_workspace.get_inventory_file_path(),\n\t)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_workspace.update_vars(vars_dict)\n\tir_workspace.update_vars(ir_plugin.get_plugin_vars())\n\n\tcli_args.extend([\"-e\", \"ansible_python_interpreter={}\".format(sys.executable)])\n\tcli_args.extend([\"-e\", \"ansible_ssh_python={}\".format(sys.executable)])\n\tcli_args.extend(\n\t\t[\"-e\", \"ansible_winrm_transport=plaintext\"])\n\n\tcli_args.extend(ir_plugin.get_plugin_args())\n\n\tansible_args = cli_args + [\"-e\", \"infrared_workspace={}\".format(\n\t\tir_workspace.workspace_path)]\n\tansible_args.extend([\"-e\", \"infrared_playbook={}\".format(\n\t\tir_plugin.playbook_file)])\n\n\tif ir_workspace.verbose:\n\t\tansible_args.append('-vvvv')\n\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_plugin.logger.info(\"Running Ansible\")\n\n\t# ansible-playbook command line arguments\n\tansible_args = [\"-i\", ir_plugin.inventory_file]\n\t# Ansible extra vars\n\tansible_vars = [\n\t\t\"--extra-vars\",\n\t\t\"@{}\".format(ir_plugin.vars_file)\n\t]\n\t# Ansible tags\n\tansible_tags = [\n\t\t\"--tags\",\n\t\t\"{}\".format(\",\".join(ir_plugin.tags))\n\t]\n\n\t# Command to run\n\tcmd = [\"ansible-playbook\"] + cli_args + ansible_args + ansible_vars + \\\n\t\tansible_tags\n\n\tir_plugin.logger.info(\"Running ansible-playbook command: {}\".format(cmd))\n\n\t# Running the playbook\n\tir_plugin.logger.info(\"Running the playbooks\")\n\ttry:\n\t\tresult = subprocess.check_output(\n\t\t\tcmd, stderr=subprocess.STDOUT, c\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tir_workspace.logger.debug(\n\t\t\"Running Ansible playbook with the following vars: {0}\".format(\n\t\t\tstr(vars_dict)))\n\tir_workspace.logger.debug(\n\t\t\"Running Ansible playbook with the following cli args: {0}\".format(\n\t\t\tstr(cli_args)))\n\tir_workspace.logger.info(\n\t\t\"Executing playbook: {0}\".format(ir_plugin.get_playbook_path()))\n\treturn ir_workspace.run_ansible_playbook(\n\t\tir_plugin.get_playbook_path(),\n\t\tcli_args,\n\t\textra_vars=vars_dict,\n\t\tverbosity=cli_args.verbosity)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t# Use the Ansible cli module to run the playbook\n\tcli_args += [\n\t\t\"-i\", ir_workspace.inventory_path,\n\t\t\"--extra-vars\", json.dumps(vars_dict),\n\t\t\"--forks\", str(ir_plugin.parallel_count)]\n\tcli_args += ir_plugin.cli_args\n\tansible_results = ansible_runner.run(\n\t\tprivate_data_dir=ir_workspace.workdir,\n\t\tproject_dir=ir_plugin.plugin_path,\n\t\tplaybook=ir_plugin.playbook,\n\t\tident=ir_plugin.name,\n\t\tverbosity=ir_plugin.verbosity,\n\t\tcli_args=cli_args)\n\n\tif ansible_results.is_successful():\n\t\tir_logger.info(\"Playbook '{}' ran successfully\".format(\n\t\t\tir_plugin.playbook))\n\telse:\n\t\tir_logger.error(\"Playbook '{}' failed\".format(ir_plugin.playbook))\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t# pylint: disable=too-many-locals\n\t# pylint: disable=unused-argument\n\n\t# adding the cli arguments to the list\n\tcli_args = cli_args.split()\n\t# adding the extra vars\n\tcli_args.append('--extra-vars')\n\tcli_args.append(json.dumps(vars_dict))\n\n\t# adding the --private-key arg if exists\n\tprivate_key = ir_workspace.get_setting('private_key')\n\tif private_key:\n\t\tcli_args.append('--private-key={}'.format(private_key))\n\n\t# adding the --ssh-common-args arg if exists\n\tssh_common_args = ir_workspace.get_setting('ssh_common_args')\n\tif ssh_common_args:\n\t\tcli_args.append('--ssh-common-args={}'.format(ssh_common_args))\n\n\t# adding the --ssh-extra-args arg if exists\n\tssh_extra_args = ir_workspace.get_setting('ssh_extra\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\tlogger.debug(\"Using cli_args: {}\".format(cli_args))\n\tlogger.debug(\n\t\t\"Using vars_dict: {}\".format(json.dumps(vars_dict, indent=4)))\n\n\t# TODO: replace this with AnsiblePlaybookExecutor\n\tif ir_workspace.is_test_mode():\n\t\tlogger.info(\"Test mode enabled, skipping playbook execution.\")\n\t\treturn (0, [], [])\n\n\tir_plugin.set_vars_to_file(vars_dict)\n\n\tansible_playbook_path = \\\n\t\tir_plugin.get_playbook_path(cli_args[0])\n\tif not ansible_playbook_path:\n\t\traise exceptions.InfraredPluginError(\n\t\t\t\"Playbook not found: {}\".format(cli_args[0]))\n\n\t# TODO: add support for ansible.cfg\n\ttry:\n\t\tir_workspace.run_ansible_playbook(\n\t\t\tansible_playbook_path,\n\t\t\tcli_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n\t# ###########################################################\n\t# #### We need to get the name of the current playbook #### #\n\t# ###########################################################\n\ttry:\n\t\tcli_args.remove('--debug')\n\t\tcli_args.insert(0, '--debug')\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tcli_args.remove('--skip-tags')\n\t\tcli_args.insert(0, '--skip-tags')\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tcli_args.remove('--tags')\n\t\tcli_args.insert(0, '--tags')\n\texcept ValueError:\n\t\tpass\n\n\ttry:\n\t\tcli_args.remove('-e')\n\t\tcli_args.insert(0, '-e')\n\texcept ValueError:\n\t\tpass\n\n\tcli_args.insert(0, 'ansible-playbook')\n\tcmd = cli_args\n\targs = cmd[1:]\n\n\t# #######################################################\n\t# #### We need to get the name of the current plugin\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/inspector/inspector.py", "project": "redhat-openstack/infrared", "name": "_convert_non_cli_args", "docstring": "Casts arguments to correct types by modifying values_dict param.\n\nBy default all the values are strings.\n\n:param parser_name: The command name, e.g. main, virsh, ospd, etc\n:param values_dict: The dict of with arguments", "ques_id": "63060ada73426c38ae68ad31", "code": ["def _convert_non_cli_args(self, parser_name, values_dict):\n\tparams_to_cast = {\n\t\t'main': {\n\t\t\t'log_level': int,\n\t\t\t'log_level_console': int,\n\t\t\t'log_level_file': int,\n\t\t\t'sleep_time_on_error': int,\n\t\t\t'attempts_on_error': int,\n\t\t\t'iterations': int,\n\t\t\t'iterations_on_error': int,\n\t\t\t'iterations_on_failure': int,\n\t\t\t'iterations_on_success': int,\n\t\t\t'iterations_on_timeout': int,\n\t\t\t'timeout': int,\n\t\t\t'count_threads_on_start': int,\n\t\t\t'count_threads_on_stop': int,\n\t\t\t'count_threads_on_error': int,\n\t\t\t'count_threads_on_failure': int,\n\t\t\t'count_threads_on_success': int,\n\t\t\t'count_threads_on_timeout': int,\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key in values_dict:\n\t\t# Do nothing if the key is not in the config\n\t\tif key not in self.config[parser_name]:\n\t\t\tcontinue\n\n\t\t# Cast the value to the type in the config\n\t\tvalues_dict[key] = self.config[parser_name][key]['type'](values_dict[key])\n\n\treturn values_dict\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tcast_dict = {\n\t\t'set_memory': {\n\t\t\t'memory': str,\n\t\t\t'unit': str,\n\t\t},\n\t\t'create': {\n\t\t\t'name': str,\n\t\t\t'disk': str,\n\t\t\t'memory': str,\n\t\t\t'unit': str,\n\t\t\t'vcpus': int,\n\t\t\t'network': str,\n\t\t\t'bridge': str,\n\t\t\t'cdrom': str,\n\t\t},\n\t\t'list': {\n\t\t\t'name': str,\n\t\t},\n\t\t'start': {\n\t\t\t'name': str,\n\t\t\t'disk': str,\n\t\t\t'memory': str,\n\t\t\t'unit': str,\n\t\t\t'vcpus': int,\n\t\t\t'network': str,\n\t\t\t'bridge': str,\n\t\t\t'cdrom': str,\n\t\t},\n\t\t'destroy': {\n\t\t\t'name': str,\n\t\t},\n\t\t\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg in self.command_list[parser_name]:\n\t\tif arg.type == bool and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = get_bool_from_string(values_dict[arg.dest])\n\n\t\tif arg.type == int and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = int(values_dict[arg.dest])\n\n\t\tif arg.type == float and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = float(values_dict[arg.dest])\n\n\t\tif arg.type == list and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = values_dict[arg.dest].split(',')\n\n\t\tif arg.type == dict and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = eval(values_dict[arg.dest])\n\n\treturn values_dict\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tpass\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tif parser_name == \"main\" and key == \"file\":\n\t\t\tif value:\n\t\t\t\t# only if we have a value, otherwise it will fail\n\t\t\t\tvalues_dict[key] = value.name\n\t\telif parser_name == \"main\" and key == \"scripts\":\n\t\t\tif value:\n\t\t\t\tvalues_dict[key] = value.name\n\t\telif parser_name == \"main\" and key == \"output\":\n\t\t\tif value:\n\t\t\t\tvalues_dict[key] = value.name\n\t\telif parser_name == \"main\" and key == \"logging_level\":\n\t\t\tvalues_dict[key] = value.upper()\n\t\telif parser_name == \"main\" and key == \"verbose\":\n\t\t\tvalues_dict[key] = value.upper()\n\t\telif parser_name == \"main\" and key == \"timeout\":\n\t\t\tif value is not None:\n\t\t\t\tvalues_dict[key]\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\t# if this parser is a sub-parser, the values_dict will have a key\n\t# 'command' that refers to the sub-command name\n\t# we only cast the arguments of the sub-command\n\tif 'command' in values_dict:\n\t\tif values_dict['command'] in self.parsers[parser_name]._actions:\n\t\t\tfor action in self.parsers[parser_name]._actions:\n\t\t\t\tif action.dest == 'command':\n\t\t\t\t\tparser_name = values_dict['command']\n\t\t\t\t\tbreak\n\n\tfor arg in self.parsers[parser_name]._actions:\n\t\t# check if the arg is of type int\n\t\tif arg.type == int:\n\t\t\ttry:\n\t\t\t\tvalues_dict[arg.dest] = int(values_dict[arg.dest])\n\t\t\texcept:\n\t\t\t\tpass\n\t\t# check if the arg is of type float\n\t\telif arg.type == float:\n\t\t\ttry:\n\t\t\t\tvalues_dict[\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg, value in values_dict.items():\n\t\tif value is None:\n\t\t\t# Skip None value if it was not provided by user\n\t\t\tcontinue\n\n\t\t# Try to cast value to correct type\n\t\ttry:\n\t\t\tif arg in self.cli_args[parser_name]:\n\t\t\t\t# Get the argument from cli_args\n\t\t\t\targ_obj = self.cli_args[parser_name][arg]\n\t\t\t\t# Cast the value\n\t\t\t\tvalues_dict[arg] = arg_obj.type(value)\n\t\texcept ValueError:\n\t\t\t# If we failed to cast the value, we will just leave it as it is\n\t\t\tpass\n\n\treturn values_dict\n\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tif self.argparser_map[parser_name].get(key) is None:\n\t\t\tcontinue\n\t\tif self.argparser_map[parser_name][key]['type'] == 'int':\n\t\t\tvalues_dict[key] = int(value)\n\t\telif self.argparser_map[parser_name][key]['type'] == 'bool':\n\t\t\tvalues_dict[key] = True if value == 'True' else False\n\t\telif self.argparser_map[parser_name][key]['type'] == 'str':\n\t\t\tvalues_dict[key] = str(value)\n\t\telif self.argparser_map[parser_name][key]['type'] == 'list':\n\t\t\tvalues_dict[key] = value.split(',')\n\t\t# TODO: add float, hex, etc.\n\treturn values_dict\n\n", "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tvalues_dict[key] = self._convert_arg(parser_name, key, value)\n\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tparams_to_cast = {\n\t\t'main': {\n\t\t\t'log_level': int,\n\t\t\t'log_level_console': int,\n\t\t\t'log_level_file': int,\n\t\t\t'sleep_time_on_error': int,\n\t\t\t'attempts_on_error': int,\n\t\t\t'iterations': int,\n\t\t\t'iterations_on_error': int,\n\t\t\t'iterations_on_failure': int,\n\t\t\t'iterations_on_success': int,\n\t\t\t'iterations_on_timeout': int,\n\t\t\t'timeout': int,\n\t\t\t'count_threads_on_start': int,\n\t\t\t'count_threads_on_stop': int,\n\t\t\t'count_threads_on_error': int,\n\t\t\t'count_threads_on_failure': int,\n\t\t\t'count_threads_on_success': int,\n\t\t\t'count_threads_on_timeout': int,\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key in values_dict:\n\t\t# Do nothing if the key is not in the config\n\t\tif key not in self.config[parser_name]:\n\t\t\tcontinue\n\n\t\t# Cast the value to the type in the config\n\t\tvalues_dict[key] = self.config[parser_name][key]['type'](values_dict[key])\n\n\treturn values_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tcast_dict = {\n\t\t'set_memory': {\n\t\t\t'memory': str,\n\t\t\t'unit': str,\n\t\t},\n\t\t'create': {\n\t\t\t'name': str,\n\t\t\t'disk': str,\n\t\t\t'memory': str,\n\t\t\t'unit': str,\n\t\t\t'vcpus': int,\n\t\t\t'network': str,\n\t\t\t'bridge': str,\n\t\t\t'cdrom': str,\n\t\t},\n\t\t'list': {\n\t\t\t'name': str,\n\t\t},\n\t\t'start': {\n\t\t\t'name': str,\n\t\t\t'disk': str,\n\t\t\t'memory': str,\n\t\t\t'unit': str,\n\t\t\t'vcpus': int,\n\t\t\t'network': str,\n\t\t\t'bridge': str,\n\t\t\t'cdrom': str,\n\t\t},\n\t\t'destroy': {\n\t\t\t'name': str,\n\t\t},\n\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg in self.command_list[parser_name]:\n\t\tif arg.type == bool and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = get_bool_from_string(values_dict[arg.dest])\n\n\t\tif arg.type == int and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = int(values_dict[arg.dest])\n\n\t\tif arg.type == float and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = float(values_dict[arg.dest])\n\n\t\tif arg.type == list and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = values_dict[arg.dest].split(',')\n\n\t\tif arg.type == dict and arg.dest in values_dict:\n\t\t\tvalues_dict[arg.dest] = eval(values_dict[arg.dest])\n\n\treturn values_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tpass\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tif parser_name == \"main\" and key == \"file\":\n\t\t\tif value:\n\t\t\t\t# only if we have a value, otherwise it will fail\n\t\t\t\tvalues_dict[key] = value.name\n\t\telif parser_name == \"main\" and key == \"scripts\":\n\t\t\tif value:\n\t\t\t\tvalues_dict[key] = value.name\n\t\telif parser_name == \"main\" and key == \"output\":\n\t\t\tif value:\n\t\t\t\tvalues_dict[key] = value.name\n\t\telif parser_name == \"main\" and key == \"logging_level\":\n\t\t\tvalues_dict[key] = value.upper()\n\t\telif parser_name == \"main\" and key == \"verbose\":\n\t\t\tvalues_dict[key] = value.upper()\n\t\telif parser_name == \"main\" and key == \"timeout\":\n\t\t\tif value is not None:\n\t\t\t\tvalues_dict[key]\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\t# if this parser is a sub-parser, the values_dict will have a key\n\t# 'command' that refers to the sub-command name\n\t# we only cast the arguments of the sub-command\n\tif 'command' in values_dict:\n\t\tif values_dict['command'] in self.parsers[parser_name]._actions:\n\t\t\tfor action in self.parsers[parser_name]._actions:\n\t\t\t\tif action.dest == 'command':\n\t\t\t\t\tparser_name = values_dict['command']\n\t\t\t\t\tbreak\n\n\tfor arg in self.parsers[parser_name]._actions:\n\t\t# check if the arg is of type int\n\t\tif arg.type == int:\n\t\t\ttry:\n\t\t\t\tvalues_dict[arg.dest] = int(values_dict[arg.dest])\n\t\t\texcept:\n\t\t\t\tpass\n\t\t# check if the arg is of type float\n\t\telif arg.type == float:\n\t\t\ttry:\n\t\t\t\tvalues_dict[\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor arg, value in values_dict.items():\n\t\tif value is None:\n\t\t\t# Skip None value if it was not provided by user\n\t\t\tcontinue\n\n\t\t# Try to cast value to correct type\n\t\ttry:\n\t\t\tif arg in self.cli_args[parser_name]:\n\t\t\t\t# Get the argument from cli_args\n\t\t\t\targ_obj = self.cli_args[parser_name][arg]\n\t\t\t\t# Cast the value\n\t\t\t\tvalues_dict[arg] = arg_obj.type(value)\n\t\texcept ValueError:\n\t\t\t# If we failed to cast the value, we will just leave it as it is\n\t\t\tpass\n\n\treturn values_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tif self.argparser_map[parser_name].get(key) is None:\n\t\t\tcontinue\n\t\tif self.argparser_map[parser_name][key]['type'] == 'int':\n\t\t\tvalues_dict[key] = int(value)\n\t\telif self.argparser_map[parser_name][key]['type'] == 'bool':\n\t\t\tvalues_dict[key] = True if value == 'True' else False\n\t\telif self.argparser_map[parser_name][key]['type'] == 'str':\n\t\t\tvalues_dict[key] = str(value)\n\t\telif self.argparser_map[parser_name][key]['type'] == 'list':\n\t\t\tvalues_dict[key] = value.split(',')\n\t\t# TODO: add float, hex, etc.\n\treturn values_dict\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def _convert_non_cli_args(self, parser_name, values_dict):\n\tfor key, value in values_dict.items():\n\t\tvalues_dict[key] = self._convert_arg(parser_name, key, value)\n\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "tests/test_plugins.py", "project": "redhat-openstack/infrared", "name": "get_plugin_spec_flatten_dict", "docstring": "Creates a flat dict from the plugin spec\n\n:param plugin_dir: A path to the plugin's dir\n:return: A flatten dictionary contains the plugin's properties", "ques_id": "63060b1a73426c38ae68ad3e", "code": ["def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_dict = get_plugin_spec_dict(plugin_dir)\n\treturn flatten_dict(plugin_spec_dict)\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_flatten_dict = {}\n\n\twith open(os.path.join(plugin_dir, \"plugin_spec.yaml\"), 'r') as fp:\n\t\tplugin_spec_yaml = yaml.load(fp)\n\n\tfor key, value in plugin_spec_yaml.items():\n\t\tplugin_spec_flatten_dict[key] = value\n\n\treturn plugin_spec_flatten_dict\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\treturn flatten_yaml_dict(plugin_spec)\n\t\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_path = os.path.join(plugin_dir, \"plugin_spec.yaml\")\n\twith open(plugin_spec_path, \"r\") as plugin_spec_file:\n\t\tplugin_spec_dict = yaml.load(plugin_spec_file)\n\treturn plugin_spec_dict\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n\twith open(plugin_spec_file, 'r') as f:\n\t\tplugin_spec_dict = yaml.load(f.read())\n\t\t# flatten the dict\n\t\tflatten_dict = dict()\n\t\tfor k, v in plugin_spec_dict.iteritems():\n\t\t\tif isinstance(v, dict):\n\t\t\t\tfor vk, vv in v.iteritems():\n\t\t\t\t\tflatten_dict['{}_{}'.format(k, vk)] = vv\n\t\t\telse:\n\t\t\t\tflatten_dict[k] = v\n\t\treturn flatten_dict\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin_spec.yml\")\n\ttry:\n\t\twith open(plugin_spec_file, 'r') as f:\n\t\t\tplugin_yaml = yaml.load(f)\n\texcept:\n\t\traise Exception(\"Error in parsing plugin spec file\")\n\tplugin_flatten_dict = {}\n\tif plugin_yaml and \"plugins\" in plugin_yaml:\n\t\tfor plugin_info in plugin_yaml[\"plugins\"]:\n\t\t\tplugin_flatten_dict[plugin_info[\"plugin_name\"]] = _flatten_dict(plugin_info)\n\treturn plugin_flatten_dict\n\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\ttry:\n\t\twith open(os.path.join(plugin_dir, 'plugin.yaml'), 'rU') as f:\n\t\t\tplugin_info = yaml.load(f.read())\n\texcept IOError:\n\t\tlogging.exception('Exception while reading plugin.yaml:')\n\t\traise\n\n\t# Special handling of icon field\n\tif 'icon' in plugin_info:\n\t\tif os.path.isabs(plugin_info['icon']):\n\t\t\tplugin_info['icon'] = os.path.basename(plugin_info['icon'])\n\n\t# Special handling of screenshot field\n\tif 'screenshots' in plugin_info:\n\t\tfor i, screenshot in enumerate(plugin_info['screenshots']):\n\t\t\tif os.path.isabs(screenshot):\n\t\t\t\tplugin_info['screenshots'][i] = os.path.basename(screenshot)\n\n\tplugin_info['plugin_dir'] = plugin_dir\n\tplugin_info['plugin_hash'] = get_plugin_hash(plugin_dir)\n\tplugin\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_flatten_dict = {}\n\tplugin_spec_flatten_dict['plugin_dir'] = plugin_dir\n\tplugin_spec_flatten_dict['plugin_name'] = os.path.basename(plugin_dir)\n\tplugin_spec_flatten_dict['plugin_spec'] = plugin_spec_flatten_dict['plugin_dir'] + '/plugin.spec'\n\tplugin_spec_flatten_dict['plugin_spec_yaml'] = plugin_spec_flatten_dict['plugin_dir'] + '/plugin.spec.yaml'\n\tplugin_spec_flatten_dict['plugin_spec_yaml_dict'] = {}\n\tplugin_spec_flatten_dict['plugin_spec_dict'] = {}\n\n\tif os.path.isfile(plugin_spec_flatten_dict['plugin_spec_yaml']): # Use YAML if available\n\t\tplugin_spec_flatten_dict['plugin_spec_yaml_dict'] = get_plugin_spec_flatten_yaml_dict(plugin_spec_flatten_dict['plugin_spec_\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tspec_file_path = os.path.join(plugin_dir, 'plugin_spec.yaml')\n\ttry:\n\t\twith open(spec_file_path, 'r') as spec_file:\n\t\t\tspec_dict = yaml.load(spec_file)\n\t\t\treturn flatten_dict(spec_dict)\n\texcept:\n\t\traise PluginSpecError('Plugin spec file {} not found.'.format(spec_file_path))\n\n", "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin.yaml')\n\tplugin_spec_dict = yaml.load(open(plugin_spec_file))\n\t\n\treturn plugin_spec_dict\n\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_dict = get_plugin_spec_dict(plugin_dir)\n\treturn flatten_dict(plugin_spec_dict)\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_flatten_dict = {}\n\n\twith open(os.path.join(plugin_dir, \"plugin_spec.yaml\"), 'r') as fp:\n\t\tplugin_spec_yaml = yaml.load(fp)\n\n\tfor key, value in plugin_spec_yaml.items():\n\t\tplugin_spec_flatten_dict[key] = value\n\n\treturn plugin_spec_flatten_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec = get_plugin_spec(plugin_dir)\n\treturn flatten_yaml_dict(plugin_spec)\n\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_path = os.path.join(plugin_dir, \"plugin_spec.yaml\")\n\twith open(plugin_spec_path, \"r\") as plugin_spec_file:\n\t\tplugin_spec_dict = yaml.load(plugin_spec_file)\n\treturn plugin_spec_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin.spec')\n\twith open(plugin_spec_file, 'r') as f:\n\t\tplugin_spec_dict = yaml.load(f.read())\n\t\t# flatten the dict\n\t\tflatten_dict = dict()\n\t\tfor k, v in plugin_spec_dict.iteritems():\n\t\t\tif isinstance(v, dict):\n\t\t\t\tfor vk, vv in v.iteritems():\n\t\t\t\t\tflatten_dict['{}_{}'.format(k, vk)] = vv\n\t\t\telse:\n\t\t\t\tflatten_dict[k] = v\n\t\treturn flatten_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_file = os.path.join(plugin_dir, \"plugin_spec.yml\")\n\ttry:\n\t\twith open(plugin_spec_file, 'r') as f:\n\t\t\tplugin_yaml = yaml.load(f)\n\texcept:\n\t\traise Exception(\"Error in parsing plugin spec file\")\n\tplugin_flatten_dict = {}\n\tif plugin_yaml and \"plugins\" in plugin_yaml:\n\t\tfor plugin_info in plugin_yaml[\"plugins\"]:\n\t\t\tplugin_flatten_dict[plugin_info[\"plugin_name\"]] = _flatten_dict(plugin_info)\n\treturn plugin_flatten_dict\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\ttry:\n\t\twith open(os.path.join(plugin_dir, 'plugin.yaml'), 'rU') as f:\n\t\t\tplugin_info = yaml.load(f.read())\n\texcept IOError:\n\t\tlogging.exception('Exception while reading plugin.yaml:')\n\t\traise\n\n\t# Special handling of icon field\n\tif 'icon' in plugin_info:\n\t\tif os.path.isabs(plugin_info['icon']):\n\t\t\tplugin_info['icon'] = os.path.basename(plugin_info['icon'])\n\n\t# Special handling of screenshot field\n\tif 'screenshots' in plugin_info:\n\t\tfor i, screenshot in enumerate(plugin_info['screenshots']):\n\t\t\tif os.path.isabs(screenshot):\n\t\t\t\tplugin_info['screenshots'][i] = os.path.basename(screenshot)\n\n\tplugin_info['plugin_dir'] = plugin_dir\n\tplugin_info['plugin_hash'] = get_plugin_hash(plugin_dir)\n\tplugin\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_flatten_dict = {}\n\tplugin_spec_flatten_dict['plugin_dir'] = plugin_dir\n\tplugin_spec_flatten_dict['plugin_name'] = os.path.basename(plugin_dir)\n\tplugin_spec_flatten_dict['plugin_spec'] = plugin_spec_flatten_dict['plugin_dir'] + '/plugin.spec'\n\tplugin_spec_flatten_dict['plugin_spec_yaml'] = plugin_spec_flatten_dict['plugin_dir'] + '/plugin.spec.yaml'\n\tplugin_spec_flatten_dict['plugin_spec_yaml_dict'] = {}\n\tplugin_spec_flatten_dict['plugin_spec_dict'] = {}\n\n\tif os.path.isfile(plugin_spec_flatten_dict['plugin_spec_yaml']): # Use YAML if available\n\t\tplugin_spec_flatten_dict['plugin_spec_yaml_dict'] = get_plugin_spec_flatten_yaml_dict(plugin_spec_flatten_dict['plugin_spec_\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tspec_file_path = os.path.join(plugin_dir, 'plugin_spec.yaml')\n\ttry:\n\t\twith open(spec_file_path, 'r') as spec_file:\n\t\t\tspec_dict = yaml.load(spec_file)\n\t\t\treturn flatten_dict(spec_dict)\n\texcept:\n\t\traise PluginSpecError('Plugin spec file {} not found.'.format(spec_file_path))\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def get_plugin_spec_flatten_dict(plugin_dir):\n\tplugin_spec_file = os.path.join(plugin_dir, 'plugin.yaml')\n\tplugin_spec_dict = yaml.load(open(plugin_spec_file))\n\t\n\treturn plugin_spec_dict\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/core/services/ansible_config.py", "project": "redhat-openstack/infrared", "name": "inject_config", "docstring": "Set the environment variable for config path, if it is undefined.", "ques_id": "63060b1b73426c38ae68ad42", "code": ["def inject_config(self):\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\treturn True\n\n", "def inject_config(self):\n\tif not self.ansible_config_path:\n\t\treturn\n\tif not \"ANSIBLE_CONFIG\" in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\n", "def inject_config(self):\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\tif not os.environ.get(\"ANSIBLE_CONFIG\"):\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n", "def inject_config(self):\n\tif not \"ANSIBLE_CONFIG\" in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\t\tprint(\"Injecting config file at %s\" % self.ansible_config_path)\n\n", "def inject_config(self):\n\tif not os.environ.get('ANSIBLE_CONFIG'):\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\tif not os.path.isfile(self.ansible_config_path):\n\t\traise Exception(\"ANSIBLE_CONFIG path does not exist: %s\" % self.ansible_config_path)\n\n\treturn self.ansible_config_path\n\n\n", "def inject_config(self):\n\tif not os.environ.get('ANSIBLE_CONFIG'):\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\n", "def inject_config(self):\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "def inject_config(self):\n\tif not \"ANSIBLE_CONFIG\" in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n"], "level": "class_runnable", "generate_results": [{"generate_code": "def inject_config(self):\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\treturn True\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not self.ansible_config_path:\n\t\treturn\n\tif not \"ANSIBLE_CONFIG\" in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not os.environ.get(\"ANSIBLE_CONFIG\"):\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not \"ANSIBLE_CONFIG\" in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\t\tprint(\"Injecting config file at %s\" % self.ansible_config_path)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not os.environ.get('ANSIBLE_CONFIG'):\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif \"ANSIBLE_CONFIG\" not in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n\tif not os.path.isfile(self.ansible_config_path):\n\t\traise Exception(\"ANSIBLE_CONFIG path does not exist: %s\" % self.ansible_config_path)\n\n\treturn self.ansible_config_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not os.environ.get('ANSIBLE_CONFIG'):\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif 'ANSIBLE_CONFIG' not in os.environ:\n\t\tos.environ['ANSIBLE_CONFIG'] = self.ansible_config_path\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def inject_config(self):\n\tif not \"ANSIBLE_CONFIG\" in os.environ:\n\t\tos.environ[\"ANSIBLE_CONFIG\"] = self.ansible_config_path\n\n", "is_pass": false, "return_code": 1}]}
{"file_path": "infrared/main.py", "project": "redhat-openstack/infrared", "name": "extend_cli", "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.", "ques_id": "63060b1b73426c38ae68ad43", "code": ["def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help='spec help')\n\tparser.add_argument('-l', '--list', action='store_true', help='list the available specifications')\n\tparser.add_argument('-s', '--show', action='store_true', help='show the current specification')\n\tparser.add_argument('-c', '--check', action='store_true', help='check the validity of the current specification')\n\tparser.add_argument('-S', '--set', help='set the current specification')\n\tparser.add_argument('-r', '--refresh', action='store_true', help='refresh the current specification')\n\tparser.add_argument('-v', '--verbose', action='store_true', help='be verbose')\n\tparser.add_argument('-vv', '--very-verbose', action='store_true', help='be very verbose')\n\n\treturn parser\n\n", "def extend_cli(self, root_subparsers):\n\tsuper(SpecCommand, self).extend_cli(root_subparsers)\n\t_subparser = self.subparser\n\n\t_subparser.add_argument('specfile',\n\t\t\t\t\t\t\tmetavar='specfile',\n\t\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\t\thelp='Path to the specfile to be parsed')\n\n\t_subparser.add_argument('--yaml',\n\t\t\t\t\t\t\tdest='yaml',\n\t\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\t\tdefault=False,\n\t\t\t\t\t\t\thelp='Print specfile in YAML format')\n\n\t_subparser.add_argument('--json',\n\t\t\t\t\t\t\tdest='json',\n\t\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\t\tdefault=False,\n\t\t\t\t\t\t\thelp='Print specfile in JSON format')\n\n\t_subparser.add_argument('--format',\n\t\t\t\t\t\t\tmetavar='format',\n\n", "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser(\"spec\", help=\"spec related commands\")\n\tsubparsers = parser.add_subparsers(dest=\"spec_command\")\n\n\t# spec run\n\tparser_run = subparsers.add_parser(\"run\", help=\"run the spec\")\n\tparser_run.add_argument(\"-f\", \"--force\", action=\"store_true\", help=\"force the spec to run even if it is not due\")\n\tparser_run.add_argument(\"-n\", \"--no-purge\", action=\"store_true\", help=\"prevent the spec from purging its inventory\")\n\tparser_run.add_argument(\"spec_identifier\", help=\"the spec identifier to run\")\n\tparser_run.add_argument(\"run_identifier\", help=\"the spec run identifier\")\n\tparser_run.add_argument(\"-o\", \"--options\", help=\"provide additional options\")\n\tparser_run.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"enable debug logging\")\n\tparser_run.set_defaults(func=\n", "def extend_cli(self, root_subparsers):\n\tparser_spec = root_subparsers.add_parser('spec', help='manage spec')\n\n\t# Add subcommands\n\tspec_subparsers = parser_spec.add_subparsers(title='spec sub-command', dest='spec_command')\n\tself.add_spec_add_cli(spec_subparsers)\n\tself.add_spec_del_cli(spec_subparsers)\n\tself.add_spec_download_cli(spec_subparsers)\n\tself.add_spec_list_cli(spec_subparsers)\n\tself.add_spec_list_local_cli(spec_subparsers)\n\tself.add_spec_list_remote_cli(spec_subparsers)\n\tself.add_spec_sync_cli(spec_subparsers)\n\n", "def extend_cli(self, root_subparsers):\n\tself.subparsers = root_subparsers.add_parser('spec', help='Manage specifications')\n\tself.subparsers.set_defaults(func=self.run)\n\n\tsubparsers = self.subparsers.add_subparsers()\n\tsubparsers.required = True\n\tsubparsers.dest = 'spec_operation'\n\n\tlist_subparser = subparsers.add_parser('list', help='List specifications')\n\tlist_subparser.set_defaults(func=self.list_)\n\n\tlist_subparser.add_argument('--name',\n\t\t\t\t\t\t\t\thelp='List specifications by name')\n\n\tget_subparser = subparsers.add_parser('get', help='Get a specific spec')\n\tget_subparser.set_defaults(func=self.get)\n\n\tget_subparser.add_argument('name',\n\t\t\t\t\t\t\t\thelp='Get a specific spec')\n\n\tget_subparser.add_argument('-f', '--format',\n\t\t\t\t\n", "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp=\"Manage the specification of the project.\"\n\t)\n\tsubparsers = parser.add_subparsers(help='spec commands')\n\n\t# create the parser for the \"show\" command\n\tparser_show = subparsers.add_parser(\n\t\t'show',\n\t\thelp='Show the current specification.'\n\t)\n\tparser_show.set_defaults(func=self.cmd_show)\n\n\t# create the parser for the \"save\" command\n\tparser_save = subparsers.add_parser(\n\t\t'save',\n\t\thelp='Save the specification to a file.'\n\t)\n\tparser_save.add_argument(\n\t\t'-f', '--force',\n\t\tdest='force',\n\t\tdefault=False,\n\t\taction='store_true',\n\t\thelp='Overwrite existing file.'\n\t)\n\tparser_save.add_argument(\n\t\t'-p', '--pretty',\n\t\tdest='pretty',\n\t\tdefault=False\n", "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help=\"The spec command\")\n\tsubparsers = parser.add_subparsers(title='subcommands', help='')\n\n\t# Create the parser for the spec command\n\tparser_spec = subparsers.add_parser('spec', help=\"The spec command\")\n\tparser_spec.add_argument('-v', '--verbose', action='count', default=0, help='increase the verbosity level')\n\tparser_spec.add_argument('--debug', action='store_true', help='run the command in debug mode')\n\tparser_spec.add_argument('--as-json', action='store_true', help='output as json')\n\tparser_spec.add_argument('--config', '-c', default=None, help='use the specified config file')\n\tparser_spec.add_argument('--spec', '-s', default=None, help='use the specified spec file')\n\tparser_spec.add_argument('--package', '-p', default=None, help='use the specified package file')\n", "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser('spec',\n\t\thelp='Specification related commands')\n\tsubparser.add_argument('spec',\n\t\thelp='specification file to operate on')\n\tsubparser.add_argument('--add-file', '-a',\n\t\thelp='add a file to the spec')\n\tsubparser.add_argument('--remove-file', '-r',\n\t\thelp='remove a file from the spec')\n\tsubparser.add_argument('--add-package', '-p',\n\t\thelp='add a package to the spec')\n\tsubparser.add_argument('--remove-package', '-P',\n\t\thelp='remove a package from the spec')\n\tsubparser.add_argument('--add-patch', '-t',\n\t\thelp='add a patch to the spec')\n\tsubparser.add_argument('--remove-patch', '-T',\n\t\thelp='remove a patch from the spec')\n\tsubparser.add_argument('--add-dependency', '-d',\n\t\thelp='add a dependency\n", "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser(\n\t\t\"spec\",\n\t\thelp=\"Generates a project template\",\n\t\tdescription=\"Generates a project template\"\n\t)\n\tsubparser.add_argument(\n\t\t\"name\",\n\t\ttype=str,\n\t\tnargs='?',\n\t\thelp=\"The name of the project.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--template\",\n\t\ttype=str,\n\t\thelp=\"The name of the template to use.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--path\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"The path to generate the project into.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--verbose\",\n\t\taction='store_true',\n\t\thelp=\"Prints the output to the console.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--force\",\n\t\taction='store_true',\n\t\thelp=\"Overwrites any existing files.\"\n\t)\n", "def extend_cli(self, root_subparsers):\n\tsubparsers = root_subparsers.add_parser(\"spec\", aliases=[\"specs\"], help=\"spec related commands\")\n\tsubparsers.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"run in verbose mode\")\n\n\tsubsubparsers = subparsers.add_subparsers(dest=\"subcommand\")\n\n\tsubsubparsers.add_parser(\"ls\", aliases=[\"list\"], help=\"list specs\")\n\n\tsubsubparsers.add_parser(\"rm\", aliases=[\"remove\"], help=\"remove a spec\")\n\n\tsubsubparsers.add_parser(\"add\", help=\"add a spec\")\n\n\tsubsubparsers.add_parser(\"update\", help=\"update a spec\")\n\n\tsubsubparsers.add_parser(\"check\", help=\"check a spec\")\n\n\tsubsubparsers.add_parser(\"show\", help=\"show the details of a spec\")\n\n\tsubsubparsers.add_parser(\"export\", help=\"export a spec to a file\")\n\n\tsubsubparsers.add_parser(\"import\",\n"], "level": "project_runnable", "generate_results": [{"generate_code": "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help='spec help')\n\tparser.add_argument('-l', '--list', action='store_true', help='list the available specifications')\n\tparser.add_argument('-s', '--show', action='store_true', help='show the current specification')\n\tparser.add_argument('-c', '--check', action='store_true', help='check the validity of the current specification')\n\tparser.add_argument('-S', '--set', help='set the current specification')\n\tparser.add_argument('-r', '--refresh', action='store_true', help='refresh the current specification')\n\tparser.add_argument('-v', '--verbose', action='store_true', help='be verbose')\n\tparser.add_argument('-vv', '--very-verbose', action='store_true', help='be very verbose')\n\n\treturn parser\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tsuper(SpecCommand, self).extend_cli(root_subparsers)\n\t_subparser = self.subparser\n\n\t_subparser.add_argument('specfile',\n\t\t\t\t\t\t\tmetavar='specfile',\n\t\t\t\t\t\t\ttype=str,\n\t\t\t\t\t\t\thelp='Path to the specfile to be parsed')\n\n\t_subparser.add_argument('--yaml',\n\t\t\t\t\t\t\tdest='yaml',\n\t\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\t\tdefault=False,\n\t\t\t\t\t\t\thelp='Print specfile in YAML format')\n\n\t_subparser.add_argument('--json',\n\t\t\t\t\t\t\tdest='json',\n\t\t\t\t\t\t\taction='store_true',\n\t\t\t\t\t\t\tdefault=False,\n\t\t\t\t\t\t\thelp='Print specfile in JSON format')\n\n\t_subparser.add_argument('--format',\n\t\t\t\t\t\t\tmetavar='format',\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser(\"spec\", help=\"spec related commands\")\n\tsubparsers = parser.add_subparsers(dest=\"spec_command\")\n\n\t# spec run\n\tparser_run = subparsers.add_parser(\"run\", help=\"run the spec\")\n\tparser_run.add_argument(\"-f\", \"--force\", action=\"store_true\", help=\"force the spec to run even if it is not due\")\n\tparser_run.add_argument(\"-n\", \"--no-purge\", action=\"store_true\", help=\"prevent the spec from purging its inventory\")\n\tparser_run.add_argument(\"spec_identifier\", help=\"the spec identifier to run\")\n\tparser_run.add_argument(\"run_identifier\", help=\"the spec run identifier\")\n\tparser_run.add_argument(\"-o\", \"--options\", help=\"provide additional options\")\n\tparser_run.add_argument(\"-d\", \"--debug\", action=\"store_true\", help=\"enable debug logging\")\n\tparser_run.set_defaults(func=\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tparser_spec = root_subparsers.add_parser('spec', help='manage spec')\n\n\t# Add subcommands\n\tspec_subparsers = parser_spec.add_subparsers(title='spec sub-command', dest='spec_command')\n\tself.add_spec_add_cli(spec_subparsers)\n\tself.add_spec_del_cli(spec_subparsers)\n\tself.add_spec_download_cli(spec_subparsers)\n\tself.add_spec_list_cli(spec_subparsers)\n\tself.add_spec_list_local_cli(spec_subparsers)\n\tself.add_spec_list_remote_cli(spec_subparsers)\n\tself.add_spec_sync_cli(spec_subparsers)\n\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tself.subparsers = root_subparsers.add_parser('spec', help='Manage specifications')\n\tself.subparsers.set_defaults(func=self.run)\n\n\tsubparsers = self.subparsers.add_subparsers()\n\tsubparsers.required = True\n\tsubparsers.dest = 'spec_operation'\n\n\tlist_subparser = subparsers.add_parser('list', help='List specifications')\n\tlist_subparser.set_defaults(func=self.list_)\n\n\tlist_subparser.add_argument('--name',\n\t\t\t\t\t\t\t\thelp='List specifications by name')\n\n\tget_subparser = subparsers.add_parser('get', help='Get a specific spec')\n\tget_subparser.set_defaults(func=self.get)\n\n\tget_subparser.add_argument('name',\n\t\t\t\t\t\t\t\thelp='Get a specific spec')\n\n\tget_subparser.add_argument('-f', '--format',\n\t\t\t\t\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser(\n\t\t'spec',\n\t\thelp=\"Manage the specification of the project.\"\n\t)\n\tsubparsers = parser.add_subparsers(help='spec commands')\n\n\t# create the parser for the \"show\" command\n\tparser_show = subparsers.add_parser(\n\t\t'show',\n\t\thelp='Show the current specification.'\n\t)\n\tparser_show.set_defaults(func=self.cmd_show)\n\n\t# create the parser for the \"save\" command\n\tparser_save = subparsers.add_parser(\n\t\t'save',\n\t\thelp='Save the specification to a file.'\n\t)\n\tparser_save.add_argument(\n\t\t'-f', '--force',\n\t\tdest='force',\n\t\tdefault=False,\n\t\taction='store_true',\n\t\thelp='Overwrite existing file.'\n\t)\n\tparser_save.add_argument(\n\t\t'-p', '--pretty',\n\t\tdest='pretty',\n\t\tdefault=False\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tparser = root_subparsers.add_parser('spec', help=\"The spec command\")\n\tsubparsers = parser.add_subparsers(title='subcommands', help='')\n\n\t# Create the parser for the spec command\n\tparser_spec = subparsers.add_parser('spec', help=\"The spec command\")\n\tparser_spec.add_argument('-v', '--verbose', action='count', default=0, help='increase the verbosity level')\n\tparser_spec.add_argument('--debug', action='store_true', help='run the command in debug mode')\n\tparser_spec.add_argument('--as-json', action='store_true', help='output as json')\n\tparser_spec.add_argument('--config', '-c', default=None, help='use the specified config file')\n\tparser_spec.add_argument('--spec', '-s', default=None, help='use the specified spec file')\n\tparser_spec.add_argument('--package', '-p', default=None, help='use the specified package file')\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser('spec',\n\t\thelp='Specification related commands')\n\tsubparser.add_argument('spec',\n\t\thelp='specification file to operate on')\n\tsubparser.add_argument('--add-file', '-a',\n\t\thelp='add a file to the spec')\n\tsubparser.add_argument('--remove-file', '-r',\n\t\thelp='remove a file from the spec')\n\tsubparser.add_argument('--add-package', '-p',\n\t\thelp='add a package to the spec')\n\tsubparser.add_argument('--remove-package', '-P',\n\t\thelp='remove a package from the spec')\n\tsubparser.add_argument('--add-patch', '-t',\n\t\thelp='add a patch to the spec')\n\tsubparser.add_argument('--remove-patch', '-T',\n\t\thelp='remove a patch from the spec')\n\tsubparser.add_argument('--add-dependency', '-d',\n\t\thelp='add a dependency\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tsubparser = root_subparsers.add_parser(\n\t\t\"spec\",\n\t\thelp=\"Generates a project template\",\n\t\tdescription=\"Generates a project template\"\n\t)\n\tsubparser.add_argument(\n\t\t\"name\",\n\t\ttype=str,\n\t\tnargs='?',\n\t\thelp=\"The name of the project.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--template\",\n\t\ttype=str,\n\t\thelp=\"The name of the template to use.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--path\",\n\t\ttype=str,\n\t\tdefault=None,\n\t\thelp=\"The path to generate the project into.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--verbose\",\n\t\taction='store_true',\n\t\thelp=\"Prints the output to the console.\"\n\t)\n\tsubparser.add_argument(\n\t\t\"--force\",\n\t\taction='store_true',\n\t\thelp=\"Overwrites any existing files.\"\n\t)\n", "is_pass": false, "return_code": 1}, {"generate_code": "def extend_cli(self, root_subparsers):\n\tsubparsers = root_subparsers.add_parser(\"spec\", aliases=[\"specs\"], help=\"spec related commands\")\n\tsubparsers.add_argument(\"-v\", \"--verbose\", action=\"store_true\", help=\"run in verbose mode\")\n\n\tsubsubparsers = subparsers.add_subparsers(dest=\"subcommand\")\n\n\tsubsubparsers.add_parser(\"ls\", aliases=[\"list\"], help=\"list specs\")\n\n\tsubsubparsers.add_parser(\"rm\", aliases=[\"remove\"], help=\"remove a spec\")\n\n\tsubsubparsers.add_parser(\"add\", help=\"add a spec\")\n\n\tsubsubparsers.add_parser(\"update\", help=\"update a spec\")\n\n\tsubsubparsers.add_parser(\"check\", help=\"check a spec\")\n\n\tsubsubparsers.add_parser(\"show\", help=\"show the details of a spec\")\n\n\tsubsubparsers.add_parser(\"export\", help=\"export a spec to a file\")\n\n\tsubsubparsers.add_parser(\"import\",\n", "is_pass": false, "return_code": 1}]}
